{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9684feb-a8d9-454e-8918-76d7ad90e4d8",
   "metadata": {},
   "source": [
    "## Falcon-7b in 4bit\n",
    "\n",
    "In this notebook, we can use llama-index with the [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model. We also use [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes) to load the model in 4bit (model loads in about just under 8GB in memory rather than 16GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d8d9be-ebb8-4b6d-9199-62246dac7b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 15 10:51:27 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   31C    P0    25W / 250W |     81MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1147      G   /usr/lib/xorg/Xorg                 80MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0055833d-3ef1-4695-87e6-ce4674afa671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    PromptHelper,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    Document\n",
    ")\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39ba954-04d3-4174-9c4d-cf6123325b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.30.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2c038-7cce-4e77-bea9-6227b7adfd12",
   "metadata": {},
   "source": [
    "The next part needs `bitsandbytes`, so pip install if not - further note that `BitsAndBytesConfig` is a new class in `transformers`, so make sure that you're running a recent enough version. We're running 4.30.2 here which will be fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65dc8104-c8c2-4501-b3b5-f952367b322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af48017-18fb-44fc-b71c-6ac43090adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.read_csv(\"../../data/wiki-scraped.csv\")\n",
    "handbook = pd.read_csv(\"../../data/handbook-scraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81358315-7be6-4589-a190-b31fc8bc694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = list(wiki[\"body\"].astype(\"str\")) + list(handbook[\"body\"].astype(\"str\"))\n",
    "documents = [Document(t) for t in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f3494-e82e-4349-9097-77261eaf00a4",
   "metadata": {},
   "source": [
    "## Using falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd370d9-d48a-4c4b-8b91-364c0f7b4464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /anaconda/envs/reginald/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /anaconda/envs/reginald/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /anaconda/envs/reginald/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/reginald/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f58cd856d4eb9b4810c7eab68e9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32b2444-dfcc-4e07-a91c-d2f8f47f8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 15 10:51:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   32C    P0    42W / 250W |   5639MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1147      G   /usr/lib/xorg/Xorg                 80MiB |\n",
      "|    0   N/A  N/A    155302      C   .../envs/reginald/bin/python     5555MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c7e151-8066-40bb-aecc-e75a5170e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "falcon_7b = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    model_name: str\n",
    "    pipeline: transformers.pipelines.text_generation.TextGenerationPipeline\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt, stop=None):\n",
    "        return self.pipeline(prompt, max_new_tokens=9999)[0][\"generated_text\"]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> dict:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model_name\": self.model_name}\n",
    "    \n",
    "llm_predictor_falcon_7b = LLMPredictor(llm=CustomLLM(model_name=model_name, pipeline=falcon_7b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e3f6ef-2629-4abf-b157-228204d76828",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015ad272-f041-463b-8fb8-acd06d80599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set maximum chunk overlap\n",
    "chunk_size = 1024\n",
    "chunk_overlap_ratio = 0.1\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=max_input_size,\n",
    "    num_output=num_output,\n",
    "    chunk_size_limit=chunk_size,\n",
    "    chunk_overlap_ratio=chunk_overlap_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1282314-d452-4a0c-9264-3f9f6fa7b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor_falcon_7b,\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "query_engine_falcon_7b = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2970fa93-8458-4186-b802-4924fa35891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/reginald/lib/python3.11/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original question is as follows: what should a new starter in REG do?\n",
      "We have provided an existing answer: Context information is below. \n",
      "---------------------\n",
      "REG Buddy Sign Up Sheet\n",
      "This page is for organising sign-ups and matches to REG's Buddy-System . Everyone new-starter gets two REG buddies. In Jan 2022 we also had one external buddy. This helps to enforce that 'Turing' is bigger than 'REG'. External buddying is a little more social in nature, whereas internal buddies can wear both hats and also may offer informal technical help. However, note that the external buddying needs to be bidirectional and historically there hasn't been many volunteers from REG to the wider Turing, so external buddying should be considered an optional nice-to-have.\n",
      "If your preference for being a buddy has changed, please contact the person in charge of onboarding wiki page .\n",
      "Please add your name below if you'd like to be considered as a buddy for a new starter.\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Andy Smith\n",
      "\n",
      "\n",
      "Helen Duncan\n",
      "\n",
      "\n",
      "Sam Greenbury\n",
      "\n",
      "\n",
      "Nick Barlow\n",
      "\n",
      "\n",
      "Callum Mole\n",
      "\n",
      "\n",
      "Jack Roberts (ARC)\n",
      "\n",
      "\n",
      "Camila Rangel Smith\n",
      "\n",
      "\n",
      "Lydia France\n",
      "\n",
      "\n",
      "Ed Chalstrey\n",
      "\n",
      "\n",
      "Aoife Hughes\n",
      "\n",
      "\n",
      "Matt Craddock\n",
      "\n",
      "\n",
      "Pam Wochner\n",
      "\n",
      "\n",
      "Ryan Chan\n",
      "\n",
      "\n",
      "James Robinson\n",
      "\n",
      "\n",
      "Fede Nanni\n",
      "\n",
      "\n",
      "Martin Stoffel\n",
      "\n",
      "\n",
      "David Llewellyn-Jones\n",
      "\n",
      "\n",
      "Rosie Wood\n",
      "\n",
      "\n",
      "Katriona Goldmann\n",
      "\n",
      "\n",
      "Greg Mingas\n",
      "\n",
      "\n",
      "Markus Hauru\n",
      "\n",
      "\n",
      "Luke Hare\n",
      "\n",
      "\n",
      "Nathan Simpson\n",
      "\n",
      "\n",
      "\n",
      "Matches\n",
      "The only constraints to being a buddy are that the buddy is not the line manager or on the new-starters first project. Preferably, at least one REG buddy will also be available on the morning of the first day to great the new-starter.\n",
      "Internal\n",
      "| New Starter |  Buddy 1 | Buddy 2 | Buddy 3 (external) | | -------- | -------- | -------- | ---- | | Boyko Vodenicharski - ARC (15 May 23)| Andy Smith | Nick Barlow | N/A | | Katie Awty Carroll - ARC (24 Apr 23)| Camila Rangel Smith | Radka Jersakova | N/A | | Carlos Gavidia-Calderon (3 Apr 23)| Katriona Goldmann | Markus Hauru | N/A | | Nathan Simpson (3 Apr 23)| Greg Mingas | Luke Hare | N/A | | Edmund Heath - ARC (3 Apr 23)| Ed Chalstrey | Helen Duncan | N/A | | Jo Knight - ARC (13 Mar 23)| Rosie Wood | David Llewellyn-Jones | N/A | | David Llewellyn-Jones (13 Feb 23)| Pam Wochner | Ryan Chan | N/A | |  Levan Bokeria (6 Feb 23) | Aoife Hughes | Matt Craddock | N/A | |  Jonathan Yong (23 Jan 23) | Ed Chalstrey | Lydia France | N/A | |  Martin Stoffel (16 Jan 23) | Markus Hauru | Camila Rangel Smith | N/A | |  Rosie Wood (16 Jan 23) | Greg Mingas | Katriona Goldmann | N/A | |  Ed Chapman (11 Jan 23) | Callum Mole | Helen Duncan | N/A | |  Eseoghene Ben-Iwhiwhu (11 Jan 23) | Nick Barlow | Luke Hare | N/A | |  Joseph Palmer (10 Jan 23) | Sam Greenbury | Jack Roberts (ARC) | N/A | |  Katriona (7 November  22) | Aoife | Andy | N/A | |  Isabel (31 October 22) | Camila | Markus | N/A | |  Phil (ARC, 3 October 22) | Andy | David | N/A | |  Ryan (1 September 22) | Fede | James R | N/A | |  Matt (1 September 22) | Kasra | Ed | N/A | |  David SJ (15 August 22) | Callum | Luke | N/A | |  Aoife (1 August 22) | Nick | Sam | N/A | |  Ibrahim (4 July 22) | Markus | Pam | Giuseppe Degan Di Dieco | |  Griff (1 June 22) |   Ed   |  Kasra  |  N/A  | |  Sam (23 May\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the question: what should a new starter in REG do?\n",
      "Given the context information and not prior knowledge, answer the question: what should a new starter in REG do?\n",
      "A new starter in REG should create a personal website and/or a Github account to showcase their work and to communicate with other new-starters.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "| |  David SJ (15 August 22) | Callum | Luke | N/A | |  Aoife (1 August 22) | Nick | Sam | N/A | |  Ibrahim (4 July 22) | Markus | Pam | Giuseppe Degan Di Dieco | |  Griff (1 June 22) |   Ed   |  Kasra  |  N/A  | |  Sam (23 May 22)\n",
      "\n",
      "The new starter page has moved to the REG handbook: https://alan-turing-institute.github.io/REG-handbook/docs/onboarding/new_joiners/first_few_days/\n",
      "See also the systems setup page in the handbook: https://alan-turing-institute.github.io/REG-handbook/docs/onboarding/new_joiners/systems_set_up/\n",
      "Wherever you found a link to this page, it should probably be replaced with a link to one of the above pages.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n",
      "The original question is as follows: what should a new starter in REG do?\n",
      "We have provided an existing answer: Context information is below. \n",
      "---------------------\n",
      "REG Buddy Sign Up Sheet\n",
      "This page is for organising sign-ups and matches to REG's Buddy-System . Everyone new-starter gets two REG buddies. In Jan 2022 we also had one external buddy. This helps to enforce that 'Turing' is bigger than 'REG'. External buddying is a little more social in nature, whereas internal buddies can wear both hats and also may offer informal technical help. However, note that the external buddying needs to be bidirectional and historically there hasn't been many volunteers from REG to the wider Turing, so external buddying should be considered an optional nice-to-have.\n",
      "If your preference for being a buddy has changed, please contact the person in charge of onboarding wiki page .\n",
      "Please add your name below if you'd like to be considered as a buddy for a new starter.\n",
      "\n",
      "\n",
      "\n",
      "Name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Andy Smith\n",
      "\n",
      "\n",
      "Helen Duncan\n",
      "\n",
      "\n",
      "Sam Greenbury\n",
      "\n",
      "\n",
      "Nick Barlow\n",
      "\n",
      "\n",
      "Callum Mole\n",
      "\n",
      "\n",
      "Jack Roberts (ARC)\n",
      "\n",
      "\n",
      "Camila Rangel Smith\n",
      "\n",
      "\n",
      "Lydia France\n",
      "\n",
      "\n",
      "Ed Chalstrey\n",
      "\n",
      "\n",
      "Aoife Hughes\n",
      "\n",
      "\n",
      "Matt Craddock\n",
      "\n",
      "\n",
      "Pam Wochner\n",
      "\n",
      "\n",
      "Ryan Chan\n",
      "\n",
      "\n",
      "James Robinson\n",
      "\n",
      "\n",
      "Fede Nanni\n",
      "\n",
      "\n",
      "Martin Stoffel\n",
      "\n",
      "\n",
      "David Llewellyn-Jones\n",
      "\n",
      "\n",
      "Rosie Wood\n",
      "\n",
      "\n",
      "Katriona Goldmann\n",
      "\n",
      "\n",
      "Greg Mingas\n",
      "\n",
      "\n",
      "Markus Hauru\n",
      "\n",
      "\n",
      "Luke Hare\n",
      "\n",
      "\n",
      "Nathan Simpson\n",
      "\n",
      "\n",
      "\n",
      "Matches\n",
      "The only constraints to being a buddy are that the buddy is not the line manager or on the new-starters first project. Preferably, at least one REG buddy will also be available on the morning of the first day to great the new-starter.\n",
      "Internal\n",
      "| New Starter |  Buddy 1 | Buddy 2 | Buddy 3 (external) | | -------- | -------- | -------- | ---- | | Boyko Vodenicharski - ARC (15 May 23)| Andy Smith | Nick Barlow | N/A | | Katie Awty Carroll - ARC (24 Apr 23)| Camila Rangel Smith | Radka Jersakova | N/A | | Carlos Gavidia-Calderon (3 Apr 23)| Katriona Goldmann | Markus Hauru | N/A | | Nathan Simpson (3 Apr 23)| Greg Mingas | Luke Hare | N/A | | Edmund Heath - ARC (3 Apr 23)| Ed Chalstrey | Helen Duncan | N/A | | Jo Knight - ARC (13 Mar 23)| Rosie Wood | David Llewellyn-Jones | N/A | | David Llewellyn-Jones (13 Feb 23)| Pam Wochner | Ryan Chan | N/A | |  Levan Bokeria (6 Feb 23) | Aoife Hughes | Matt Craddock | N/A | |  Jonathan Yong (23 Jan 23) | Ed Chalstrey | Lydia France | N/A | |  Martin Stoffel (16 Jan 23) | Markus Hauru | Camila Rangel Smith | N/A | |  Rosie Wood (16 Jan 23) | Greg Mingas | Katriona Goldmann | N/A | |  Ed Chapman (11 Jan 23) | Callum Mole | Helen Duncan | N/A | |  Eseoghene Ben-Iwhiwhu (11 Jan 23) | Nick Barlow | Luke Hare | N/A | |  Joseph Palmer (10 Jan 23) | Sam Greenbury | Jack Roberts (ARC) | N/A | |  Katriona (7 November  22) | Aoife | Andy | N/A | |  Isabel (31 October 22) | Camila | Markus | N/A | |  Phil (ARC, 3 October 22) | Andy | David | N/A | |  Ryan (1 September 22) | Fede | James R | N/A | |  Matt (1 September 22) | Kasra | Ed | N/A | |  David SJ (15 August 22) | Callum | Luke | N/A | |  Aoife (1 August 22) | Nick | Sam | N/A | |  Ibrahim (4 July 22) | Markus | Pam | Giuseppe Degan Di Dieco | |  Griff (1 June 22) |   Ed   |  Kasra  |  N/A  | |  Sam (23 May\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the question: what should a new starter in REG do?\n",
      "Given the context information and not prior knowledge, answer the question: what should a new starter in REG do?\n",
      "A new starter in REG should create a personal website and/or a Github account to showcase their work and to communicate with other new-starters.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "| |  David SJ (15 August 22) | Callum | Luke | N/A | |  Aoife (1 August 22) | Nick | Sam | N/A | |  Ibrahim (4 July 22) | Markus | Pam | Giuseppe Degan Di Dieco | |  Griff (1 June 22) |   Ed   |  Kasra  |  N/A  | |  Sam (23 May 22)\n",
      "\n",
      "The new starter page has moved to the REG handbook: https://alan-turing-institute.github.io/REG-handbook/docs/onboarding/new_joiners/first_few_days/\n",
      "See also the systems setup page in the handbook: https://alan-turing-institute.github.io/REG-handbook/docs/onboarding/new_joiners/systems_set_up/\n",
      "Wherever you found a link to this page, it should probably be replaced with a link to one of the above pages.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_falcon_7b.query(\"what should a new starter in REG do?\")\n",
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reginald",
   "language": "python",
   "name": "reginald"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
