import json
import os

import nest_asyncio
import pandas as pd
from llama_index.core.evaluation import CorrectnessEvaluator
from llama_index.llms.ollama import Ollama

from reginald.models.setup_llm import setup_llm

nest_asyncio.apply()


def load_multiline_jsonl(file_path):
    """
    Loads in the questions and answers from a JSON file where each line contains a JSON object.

    Args:
        file_path (str): The path to the JSON Lines file.

    Returns:
        list: A list of JSON objects loaded from the file.
    """
    objects = []
    buffer = ""
    with open(file_path, "r") as file:
        for line in file:
            buffer += line
            try:
                obj = json.loads(buffer)
                objects.append(obj)
                buffer = ""  # Reset the buffer after a successful load
            except json.JSONDecodeError:
                # If a JSONDecodeError occurs, it means the current buffer is not a complete JSON object yet
                continue
    return objects


def load_data(file_path):
    return load_multiline_jsonl(file_path)


# def process_query(query, response_model):
#     """
#     Queries Reginald with the given query using the specified response model.

#     Args:
#         query (str): The query to be processed.
#         response_model: The model used to process the query and generate a response.

#     Returns:
#         str: The response generated by Reginald for the given query.
#     """
#     print(f"Processing query: {query}")
#     response = response_model.direct_message(message=query, user_id="")
#     return response.message

# def collect_responses(data, response_model):
#     """
#     Collects responses from Reginald for each query in the given data using the specified response model.

#     Args:
#         data (list): A list of objects containing queries.
#         response_model: The model used to process the queries and generate responses.

#     Returns:
#         list: A list of dictionaries, where each dictionary contains the query and its corresponding response.
#     """
#     responses = []
#     for obj in data:
#         query = obj.get('prompt')
#         if query:
#             response = process_query(query, response_model)
#             responses.append({
#                 "query": query,
#                 "response": response
#             })
#     return responses

# def evaluate_correctness(responses, correct_answers):
#     '''
#     Evaluates the correctness of generated answers against the correct answers.

#     Args:
#         responses (list): A list of dictionaries containing the generated answers
#         and corresponding queries. Each dictionary should have the following keys:
#             - "query": The query for which the answer was generated.
#             - "response": The generated answer for the query.
#         correct_answers (list): A list of correct answers corresponding to the
#         queries in the responses list.

#     Returns:
#         list: A list of dictionaries containing the evaluation results for each query.
#             Each dictionary has the following keys:
#                 - "query": The query for which the evaluation was performed.
#                 - "generated_answer": The generated answer for the query.
#                 - "correct_answer": The correct answer for the query.
#                 - "is_correct": A boolean indicating whether the generated answer
#                 is correct or not.
#     '''
#     evaluation_results = []
#     for i in range(len(responses)):
#         response = responses[i]
#         query = response["query"]
#         generated_answer = response["response"]
#         correct_answer = correct_answers[i]
#         is_correct = generated_answer.strip().lower() == correct_answer.strip().lower()
#         evaluation_results.append({
#             "query": query,
#             "generated_answer": generated_answer,
#             "correct_answer": correct_answer,
#             "is_correct": is_correct
#         })
#     return evaluation_results

if __name__ == "__main__":
    project_dir = "../../"

    # Load in data and models
    data_file_path = os.path.join(
        project_dir, "data/handbook_qa/data/qAndA_subset.jsonl"
    )

    data = load_data(data_file_path)
    print(f"Loaded {len(data)} question-answer sets from the dataset.")

    eval_questions = [x["prompt"] for x in data]
    gpt3_answers = [x["completion"] for x in data]

    response_model = setup_llm(
        model="llama-index-llama-cpp",
        model_name=os.path.join(
            project_dir,
            "notebooks/llama-index-hackweek2024/../../../llama-2-7b-chat.Q4_K_M.gguf",
        ),
        data_dir=os.path.join(project_dir, "data"),
        which_index="handbook",
    )

    ollama_llm = Ollama(model="llama2:7b-chat", request_timeout=60.0)
    evaluator = CorrectnessEvaluator(llm=ollama_llm)

    # Get the Reginald responses and evaluate correctness
    outputs = []
    for i in range(len(eval_questions)):
        print(i)
        query = eval_questions[i]
        reference = gpt3_answers[i]

        print("Asking Reginald: ", query)
        reginald_resp = response_model.direct_message(message=query, user_id="")
        reginald_resp = reginald_resp.message

        result = evaluator.evaluate(
            query=query,
            response=reginald_resp,
            contexts=[reference],
        )

        outputs.append(result)

    # convert to a pandas dataframe
    output_df = pd.DataFrame(
        {
            "query": [x.query for x in outputs],
            "reginald_response": [x.response for x in outputs],
            "gpt3_response": gpt3_answers,
            "correct": [x.passing for x in outputs],
            "reasoning": [x.feedback for x in outputs],
            "score": [x.score for x in outputs],
        }
    )

    # Output results
    output_path = os.path.join(
        project_dir, "notebooks/llama-index-hackweek2024/data/evaluation_results.csv"
    )
    output_df.to_csv(output_path, index=False)

    print("Done")
