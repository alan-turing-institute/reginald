{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions and Answers using Llama 2\n",
    "\n",
    "Model: Llama 2 7b-chat\n",
    "\n",
    "Data: Handbook test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reginald.models.setup_llm import setup_llm\n",
    "\n",
    "from reginald.models.models.llama_index import (\n",
    "    setup_settings,\n",
    "    LlamaIndexLlamaCPP,\n",
    "    set_global_tokenizer,\n",
    "    compute_default_chunk_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ollama\n",
    "ollama_llm = Ollama(model=\"llama2:7b-chat\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = compute_default_chunk_size(\n",
    "    max_input_size=4096, k=3\n",
    ")  # calculate chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\"\n",
    ").encode  # load tokenizer\n",
    "set_global_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23df7cb52d24c81bb4530e0d77e932f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c06cad3501480b8a5fd12cb58fbb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc5d81e7d9942209fed97447becec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1140d72716148d7824084bb6337d83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706cb0b46111479a8586d519eee7bd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd83b12309646838bce969d1a1f3597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8882e5f457a540698ffeae2be7d6ec7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdb45d47e524f52bd6e418278b211f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729131f4198a46f1a50abc166b8866ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e4dd59355643008d7b39dbb9844b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c6951ca9834224a31c90888a9a9863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "settings = setup_settings(\n",
    "    llm=ollama_llm,\n",
    "    max_input_size=4096,\n",
    "    num_output=512,\n",
    "    chunk_overlap_ratio=0.1,\n",
    "    chunk_size=chunk_size,\n",
    "    k=3,\n",
    "    tokenizer=tokenizer,\n",
    ")  # these are settings for the storage context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load in the handbook\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"../../data/llama_index_indices/handbook/\"\n",
    ")\n",
    "\n",
    "vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    settings=settings,\n",
    ")  # load the data index from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load in the example Paul Graham data\n",
    "# reader = SimpleDirectoryReader(\"./data/paul_graham/\")\n",
    "# documents = reader.load_data()\n",
    "# vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727be2b05d7446ee8b360bf90eded017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kgoldmann/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:213: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
      "  return cls(\n"
     ]
    }
   ],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    vector_index.docstore.docs.values(),\n",
    "    num_questions_per_chunk=1,\n",
    "    show_progress=True,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.55s/it]\n",
      "/Users/kgoldmann/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:310: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
     ]
    }
   ],
   "source": [
    "eval_questions = dataset_generator.generate_questions_from_nodes(\n",
    "    5\n",
    ")  # generate questions from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Of course! Here are 5 questions based on the provided context information:',\n",
       " 'What is the purpose of the \"Contributing\" section in the REG Handbook?',\n",
       " 'Who is the intended audience for the guide in the \"Contributing\" section?',\n",
       " 'How does the guide aim to make contributing to the handbook user-friendly?',\n",
       " 'What is the main focus of the guide in terms of providing links for further reading?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_gpt4 = RelevancyEvaluator(llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every value in eval_questions, get the response from the llm\n",
    "all_dfs = []\n",
    "\n",
    "for i in range(len(eval_questions)):\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    response_vector = query_engine.query(eval_questions[i])\n",
    "    eval_result = evaluator_gpt4.evaluate_response(\n",
    "        query=eval_questions[i], response=response_vector\n",
    "    )\n",
    "\n",
    "    # append to the all_dfs list\n",
    "    all_dfs.append({'eval_result': eval_result,\n",
    "                    'question': eval_questions[i],\n",
    "                    'response': response_vector})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the different elements\n",
    "questions = [x['question'] for x in all_dfs]\n",
    "response = [x['response'] for x in all_dfs]\n",
    "sources = [x['response'].source_nodes for x in all_dfs]\n",
    "#sources = [x[0].node.get_content() for x in sources]\n",
    "\n",
    "# for the each sources, get the content as one string deliminated by ;\n",
    "sources = [\";\".join([x.get_content() for x in y]) for y in sources]\n",
    "\n",
    "match = [x['eval_result'].passing for x in all_dfs] # does the response match the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the questions\n",
    "combined_df = pd.DataFrame({'questions': questions, 'response': response, 'sources': sources, 'match': match})\n",
    "combined_df.to_csv(\"data/handbook_QandA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f9c30_row0_col0, #T_f9c30_row0_col1, #T_f9c30_row0_col2, #T_f9c30_row0_col3, #T_f9c30_row1_col0, #T_f9c30_row1_col1, #T_f9c30_row1_col2, #T_f9c30_row1_col3, #T_f9c30_row2_col0, #T_f9c30_row2_col1, #T_f9c30_row2_col2, #T_f9c30_row2_col3, #T_f9c30_row3_col0, #T_f9c30_row3_col1, #T_f9c30_row3_col2, #T_f9c30_row3_col3, #T_f9c30_row4_col0, #T_f9c30_row4_col1, #T_f9c30_row4_col2, #T_f9c30_row4_col3 {\n",
       "  inline-size: 600px;\n",
       "  overflow-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f9c30\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f9c30_level0_col0\" class=\"col_heading level0 col0\" >questions</th>\n",
       "      <th id=\"T_f9c30_level0_col1\" class=\"col_heading level0 col1\" >response</th>\n",
       "      <th id=\"T_f9c30_level0_col2\" class=\"col_heading level0 col2\" >sources</th>\n",
       "      <th id=\"T_f9c30_level0_col3\" class=\"col_heading level0 col3\" >match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c30_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f9c30_row0_col0\" class=\"data row0 col0\" >Of course! Here are 5 questions based on the provided context information:</td>\n",
       "      <td id=\"T_f9c30_row0_col1\" class=\"data row0 col1\" >Of course, I'm here to help! Based on the provided context information, here are five questions and their answers:\n",
       "\n",
       "1. What is the purpose of creating reading groups within the team?\n",
       "Answer: The purpose of creating reading groups is to learn about a topic, either as part of a project or for general interest. Reading groups may meet weekly to discuss a chapter from a book, paper, or work together on implementing something.\n",
       "2. Where can I find information about current reading groups and their details?\n",
       "Answer: You can find information about current reading groups and their details on the Data Science Skills wiki.\n",
       "3. Are there any other informal groups related to the Turing Institute?\n",
       "Answer: Yes, there are also Turing Interest Groups and many other informal groups around the Turing.\n",
       "4. How does the team work together to implement something?\n",
       "Answer: The team may work together on implementing something through reading groups, where members create reading groups to learn about a topic and discuss it weekly.\n",
       "5. Where can I find material for reading groups?\n",
       "Answer: Material for reading groups is stored in the Data Science Skills repository.</td>\n",
       "      <td id=\"T_f9c30_row0_col2\" class=\"data row0 col2\" >Reading Groups As part of projects or for general interest, members of the team create reading groups to learn about a topic. Reading groups may meet on weekly basis to discuss a chapter from a book, a paper, or to work together on implementing something. A list of current reading groups and information about them can be found on the Data Science Skills wiki . Material for the reading groups is stored in the Data Science Skills repository There are also Turing Interest Groups and many other informal groups around the Turing. ;How We Work </td>\n",
       "      <td id=\"T_f9c30_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c30_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f9c30_row1_col0\" class=\"data row1 col0\" >What is the purpose of the \"Contributing\" section in the REG Handbook?</td>\n",
       "      <td id=\"T_f9c30_row1_col1\" class=\"data row1 col1\" >The purpose of the \"Contributing\" section in the REG Handbook is to provide a platform for individuals to discuss and share ideas related to the handbook. The section encourages open communication and collaboration, allowing participants to brainstorm, ask questions, and report any issues they encounter. By fostering an environment where people can freely express themselves, the Contributing section helps ensure that the handbook remains a dynamic and evolving resource.</td>\n",
       "      <td id=\"T_f9c30_row1_col2\" class=\"data row1 col2\" >Projects This section describes how we coordinate our project work. It does not go into details of how individual projects are managed, but how REG coordinates its commitments to the range of projects it’s involved in. ;Discussions and Issues Contributing does not only mean adding code or writing pages. Being involved in reporting issues and discussing ideas are important and valuable aspects to contributing. The handbook uses both issues and discussions on GitHub. Discussions The handbook Discussions\n",
       "are the best place for informal talk about the handbook. You should feel welcome to create a discussion on any relevant topic, without the formality of an issue. Good examples of discussions are Any questions Possible bugs (does anyone else have this problem?) Chapter suggestions Looking for collaborators Community support Any other on-topic talk Issues The issue tracker is best used for development work. This is because issues integrate well with GitHub development tools like projects, pull requests, assignments and so on. Each issue should ideally represent a well-defined, self-contained piece of work suitable to become a single pull request. Good examples of issues are Bug reports with technical detail Developed chapter proposals Feature requests (such as new shortcodes) Specific ideas for changes When opening an issue, pick a suitable template (if any) to make the process easier. </td>\n",
       "      <td id=\"T_f9c30_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c30_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f9c30_row2_col0\" class=\"data row2 col0\" >Who is the intended audience for the guide in the \"Contributing\" section?</td>\n",
       "      <td id=\"T_f9c30_row2_col1\" class=\"data row2 col1\" >Based on the context provided, it appears that the intended audience for the guide in the \"Contributing\" section is members of the team creating the REG Handbook. The guide provides instructions for reporting issues and discussing ideas related to the handbook, which suggests that the target audience is individuals who are actively contributing to the handbook's development and maintenance.</td>\n",
       "      <td id=\"T_f9c30_row2_col2\" class=\"data row2 col2\" >Discussions and Issues Contributing does not only mean adding code or writing pages. Being involved in reporting issues and discussing ideas are important and valuable aspects to contributing. The handbook uses both issues and discussions on GitHub. Discussions The handbook Discussions\n",
       "are the best place for informal talk about the handbook. You should feel welcome to create a discussion on any relevant topic, without the formality of an issue. Good examples of discussions are Any questions Possible bugs (does anyone else have this problem?) Chapter suggestions Looking for collaborators Community support Any other on-topic talk Issues The issue tracker is best used for development work. This is because issues integrate well with GitHub development tools like projects, pull requests, assignments and so on. Each issue should ideally represent a well-defined, self-contained piece of work suitable to become a single pull request. Good examples of issues are Bug reports with technical detail Developed chapter proposals Feature requests (such as new shortcodes) Specific ideas for changes When opening an issue, pick a suitable template (if any) to make the process easier. ;Reading Groups As part of projects or for general interest, members of the team create reading groups to learn about a topic. Reading groups may meet on weekly basis to discuss a chapter from a book, a paper, or to work together on implementing something. A list of current reading groups and information about them can be found on the Data Science Skills wiki . Material for the reading groups is stored in the Data Science Skills repository There are also Turing Interest Groups and many other informal groups around the Turing. </td>\n",
       "      <td id=\"T_f9c30_row2_col3\" class=\"data row2 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c30_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f9c30_row3_col0\" class=\"data row3 col0\" >How does the guide aim to make contributing to the handbook user-friendly?</td>\n",
       "      <td id=\"T_f9c30_row3_col1\" class=\"data row3 col1\" >The guide aims to make contributing to the handbook user-friendly by providing various ways for users to participate in the project. Here are some ways the guide makes contributing user-friendly:\n",
       "\n",
       "1. Informal talk: The guide encourages users to create discussions on relevant topics without the formality of an issue. This provides a welcoming space for users to ask questions, share ideas, and collaborate with others.\n",
       "2. Local editing: Users can edit pages locally before opening an issue in the GitHub web editor. This allows users to make small changes easily.\n",
       "3. YAML front matter: The guide provides clear instructions on how to format front matter YAML, which is used to define various metadata related to a page. This helps users understand how to properly structure their content.\n",
       "4. Shortcodes: The guide includes a list of useful built-in shortcodes and theme shortcodes that can be included in Markdown content files. These shortcodes make it easier for users to include complex features in their content without having to write raw HTML code.\n",
       "5. Repository links: The guide provides an example of how to create a link to a file or directory in the handbook's repository. This allows users to easily navigate and explore the repository.\n",
       "\n",
       "Overall, the guide makes contributing user-friendly by providing clear instructions, examples, and ways for users to participate in the project.</td>\n",
       "      <td id=\"T_f9c30_row3_col2\" class=\"data row3 col2\" >Discussions and Issues Contributing does not only mean adding code or writing pages. Being involved in reporting issues and discussing ideas are important and valuable aspects to contributing. The handbook uses both issues and discussions on GitHub. Discussions The handbook Discussions\n",
       "are the best place for informal talk about the handbook. You should feel welcome to create a discussion on any relevant topic, without the formality of an issue. Good examples of discussions are Any questions Possible bugs (does anyone else have this problem?) Chapter suggestions Looking for collaborators Community support Any other on-topic talk Issues The issue tracker is best used for development work. This is because issues integrate well with GitHub development tools like projects, pull requests, assignments and so on. Each issue should ideally represent a well-defined, self-contained piece of work suitable to become a single pull request. Good examples of issues are Bug reports with technical detail Developed chapter proposals Feature requests (such as new shortcodes) Specific ideas for changes When opening an issue, pick a suitable template (if any) to make the process easier. ;Editing a Page If you followed the instructions in the Getting Started section to checkout the repository and serve the handbook locally you can edit a page locally. However, as you may have noticed, at the bottom of each page is a link to edit the page in the GitHub web editor if you would prefer. This may be easy for making small changes. Pages Each page is a Markdown file with YAML front matter followed by the page contents in Markdown. Front Matter The front matter is used to define various pieces of metadata related to a page. The front matter appears at the top of a content file. In the handbook we format front matter as YAML, preceded and followed by three hyphens. --- title : \"Example Page\"\n",
       "weight : 1\n",
       "--- The full YAML specification is long and comprehensive. The most important thing to understand here is that the front matter YAML consists of keys and values separated by a hyphen. For example, in the expression weight: 1 , weight is the key with a value of 1 . If you created a page using hugo new then some boilerplate front matter with explanatory comments should already be present. If you are editing an existing page there should already be front matter. Most of the time, the only keys you will need to consider are title and weight . title The title of a page as it appears in the navigation menu weight Determines the order of pages in the navigation menu. Smaller numbers appear first The Hugo documentation details a set of predefined front matter keys . Other valid fields related to the handbook theme are documented in the theme’s README . Content After the front matter is the content section. --- title: \"Example Page\" weight: 1 ---\n",
       "| content section | The content section is formatted in Markdown. Markdown Guide outlines the basic and extended . Not all of the extended Markdown syntax may be implemented by Hugo’s Markdown renderer Shortcodes Shortcodes\n",
       "are templates (which can be parametrised) and included the content section. They can be particularly useful for including more complex features than Markdown’s simple syntax will allow. Using a shortcode is preferable to including raw HTML in a content file. Shortcodes can called in Markdown content files the following way {{% shortcode parameter %}} Some shortcodes may have an opening and closing tag, similar to HTML {{% shortcode %}} content to process {{% /shortcode %}} Hugo Shortcodes Hugo has a set of useful built-in shortcodes . In particular figure gist highlight param ref and relref Theme Shortcodes The theme has a number of shortcodes which may be helpful. In particular hints expand KaTeX tabs Repository Shortcodes Shortcodes can be included in the handbooks repository. For more information see Creating Shortcodes repo_link Create a link to a file or directory in the handbook’s repository. {{% repo_link path=\"README.md\" text=\"README\" %}} path Path to the file or directory relative to the root of the repository text Text of the link (can include Markdown formatting) </td>\n",
       "      <td id=\"T_f9c30_row3_col3\" class=\"data row3 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9c30_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f9c30_row4_col0\" class=\"data row4 col0\" >What is the main focus of the guide in terms of providing links for further reading?</td>\n",
       "      <td id=\"T_f9c30_row4_col1\" class=\"data row4 col1\" >Based on the provided context information, it seems that the main focus of the guide is to provide links for further reading related to the handbook's content. The guide provides information on how to create shortcodes, which are used to include links to files or directories in the handbook's repository. Additionally, the guide mentions that there are built-in shortcodes available in Hugo, as well as theme shortcodes that may be helpful. Overall, the main focus of the guide appears to be on providing resources for readers to explore further and learn more about the topics covered in the handbook.</td>\n",
       "      <td id=\"T_f9c30_row4_col2\" class=\"data row4 col2\" >Discussions and Issues Contributing does not only mean adding code or writing pages. Being involved in reporting issues and discussing ideas are important and valuable aspects to contributing. The handbook uses both issues and discussions on GitHub. Discussions The handbook Discussions\n",
       "are the best place for informal talk about the handbook. You should feel welcome to create a discussion on any relevant topic, without the formality of an issue. Good examples of discussions are Any questions Possible bugs (does anyone else have this problem?) Chapter suggestions Looking for collaborators Community support Any other on-topic talk Issues The issue tracker is best used for development work. This is because issues integrate well with GitHub development tools like projects, pull requests, assignments and so on. Each issue should ideally represent a well-defined, self-contained piece of work suitable to become a single pull request. Good examples of issues are Bug reports with technical detail Developed chapter proposals Feature requests (such as new shortcodes) Specific ideas for changes When opening an issue, pick a suitable template (if any) to make the process easier. ;Editing a Page If you followed the instructions in the Getting Started section to checkout the repository and serve the handbook locally you can edit a page locally. However, as you may have noticed, at the bottom of each page is a link to edit the page in the GitHub web editor if you would prefer. This may be easy for making small changes. Pages Each page is a Markdown file with YAML front matter followed by the page contents in Markdown. Front Matter The front matter is used to define various pieces of metadata related to a page. The front matter appears at the top of a content file. In the handbook we format front matter as YAML, preceded and followed by three hyphens. --- title : \"Example Page\"\n",
       "weight : 1\n",
       "--- The full YAML specification is long and comprehensive. The most important thing to understand here is that the front matter YAML consists of keys and values separated by a hyphen. For example, in the expression weight: 1 , weight is the key with a value of 1 . If you created a page using hugo new then some boilerplate front matter with explanatory comments should already be present. If you are editing an existing page there should already be front matter. Most of the time, the only keys you will need to consider are title and weight . title The title of a page as it appears in the navigation menu weight Determines the order of pages in the navigation menu. Smaller numbers appear first The Hugo documentation details a set of predefined front matter keys . Other valid fields related to the handbook theme are documented in the theme’s README . Content After the front matter is the content section. --- title: \"Example Page\" weight: 1 ---\n",
       "| content section | The content section is formatted in Markdown. Markdown Guide outlines the basic and extended . Not all of the extended Markdown syntax may be implemented by Hugo’s Markdown renderer Shortcodes Shortcodes\n",
       "are templates (which can be parametrised) and included the content section. They can be particularly useful for including more complex features than Markdown’s simple syntax will allow. Using a shortcode is preferable to including raw HTML in a content file. Shortcodes can called in Markdown content files the following way {{% shortcode parameter %}} Some shortcodes may have an opening and closing tag, similar to HTML {{% shortcode %}} content to process {{% /shortcode %}} Hugo Shortcodes Hugo has a set of useful built-in shortcodes . In particular figure gist highlight param ref and relref Theme Shortcodes The theme has a number of shortcodes which may be helpful. In particular hints expand KaTeX tabs Repository Shortcodes Shortcodes can be included in the handbooks repository. For more information see Creating Shortcodes repo_link Create a link to a file or directory in the handbook’s repository. {{% repo_link path=\"README.md\" text=\"README\" %}} path Path to the file or directory relative to the root of the repository text Text of the link (can include Markdown formatting) </td>\n",
       "      <td id=\"T_f9c30_row4_col3\" class=\"data row4 col3\" >True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2c312c290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df_out = combined_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "display(combined_df_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Reginal the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/kgoldmann/Library/Caches/llama_index/models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3891.24 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "response_model = setup_llm(\n",
    "    model=\"llama-index-llama-cpp\",\n",
    "    model_name=\"../../../llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    data_dir=\"../../data/\",\n",
    "    which_index=\"handbook\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    resp = response_model.direct_message(message=que, user_id=\"\")\n",
    "    return(resp.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14150.19 ms\n",
      "llama_print_timings:      sample time =      17.09 ms /   139 runs   (    0.12 ms per token,  8131.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12778.71 ms /   887 tokens (   14.41 ms per token,    69.41 tokens per second)\n",
      "llama_print_timings:        eval time =   10230.16 ms /   138 runs   (   74.13 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time =   23306.57 ms /  1025 tokens\n",
      "WARNING:root:Was expecting a backend response with a regular expression but couldn't find a match.\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14150.19 ms\n",
      "llama_print_timings:      sample time =      27.86 ms /   242 runs   (    0.12 ms per token,  8687.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18033.65 ms /  1264 tokens (   14.27 ms per token,    70.09 tokens per second)\n",
      "llama_print_timings:        eval time =   18838.52 ms /   241 runs   (   78.17 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =   37403.11 ms /  1505 tokens\n",
      "WARNING:root:Was expecting a backend response with a regular expression but couldn't find a match.\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14150.19 ms\n",
      "llama_print_timings:      sample time =      24.18 ms /   210 runs   (    0.12 ms per token,  8683.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26655.45 ms /  1874 tokens (   14.22 ms per token,    70.30 tokens per second)\n",
      "llama_print_timings:        eval time =   16797.33 ms /   209 runs   (   80.37 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   43923.04 ms /  2083 tokens\n",
      "WARNING:root:Was expecting a backend response with a regular expression but couldn't find a match.\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14150.19 ms\n",
      "llama_print_timings:      sample time =      42.91 ms /   378 runs   (    0.11 ms per token,  8808.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32785.96 ms /  2109 tokens (   15.55 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =   31854.82 ms /   377 runs   (   84.50 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   65518.67 ms /  2486 tokens\n",
      "WARNING:root:Was expecting a backend response with a regular expression but couldn't find a match.\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14150.19 ms\n",
      "llama_print_timings:      sample time =      26.66 ms /   238 runs   (    0.11 ms per token,  8925.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7371.21 ms /   404 tokens (   18.25 ms per token,    54.81 tokens per second)\n",
      "llama_print_timings:        eval time =   20656.59 ms /   237 runs   (   87.16 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =   28563.72 ms /   641 tokens\n",
      "WARNING:root:Was expecting a backend response with a regular expression but couldn't find a match.\n"
     ]
    }
   ],
   "source": [
    "all_questions = combined_df['questions'].tolist()\n",
    "reg_responses = []\n",
    "for que in all_questions:\n",
    "        reg_responses.append(get_response(que))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Great! Here are five questions based on the provided context information:\n",
      "1. What is the main purpose of the \"Contributing\" section in the REG Handbook?\n",
      "2. How does the section help create an inclusive and welcoming environment for new starters at REG?\n",
      "3. Can you explain the significance of the \"Buddy System\" mentioned in the section?\n",
      "4. What are some of the key technologies or tools mentioned in the section that new employees may need to know about?\n",
      "5. How does the section encourage new employees to explore their interests and expand their skill sets within the context of their work at REG?\n",
      "\n",
      "\n",
      "I read the following documents to compose this answer:\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/regular_events/reading_groups/ (similarity: 0.21)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/how_we_work/ (similarity: 0.2)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/regular_events/ (similarity: 0.17)\n",
      "\n",
      "\n",
      "  The purpose of the \"Contributing\" section in the REG (Research Environment Group) Handbook is to provide guidance for individuals who want to contribute to the handbook, including information on how to add or edit content without getting bogged down in technical details. Specifically, the section aims to:\n",
      "1. Provide an overview of the different ways to contribute to the handbook, such as adding new pages, editing existing ones, or reporting issues.\n",
      "2. Explain the basic workflow for contributing to the handbook, including how to create a new page, edit an existing one, or report an issue.\n",
      "3. Offer tips and best practices for writing clear and concise content that is easy for others to understand and use.\n",
      "4. Discuss the importance of following the handbook's style guide and using consistent formatting throughout.\n",
      "5. Emphasize the value of collaborating with other contributors and welcoming feedback from the community.\n",
      "By providing this information, the \"Contributing\" section aims to make it easier for new contributors to get started and feel confident in their ability to contribute to the handbook.\n",
      "\n",
      "\n",
      "I read the following documents to compose this answer:\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/projects/ (similarity: 0.5)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/discussions_and_issues/ (similarity: 0.5)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/ (similarity: 0.45)\n",
      "\n",
      "\n",
      "  The intended audience for the \"Contributing\" section of the REG (Research Environment Group) Handbook appears to be individuals who are interested in contributing to the handbook, including:\n",
      "1. New starters at REG: The section provides guidance on how to contribute to the handbook, which can be helpful for new employees who are not familiar with the group's processes and tools.\n",
      "2. Existing contributors: The section may also be useful for existing contributors who want to learn more about the handbook's structure, style guide, and workflow.\n",
      "3. External collaborators: The section mentions opportunities for side projects and skill development, which may be of interest to external collaborators who want to contribute to the group's mission beyond just editing the handbook.\n",
      "Overall, the intended audience for the \"Contributing\" section appears to be individuals who are interested in participating in the REG Handbook project and want to learn more about how to contribute effectively.\n",
      "\n",
      "\n",
      "I read the following documents to compose this answer:\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/discussions_and_issues/ (similarity: 0.46)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/regular_events/reading_groups/ (similarity: 0.38)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributors/ (similarity: 0.36)\n",
      "\n",
      "\n",
      "  The \"Contributing\" section of the REG (Research Environment Group) Handbook aims to make contributing to the handbook user-friendly by providing clear and concise instructions, tips, and best practices for new contributors. Here are some ways the guide aims to achieve this:\n",
      "1. Simple language: The guide uses simple language and avoids technical jargon, making it easy for new contributors to understand the concepts and processes involved in contributing to the handbook.\n",
      "2. Step-by-step workflow: The section provides a step-by-step workflow for contributing to the handbook, including how to create a new page, edit an existing one, or report an issue. This makes it easy for new contributors to follow along and understand the process.\n",
      "3. Tips and best practices: The guide offers tips and best practices for writing clear and concise content, formatting pages consistently, and collaborating with other contributors. These tips can help new contributors create high-quality content that is easy for others to understand and use.\n",
      "4. Links to additional resources: The section includes links to additional resources, such as the handbook's style guide and a list of common issues, which can provide further guidance and support for new contributors.\n",
      "5. Encouragement and support: The guide encourages new contributors to collaborate with other members of the community and provides opportunities for side projects and skill development. This can help new contributors feel more connected and supported in their contributions to the handbook.\n",
      "Overall, the \"Contributing\" section of the REG Handbook is designed to be user-friendly and accessible to new contributors, providing them with the information and support they need to make meaningful contributions to the project.\n",
      "\n",
      "\n",
      "I read the following documents to compose this answer:\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/discussions_and_issues/ (similarity: 0.55)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/editing_a_page/ (similarity: 0.5)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/ (similarity: 0.5)\n",
      "\n",
      "\n",
      "  The \"Contributing\" section of the REG (Research Environment Group) Handbook provides links to additional resources for further reading in order to:\n",
      "1. Support new contributors: The guide includes links to resources that can help new contributors learn more about the handbook's structure, style guide, and workflow, as well as common issues and how to resolve them.\n",
      "2. Provide context: The section includes links to relevant documentation and guides that can provide context for new contributors, such as the handbook's purpose, target audience, and editing philosophy.\n",
      "3. Facilitate collaboration: The guide includes links to tools and resources that can help contributors collaborate more effectively, such as version control systems and project management software.\n",
      "4. Encourage skill development: The section includes links to tutorials and guides that can help contributors develop new skills, such as writing for the web or using specific tools and technologies.\n",
      "By providing these links, the guide aims to support new contributors in their efforts to learn and contribute to the handbook, and to facilitate collaboration and skill development within the community.\n",
      "\n",
      "\n",
      "I read the following documents to compose this answer:\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/discussions_and_issues/ (similarity: 0.45)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/editing_a_page/ (similarity: 0.4)\n",
      "\n",
      "https://alan-turing-institute.github.io/REG-handbook/docs/contributing/ (similarity: 0.39)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in reg_responses:\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['reginald_response'] = reg_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>response</th>\n",
       "      <th>sources</th>\n",
       "      <th>match</th>\n",
       "      <th>reginald_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Of course! Here are 5 questions based on the p...</td>\n",
       "      <td>Of course, I'm here to help! Based on the prov...</td>\n",
       "      <td>Reading Groups As part of projects or for gene...</td>\n",
       "      <td>True</td>\n",
       "      <td>Great! Here are five questions based on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the purpose of the \"Contributing\" sect...</td>\n",
       "      <td>The purpose of the \"Contributing\" section in t...</td>\n",
       "      <td>Projects This section describes how we coordin...</td>\n",
       "      <td>True</td>\n",
       "      <td>The purpose of the \"Contributing\" section in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who is the intended audience for the guide in ...</td>\n",
       "      <td>Based on the context provided, it appears that...</td>\n",
       "      <td>Discussions and Issues Contributing does not o...</td>\n",
       "      <td>True</td>\n",
       "      <td>The intended audience for the \"Contributing\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the guide aim to make contributing to...</td>\n",
       "      <td>The guide aims to make contributing to the han...</td>\n",
       "      <td>Discussions and Issues Contributing does not o...</td>\n",
       "      <td>True</td>\n",
       "      <td>The \"Contributing\" section of the REG (Resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the main focus of the guide in terms o...</td>\n",
       "      <td>Based on the provided context information, it ...</td>\n",
       "      <td>Discussions and Issues Contributing does not o...</td>\n",
       "      <td>True</td>\n",
       "      <td>The \"Contributing\" section of the REG (Resea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  Of course! Here are 5 questions based on the p...   \n",
       "1  What is the purpose of the \"Contributing\" sect...   \n",
       "2  Who is the intended audience for the guide in ...   \n",
       "3  How does the guide aim to make contributing to...   \n",
       "4  What is the main focus of the guide in terms o...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Of course, I'm here to help! Based on the prov...   \n",
       "1  The purpose of the \"Contributing\" section in t...   \n",
       "2  Based on the context provided, it appears that...   \n",
       "3  The guide aims to make contributing to the han...   \n",
       "4  Based on the provided context information, it ...   \n",
       "\n",
       "                                             sources  match  \\\n",
       "0  Reading Groups As part of projects or for gene...   True   \n",
       "1  Projects This section describes how we coordin...   True   \n",
       "2  Discussions and Issues Contributing does not o...   True   \n",
       "3  Discussions and Issues Contributing does not o...   True   \n",
       "4  Discussions and Issues Contributing does not o...   True   \n",
       "\n",
       "                                   reginald_response  \n",
       "0    Great! Here are five questions based on the ...  \n",
       "1    The purpose of the \"Contributing\" section in...  \n",
       "2    The intended audience for the \"Contributing\"...  \n",
       "3    The \"Contributing\" section of the REG (Resea...  \n",
       "4    The \"Contributing\" section of the REG (Resea...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Response at 0x2ae0ba290>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turing the response_vector into an object with attribute response\n",
    "class Response:\n",
    "    def __init__(self, response):\n",
    "        self.response = response\n",
    "        self.get_content = response\n",
    "        self.source_nodes = response\n",
    "\n",
    "response_objects = [Response(x) for x in reg_responses]\n",
    "response_objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Great! Here are five questions based on the provided context information:\\n1. What is the main purpose of the \"Contributing\" section in the REG Handbook?\\n2. How does the section help create an inclusive and welcoming environment for new starters at REG?\\n3. Can you explain the significance of the \"Buddy System\" mentioned in the section?\\n4. What are some of the key technologies or tools mentioned in the section that new employees may need to know about?\\n5. How does the section encourage new employees to explore their interests and expand their skill sets within the context of their work at REG?\\n\\n\\nI read the following documents to compose this answer:\\nhttps://alan-turing-institute.github.io/REG-handbook/docs/regular_events/reading_groups/ (similarity: 0.21)\\n\\nhttps://alan-turing-institute.github.io/REG-handbook/docs/how_we_work/ (similarity: 0.2)\\n\\nhttps://alan-turing-institute.github.io/REG-handbook/docs/regular_events/ (similarity: 0.17)'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_objects[0].get_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for EvaluationResult\ncontexts\n  value is not a valid sequence (type=type_error.sequence)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m reg_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_questions)):\n\u001b[0;32m----> 4\u001b[0m     eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator_gpt4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_responses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msources\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(eval_result\u001b[38;5;241m.\u001b[39mpassing))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# for i in range(len(all_questions)):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     query_engine = vector_index.as_query_engine()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     response_vector = reg_responses[i]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#                     'question': all_questions[i],\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#                     'response': response_vector})\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/base.py:62\u001b[0m, in \u001b[0;36mBaseEvaluator.evaluate\u001b[0;34m(self, query, response, contexts, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     51\u001b[0m     query: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     55\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EvaluationResult:\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run evaluation with query string, retrieved contexts,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    and generated response string.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Subclasses can override this method to provide custom evaluation logic and\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    take in additional arguments.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/llama_index/core/async_utils.py:30\u001b[0m, in \u001b[0;36masyncio_run\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mrun(coro)\n",
      "File \u001b[0;32m~/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/llama_index/core/evaluation/relevancy.py:136\u001b[0m, in \u001b[0;36mRelevancyEvaluator.aevaluate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe response is invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m     passing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEvaluationResult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpassing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeedback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_response_txt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/reginald/.venv/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for EvaluationResult\ncontexts\n  value is not a valid sequence (type=type_error.sequence)"
     ]
    }
   ],
   "source": [
    "reg_dfs = []\n",
    "\n",
    "for i in range(len(eval_questions)):\n",
    "    query_engine = vector_index.as_query_engine()\n",
    "    response_vector = query_engine.query(eval_questions[i])\n",
    "    eval_result = .evaluate_response(\n",
    "        query=eval_questions[i], response=response_vector\n",
    "    )\n",
    "\n",
    "# for i in range(len(all_questions)):\n",
    "#     query_engine = vector_index.as_query_engine()\n",
    "#     response_vector = reg_responses[i]\n",
    "#     eval_result = evaluator_gpt4.evaluate_response(\n",
    "#         query=all_questions[i], response=response_objects[i]\n",
    "#     )\n",
    "\n",
    "#     # append to the all_dfs list\n",
    "#     reg_dfs.append({'eval_result': eval_result,\n",
    "#                     'question': all_questions[i],\n",
    "#                     'response': response_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
