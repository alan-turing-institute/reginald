{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efa0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    LangchainEmbedding,\n",
    "    VectorStoreIndex,\n",
    "    PromptHelper,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    Document\n",
    ")\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39695618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.21'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index\n",
    "llama_index.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2138e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rchan/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/llama_index_proper/llama_index/__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_index.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b085fc",
   "metadata": {},
   "source": [
    "Note: notebook assumes that in the reginald directory, there is a `gguf_models/` folder. Here we've downloaded the quantized 4-bit version of Llama2-13b-chat from [`TheBloke/Llama-2-13B-chat-GGML`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML). \n",
    "\n",
    "Note that we're currently running a version of `llama-cpp-python` which no longer supports `ggmmlv3` model formats and has changed to `gguf`. We need to convert the above to `gguf` format using the `convert-llama-ggmlv3-to-gguf.py` script in [`llama.cpp`](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "## Quick example with llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3044b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_2_13b_chat_path = \"../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ad130",
   "metadata": {},
   "source": [
    "## Using metal acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21bee96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 363 tensors from ../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.35.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.36.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:           blk.37.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:           blk.38.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:           blk.39.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                        general.description str     \n",
      "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   5:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 2048\n",
      "llm_load_print_meta: n_ctx          = 512\n",
      "llm_load_print_meta: n_embd         = 5120\n",
      "llm_load_print_meta: n_head         = 40\n",
      "llm_load_print_meta: n_head_kv      = 40\n",
      "llm_load_print_meta: n_layer        = 40\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 5.0e-06\n",
      "llm_load_print_meta: n_ff           = 13824\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 13B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium (guessed)\n",
      "llm_load_print_meta: model size     = 13.02 B\n",
      "llm_load_print_meta: general.name   = llama-2-13b-chat.ggmlv3.q4_K_S.bin\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MB\n",
      "llm_load_tensors: mem required  = 7024.01 MB (+  400.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rchan/opt/miniconda3/envs/reginald/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x11cba37b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x11cba3a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x11cba3c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x11cba3ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x11cba4130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x11cba4390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x11cba45f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x11cba4850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x11cba4ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11cba4d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x11cba4f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11cba51d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x11cba5430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x11cba5690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x11cba58f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x11cba5b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x11cba5db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x11cba6010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x11cba6270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x11cba64d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x11cba6730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x11cba6d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x11cba71c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11cba7780 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x11cba7d40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11cba8300 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11cba88c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11cba9080 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11cba98a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11cba9e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x11cbaa460 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x11cbaaa60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x11cbab060 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x11cbab660 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x11cbabc60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x11cbac260 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x11cbac860 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x11cbace60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x11cbad460 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x11cbad7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x11cbadf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11cbae5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11cbaece0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11cbaf3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: compute buffer total size =   91.47 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.61 MB, ( 7025.05 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 7026.53 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, ( 7428.53 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    90.02 MB, ( 7518.55 / 21845.34)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=llama_2_13b_chat_path, n_gpu_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "885156d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_example = \"Name all the planets in the solar system and state their distances to the sun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bee457a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   610.65 ms\n",
      "llama_print_timings:      sample time =   196.05 ms /   268 runs   (    0.73 ms per token,  1367.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   610.63 ms /    17 tokens (   35.92 ms per token,    27.84 tokens per second)\n",
      "llama_print_timings:        eval time = 14795.14 ms /   267 runs   (   55.41 ms per token,    18.05 tokens per second)\n",
      "llama_print_timings:       total time = 15977.86 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt_example,\n",
    "             max_tokens=512,\n",
    "             echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acef5902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-0026dd42-1853-4a6c-8e46-5a6e71740986', 'object': 'text_completion', 'created': 1694121339, 'model': '../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin', 'choices': [{'text': \"Name all the planets in the solar system and state their distances to the sun.\\n\\nThere are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Here is a list of each planet along with its distance from the Sun (in astronomical units or AU):\\n\\n1. Mercury - 0.4 AU (very close to the Sun)\\n2. Venus - 1.0 AU (just inside Earth's orbit)\\n3. Earth - 1.0 AU (the distance from the Earth to the Sun is called an astronomical unit, or AU)\\n4. Mars - 1.6 AU (about 1.5 times the distance from the Earth to the Sun)\\n5. Jupiter - 5.2 AU (about 5 times the distance from the Earth to the Sun)\\n6. Saturn - 9.5 AU (almost twice the distance from the Earth to the Sun)\\n7. Uranus - 19.0 AU (about 4 times the distance from the Earth to the Sun)\\n8. Neptune - 30.1 AU (more than 3 times the distance from the Earth to the Sun)\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 17, 'completion_tokens': 267, 'total_tokens': 284}}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1d16ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name all the planets in the solar system and state their distances to the sun.\n",
      "\n",
      "There are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Here is a list of each planet along with its distance from the Sun (in astronomical units or AU):\n",
      "\n",
      "1. Mercury - 0.4 AU (very close to the Sun)\n",
      "2. Venus - 1.0 AU (just inside Earth's orbit)\n",
      "3. Earth - 1.0 AU (the distance from the Earth to the Sun is called an astronomical unit, or AU)\n",
      "4. Mars - 1.6 AU (about 1.5 times the distance from the Earth to the Sun)\n",
      "5. Jupiter - 5.2 AU (about 5 times the distance from the Earth to the Sun)\n",
      "6. Saturn - 9.5 AU (almost twice the distance from the Earth to the Sun)\n",
      "7. Uranus - 19.0 AU (about 4 times the distance from the Earth to the Sun)\n",
      "8. Neptune - 30.1 AU (more than 3 times the distance from the Earth to the Sun)\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865df6bf",
   "metadata": {},
   "source": [
    "## Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db096045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 363 tensors from ../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensorAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.35.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.36.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:           blk.37.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:           blk.38.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:           blk.39.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                        general.description str     \n",
      "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   5:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 2048\n",
      "llm_load_print_meta: n_ctx          = 512\n",
      "llm_load_print_meta: n_embd         = 5120\n",
      "llm_load_print_meta: n_head         = 40\n",
      "llm_load_print_meta: n_head_kv      = 40\n",
      "llm_load_print_meta: n_layer        = 40\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 5.0e-06\n",
      "llm_load_print_meta: n_ff           = 13824\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 13B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium (guessed)\n",
      "llm_load_print_meta: model size     = 13.02 B\n",
      "llm_load_print_meta: general.name   = llama-2-13b-chat.ggmlv3.q4_K_S.bin\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MB\n",
      "llm_load_tensors: mem required  = 7024.01 MB (+  400.00 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.47 MB\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(model_path=llama_2_13b_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "291a4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  1480.25 ms\n",
      "llama_print_timings:      sample time =   162.22 ms /   212 runs   (    0.77 ms per token,  1306.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1480.21 ms /    17 tokens (   87.07 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:        eval time = 20115.90 ms /   211 runs   (   95.34 ms per token,    10.49 tokens per second)\n",
      "llama_print_timings:       total time = 22063.41 ms\n"
     ]
    }
   ],
   "source": [
    "output = llm(prompt_example,\n",
    "             max_tokens=512,\n",
    "             echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdce188",
   "metadata": {},
   "source": [
    "By inspection, we can see that the metal acceleration is faster as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b74226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name all the planets in the solar system and state their distances to the sun.\n",
      "1. Mercury - 58 million kilometers (36 million miles)\n",
      "2. Venus - 108 million kilometers (67 million miles)\n",
      "3. Earth - 149.6 million kilometers (92.96 million miles)\n",
      "4. Mars - 225 million kilometers (140 million miles)\n",
      "5. Jupiter - 778.3 million kilometers (483.8 million miles)\n",
      "6. Saturn - 1.4 billion kilometers (870 million miles)\n",
      "7. Uranus - 2.9 billion kilometers (1.8 billion miles)\n",
      "8. Neptune - 4.5 billion kilometers (2.8 billion miles)\n",
      "\n",
      "Note that the distance of each planet from the Sun is measured in terms of their average distance, as the orbits of the planets are not perfectly circular and the distances vary slightly over the course of a year.\n"
     ]
    }
   ],
   "source": [
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b606c",
   "metadata": {},
   "source": [
    "## Using Llama2 in `llama-index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8f0f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 363 tensors from ../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:           blk.34.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:           blk.35.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:           blk.36.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:           blk.37.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:           blk.38.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:           blk.39.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                        general.description str     \n",
      "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   5:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 2048\n",
      "llm_load_print_meta: n_ctx          = 3900\n",
      "llm_load_print_meta: n_embd         = 5120\n",
      "llm_load_print_meta: n_head         = 40\n",
      "llm_load_print_meta: n_head_kv      = 40\n",
      "llm_load_print_meta: n_layer        = 40\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 5.0e-06\n",
      "llm_load_print_meta: n_ff           = 13824\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 13B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium (guessed)\n",
      "llm_load_print_meta: model size     = 13.02 B\n",
      "llm_load_print_meta: general.name   = llama-2-13b-chat.ggmlv3.q4_K_S.bin\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MB\n",
      "llm_load_tensors: mem required  = 7024.01 MB (+ 3046.88 MB per state)\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 3046.88 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: loading '/Users/rchan/opt/miniconda3/envs/reginald/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x14ca7e860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                        0x14ca7eac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                            0x14ca7ed20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x14ca7ef80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                          0x14ca7dc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                           0x14ca7dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                           0x14ca7fb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                           0x14ca7fde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x14ca80040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x14ca802a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x14ca80500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x14ca80760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x14ca809c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                  0x14ca80c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x14ca80e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x14ca810e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x14ca814e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x14ca81740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x14ca819a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x14ca81c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                           0x14ca81e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x14ca82450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x14ca828f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x14ca82d90 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x14ca83230 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x14ca836d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x14ca83b70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x14ca84010 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x14ca844b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x14ca84950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x14ca84e30 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x14ca85310 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x14ca857f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x14ca85cd0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x14ca861b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x14ca86690 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1488391a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x148839d60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x14883a240 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope                           0x14883a4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x14883aaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x14883b190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x14883b880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x14883bf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: compute buffer total size =  356.16 MB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.61 MB, ( 7025.11 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.48 MB, ( 7026.59 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3048.88 MB, (10075.47 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   354.70 MBAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      ", (10430.17 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=\"../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    # llama2 has a context window of 4096 tokens,\n",
    "    # but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad388e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMMetadata(context_window=3900, num_output=1024, is_chat_model=False, is_function_calling_model=False, model_name='../../gguf_models/llama-2-13b-chat.gguf.q4_K_S.bin')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107331f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "handbook = pd.read_csv(\"../../data/public/handbook-scraped.csv\")\n",
    "wiki = pd.read_csv(\"../../data/turing_internal/wiki-scraped.csv\")\n",
    "# turing = pd.read_csv(\"../../data/public/turingacuk-no-boilerplate.csv\")\n",
    "\n",
    "text_list = list(handbook[\"body\"].astype(\"str\")) + list(wiki[\"body\"].astype(\"str\"))\n",
    "documents = [Document(text=t) for t in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff676438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00032b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of output tokens\n",
    "num_output = 1024\n",
    "# set maximum input size\n",
    "context_window = 4096\n",
    "# set maximum chunk overlap\n",
    "chunk_size_limit = 512\n",
    "chunk_overlap_ratio = 0\n",
    "\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=context_window,\n",
    "    num_output=num_output,\n",
    "    chunk_size_limit=chunk_size_limit,\n",
    "    chunk_overlap_ratio=chunk_overlap_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4f3d57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=LLMPredictor(llm=llm),\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    chunk_size=chunk_size_limit,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ddbe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_mode = \"simple_summarize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e01b1",
   "metadata": {},
   "source": [
    "## \"React\" chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a8b7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"react\",\n",
    "                                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1695904a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mThought: I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'hello world'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =    42.94 ms /    59 runs   (    0.73 ms per token,  1374.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7509.19 ms /   447 tokens (   16.80 ms per token,    59.53 tokens per second)\n",
      "llama_print_timings:        eval time =  3475.79 ms /    58 runs   (   59.93 ms per token,    16.69 tokens per second)\n",
      "llama_print_timings:       total time = 11105.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mObservation:   Hello! As a helpful, respectful, and honest assistant, I'm here to assist you with any questions or requests you may have. Based on the context information provided, it seems like you are looking for information about the Turing Institute and its various communications channels and events.\n",
      "\n",
      "To start, I can provide you with some general information about the Turing Institute and its activities. The Turing Institute is a research centre based in the UK that focuses on the development of algorithms and computational methods for solving complex problems in various fields, such as computer science, mathematics, and biology. The institute has a strong emphasis on interdisciplinary research and collaboration, and it hosts a variety of events and workshops to facilitate these interactions.\n",
      "\n",
      "In terms of communications channels, the Turing Institute uses a variety of platforms to keep its members and collaborators informed about its activities and research progress. These include email lists, Slack channels, and a website with information about ongoing projects, events, and research updates.\n",
      "\n",
      "Regarding events, the Turing Institute hosts a variety of activities throughout the year, including tech talks, workshops, and conferences. These events cover a range of topics related to the institute's research areas, and they are often open to members and non-members alike. You can find information about upcoming events on the Turing Institute's website or by checking the shared REG calendar.\n",
      "\n",
      "If you have any specific questions or requests, please feel free to ask, and I will do my best to assist you based on the information available to me.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =   244.08 ms /   339 runs   (    0.72 ms per token,  1388.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8954.56 ms /   537 tokens (   16.68 ms per token,    59.97 tokens per second)\n",
      "llama_print_timings:        eval time = 21652.52 ms /   338 runs   (   64.06 ms per token,    15.61 tokens per second)\n",
      "llama_print_timings:       total time = 31331.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mThought: I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'what are the research areas of the Turing Institute?'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =    32.78 ms /    46 runs   (    0.71 ms per token,  1403.21 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15197.21 ms /   832 tokens (   18.27 ms per token,    54.75 tokens per second)\n",
      "llama_print_timings:        eval time =  2972.41 ms /    45 runs   (   66.05 ms per token,    15.14 tokens per second)\n",
      "llama_print_timings:       total time = 18262.37 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =   216.46 ms /   309 runs   (    0.70 ms per token,  1427.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11918.74 ms /   689 tokens (   17.30 ms per token,    57.81 tokens per second)\n",
      "llama_print_timings:        eval time = 20660.28 ms /   308 runs   (   67.08 ms per token,    14.91 tokens per second)\n",
      "llama_print_timings:       total time = 33190.48 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mObservation:   Based on the updated context, the research areas of the Turing Institute are focused on impact-driven research in data science and AI, with a particular emphasis on diversity and openness. The institute prioritizes projects that have realistic plans for meeting the priority of reproducible research and open source software development. Additionally, the institute values collaboration across academic, social, ethnic, and racial backgrounds, as well as cross-disciplinary and cross-cultural collaborations.\n",
      "\n",
      "Some specific research areas that may be of interest to the Turing Institute include:\n",
      "\n",
      "1. Data-driven approaches to addressing societal challenges, such as healthcare, education, and environmental sustainability.\n",
      "2. Development of new AI technologies and techniques, such as machine learning, natural language processing, and computer vision.\n",
      "3. Applications of data science and AI in various domains, such as finance, transportation, and culture.\n",
      "4. Studies on the ethical, social, and economic implications of data science and AI.\n",
      "5. Collaborative research projects that bring together diverse perspectives and expertise from academia, industry, and civil society.\n",
      "\n",
      "The Turing Institute is open to a wide range of research proposals that align with its guiding principles and priorities, including pioneering approaches and innovative methodologies. The institute encourages the sharing of outputs and methodologies as openly as possible to facilitate collaboration and reach a diverse audience.\n",
      "\n",
      "In addition to these general research areas, the Turing Institute is also interested in exploring new ways of tackling research, talent, and approaches. Project briefs will need to specify the extent to which the proposed approaches or methodologies are innovative in the context of similar research being undertaken elsewhere, and a pioneering score will be assigned to each project.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =   277.50 ms /   396 runs   (    0.70 ms per token,  1427.03 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11135.08 ms /   627 tokens (   17.76 ms per token,    56.31 tokens per second)\n",
      "llama_print_timings:        eval time = 26738.70 ms /   395 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time = 38670.27 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mResponse: The research areas of the Turing Institute include data-driven approaches to addressing societal challenges, development of new AI technologies and techniques, applications of data science and AI in various domains, studies on the ethical, social, and economic implications of data science and AI, and collaborative research projects that bring together diverse perspectives and expertise from academia, industry, and civil society.\n",
      "\u001b[0mThe research areas of the Turing Institute include data-driven approaches to addressing societal challenges, development of new AI technologies and techniques, applications of data science and AI in various domains, studies on the ethical, social, and economic implications of data science and AI, and collaborative research projects that bring together diverse perspectives and expertise from academia, industry, and civil society.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =    72.06 ms /   103 runs   (    0.70 ms per token,  1429.42 tokens per second)\n",
      "llama_print_timings: prompt eval time = 27985.35 ms /  1286 tokens (   21.76 ms per token,    45.95 tokens per second)\n",
      "llama_print_timings:        eval time =  7628.91 ms /   102 runs   (   74.79 ms per token,    13.37 tokens per second)\n",
      "llama_print_timings:       total time = 35813.39 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"hello\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6fa2d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mResponse:   As a new starter in the Research Engineering Group (REG) at the Alan Turing Institute, there are several things you can do to get started and make the most of your time here:\n",
      "\n",
      "1. Familiarize yourself with the institute's research areas and ongoing projects. This will help you understand the scope of the work being done and how you can contribute.\n",
      "2. Meet with your supervisor and other members of the REG to discuss your background, interests, and goals. They can provide valuable guidance and help you get settled into your new role.\n",
      "3. Review the institute's policies and procedures to ensure you understand the expectations and requirements of your position.\n",
      "4. Attend orientation sessions and training programs offered by the institute to learn more about the Turing Institute's culture, resources, and research practices.\n",
      "5. Start exploring the tools and technologies used in the REG, such as query_engine_tool, to get a sense of the technical capabilities available to you.\n",
      "6. Begin identifying potential research projects that align with your interests and skills, and reach out to relevant researchers to discuss possibilities.\n",
      "7. Consider joining relevant working groups or workshops to connect with other researchers and stay up-to-date on the latest developments in the field.\n",
      "8. Start building your network within the institute by attending social events, seminars, and other activities that promote collaboration and knowledge sharing.\n",
      "9. Familiarize yourself with the Turing Institute's publication and dissemination processes to understand how research is shared and recognized within the institution.\n",
      "10. Stay open-minded, curious, and willing to learn, as the Turing Institute is a dynamic and interdisciplinary environment that values collaboration and innovation.\n",
      "\u001b[0m  As a new starter in the Research Engineering Group (REG) at the Alan Turing Institute, there are several things you can do to get started and make the most of your time here:\n",
      "\n",
      "1. Familiarize yourself with the institute's research areas and ongoing projects. This will help you understand the scope of the work being done and how you can contribute.\n",
      "2. Meet with your supervisor and other members of the REG to discuss your background, interests, and goals. They can provide valuable guidance and help you get settled into your new role.\n",
      "3. Review the institute's policies and procedures to ensure you understand the expectations and requirements of your position.\n",
      "4. Attend orientation sessions and training programs offered by the institute to learn more about the Turing Institute's culture, resources, and research practices.\n",
      "5. Start exploring the tools and technologies used in the REG, such as query_engine_tool, to get a sense of the technical capabilities available to you.\n",
      "6. Begin identifying potential research projects that align with your interests and skills, and reach out to relevant researchers to discuss possibilities.\n",
      "7. Consider joining relevant working groups or workshops to connect with other researchers and stay up-to-date on the latest developments in the field.\n",
      "8. Start building your network within the institute by attending social events, seminars, and other activities that promote collaboration and knowledge sharing.\n",
      "9. Familiarize yourself with the Turing Institute's publication and dissemination processes to understand how research is shared and recognized within the institution.\n",
      "10. Stay open-minded, curious, and willing to learn, as the Turing Institute is a dynamic and interdisciplinary environment that values collaboration and innovation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =   266.23 ms /   380 runs   (    0.70 ms per token,  1427.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =  2152.57 ms /   121 tokens (   17.79 ms per token,    56.21 tokens per second)\n",
      "llama_print_timings:        eval time = 24885.95 ms /   379 runs   (   65.66 ms per token,    15.23 tokens per second)\n",
      "llama_print_timings:       total time = 27796.96 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"what should a new starter in the research engineering group (REG) at the Alan Turing Institute do?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d028880",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mResponse:   No, I have not used the query engine yet. As a new starter in the Research Engineering Group at the Alan Turing Institute, I am still in the process of familiarizing myself with the tools and technologies available to me. However, I am eager to learn more about the query engine and how it can be used to support my research activities. Can you tell me more about the query engine and its capabilities?\n",
      "\u001b[0m  No, I have not used the query engine yet. As a new starter in the Research Engineering Group at the Alan Turing Institute, I am still in the process of familiarizing myself with the tools and technologies available to me. However, I am eager to learn more about the query engine and how it can be used to support my research activities. Can you tell me more about the query engine and its capabilities?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =    61.06 ms /    87 runs   (    0.70 ms per token,  1424.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   570.47 ms /    20 tokens (   28.52 ms per token,    35.06 tokens per second)\n",
      "llama_print_timings:        eval time =  5989.95 ms /    86 runs   (   69.65 ms per token,    14.36 tokens per second)\n",
      "llama_print_timings:       total time =  6726.84 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Have you used the query engine yet?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a01fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;200m\u001b[1;3mResponse:   You have asked me the following questions so far:\n",
      "\n",
      "1. What should a new starter in the Research Engineering Group (REG) at the Alan Turing Institute do?\n",
      "2. Have you used the query engine yet?\n",
      "\u001b[0m  You have asked me the following questions so far:\n",
      "\n",
      "1. What should a new starter in the Research Engineering Group (REG) at the Alan Turing Institute do?\n",
      "2. Have you used the query engine yet?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7509.25 ms\n",
      "llama_print_timings:      sample time =    34.37 ms /    49 runs   (    0.70 ms per token,  1425.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   600.34 ms /    20 tokens (   30.02 ms per token,    33.31 tokens per second)\n",
      "llama_print_timings:        eval time =  3407.13 ms /    48 runs   (   70.98 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =  4101.47 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"What have I asked you so far?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c86b3d",
   "metadata": {},
   "source": [
    "Reset chat engine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "753b4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7ca01f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** prompt: **********\n",
      " [INST] What did I ask you before? [/INST]\n",
      "********** completion_response: **********\n",
      "   I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\n",
      "********** chat: chat_response: **********\n",
      " assistant:   I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\n",
      "\u001b[38;5;200m\u001b[1;3mResponse:   I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\n",
      "\u001b[0m********** _process_actions: current_reasoning: **********\n",
      " [ResponseReasoningStep(thought='I can answer without any tools.', response=\"  I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\")]\n",
      "********** _process_actions: is_done: **********\n",
      " True\n",
      "********** chat: reasoning_steps: **********\n",
      " [ResponseReasoningStep(thought='I can answer without any tools.', response=\"  I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\")]\n",
      "********** chat: is_done: **********\n",
      " True\n",
      "  I apologize, but I don't have the ability to remember previous conversations or keep track of what was asked. Each time you interact with me, it is a new and separate conversation. If you would like to ask something again, I will do my best to assist you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   931.62 ms\n",
      "llama_print_timings:      sample time =    42.12 ms /    60 runs   (    0.70 ms per token,  1424.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   288.19 ms /    14 tokens (   20.59 ms per token,    48.58 tokens per second)\n",
      "llama_print_timings:        eval time =  3228.61 ms /    59 runs   (   54.72 ms per token,    18.27 tokens per second)\n",
      "llama_print_timings:       total time =  3630.86 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"What did I ask you before?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3941e6f",
   "metadata": {},
   "source": [
    "## React engine and asking it to use query\n",
    "\n",
    "We saw that it didn't use the query engine in the above, but maybe we could force it to use it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5629ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"react\",\n",
    "                                   response_mode=response_mode,\n",
    "                                   verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ded38211",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** prompt: **********\n",
      " [INST] Please use the query engine. What should a new starter in the research engineering group do? [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** completion_response: **********\n",
      "   As a new starter in the research engineering group, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the research environment:\n",
      "\t* Learn about the research projects that the group is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the organization.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the organization's policies and procedures:\n",
      "\t* Review the organization's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the organization to help you get started with your role.\n",
      "6. Start contributing to research projects:\n",
      "\t* Begin contributing to ongoing research projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and colleagues:\n",
      "\t* Regularly communicate with your supervisor and colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the organization and begin contributing to the success of the research projects.\n",
      "********** chat_response: **********\n",
      " assistant:   As a new starter in the research engineering group, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the research environment:\n",
      "\t* Learn about the research projects that the group is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the organization.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the organization's policies and procedures:\n",
      "\t* Review the organization's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the organization to help you get started with your role.\n",
      "6. Start contributing to research projects:\n",
      "\t* Begin contributing to ongoing research projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and colleagues:\n",
      "\t* Regularly communicate with your supervisor and colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the organization and begin contributing to the success of the research projects.\n",
      "\u001b[38;5;200m\u001b[1;3mResponse:   As a new starter in the research engineering group, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the research environment:\n",
      "\t* Learn about the research projects that the group is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the organization.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the organization's policies and procedures:\n",
      "\t* Review the organization's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the organization to help you get started with your role.\n",
      "6. Start contributing to research projects:\n",
      "\t* Begin contributing to ongoing research projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and colleagues:\n",
      "\t* Regularly communicate with your supervisor and colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the organization and begin contributing to the success of the research projects.\n",
      "\u001b[0m  As a new starter in the research engineering group, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the research environment:\n",
      "\t* Learn about the research projects that the group is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the organization.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the organization's policies and procedures:\n",
      "\t* Review the organization's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the organization to help you get started with your role.\n",
      "6. Start contributing to research projects:\n",
      "\t* Begin contributing to ongoing research projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and colleagues:\n",
      "\t* Regularly communicate with your supervisor and colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the organization and begin contributing to the success of the research projects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   981.41 ms\n",
      "llama_print_timings:      sample time =   377.91 ms /   539 runs   (    0.70 ms per token,  1426.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   307.84 ms /    23 tokens (   13.38 ms per token,    74.72 tokens per second)\n",
      "llama_print_timings:        eval time = 31503.15 ms /   538 runs   (   58.56 ms per token,    17.08 tokens per second)\n",
      "llama_print_timings:       total time = 32916.09 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Please use the query engine. What should a new starter in the research engineering group do?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "098a68a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** prompt: **********\n",
      " <s>[INST] <<SYS>>\n",
      "\n",
      "You are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "You have access to a wide variety of tools. You are responsible for using\n",
      "the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools\n",
      "to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "> Tool Name: query_engine_tool\n",
      "Tool Description: Useful for running a natural language query\n",
      "against a knowledge base and get back a natural language response.\n",
      "\n",
      "Tool Args: {'title': 'DefaultToolFnSchema', 'description': 'Default tool function Schema.', 'type': 'object', 'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input']}\n",
      "\n",
      "\n",
      "## Output Format\n",
      "To answer the question, please use the following format.\n",
      "\n",
      "```\n",
      "Thought: I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of query_engine_tool)\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"text\": \"hello world\", \"num_beams\": 5})\n",
      "```\n",
      "Please use a valid JSON format for the action input. Do NOT do this {'text': 'hello world', 'num_beams': 5}.\n",
      "\n",
      "If this format is used, the user will respond in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format until you have enough information\n",
      "to answer the question without using any more tools. At that point, you MUST respond\n",
      "in the following format:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools.\n",
      "Answer: [your answer here]\n",
      "```\n",
      "\n",
      "## Current Conversation\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "Please use the query engine. What should a new starter in the research engineering group do? [/INST] assistant:   As a new starter in the research engineering group, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the research environment:\n",
      "\t* Learn about the research projects that the group is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the organization.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the organization's policies and procedures:\n",
      "\t* Review the organization's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the organization to help you get started with your role.\n",
      "6. Start contributing to research projects:\n",
      "\t* Begin contributing to ongoing research projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and colleagues:\n",
      "\t* Regularly communicate with your supervisor and colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the organization and begin contributing to the success of the research projects. [INST] I want to specifically know about a new starter in the REG team at the Turing institute [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** completion_response: **********\n",
      "   As a new starter in the Research Engineering Group (REG) at the Turing Institute, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the Turing Institute's research environment:\n",
      "\t* Learn about the research projects that the REG is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the Turing Institute.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and REG colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your REG colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the Turing Institute's policies and procedures:\n",
      "\t* Review the Turing Institute's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the Turing Institute to help you get started with your role.\n",
      "6. Start contributing to REG projects:\n",
      "\t* Begin contributing to ongoing REG projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and REG colleagues:\n",
      "\t* Regularly communicate with your supervisor and REG colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the REG at the Turing Institute and begin contributing to the success of the research projects.\n",
      "********** chat_response: **********\n",
      " assistant:   As a new starter in the Research Engineering Group (REG) at the Turing Institute, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the Turing Institute's research environment:\n",
      "\t* Learn about the research projects that the REG is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the Turing Institute.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and REG colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your REG colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the Turing Institute's policies and procedures:\n",
      "\t* Review the Turing Institute's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the Turing Institute to help you get started with your role.\n",
      "6. Start contributing to REG projects:\n",
      "\t* Begin contributing to ongoing REG projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and REG colleagues:\n",
      "\t* Regularly communicate with your supervisor and REG colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the REG at the Turing Institute and begin contributing to the success of the research projects.\n",
      "\u001b[38;5;200m\u001b[1;3mResponse:   As a new starter in the Research Engineering Group (REG) at the Turing Institute, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the Turing Institute's research environment:\n",
      "\t* Learn about the research projects that the REG is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the Turing Institute.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and REG colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your REG colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the Turing Institute's policies and procedures:\n",
      "\t* Review the Turing Institute's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the Turing Institute to help you get started with your role.\n",
      "6. Start contributing to REG projects:\n",
      "\t* Begin contributing to ongoing REG projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and REG colleagues:\n",
      "\t* Regularly communicate with your supervisor and REG colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the REG at the Turing Institute and begin contributing to the success of the research projects.\n",
      "\u001b[0m  As a new starter in the Research Engineering Group (REG) at the Turing Institute, there are several tasks and activities that you can focus on to get started with your role:\n",
      "\n",
      "1. Familiarize yourself with the Turing Institute's research environment:\n",
      "\t* Learn about the research projects that the REG is currently working on, and the technologies and tools being used.\n",
      "\t* Understand the goals and objectives of the research projects, and how they align with the overall research strategy of the Turing Institute.\n",
      "2. Set up your workspace:\n",
      "\t* Install any necessary software or tools required for your work.\n",
      "\t* Set up your computer and other equipment to ensure that you have a comfortable and productive work environment.\n",
      "3. Meet with your supervisor and REG colleagues:\n",
      "\t* Schedule a meeting with your supervisor to discuss your role, responsibilities, and expectations.\n",
      "\t* Introduce yourself to your REG colleagues and learn about their roles and areas of expertise.\n",
      "4. Familiarize yourself with the Turing Institute's policies and procedures:\n",
      "\t* Review the Turing Institute's policies and procedures related to research, engineering, and technology transfer.\n",
      "\t* Understand the intellectual property policies and how they apply to your work.\n",
      "5. Attend training and orientation sessions:\n",
      "\t* Attend any training or orientation sessions that are provided by the Turing Institute to help you get started with your role.\n",
      "6. Start contributing to REG projects:\n",
      "\t* Begin contributing to ongoing REG projects, and start learning about the research process and how to work with the team.\n",
      "7. Develop a plan for your research project:\n",
      "\t* Work with your supervisor to develop a plan for your research project, including specific objectives, methodology, and timelines.\n",
      "8. Start building your network:\n",
      "\t* Attend relevant conferences, seminars, and workshops to learn about the latest developments in your field and to build relationships with other researchers and industry experts.\n",
      "9. Keep up-to-date with relevant literature and trends:\n",
      "\t* Regularly review scientific articles, conference proceedings, and other relevant publications to stay current with the latest developments in your field.\n",
      "10. Communicate regularly with your supervisor and REG colleagues:\n",
      "\t* Regularly communicate with your supervisor and REG colleagues to provide updates on your progress, ask questions, and seek feedback.\n",
      "\n",
      "By following these steps, you can get started with your role as a research engineer in the REG at the Turing Institute and begin contributing to the success of the research projects.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   981.41 ms\n",
      "llama_print_timings:      sample time =   403.86 ms /   577 runs   (    0.70 ms per token,  1428.72 tokens per second)\n",
      "llama_print_timings: prompt eval time = 21240.71 ms /  1045 tokens (   20.33 ms per token,    49.20 tokens per second)\n",
      "llama_print_timings:        eval time = 43054.91 ms /   576 runs   (   74.75 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time = 65498.11 ms\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"I want to specifically know about a new starter in the REG team at the Turing institute\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d96826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8643927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reginald",
   "language": "python",
   "name": "reginald"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
