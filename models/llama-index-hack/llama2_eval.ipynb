{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index import Document\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import LLMPredictor, PromptHelper, ServiceContext\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from ../../../llama/llama-2-7b-chat/gguf-model-q4_0.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                        general.description str     \n",
      "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   5:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 2048\n",
      "llm_load_print_meta: n_ctx          = 3900\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 32\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 5.0e-06\n",
      "llm_load_print_meta: n_ff           = 11008\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_0 (guessed)\n",
      "llm_load_print_meta: model size     = 6.74 B\n",
      "llm_load_print_meta: general.name   = ggml-model-q4_0.bin\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 3647.96 MB (+ 1950.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 1950.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  269.22 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=\"../../../llama/llama-2-7b-chat/gguf-model-q4_0.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    wiki_scraped=pd.read_csv(\"../../data/turing_internal/wiki-scraped.csv\")\n",
    "    wiki_scraped.dropna(subset=\"body\", inplace=True)\n",
    "    wiki_scraped_text=[str(i) for i in wiki_scraped[\"body\"].values]\n",
    "\n",
    "    handbook_scraped=pd.read_csv(\"../../data/public/handbook-scraped.csv\")\n",
    "    handbook_scraped.dropna(subset=\"body\", inplace=True)\n",
    "    handbook_scraped_text=[str(i) for i in handbook_scraped[\"body\"].values]\n",
    "\n",
    "    turingacuk=pd.read_csv(\"../../data/public/turingacuk-no-boilerplate.csv\")\n",
    "    turingacuk.dropna(subset=\"body\", inplace=True)\n",
    "    turingacuk_text=[str(i) for i in turingacuk[\"body\"].values]\n",
    "\n",
    "    documents = [Document(text=i) for i in wiki_scraped_text]\n",
    "    documents.extend([Document(text=i) for i in handbook_scraped_text])\n",
    "    documents.extend([Document(text=i) for i in turingacuk_text])\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a1fefa81-3605-41ff-a673-ae1831218fbe', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='041555380e6b94a954635f369e0b94eebbb878244b785a054cad6a3090ebbb08', text=\"Our Approach to Timesheets\\n\\nTimesheets should match Forecast.\\n\\nThat is, if you are “100%” on projects, and have no service role activity, then your timesheet records 100% time on projects. For example, Town Hall meetings (which by the way is standing role no. 617, Corporate Duties: Professional Activities not elsewhere specified ) would in general not be recorded on the timesheet.\\n\\n\\nHowever, if your other duties are taking up an inordinate amount of your time then you are very much at liberty to so record them in order to enable any conversations you would like to have about the issue. That is, the timesheet is also to help you push back if you would like.\\n\\n\\nHowever, in addition, we should (again, if you like!) record any activity in support of a Programme Support Role, Turing Service Area, or REG Service Area (like training) even if you are not explicitly assigned to that role so that we can start to figure out how much time these things really take up.\\n\\n\\nRemember again that Timesheets are to help us, they are not the source of truth for charging projects. We'd prefer that they reflect reality, rather than anything we have promised. But we don’t want them to be so fine-grained that they take up all our time to fill out. So, in particular: time allocated to projects includes a reasonable amount for corporate duties. Thus we don’t bother explicitly recording the corporate duties unless they become unreasonable.\\nOur actual process is: we charge projects based on Forecast, having made sure that there aren’t any glaring discrepancies with Harvest. (eg, a project starting a month late.)\\nWhich Project should I use for which activity?\\nThis page describes where to find your project on Harvest. Every project on Harvest is labelled with a GitHub issue number, in the form hut23-999 , so you want to find the project with the correct issue number.\\nMy project is a project\\nYou should able find the project under the appropriate Client.\\nThe issue number corresponds to the project as listed in the project tracker or possibly in the 22 days project list . (There may be other places but all projects have an issue number.)\\nMy project is Programme Support, Service Area, or other activity\\nAll Programme Support roles and Service Area roles are listed on the Standing Roles kanban board . The columns in that board each have their own Client on Harvest, where you should look for the particular role. Again, all the Harvest projects should be labelled with the appropriate issue number.\\nThe clients are: - Turing Programme Support - Turing Service Areas - REG Service Areas - Corporate Duties - Personal Time \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='56b88ed4-09a7-44a2-a451-d30f69fd25ed', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e76023db7885ca57cbfdae85a7bbfdb1eb248ee38bae844dc43eb7616a0d8086', text='The buddy system page has moved to the REG handbook: https://alan-turing-institute.github.io/REG-handbook/docs/onboarding/buddy_system/\\nWherever you found the link to this page, please consider fixing that link to point to the handbook page above. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1add2f1-076e-4d39-87c1-ef3471e9b11b', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='719c5817b865923b5da2817540676af30deddc4d8d25ce0e2a9fa6e1a60c7c21', text='See this TopDesk page for info on printing at the institute.\\nTips\\nAdvice on printing via CUPS (the Common Unix Printing System, used by MacOS)\\nlpr -T \"Gargleblaster\" -o InputSlot=Tray3 -o XRAFinisherStapleOption=True -o XRFinishing=Staple -o XRStapleOpti on=1Staple -o XRStapleLocation=TopLeft -o XRAnnotationOption=Standard temp/*.pdf\\nPrint all pdfs in temp , stapling the lot, and add a small annotation saying \"Gargleblaster\" in the top left.\\n(NB: The default input tray is Tray1 but this is A3 paper on our printers.)\\nPrinting A3, double sided etc.\\nTo get additional print settings like these have a look at the instructions here: https://github.com/alan-turing-institute/knowledgebase/wiki/How-to-Print-in-A3-on-Mac\\nThe office also has a stationary cupboard with the usual office supplies. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce5bf895-9b12-4a4c-b921-bb29c82cd26c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='efb7dcdfc435ef699a21b48f03cf165a2833d8ca2c86236b7f49d38f6d42ed87', text='This page documents the first run of the \"new\" process whereby REG\\'s allocations to projects eventually end up in the Finance system. There are three steps (and several sub-steps that are not shown):\\n\\n\\nAfter month end, REG produce a \"recharge pro forma \" for the month\\n\\n\\nPMU sign off this pro forma.\\n\\n\\nFinance upload the allocations.\\n\\n\\n1. Recharge pro forma\\n\\n\\nOliver created regolith , a Racket script for producing a monthly summary from Forecast.\\n\\n\\nJames G used regolith to manually produce a pro forma for April. That involved:\\n\\ndeleting all months other than April\\ndeleting rows with no allocation\\nadding columns for PMU sign-off, proposed edits, and REG sign-off for the edits\\n\\nadding an instructions sheet\\n\\n\\nJames G asked Catherine L who from PMU should be REG\\'s working contact.\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='421f41dc-9c32-407e-b1fc-1eb465e44456', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3c46ff0feebd7a9fda353f7f6089a8a14f9f8bb4ddca72708743785697f1936e', text='2023-05-16: Wiki page first created for the 2022-23 process (we\\'ve previously used dedicated issues and discussion topics for each year). Only small changes from the 2021-22 process . Only substantive changes are the change from a 1-5 scale to a 1-3 scale for scoring objectives and qualities in Cezanne and updates to the timeline dates to reflect this years deadlines.\\nContext\\nREG is a large team, with a relatively flat line management structure. Therefore we run a team-wide standardisation process where appraisals and progression award recommendations are reviewed to ensure consistency and fairness across the team.\\nOur goal is to work within the wider HR policy and guidance (see 2022-23 HR performance review guide ) to prioritise what we think is most important for our team (fairness, transparency and a clear progression path) and provide clear guidance to achieve that.\\nPrinciples\\n\\nREG folk who are operating at an equivalent level should receive equivalent pay.\\nWe are a learning team and our default assumption is that people will generally be growing and developing in their role as they gain more experience.\\nEveryone who is performing and developing as expected for their role and their experience level within it should expect to progress through their pay band each year on top of the cost of living adjustment. This is very much in line with the guidance on salary increases from HR (see below).\\n\\n\\nHR guidance: A salary increase should be recommended to acknowledge an improvement in the skills and experience that the individual has gained over the last performance year i.e. they are performing better in the role than they were this time last year.\\n\\nMechanism\\n\\nAll team members who have been performing and developing as expected for their experience level will receive the same \"small\" progression increase in pay each year above the cost of living increase. The actual percentage increase this corresponds to is set by HR each year but the expectation is that, if performing and developing as expected each year, you would progress from the bottom to the top of your band in around 6-8 years. This is in line with the expected progression rate in the university spine point system.\\nExceptionally, where someone has progressed significantly more than what would reasonably be expected, a further adjustment may be made to ensure that Principle 1 still holds.\\nWhere someone has been promoted within the year, they will already have received their progression pay increase for the year at promotion and no further progression increase will be awarded at appraisal time.\\nIn the rare cases someone in the team is not performing or developing as expected, no progression increase will be awarded. This situation should be managed via ongoing support throughout the year. In general, this should not be a surprise at appraisal time.\\nTeam members who are at the top of their pay band are not eligible for a progression pay rise, but they are eligible for a bonus if consistently performing at a high level.\\n\\n\\nHR guidance: the employee is at the top of the banding for the role but consistently performs at a high level (one of the example reasons for recommending a recognition award)\\n\\n\\n\\n\\n\\nGuidance notes\\n\\nWe use our objectives and the discussions about progress against them in 1-2-1s and at year end appraisal time to:\\nHelp identify the important things to focus on and prioritise\\nRecognise where we are spending our effort and reflect on how we are doing in this work\\nHelp evaluate how we are performing and developing in our roles\\n\\n\\nThe last of these helps inform what a fair progression award will be for each person, helping us identify whether each individual is in the default \"progressing as expected\" group or in one of the exception groups that are significantly over- or under-performing against expectations.\\nThe appraisal process at the Turing involves each employee and their line manager assessing them against objectives set at the start of the year and personal qualities important to the institute. These are assessed on the following scale.\\n1: Fails to meet expected level\\n2: Meets expected level\\n3: Exceeds expected level\\n\\n\\nThe most important purpose of the appraisal discussion and scoring is as a tool to look back on the year and reflect on what went well, what went not so well and what we would like to change or focus on in the coming year. However, there is often some tension when scoring objectives and qualities due to concerns about the impact of the overall score on the recommendation for a progression pay rise or bonus.\\nWe will be asking REG line managers to make an overall judgement on whether each of their reports is performing and developing broadly as expected given their experience level, considering the \"year in review\" appraisal discussion they have with each report and, where useful, discussions with other team members who have insight into their work over the year.\\nFor most people, the answer will be that they have been progressing broadly as expected for their experience level and their line manager will recommend a \"small\" progression pay rise.\\nDuring last year\\'s appraisal process everyone\\'s pay was reviewed to ensure that that Principle 1 held after applying the progression pay rises. Therefore, we are as confident as we can be that everyone started the appraisal year performing and developing as expected for their position in their pay band.\\nIf, during the course of the year, there has been a substantive positive change in the person\\'s ability to perform the role which means that they are no longer aligned with their peers at an equivalent level, then a \"medium\" (or \"large\") pay rise might be warranted.\\nIf a person\\'s performance and development does not reflect a substantive and sustained increase that justifies an additional pay rise to adjust their alignment with their peers, but there is a specific one-off contribution that significantly exceeds the expectations for their experience level, a one-off bonus might be warranted.\\n\\n\\nHR guidance: The purpose of recognition awards is to recognise a piece of work or specific contribution from the past performance year which has added value to the team and wider organisation.\\n\\n\\n\\n\\nIf, during the course of the year, a person has not being performing or developing as expected, no progression pay rise will be awarded.\\nTeam members who are at the top of their pay band are not eligible for a progression pay rise, but they are eligible for a bonus if consistently performing at a high level.\\n\\n\\nHR guidance: the employee is at the top of the banding for the role but consistently performs at a high level (one of the example reasons for recommending a recognition award)\\n\\n\\n\\n\\nTeam members who have been promoted within the past 6 months will not receive a progression pay rise at appraisal time as they will have already received a progression pay rise this year as part of their promotion. They will however be eligible for a bonus if there is a particular thing they did that year that significantly exceeded the expectations for their role.\\n\\n\\nHR guidance: Staff who have been promoted within 6 months of a performance review cycle will not be eligible for further salary increases but are eligible for a recognition award.\\n\\n\\n\\n\\nTeam members who have not completed probation will not be eligible for a progression pay rise or bonus, but their pay will be reviewed at the end of probation to ensure that Principle 1 continues to hold.\\nTeam members who completed probation after 31 December will usually not undergo the appraisal process in Cezanne as they will only recently have completed their end of probation review (which for REG also includes a salary review (see the [[Probation Review]] page for details). However, they will have a similar \"year in review\" discussion with their line manager, reflecting and evaluating on their \"rest of year\" objectives set at the end of their appraisal.\\n\\nThe process\\nSee Cezanne appraisal workflow for details of the system workflow covering steps 1-2. Please print a PDF of all Cezanne forms prior to submission as it has proved tricky to get these afterwards\\n\\nTeam members complete self-assessment appraisal and share with their line manager. We would suggest team members have a discussion with their line managers about their self-assessment before submitting it on Cezanne, but the self-assessment should reflect the team member\\'s view.\\nFollowing a discussion with their reports about their self-assessment, line managers complete their assessment, submitting the assessment form and associated progression award recommendation in Cezanne by the Wednesday 31st May 2023 deadline for those who are doing the appraisal process on Cezanne.\\nLine managers send REG director PDFs of their appraisal forms and progression award recommendations by Thursday 1st June 2023 .\\nREG director and principals meet on Wednesday 7th June 2023 to review progression pay rise and bonus recommendations for consistency, making adjustments to come up with an overall set of standardised recommendations for the team that ensure that performance and development are recognised and that Principal 1 continues to hold.\\nREG director will share any adjustments to line manager\\'s recommendations arising from the REG standardisation process with line managers, who will have an opportunity to discuss these.\\nREG director will share the standardised recommendations with the salary review panel (COO, Director of People)\\nREG director will attend the Turing-wide Salary Review Panel on [2023 date TBC] to answer any questions the panel have. The panel will have both the pre-standardisation line manager recommendations and the post-standardisation REG panel recommendations, so any differences will be discussed and justified to the panel.\\nThe recommendations from the Salary Review Panel will then go the the Remuneration Committee (REPCo - a sub-committee of the Board of Trustees) for final approval. REPCo meet on [2023 date TBC] .\\nAny pay rises will be  effective from 1st July and first applied in the July payroll. One-off recognition awards will be also paid in the July payroll run.\\n\\nThe goal is that, between the principles and process outlined above, the REG standardisation panel and the Turing salary review panel, we will stand the best chance possible of ensuring that people progress in their role in a fair and transparent manner while ensuring that REG folk who are operating at an equivalent level continue to receive equivalent pay (Principle 1). ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6e34357-58b6-4ded-b934-e9ab9783cdf4', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8f9513439b3d1cbf71bf2db9d8b3aac56a14abdb11320dfa9986f7b3463efcd1', text='aligned with Turing 2.0 and the Turing Charter\\n1. Impact\\nMake a positive change in the world\\nThe Institute works with an impact-first philosophy, focusing on research that is likely to make a positive change in the world. Impact in this context is understood broadly, from increased knowledge and understanding of the way we see and operate in the world, to increased efficiency resulting from the uptake of new tools and technologies. Impact also drives the way our project delivery is managed. In assessing project proposals requiring data scientist or research engineer effort, we will prioritise approaches and methodologies we are familiar with and confident that they can produce impactful results. Each project will be assigned a score based on the impact opportunities it presents and the likelihood of realising them.\\n2. Diversity\\nReflect and celebrate the diverse nature of the world\\nIn order to be able to change the world for the better, our research must be conducted by researchers from diverse academic, social, ethnical, racial backgrounds, and using a variety of theoretical frameworks, methodologies and categories of data. Diversity breeds creativity and enables discovery and innovation to flourish. This approach is matched at project delivery level by prioritizing projects building cross-cultural and cross-disciplinary collaborations or bringing partner institutions together. Project proposals involving purely theoretical research with academics, or consultancy-type work with commercial partners, or not welcoming external contributions from outside the REG, are unlikely to score high in the diversity assessment. Conversely, projects encouraging collaboration with culturally diverse academic and non-academic teams from beyond the Turing are likely to get higher marks.\\n3. Pioneering\\nExplore new ways of tackling research, talent and approaches\\nTuring research is solidly rooted in academic excellence, scientific evidence and trusted best practice. At the same time the Institute works creatively, encouraging new ideas and unprecedented approaches to pursue scientific excellence in addressing critical global challenges. Cross-pollination of ideas and practices leading to innovative results is encouraged in collaborative projects with academic, commercial and third sector delivery partners. Project briefs will need to specify to what extent the proposed approaches or methodologies are innovative in the context of similar research being undertaken elsewhere, and a pioneering score will be assigned to each of them.\\n4. Openness\\nShare outputs and methodologies as openly as possible\\nTo ensure that our collaborative research reaches an audience as wide and diverse as possible, we believe both knowledge and the path to reaching it should be shared. Openness by default facilitates both research and software development collaboration. This extends the impact of our projects beyond the Institute, allowing outputs to be built upon by others and repurposed across domains. Whenever possible, we expect the projects we prioritise to undertake reproducible research, and the software jointly developed with us to be built and released as open source. Project proposals with realistic plans on meeting this priority will get high marks in the openness score.\\n5. Alignment with Turing 2.0\\nPrioritise activities reflecting Turing 2.0 guiding principles\\nTuring 2.0 represents the next phase of the Institute – delivering on an ambition to be a truly national institute for data science and AI. Achieving this will mean operating differently to how we do now and having a clearer focus to guide the work we undertake, across science and innovation, skills and engagement. In consultation with the Board of Trustees and Institute’s Research Leadership and Turing Management team, a set of initial guiding principles that will shape Turing 2.0 have been formulated. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88f1b575-cb03-47c0-812c-e96933ebfb76', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0260daa2062d2b3678d29426adbf41b1ac6d0024e7649252f16e85f3860e380f', text=\"Current\\n2023-24 Institute-wide HERA Bands\\nEffective: 01 April 2023 - 31 March 2024; Cost of living award from previous year: 5%\\n\\n\\n\\nRole\\nBand\\nRole Salary Min\\nRole Salary Max\\nBottom third baseline\\nMiddle third baseline\\nTop third baseline\\n\\n\\n\\n\\nPrincipal\\n6\\n£73,526\\n£84,488\\n£73,526\\n£77,180\\n£80,834\\n\\n\\nLead\\n5\\n£62,666\\n£73,297\\n£62,666\\n£66,210\\n£69,754\\n\\n\\nSenior\\n4\\n£51,476\\n£62,108\\n£51,476\\n£55,020\\n£58,564\\n\\n\\nStandard\\n3b*\\n£42,000\\n£50,916\\n£42,000\\n£44,972\\n£47,944\\n\\n\\nJunior\\n3a*\\n£38,048\\n£39,900\\n£38,048\\n£38,665\\n£39,283\\n\\n\\n\\n*Note: The Institute wide salary bands don't distinguish between 3a and 3b. This is purely a REG distinction.\\nNew starter offers: When making offers to prospective new team members, we make fixed, non-negotiable offers at one of the three fixed baseline points within the overall salary band for the role (i.e. the 0/3, 1/3, 2/3 points). When determining these offers, we consider candidates’ previous experience and consider who their closest future peers are within the existing team. Deciding which of these points represents a fair starting salary for a future colleague based on two hours of interviews can sometimes be challenging but we work very hard to be as confident as we can be that we are not being unfair to either current or future team members and, when the decision is not straightforward, we consult widely with those who interviewed the candidate to ensure we have the best possible input to make the best decision we can.\\nPromotion: On promotion, people will be appointed at the bottom of the salary range for the role.\\nHistoric\\n2022-23 Institute-wide HERA Bands\\nEffective: 01 April 2022 - 31 March 2023; Cost of living award from previous year: 5%\\n\\n\\n\\nRole\\nBand\\nRole Salary Min\\nRole Salary Max\\nBottom third baseline\\nMiddle third baseline\\nTop third baseline\\n\\n\\n\\n\\nPrincipal\\n6\\n£70,025\\n£80,465\\n£70,025\\n£73,505\\n£76,985\\n\\n\\nLead\\n5\\n£59,682\\n£69,807\\n£59,682\\n£63,057\\n£66,432\\n\\n\\nSenior\\n4\\n£49,025\\n£59,150\\n£49,025\\n£52,400\\n£55,775\\n\\n\\nStandard\\n3b*\\n£40,000\\n£48,491\\n£40,000\\n£42,830\\n£45,661\\n\\n\\nJunior\\n3a*\\n£36,236\\n£38,000\\n£36,236\\n£36,824\\n£37,412\\n\\n\\n\\n*Note: The Institute wide salary bands don't distinguish between 3a and 3b. This is purely a REG distinction.\\n2021-2022 Institute-wide HERA Bands\\nEffective: 01 April 2021 - 31 March 2022; Cost of living award from previous year: 1.5%\\nNote: In 2021-22 REG salaries were only partially aligned with the HERA bands below. Junior and Standard were fully aligned but we did not yet have the Band 5 Lead role defined and Principals had not yet been formally mapped into Band 6. Senior salaries spanned all of Band 4 and some of Band 5 due to the effect of cost of living increases from previous years on salaries within the previous REG-specific Senior salary band of £45,0000 - £60,000 (which had not had its minimum and maximum salaries adjusted in line with the cost of living increases applied to individual salaries).\\n\\n\\n\\nRole\\nBand\\nRole Salary Min\\nRole Salary Max\\nBottom third baseline\\nMiddle third baseline\\nTop third baseline\\n\\n\\n\\n\\nPrincipal\\n6\\n£66,000\\n£75,500\\n£66,000\\n£69,167\\n£72,333\\n\\n\\nLead\\n5\\n£56,000\\n£65,500\\n£56,000\\n£59,167\\n£62,333\\n\\n\\nSenior\\n4\\n£46,000\\n£55,500\\n£46,000\\n£49,167\\n£52,333\\n\\n\\nStandard\\n3b*\\n£37,000\\n£45,500\\n£37,000\\n£39,833\\n£42,667\\n\\n\\nJunior\\n3a*\\n£34,000\\n£36,000\\n£34,000\\n£34,667\\n£35,333\\n\\n\\n\\n*Note: The Institute wide salary bands don't distinguish between 3a and 3b. This is purely a REG distinction. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5288eb24-642e-4728-8c00-260d292c8500', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b567644e0138cd2d966d9bd5713419517766402148ac4bfac4e3d847604909d2', text=\"Line management in REG is a little different than in other parts of the Turing as you often will not be working directly with the people you are managing. This guide aims to explain how this relationship might work.\\nHow line management duties are assigned\\nAt (current) senior level, people are expected to line manage 1-2 standards. This will probably change with the new lead role (see the REG role matrix ). Currently, if seniors are willing to line manage more than 2 people, they should inform the recruitment lead (currently Pam).\\nAt standard level, opportunities for line management and project mentoring of junior members and interns are offered during the year. To assign such duties, we follow whenever possible time spent at standard level in REG.\\nAt both levels, we try to avoid overlaps between projects and line management, at least during probation.\\nFirst day\\nAs a line manager you will have a 1-to-1 meeting with the person you will be managing on their first day. By this point they will have had various other inductions (eg. with HR, IT, their REG buddies). You might want to focus on the following points\\n\\nintroduce what line management is at REG\\npart mentor, part escalation point, part pastoral support, part admin, can take different shapes depending on need\\n\\n\\n... and what it is isn’t\\ntask-setting, boss vibes, oversight\\n\\n\\npoint out that line management, like everything else in REG, is a collaboration\\nhave a chat about what management styles have worked well for them.\\n\\n\\nfind out their background, why they decided to join the group\\ndiscuss what their day-to-day might look like\\n80% projects (either one or two projects)\\n10% REG responsibilities (eg. service areas)\\n10% personal development\\n\\n\\nbriefly talk about how people are allocated to projects\\ncheck that they've taken a look at the new starters checklist\\nleave time for questions\\n\\nFirst week\\nLater in the first week you might want to go through some of the more technical points:\\n\\nTeam reporting structure\\nAdding yourself to the Turing GitHub organisation\\nEmoji reacting to projects\\nProjects looking for people\\nCurrently active projects\\n\\n\\nProject allocations on Forecast\\nPersonal time tracking on Harvest (noting that this is not compulsory)\\nProbation formalities\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d22e8c9d-4b36-4972-b312-18c4047184c9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='028414143d2866022c8d9800d1351cf8cd1c6e504c9d6684e78067eb2ee33ddf', text='What to do when you leave\\nThis page is for information on what you need to do if you leave the Turing.\\nLaptop Return\\nBadges and ID\\nWFH Equipment ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='767ef139-0978-4425-befb-c2f95eed2d94', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8ebbd107574f591b43b4edd52fb56b5de50ed3764f3b8c61e69a9c7f2a9364f6', text='This guide was put together after organising several workshop/teaching events. Generally the Events team and others in the Business team will be able to help with managing events, but the notes below may be useful if you ever end up self-organising an event. If you have a different experience, please edit!\\nBooking Enigma\\nEnigma fills up early, especially if you need more than one day for your event. As a general guide, look at least two months ahead if you want a reasonable amount of choice.\\n\\nLook at the Turing Calendar* and find a good time. Those marked \"Events hold\" may be free, but you\\'ll need to check with Events.\\nYou can book Enigma either via Reception or Events. For smaller meeting rooms, book through a calendar invitation or Reception.\\nReception: Fine for small events, but remember that you won\\'t get as much support with the organisation.\\nEvents team: Booking via the Events team is a longer process - you will need to fill in a five-page Event Application Form and send it to the Events team. They will get in touch within a few days to discuss dates and other arrangements. You\\'ll also get support for registration, website etc. if needed.\\nIf you are arranging a large event, you might need to book another room (usually Lovelace) for catering.\\n\\n*In Outlook, go to Organize , then click Open Shared Calendar and search for \"Turing Calendar\". Note that this calendar also includes events held elsewhere, but there isn\\'t a way to see a separate list for Enigma only. You can also add the calendars of the other meeting rooms in the same way.\\nEvent times\\nStarting after 9:30am - The main entrance to the British Library opens at 9:30am. - Once inside, attendees can get their visitor\\'s pass from Reception. - You should provide a list of attendees to Reception a couple of days in advance of the event. - Make sure to inform attendees that - There is often a queue to enter the British Library. - Bags larger than airline carry-on size are not permitted in the library.\\nStarting before 9:30am - Attendees will need to use the staff entrance. - You should provide a list of names to Reception and make sure the list also gets sent to the staff downstairs. - Someone will need to bring the attendees from the staff entrance up to the Turing. This can be inconvenient, so starting the event later is generally preferable.\\nEnding after 5:30pm - Talk to the Events team if attendees will be in the Institute later than 5:30pm. Approved members of staff will need to be present if non-Turing attendees are in the office after this time.\\nCatering\\n\\nCatering should be ordered at least two weeks before the event. Reception can send you an order form, or you can book via Turing Complete .\\nYou can make small changes to the order (for dietary requirements etc.) up to a couple of days before the event.\\nMenus and prices can be found on Turing Complete .\\nTo pay for catering, you will need to supply a nominal code and a project code. If you are using funding from an external source, talk to Reception about setting up a purchase order (the Turing will make the payment and then reclaim from the other funding source).\\nMake sure to ask about dietary requirements on the registration form.\\n\\nPublicity\\n\\nThe Events team can create websites for events.\\nThe Turing Bulletin is sent out every Thursday afternoon. Email communications@turing.ac.uk before the end of Wednesday to have your event included (<100 words).\\nSlack: #events , #interesting-events etc.\\nTwitter: @turinghut23\\n\\nRoom setup for workshops\\nCapacity guides for theatre, boardroom and other styles can be found in the meeting request form . If you hold a workshop with a different setup, please add it below so that others can use it as a guide for capacity in future.\\nGeneral notes\\n\\nIf you need a microphone, get in touch with Dan ( dwhitfield@turing.ac.uk ) well in advance of the event.\\nYou can find spare extension cables in a box on the shelves behind the door of Enigma.\\nThere might not be enough tables for workshops in Enigma - make sure to check with Reception if you need more than eight.\\n\\nEnigma\\n40 people around five large tables (Spark workshop)\\n\\nEight people per table (made up of two smaller tables).\\nSome chairs didn\\'t have a good view of screens.\\nPlenty of room for helpers to walk around.\\nExtension cables got to three tables easily.\\nMicrophone was definitely needed.\\nNot enough tables; had to borrow from Lovelace. We needed two tables to make up the final work table, and two more for catering (tea and snacks only; more tables would be needed for full catering).\\n\\n35 people around 8 small tables (RSE with Python course)\\n\\n4 or 5 people per table.\\nFairly crowded for helpers to walk around, but small tables were good for group work.\\nTables borrowed from Lovelace.\\n\\nLovelace Suite (3 rooms)\\n~18 around three tables (Binder workshop)\\n\\nCatering on tables at the back of Lovelace.\\nMicrophone would have been useful - hard to hear from back of the room.\\nThis was an unusual setup so Reception needed help moving tables and chairs.\\n\\nTo add\\n\\nRegistration. I (Louise) haven\\'t managed the registration process for any events yet. I\\'d recommend asking someone from the Events team, who may be able to manage the process for you or show you how they use Eventbrite.\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs=documents[10:20]\n",
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     27\u001b[0m metadata_extractor \u001b[39m=\u001b[39m MetadataExtractor(\n\u001b[1;32m     28\u001b[0m     extractors\u001b[39m=\u001b[39m[\n\u001b[1;32m     29\u001b[0m         \u001b[39m# TitleExtractor(nodes=5, llm=llm),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     ],\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m node_parser \u001b[39m=\u001b[39m SimpleNodeParser\u001b[39m.\u001b[39mfrom_defaults(\n\u001b[1;32m     39\u001b[0m     text_splitter\u001b[39m=\u001b[39mtext_splitter,\n\u001b[1;32m     40\u001b[0m     metadata_extractor\u001b[39m=\u001b[39mmetadata_extractor,\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m nodes \u001b[39m=\u001b[39m node_parser\u001b[39m.\u001b[39;49mget_nodes_from_documents(test_docs)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/node_parser/simple.py:104\u001b[0m, in \u001b[0;36mSimpleNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress)\u001b[0m\n\u001b[1;32m    101\u001b[0m         all_nodes\u001b[39m.\u001b[39mextend(nodes)\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         all_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata_extractor\u001b[39m.\u001b[39;49mprocess_nodes(all_nodes)\n\u001b[1;32m    106\u001b[0m     event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: all_nodes})\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m all_nodes\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/node_parser/extractors/metadata_extractors.py:119\u001b[0m, in \u001b[0;36mMetadataExtractor.process_nodes\u001b[0;34m(self, nodes, excluded_embed_metadata_keys, excluded_llm_metadata_keys)\u001b[0m\n\u001b[1;32m    117\u001b[0m     new_nodes \u001b[39m=\u001b[39m [deepcopy(node) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes]\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m extractor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextractors:\n\u001b[0;32m--> 119\u001b[0m     cur_metadata_list \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39;49mextract(new_nodes)\n\u001b[1;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m idx, node \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(new_nodes):\n\u001b[1;32m    121\u001b[0m         node\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mupdate(cur_metadata_list[idx])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/node_parser/extractors/metadata_extractors.py:286\u001b[0m, in \u001b[0;36mKeywordExtractor.extract\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    285\u001b[0m             \u001b[39m# TODO: figure out a good way to allow users to customize keyword template\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m             keywords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    287\u001b[0m                 PromptTemplate(\n\u001b[1;32m    288\u001b[0m                     template\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m{{\u001b[39;49;00m\u001b[39mcontext_str\u001b[39;49m\u001b[39m}}\u001b[39;49;00m\u001b[39m. Give \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkeywords\u001b[39m}\u001b[39;49;00m\u001b[39m unique keywords for this \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mdocument. Format as comma separated. Keywords: \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m    291\u001b[0m                 ),\n\u001b[1;32m    292\u001b[0m                 context_str\u001b[39m=\u001b[39;49mcast(TextNode, node)\u001b[39m.\u001b[39;49mtext,\n\u001b[1;32m    293\u001b[0m             )\n\u001b[1;32m    294\u001b[0m             \u001b[39m# node.metadata[\"excerpt_keywords\"] = keywords\u001b[39;00m\n\u001b[1;32m    295\u001b[0m             metadata_list\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mexcerpt_keywords\u001b[39m\u001b[39m\"\u001b[39m: keywords\u001b[39m.\u001b[39mstrip()})\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/llm_predictor/base.py:152\u001b[0m, in \u001b[0;36mLLMPredictor.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    150\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_prompt(prompt)\n\u001b[1;32m    151\u001b[0m     formatted_prompt \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[0;32m--> 152\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_llm\u001b[39m.\u001b[39;49mcomplete(formatted_prompt)\n\u001b[1;32m    153\u001b[0m     output \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[1;32m    155\u001b[0m logger\u001b[39m.\u001b[39mdebug(output)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/llms/base.py:277\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    268\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    269\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    270\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         },\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 277\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    279\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_index/llms/llama_cpp.py:197\u001b[0m, in \u001b[0;36mLlamaCPP.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_formatted:\n\u001b[1;32m    195\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_to_prompt(prompt)\n\u001b[0;32m--> 197\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(prompt\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m CompletionResponse(text\u001b[39m=\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], raw\u001b[39m=\u001b[39mresponse)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama.py:1441\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1396\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1397\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     grammar: Optional[LlamaGrammar] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1418\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m   1419\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m \n\u001b[1;32m   1421\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1442\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1443\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m   1444\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1445\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1446\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1447\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m   1448\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m   1449\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1450\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1451\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1452\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1453\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1454\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1455\u001b[0m         tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1456\u001b[0m         mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1457\u001b[0m         mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1458\u001b[0m         mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1459\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1460\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1461\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1462\u001b[0m         grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1463\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama.py:1392\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1391\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1392\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama.py:945\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m    943\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 945\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    946\u001b[0m     prompt_tokens,\n\u001b[1;32m    947\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    948\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    949\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    950\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m    951\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m    952\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m    953\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m    954\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    955\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m    956\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    957\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m    958\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    959\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m    960\u001b[0m ):\n\u001b[1;32m    961\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m    962\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama.py:765\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    762\u001b[0m     grammar\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    764\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    766\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    767\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    768\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m         grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m    779\u001b[0m     )\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    781\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids\u001b[39m.\u001b[39mtolist(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    782\u001b[0m     ):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama.py:484\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    482\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[1;32m    483\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 484\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    485\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    486\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    487\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_tokens),\n\u001b[1;32m    488\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_past),\n\u001b[1;32m    489\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    492\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/reginald-slack-ZVq5BSHv-py3.11/lib/python3.11/site-packages/llama_cpp/llama_cpp.py:788\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    782\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    783\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    786\u001b[0m     n_threads: c_int,\n\u001b[1;32m    787\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 788\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    "    # SummaryExtractor,\n",
    "    # QuestionsAnsweredExtractor,\n",
    "    # TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    # EntityExtractor,\n",
    "    MetadataFeatureExtractor,\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "class CustomExtractor(MetadataFeatureExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\": node.metadata[\"document_title\"]\n",
    "                + \"\\n\"\n",
    "                + node.metadata[\"excerpt_keywords\"]\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list\n",
    "    \n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        # TitleExtractor(nodes=5, llm=llm),\n",
    "        # QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "        # EntityExtractor(prediction_threshold=0.5),\n",
    "        # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "        KeywordExtractor(keywords=3, llm=llm),\n",
    "        # CustomExtractor()\n",
    "    ],\n",
    ")\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter,\n",
    "    metadata_extractor=metadata_extractor,\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_service_context(\n",
    "        model, \n",
    "        max_input_size=1024,\n",
    "        num_output=128,\n",
    "        chunk_size_lim=512,\n",
    "        overlap_ratio=0.1\n",
    "    ):\n",
    "    llm_predictor=LLMPredictor(llm=model)\n",
    "    prompt_helper=PromptHelper(max_input_size,num_output,overlap_ratio,chunk_size_lim)\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper, embed_model=\"local\")\n",
    "    return service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = create_service_context(llm)\n",
    "index = VectorStoreIndex(nodes, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import ResponseEvaluator, QueryResponseEvaluator\n",
    "\n",
    "source_evaluator = ResponseEvaluator(service_context=service_context)\n",
    "query_evaluator = QueryResponseEvaluator(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Who is Ryan Chan?\"\n",
    "response = query_engine.query(query)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_evaluator.evaluate(response))\n",
    "print(query_evaluator.evaluate(query, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "data_generator = DatasetGenerator(nodes, service_context=service_context, num_questions_per_chunk=3)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
