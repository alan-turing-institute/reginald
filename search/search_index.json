{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reginald","text":"<p>The Reginald project consists of:</p> <pre><code>\u251c\u2500\u2500 azure\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts to setup Reginald infrastructure on Azure\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 directory to store llama-index data indexes and other public Turing data\n\u251c\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 scripts for building a Docker images for both Reginald app and Slack-bot only app\n\u251c\u2500\u2500 notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 data processing notebooks\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 development notebooks for llama-index Reginald models\n\u2514\u2500\u2500 reginald\n    \u2514\u2500\u2500 models: scripts for setting up query and chat engines\n    \u2514\u2500\u2500 slack_bot: scripts for setting up Slack bot\n    \u2514\u2500\u2500 scripts for setting up end to end Slack bot with query engine\n</code></pre>"},{"location":"#slack-bot","title":"Slack bot","text":"<p>This is a simple Slack bot written in Python that listens for direct messages and @mentions in any channel it is in and responds with a message and an emoji. The bot uses web sockets for communication. How the bot responds to messages is determined by the response engine that is set up - see the models README for more details of the models available. The main models we use are: -  <code>llama-index-llama-cpp</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses a quantised LLM (implemented using <code>llama-python-cpp</code>) to generate a response - <code>llama-index-hf</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses an LLM from Huggingface to generate a response - <code>llama-index-gpt-azure</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses the Azure OpenAI API to query a LLM to generate a response</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This project uses Poetry for dependency management. Make sure you have Poetry installed on your machine.</p>"},{"location":"#install-the-project-dependencies","title":"Install the project dependencies:","text":"<pre><code>poetry install --all-extras\n</code></pre> <p>If you only want to run a subset of the available packages then use:</p> <ul> <li>for the LLM-only and Slack-bot-only setup: <code>--extras api_bot</code></li> <li>for the Azure configuration: <code>--extras azure</code></li> <li>for running notebooks regarding using fine-tuning: <code>--extras ft_notebooks</code></li> <li>for running notebooks regarding using <code>llama-index</code>: <code>--extras llama_index_notebooks</code></li> </ul> <p>Without installing extras, you will have the packages required in order to run the full Reginald model on your machine.</p>"},{"location":"#install-the-pre-commit-hooks","title":"Install the pre-commit hooks","text":"<pre><code>pre-commit install\n</code></pre>"},{"location":"#obtaining-slack-tokens","title":"Obtaining Slack tokens","text":"<p>To set up the Slack bot, you must set Slack bot environment variables. To obtain them from Slack, follow the steps below:</p> <ol> <li> <p>Set up the bot in Slack: Socket Mode Client.</p> </li> <li> <p>To connect to Slack, the bot requires an app token and a bot token. Put these into into a <code>.env</code> file:</p> <pre><code>echo \"SLACK_BOT_TOKEN='your-bot-user-oauth-access-token'\" &gt;&gt; .env\necho \"SLACK_APP_TOKEN='your-app-level-token'\" &gt;&gt; .env\n</code></pre> </li> <li> <p>Activate the virtual environment:</p> <pre><code>poetry shell\n</code></pre> </li> </ol>"},{"location":"#github-access-tokens","title":"GitHub access tokens","text":"<p>We are currently using <code>llama-hub</code> GitHub readers for creating our data indexes and pulling from relevant repos for issues and files. As a prerequisite, you will need to generate a \u201cclassic\u201d personal access token with the <code>repo</code> and <code>read:org</code> scopes - see here for instructions for creating and obtaining your personal access token.</p> <p>Once, you do this, simply add this to your <code>.env</code> file:</p> <pre><code>echo \"GITHUB_TOKEN='your-github-personal-access-token'\" &gt;&gt; .env\n</code></pre>"},{"location":"#running-reginald-locally-without-slack","title":"running Reginald locally (without Slack)","text":"<p>It is possible to run the Reginald model locally and interact with it completely through the command line via the <code>reginald chat</code> CLI - note that this is a wrapper around the <code>reginald.run.run_chat_interact</code> function. To see CLI arguments:</p> <pre><code>reginald chat --help\n</code></pre> <p>For example with using the <code>llama-index-llama-cpp</code> model running Llama-2-7b-Chat (quantised to 4bit), you can run:</p> <pre><code>reginald chat \\\n  --model llama-index-llama-cpp \\\n  --model-name https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook \\\n  --n-gpu-layers 2\n</code></pre> <p>For an example with using the <code>llama-index-ollama</code> model running Llama3, you can run:</p> <pre><code>reginald chat \\\n  --model llama-index-ollama \\\n  --model-name llama3 \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook\n</code></pre> <p>where you have set the <code>OLLAMA_API_ENDPOINT</code> environment variable to the endpoint of the OLLAMA API.</p> <p>For examples of running each of our different models, see the models README.</p> <p>The <code>reginald run_all</code> CLI takes in several arguments such as: - <code>--model</code> (<code>-m</code>): to select the type of model to use (see the models README for the list of models available) - <code>--model-name</code> (<code>-n</code>): to select the sub-model to use within the model selected     - For <code>llama-index-llama-cpp</code> and <code>llama-index-hf</code> models, this specifies the LLM (or path to that model) which we would like to use     - For <code>chat-completion-azure</code> and <code>llama-index-gpt-azure</code>, this refers to the deployment name on Azure     - For <code>chat-completion-openai</code> and <code>llama-index-gpt-openai</code>, this refers to the model/engine name on OpenAI</p> <p>There are some CLI arguments specific to only the <code>llama-index</code> models: - <code>--mode</code>: to determine whether to use \u2018query\u2019 or \u2018chat\u2019 engine - <code>--data-dir</code> (<code>-d</code>): specify the data directory location - <code>--which-index</code> (<code>-w</code>): specify the directory name for looking up/writing data index (for <code>llama-index</code> models) - <code>--force-new-index</code> (<code>-f</code>): whether or not to force create a new data index</p> <p>There are some CLI arguments specific to only the <code>llama-index-llama-cpp</code> and <code>llama-index-hf</code> models: - <code>--max-input-size</code> (<code>-max</code>): maxumum input size of LLM</p> <p>There are some CLI arguments specific to only the <code>llama-index-llama-cpp</code> model: - <code>--is-path</code> (<code>-p</code>): whether or not the model-name passed is a path to the model - <code>--n-gpu-layers</code> (<code>-ngl</code>): number of layers to offload to GPU if using <code>llama-index-llama-cpp</code> model</p> <p>There are some CLI arguments specific to only the <code>llama-index-hf</code> model: - <code>--device</code> (<code>-dev</code>): device to host Huggingface model if using <code>llama-index-hf</code> model</p> <p>Note: specifying CLI arguments will override any environment variables set.</p>"},{"location":"#running-the-reginald-bot-locally-with-slack","title":"Running the Reginald bot locally with Slack","text":"<p>In order to run the full Reginald app locally (i.e. setting up the full response engine along with the Slack bot), you can follow the steps below:</p> <ol> <li> <p>Set environment variables (for more details on environtment variables, see the environment variables README):</p> <pre><code>source .env\n</code></pre> </li> <li> <p>Run the bot using <code>reginald run_all</code> - note that this is a wrapper around the <code>reginald.run.run_full_pipeline</code> function. To see CLI arguments:</p> <pre><code>reginald run_all --help\n</code></pre> </li> </ol> <p>For example, to set up a <code>llama-index-llama-cpp</code> chat engine model running Llama-2-7b-Chat (quantised to 4bit), you can run:</p> <pre><code>reginald run_all \\\n  --model llama-index-llama-cpp \\\n  --model-name https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook \\\n  --n-gpu-layers 2\n</code></pre> <p>The bot will now listen for @mentions in the channels it\u2019s added to and respond with a simple message.</p>"},{"location":"#running-the-response-engine-and-slack-bot-separately","title":"Running the response engine and Slack bot separately","text":"<p>There are some cases where you\u2019d want to run the response engine and Slack bot separately. For instance, with the <code>llama-index-llama-cpp</code> and <code>llama-index-hf</code> models, you are hosting your own LLM which you might want to host on a machine with GPUs. The Slack bot can then be run on a separate (more cost-efficient) machine. Doing this allows you to change the model or machine running the model without having to change the Slack bot.</p> <p>To do this, you can follow the steps below:</p> <ul> <li> <p>On the machine where you want to run the response engine, run the following command:</p> <ol> <li>Set up environment variables for the response engine (for more details on environtment variables, see the environment variables README):</li> </ol> <pre><code>source .response_engine_env\n</code></pre> <ol> <li>Set up response engine using <code>reginald app</code> - note that this is a wrapper around the <code>reginald.run.run_reginald_app</code> function. To see CLI arguments:</li> </ol> <pre><code>reginald app --help\n</code></pre> <p>This command uses many of the same CLI arguments as described above. For example to set up a <code>llama-index-llama-cpp</code> chat engine model running Llama-2-7b-Chat (quantised to 4bit), you can run:</p> <pre><code>reginald app \\\n    --model llama-index-llama-cpp \\\n    --model-name https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \\\n    --mode chat \\\n    --data-dir data/ \\\n    --which-index handbook \\\n    --n-gpu-layers 2\n</code></pre> </li> <li> <p>On the machine where you want to run the Slack bot, run the following command:</p> <ol> <li>Set up environment variables for the Slack bot (for more details on environtment variables, see the environment variables README):</li> </ol> <p><pre><code>source .slack_bot_env\n</code></pre> 2. Set up Slack bot using <code>reginald bot</code> - note that this is a wrapper around the <code>reginald.run.run_bot</code> function. To see CLI arguments:</p> <pre><code>reginald bot --help\n</code></pre> <p>This command takes in an emoji to respond with. For example, to set up a Slack bot that responds with the <code>:llama:</code> emoji, you can run:</p> <pre><code>reginald bot --emoji llama\n</code></pre> </li> </ul>"},{"location":"#running-the-bot-in-docker","title":"Running the bot in Docker","text":"<p>For full details of Docker setup, see the Docker README.</p>"},{"location":"#running-the-bot-in-azure","title":"Running the bot in Azure","text":"<ol> <li> <p>Go to the <code>azure</code> directory</p> </li> <li> <p>Ensure that you have installed <code>Pulumi</code> and the <code>Azure CLI</code></p> </li> <li> <p>Setup the Pulumi backend and deploy</p> </li> </ol> <pre><code>./setup.sh &amp;&amp; AZURE_KEYVAULT_AUTH_VIA_CLI=true pulumi up -y\n</code></pre>"},{"location":"ENVIRONMENT_VARIABLES/","title":"Configuration","text":""},{"location":"ENVIRONMENT_VARIABLES/#environment-variables-for-running-reginald","title":"Environment variables for running Reginald","text":"<p>To set up the Reginald app (which consists of both the full response engine along with the Slack bot), you can use the <code>reginald run_all</code> on the terminal. To see the CLI arguments, you can simply run:</p> <pre><code>reginald run_all --help\n</code></pre> <p>Note: specifying CLI arguments will override any environment variables set.</p> <p>Below are the key environment variables that must be set:</p>"},{"location":"ENVIRONMENT_VARIABLES/#slack-bot-tokens","title":"Slack bot tokens","text":"<p>You must set the Slack bot environment variables (see the main README for information on obtaining them from Slack): - <code>SLACK_APP_TOKEN</code>: app token for Slack - <code>SLACK_BOT_TOKEN</code>: bot token for Slack</p>"},{"location":"ENVIRONMENT_VARIABLES/#openai-azure-openai-api-keys","title":"OpenAI / Azure OpenAI API keys","text":"<p>If you\u2019re using a model which uses the OpenAI API, you must set the OPENAI_API_KEY environment variable:</p> <ul> <li><code>OPENAI_API_KEY</code>: API key for OpenAI if using <code>chat-completion-openai</code> or <code>llama-index-gpt-openai</code> models</li> </ul> <p>If you\u2019re using a model which uses Azure\u2019s OpenAI instance, you must set the following environment variables: - <code>OPENAI_AZURE_API_BASE</code>: API base for Azure OpenAI if using <code>chat-completion-azure</code> or <code>llama-index-gpt-azure</code> models - <code>OPENAI_AZURE_API_KEY</code>: API key for Azure OpenAI if using <code>chat-completion-azure</code> or <code>llama-index-gpt-azure</code> models</p>"},{"location":"ENVIRONMENT_VARIABLES/#github-tokens","title":"GitHub tokens","text":"<p>For creating a data index, you must set the GitHub token environment variable <code>GITHUB_TOKEN</code> (see the main README for information on obtaining them from GitHub): - <code>GITHUB_TOKEN</code>: GitHub access token</p>"},{"location":"ENVIRONMENT_VARIABLES/#model-environment-variables","title":"Model environment variables","text":"<p>Lastly, to avoid using CLI variables and be able to simply use <code>reginald run_all</code>, you can also set the following variables too:</p> <ul> <li><code>REGINALD_MODEL</code>: name of model to use (see the models README) for the list of models available</li> <li><code>REGINALD_MODEL_NAME</code>: name of sub-model to use with the one requested if not using <code>hello</code> model.<ul> <li>For <code>llama-index-llama-cpp</code> and <code>llama-index-hf</code> models, this specifies the LLM (or path to that model) which we would like to use</li> <li>For <code>chat-completion-azure</code> and <code>llama-index-gpt-azure</code>, this refers to the deployment name on Azure</li> <li>For <code>chat-completion-openai</code> and <code>llama-index-gpt-openai</code>, this refers to the model/engine name on OpenAI</li> </ul> </li> <li><code>LLAMA_INDEX_MODE</code>: mode to use (\u201cquery\u201d or \u201cchat\u201d) if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_DATA_DIR</code>: data directory if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_WHICH_INDEX</code>: index to use (\u201chandbook\u201d, \u201cwikis\u201d, \u201cpublic\u201d, \u201creg\u201d or \u201call_data\u201d) if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_FORCE_NEW_INDEX</code>: whether to force a new index if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_MAX_INPUT_SIZE</code>: max input size if using <code>llama-index-llama-cpp</code> or <code>llama-index-hf</code> model</li> <li><code>LLAMA_INDEX_IS_PATH</code>: whether to treat REGINALD_MODEL_NAME as a path if using <code>llama-index-llama-cpp</code> model</li> <li><code>LLAMA_INDEX_N_GPU_LAYERS</code>: number of GPU layers if using <code>llama-index-llama-cpp</code> model</li> <li><code>LLAMA_INDEX_DEVICE</code>: device to use if using <code>llama-index-hf</code> model</li> </ul>"},{"location":"ENVIRONMENT_VARIABLES/#using-an-environment-file","title":"Using an environment file","text":"<p>Rather than passing in the environment variables on the command line, you can use an environment file, e.g. <code>.env</code>, and set the variables using:</p> <pre><code>source .env\n</code></pre>"},{"location":"ENVIRONMENT_VARIABLES/#environment-variables-for-running-only-the-response-engine","title":"Environment variables for running only the response engine","text":"<p>To set up the Reginald response engine (without the Slack bot), you can use the <code>reginald run_all_engine</code> on the terminal. To see the CLI arguments, you can simply run:</p> <pre><code>reginald run_all_api_llm --help\n</code></pre> <p>The CLI arguments are largely the same as <code>reginald run_all</code> except that the Slack bot tokens are not required (as they will be used to set up the Slack bot which will call the response engine via an API that is set up using <code>reginald run_all_api_llm</code>). You can also use the same environment variables as <code>reginald run_all</code> except for the Slack bot tokens.</p> <p>You can still use the same <code>.env</code> file that you used for <code>reginald run_all</code> to set up the environment variables or choose to have a separate <code>.response_engine_env</code> file to store the environment variables required for the response engine set up.</p>"},{"location":"ENVIRONMENT_VARIABLES/#environment-variables-for-running-only-the-slack-bot","title":"Environment variables for running only the Slack-bot","text":"<p>To set up the Reginald Slack bot (without the response engine), you can use the <code>reginald run_all_api_bot</code> on the terminal. To see the CLI arguments, you can simply run:</p> <pre><code>reginald run_all_api_bot --help\n</code></pre> <p>This command takes in an emoji to respond with and will set up a Slack bot that responds with the specified emoji (by default, this is the :rocket: emoji if no emoji is specified). You can also set an environment variable for the emoji to respond with using <code>REGINALD_EMOJI</code>.</p> <p>You can use the same <code>.env</code> file that you used for <code>reginald run_all</code> to set up the environment variables or choose to have a separate <code>.slack_bot_env</code> file to store the environment variables required for the Slack bot set up. This must include the Slack bot tokens.</p>"},{"location":"MODELS/","title":"Reginald Models","text":"<p>We currently have the following models available:</p> <ul> <li><code>hello</code>: a simple model which responds to a message with a greeting and an emoji</li> <li><code>llama-index-llama-cpp</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses a quantised LLM (implemented using <code>llama-python-cpp</code>) to generate a response</li> <li><code>llama-index-hf</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses an LLM from Huggingface to generate a response</li> <li><code>llama-index-gpt-azure</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses the Azure OpenAI API to query a LLM to generate a response</li> <li><code>llama-index-gpt-openai</code>: a model which uses the <code>llama-index</code> library to query a data index and then uses the OpenAI API to query a LLM to generate a response</li> <li><code>chat-completion-azure</code>: a chat completion model which uses the Azure OpenAI API to query a LLM to generate a response (does not use <code>llama-index</code>)</li> <li><code>chat-completion-openai</code>: a chat completion model which uses the OpenAI API to query a LLM to generate a response (does not use <code>llama-index</code>)</li> </ul>"},{"location":"MODELS/#llama-index-models","title":"<code>llama-index</code> Models","text":"<p>The library has several models which use the <code>llama-index</code> library which allow us to easily augment an LLM with our own data. In particular, we use <code>llama-index</code> to ingest several data sources, including several public sources:</p> <ul> <li>The Research Engineering Group (REG) Handbook</li> <li>The Turing Way</li> <li>The Research Software Engineering (RSE) course ran by REG</li> <li>The Research Data Science (RDS) course ran by REG</li> <li>The public Turing website</li> </ul> <p>And also some private sources from our private GitHub repositories (using the repo\u2019s Wiki pages, issues and some selected files).</p> <p>All of these (besides the public Turing website) are loaded using <code>llama-hub</code> GitHub readers. Hence, when we are first building up the data index, we must set up the GitHub access tokens (see the README for more details), and you will only be able to build the <code>all_data</code> data index if you have access to our private repositories.</p>"},{"location":"MODELS/#data-index-options","title":"Data index options","text":"<p>When running the Reginald Slack bot, you can specify which data index to use using the <code>LLAMA_INDEX_WHICH_INDEX</code> environment variable (see the environment variables README for more details). The options are: - <code>handbook</code>: only builds an index with the public REG handbook - <code>wikis</code>: only builds an index with private REG repo Wiki pages - <code>public</code>: builds an index with the all the public data listed above - <code>all_data</code>: builds an index with all the data listed above including data from our private repo</p> <p>Once a data index has been built, it will be saved in the <code>data</code> directory specified in the <code>reginald run_all</code> (or <code>reginald run_all_api_llm</code>) CLI arguments or the <code>LLAMA_INDEX_DATA_DIR</code> environment variable. If you want to force a new index to be built, you can use the <code>--force-new-index</code> or <code>-f</code> flag, or you can set the <code>LLAMA_INDEX_FORCE_NEW_INDEX</code> environment variable to <code>True</code>.</p> <p>There are several options of the LLM to use with the <code>llama-index</code> models, some of which we have implemented in this library and which we discuss below.</p>"},{"location":"MODELS/#llama-index-models-with-self-hosted-llm","title":"<code>llama-index</code> models with self-hosted LLM","text":"<p>We have two models which involve hosting the LLM ourselves and using the <code>llama-index</code> library to query the data index and then generate a response using the LLM. These models are:</p>"},{"location":"MODELS/#llama-index-llama-cpp-model","title":"<code>llama-index-llama-cpp</code> Model","text":"<p>This model uses the <code>llama-cpp-python</code> library to host a quantised LLM. In our case, we have been using quantised versions of Meta\u2019s Llama-2 model uploaded by TheBloke on Huggingface\u2019s model hub. An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model llama-index-llama-cpp \\\n  --model-name https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook \\\n  --max-input-size 4096 \\\n  --n-gpu-layers 2\n</code></pre> <p>Note that the <code>--n-gpu-layers</code> argument is optional and specifies the number of layers to offload to the GPU. If not specified, it will default to 0. See the <code>llama-cpp-python</code> README to see how you can install the library with hardware acceleration.</p> <p>Running this in a root of this repository will automatically pick up the data indices for the handbook in <code>data/llama_index_indices/handbook</code> directory.</p> <p>Running this command requires about 7GB of RAM. We were able to run this on our M1 Pro (32GB) macbook pros with no issues and were able to run the Llama-2-13B-chat model too.</p> <p>If you wish to download the quantised model (as a <code>.gguf</code> file) and host it yourself, you can do so by passing the file name to the <code>--model-name</code> argument and using the <code>--is-path</code> flag (alternatively, you can re-run the above but first set the environment variable <code>LLAMA_INDEX_IS_PATH</code> to <code>True</code>):</p> <pre><code>reginald run_all \\\n  --model llama-index-llama-cpp \\\n  --model-name gguf_models/llama-2-7b-chat.Q4_K_M.gguf \\\n  --is-path \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook \\\n  --max-input-size 4096 \\\n  --n-gpu-layers 2\n</code></pre> <p>given that the <code>llama-2-7b-chat.Q4_K_M.gguf</code> file is in a <code>gguf_models</code> directory.</p>"},{"location":"MODELS/#llama-index-hf-model","title":"<code>llama-index-hf</code> Model","text":"<p>This model uses an LLM from Huggingface to generate a response. An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model llama-index-hf \\\n  --model-name microsoft/phi-1_5 \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook \\\n  --max-input-size 2048 \\\n  --device auto\n</code></pre> <p>Note currently the <code>microsoft/phi-1_5</code> model has a predefined maximum length of 2048 context length. Hence, we must set the <code>--max-input-size</code> argument to be less than or equal to 2048 as the default value for this argument is 4096 as we tend to use the <code>llama-cpp-python</code> model more. We also set the <code>--device</code> argument to be <code>auto</code> so that the model will be run on any hardware acceleration if available.</p>"},{"location":"MODELS/#llama-index-models-using-an-api","title":"<code>llama-index</code> models using an API","text":"<p>We have two models which use an API to query a LLM to generate a response. These models are:</p>"},{"location":"MODELS/#llama-index-gpt-azure-model","title":"<code>llama-index-gpt-azure</code> Model","text":"<p>To use this model, you must set the following environment variables: - <code>OPENAI_AZURE_API_BASE</code>: API base for Azure OpenAI - <code>OPENAI_AZURE_API_KEY</code>: API key for Azure OpenAI</p> <p>An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model llama-index-gpt-azure \\\n  --model-name \"reginald-gpt35-turbo\" \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook\n</code></pre> <p>Note that <code>\"reginald-gpt35-turbo\"</code> is the name of our deployment of the \u201cgpt-3.5-turbo\u201d model on Azure. This probably is different on your deployment and resource group on Azure.</p>"},{"location":"MODELS/#llama-index-gpt-openai-model","title":"<code>llama-index-gpt-openai</code> Model","text":"<p>To use this model, you must set the <code>OPENAI_API_KEY</code> environment variable and set this to be an API key for OpenAI.</p> <p>An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model llama-index-gpt-openai \\\n  --model-name \"gpt-3.5-turbo\" \\\n  --mode chat \\\n  --data-dir data/ \\\n  --which-index handbook\n</code></pre>"},{"location":"MODELS/#chat-completion-models","title":"<code>chat-completion</code> Models","text":"<p>The library also has several models which use the OpenAI API (or the Azure OpenAI API) to query a LLM to generate a response. These models do not use the <code>llama-index</code> library and hence do not use a data index - these are purely chat completion models.</p>"},{"location":"MODELS/#chat-completion-azure-model","title":"<code>chat-completion-azure</code> Model","text":"<p>To use this model, you must set the following environment variables: - <code>OPENAI_AZURE_API_BASE</code>: API base for Azure OpenAI - <code>OPENAI_AZURE_API_KEY</code>: API key for Azure OpenAI</p> <p>An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model chat-completion-azure \\\n  --model-name \"reginald-curie\"\n</code></pre> <p>Note that <code>\"reginald-curie\"</code> is the name of our deployment of a fine-tuned model on Azure. This probably is different on your deployment and resource group on Azure. With Azure\u2019s AI Studio, it is possible to fine-tune your own model with Q&amp;A pairs (see here for more details).</p>"},{"location":"MODELS/#chat-completion-openai-model","title":"<code>chat-completion-openai</code> Model","text":"<p>To use this model, you must set the <code>OPENAI_API_KEY</code> environment variable and set this to be an API key for OpenAI.</p> <p>An example of running this model locally is:</p> <pre><code>reginald run_all \\\n  --model chat-completion-openai \\\n  --model-name \"gpt-3.5-turbo\"\n</code></pre>"},{"location":"azure/","title":"Index","text":"<p>You will need to install</p> <ul> <li>Pulumi</li> <li>Azure CLI</li> </ul> <p>in order to run this code.</p> <ol> <li>Have a .pulumi_env file (in this directory) including the variables that you need.</li> </ol> <p>For the <code>api_bot</code> model, you will need to have the following variables:</p> <pre><code>REGINALD_SLACK_APP_TOKEN\nREGINALD_SLACK_BOT_TOKEN\nREGINALD_API_URL\nGITHUB_TOKEN\n</code></pre> <p>For the <code>hack_week</code> model, you will need to have the following variables:</p> <pre><code>OPENAI_AZURE_API_BASE\nOPENAI_AZURE_API_KEY\nCOMPLETION_SLACK_APP_TOKEN\nCOMPLETION_SLACK_BOT_TOKEN\nGPT_AZURE_SLACK_APP_TOKEN\nGPT_AZURE_SLACK_BOT_TOKEN\nGITHUB_TOKEN\nOPENAI_API_KEY (optional)\n</code></pre> <ol> <li>Move into the folder for the model you want to deploy, e.g. for the <code>api_bot</code> model:</li> </ol> <pre><code>cd api_bot\n</code></pre> <ol> <li>Setup the Pulumi backend (if it already exists this will just check that you have access)</li> </ol> <pre><code>./setup.sh\n</code></pre> <p>This may create a local <code>.secrets</code> file. This file contains the secrets that you need to deploy the model and includes the environment variables needed for the container instance (see Step 1.). You will need to source this file before deploying in the next step.</p> <ol> <li>Deploy with Pulumi</li> </ol> <pre><code>source .secrets\nAZURE_KEYVAULT_AUTH_VIA_CLI=true pulumi up\n</code></pre>"},{"location":"docker/","title":"Index","text":"<p>To build this Docker image you need to be in the root of the repository (i.e. the parent directory of this one).</p>"},{"location":"docker/#reginald-model","title":"Reginald model","text":"<p>The following command will build it the image for the (full) Reginald model which has the model + slack bot and tag it as <code>reginald:latest</code>:</p> <pre><code>docker build . -t reginald:latest -f docker/reginald/Dockerfile\n</code></pre> <p>The following environment variables can be used by this image:</p> <ul> <li><code>REGINALD_MODEL</code>: name of model to use</li> <li><code>REGINALD_MODEL_NAME</code>: name of sub-model to use with the one requested if not using <code>hello</code> model.<ul> <li>For <code>llama-index-llama-cpp</code> and `llama-index-hf`` models, this specifies the LLM (or path to that model) which we would like to use</li> <li>For <code>chat-completion-azure</code> and <code>llama-index-gpt-azure</code>, this refers to the deployment name on Azure</li> <li>For <code>chat-completion-openai</code> and <code>llama-index-gpt-openai</code>, this refers to the model/engine name on OpenAI</li> </ul> </li> <li><code>LLAMA_INDEX_MODE</code>: mode to use (\u201cquery\u201d or \u201cchat\u201d) if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_DATA_DIR</code>: data directory if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_WHICH_INDEX</code>: index to use (\u201chandbook\u201d, \u201cwikis\u201d, \u201cpublic\u201d, \u201creg\u201d or \u201call_data\u201d) if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_FORCE_NEW_INDEX</code>: whether to force a new index if using <code>llama-index</code> model</li> <li><code>LLAMA_INDEX_MAX_INPUT_SIZE</code>: max input size if using <code>llama-index-llama-cpp</code> or <code>llama-index-hf</code> model</li> <li><code>LLAMA_INDEX_IS_PATH</code>: whether to treat REGINALD_MODEL_NAME as a path if using <code>llama-index-llama-cpp</code> model</li> <li><code>LLAMA_INDEX_N_GPU_LAYERS</code>: number of GPU layers if using <code>llama-index-llama-cpp</code> model</li> <li><code>LLAMA_INDEX_DEVICE</code>: device to use if using <code>llama-index-hf</code> model</li> <li><code>OPENAI_API_KEY</code>: API key for OpenAI if using <code>chat-completion-openai</code> or <code>llama-index-gpt-openai</code> models</li> <li><code>OPENAI_AZURE_API_BASE</code>: API base for Azure OpenAI if using <code>chat-completion-azure</code> or <code>llama-index-gpt-azure</code> models</li> <li><code>OPENAI_AZURE_API_KEY</code>: API key for Azure OpenAI if using <code>chat-completion-azure</code> or <code>llama-index-gpt-azure</code> models</li> <li><code>SLACK_APP_TOKEN</code>: app token for Slack</li> <li><code>SLACK_BOT_TOKEN</code>: bot token for Slack</li> </ul> <p>To run you can use the following command (to run the <code>hello</code> model):</p> <pre><code>docker run -e REGINALD_MODEL=hello -e SLACK_APP_TOKEN=&lt;slack-app-token&gt; -e SLACK_BOT_TOKEN=&lt;slack-bot-token&gt; reginald:latest\n</code></pre>"},{"location":"docker/#slack-bot-only","title":"Slack bot only","text":"<p>The following command will build it the image for the Slack bot only and tag it as <code>reginald-slack-bot:latest</code>:</p> <pre><code>docker build . -t reginald-slack-bot:latest -f docker/slack_bot/Dockerfile\n</code></pre> <p>The following environment variables will be used by this image:</p> <ul> <li><code>REGINALD_EMOJI</code>: emoji to use for bot</li> <li><code>SLACK_APP_TOKEN</code>: app token for Slack</li> <li><code>SLACK_BOT_TOKEN</code>: bot token for Slack</li> </ul> <p>To run you can use the following command:</p> <pre><code>docker run -e REGINALD_EMOJI=wave -e SLACK_APP_TOKEN=&lt;slack-app-token&gt; -e SLACK_BOT_TOKEN=&lt;slack-bot-token&gt; reginald-slack-bot:latest\n</code></pre>"},{"location":"docker/#using-an-environment-file","title":"Using an environment file","text":"<p>Rather than passing in the environment variables on the command line using the <code>-e</code> flag in <code>docker run</code>, you can use an environment file:</p> <pre><code>docker run --env-file .env reginald:latest\n</code></pre> <p>where <code>.env</code> is a file containing the environment variables, e.g. for running the <code>llama-index-llama-cpp</code> model using the <code>handbook</code> index:</p> <pre><code>REGINALD_MODEL=llama-index-llama-cpp\nREGINALD_MODEL_NAME=https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\nLLAMA_INDEX_MODE=chat\nLLAMA_INDEX_DATA_DIR=data\nLLAMA_INDEX_WHICH_INDEX=handbook\nLLAMA_INDEX_MAX_INPUT_SIZE=2048\nSLACK_APP_TOKEN=&lt;slack-app-token&gt;\nSLACK_BOT_TOKEN=&lt;slack-bot-token&gt;\n</code></pre>"},{"location":"meeting_notes/Reginald_08_24_23/","title":"Reginald 24/08/23","text":""},{"location":"meeting_notes/Reginald_08_24_23/#notes","title":"Notes","text":"<ul> <li>Quick overview of existing Reginald Work<ul> <li>Reginald (Handbook): fine-tune a Q&amp;A LLM using the REG handbook</li> <li>Reginald (Llama): use something called llama-index</li> <li>Quick recap of the type of questions it answered</li> </ul> </li> <li>Discussion of work to do this month<ul> <li>Mainly focus on model development</li> <li>Getting onto Turing slack would be nice, but important to get the model working well first<ul> <li>Not the end of the world with the bot existing in another Slack workspace for now</li> <li>Time better spent to learn about LLMs and playing around with libraries rather than looking into technicalities of slack and IT policy</li> </ul> </li> <li>Models to look at<ul> <li>RAG</li> <li>Using LLama2 rather than OpenAI API</li> </ul> </li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_08_24_23/#actions","title":"Actions","text":"<ul> <li>Start playing around with llama-index again and read up on alternative models</li> <li>Look into using an open-source model rather than OpenAI</li> <li>Admin for Ryan:<ul> <li>Get Yi-Ling added to Reginald repo</li> <li>Add Rosie to Azure subscription</li> <li>Create regular calendar invite</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_08_31_23/","title":"Reginald 31/08/23","text":""},{"location":"meeting_notes/Reginald_08_31_23/#notes","title":"Notes","text":"<ul> <li>Rosie been working with using llama-index with LLama2 (currently running locally with use of llama-cpp)<ul> <li>Went through Rosie\u2019s development notebook</li> </ul> </li> <li>Also had a play around with llama-index chat engine<ul> <li>Went through some examples</li> <li>With the chat engine, the model will try to determine if it should query the database (i.e. construct a prompt with some text from the database) or should it just send something to the LLM<ul> <li>Can we investigate more about how it does this?</li> </ul> </li> </ul> </li> <li>Is it possible to obtain the prompt to the LLM<ul> <li>We know that llama-index does some prompt engineering, but would it be possible to look at that?</li> <li>Also in cases when it tries to refine the answer - does it actually give much benefit?</li> </ul> </li> <li>Yi-Ling has been looking at llama-index and ideas of how to evaluate the models to see if it\u2019s using the knowledge base effectively<ul> <li>We should compile some \u201cmodel\u201d question and answer pairs that we hope our model to get right</li> <li>Start to think about being able to systematically compare models and approaches</li> </ul> </li> <li>Discussion about documentation<ul> <li>Start uploading meeting notes</li> <li>Start documenting the models we try and our experiences with them</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_08_31_23/#actions","title":"Actions","text":"<ul> <li>Continue with chat engine hacking<ul> <li>There are several chat engine choices given by llama-index<ul> <li>Compare different approaches and figure out what is the most appropriate</li> <li>Try to figure out when the model is querying and when it is</li> <li>Try to figure out how it makes the decision of whether or not to just have a conversation or not</li> <li>How does this work with small context lengths?<ul> <li>How is it remembering/tracking conversation history?</li> </ul> </li> </ul> </li> </ul> </li> <li>Maybe start working on Azure rather than local<ul> <li>Could be able to run a larger model</li> <li>Might not need GPU if only inference and only for dev</li> <li>Maybe GPU long term?</li> </ul> </li> <li>Admin:<ul> <li>Start meeting note uploads (Ryan)</li> <li>Start model documentation (all)</li> <li>Start project board (Rosie, Levan)</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_09_08_23/","title":"Reginald 08/09/23","text":""},{"location":"meeting_notes/Reginald_09_08_23/#notes","title":"Notes","text":"<ul> <li>Ryan and Rosie almost finishing a PR contribution to llama-index for the following issue: https://github.com/jerryjliu/llama_index/issues/7596<ul> <li>ReAct engine seems to be a bit broken after this due to the default system prompt<ul> <li>Potentially can make another issue and PR to fix</li> </ul> </li> </ul> </li> <li>Other potential llama-index contributions<ul> <li>Some llama-cpp chat engine example notebooks - there\u2019s not many (if at all) that exist in the repo and they\u2019re very welcome to more examples</li> <li>Ryan to try fix old issue posted during Hackweek (https://github.com/jerryjliu/llama_index/issues/6465)</li> </ul> </li> <li>Rosie been working on getting GitHub issues and files from within our Hut23<ul> <li>Will need to think about what is the personal token that we should use<ul> <li>Maybe look at if there\u2019s an instituitional token that exists</li> </ul> </li> <li>Will need to upload this to Azure as a secret</li> </ul> </li> <li>Idea: look into logging how good the answers are and save into a database<ul> <li>For future, we can maybe use this with a RLHF fine-tuning of our LLM</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_09_08_23/#actions","title":"Actions","text":"<ul> <li>Ryan &amp; Rosie: look at putting this azure<ul> <li>Figure out how to have multiple chat instances at the same time</li> <li>Can start with running the bot locally</li> </ul> </li> <li>Rosie to think about using the reader for pure markdown files so no need to process them to csv<ul> <li>Alternatively, to figure out how the csv files were created from pure markdown files</li> <li>Maybe it was from Andy\u2019s scripts to pull data from wiki and handbook</li> </ul> </li> <li>Ryan to see if we can update the OpenAI code and compare with Llama2</li> <li>Ryan &amp; Rosie: Try to get a quantized Llama2-70b running (for Tomas)</li> <li>Merge Rosie\u2019s current PRs at some point next week</li> </ul>"},{"location":"meeting_notes/Reginald_09_15_23/","title":"Reginald 15/09/23","text":""},{"location":"meeting_notes/Reginald_09_15_23/#notes","title":"Notes","text":"<ul> <li>Updates on recent llama-index contributions<ul> <li>Our PR has been merged into llama-index: https://github.com/jerryjliu/llama_index/pull/7597#issuecomment-1712010196</li> <li>Some other contributions to llama-index over this last week<ul> <li>PR for fixing an issue that came up during hackweek was merged into llama-index: https://github.com/jerryjliu/llama_index/pull/7607</li> <li>A PR to llama-index for updating their llama-cpp example in docs: https://github.com/jerryjliu/llama_index/pull/7616</li> <li>A PR to fix a bug introduced to LlamaCPP: https://github.com/jerryjliu/llama_index/pull/7650</li> </ul> </li> <li>More llama-index contributions?<ul> <li>We could maybe contribute some end-to-end examples to the repo too</li> </ul> </li> </ul> </li> <li>Updates on Reginald (Rosie, Ryan)<ul> <li>Updated our hackweek code for implementing the query engine<ul> <li>Previous code had some old classes in llama-index which are depreciating in the future</li> </ul> </li> <li>Llama-2 CPP added to the available models</li> <li>Ddded chat engine functionality to slack bot</li> <li>Been running things locally, so need to update the bot that is running on Azure<ul> <li>Maybe we can keep the OpenAI one but make sure that is only using public data</li> <li>Create a new bot which has all private data (along with the GitHub readers that Rosie\u2019s been looking at)</li> </ul> </li> </ul> </li> <li>Some current issues<ul> <li>Queuing system<ul> <li>Currently it\u2019s possible to crash it by sending multiple requests while it\u2019s still processing the previous</li> <li>Rosie has made good progress on fixing this, but currently it will only emoji and respond to one message. Would be good to emoji immediately on all but queue the queries to the LLM</li> </ul> </li> <li>Multiple chat instances<ul> <li>Each user to have a chat history</li> <li>Some issues to consider<ul> <li>When do we refresh a chat history for a user?<ul> <li>Maybe add option for user to acknowledge that their query was answered - manual refresh of history</li> </ul> </li> <li>Timer that deletes chat history after a certain time</li> <li>Are there any privacy concerns of us hosting the chat history?</li> <li>How do we queue queries coming from different users?<ul> <li>Maybe best to just do it by time they posted that query. Hopefully the responses don\u2019t take too long anyway</li> <li>How many users do we actually expect to be using it at the same time?</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Demo for next week<ul> <li>Aim to have minimal Llama-2-CPP model running on Azure<ul> <li>If not, we can run it locally</li> <li>Will still have the OpenAI running as well</li> </ul> </li> <li>Rosie: one idea is to write a notebook example that reads in PDF reports and compare it with just using ChatGPT</li> <li>Do we need slides?</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_09_15_23/#actions","title":"Actions","text":"<ul> <li>Ryan to message James R to discuss Azure</li> <li>Rosie to continuing looking at queuing system</li> <li>Rosie and Ryan to continue thinking about how we deal with multiple users of the chat engine</li> <li>Focus on making the demo smooth</li> </ul>"},{"location":"meeting_notes/Reginald_09_22_23/","title":"Reginald 22/09/23","text":""},{"location":"meeting_notes/Reginald_09_22_23/#notes","title":"Notes","text":"<ul> <li>Well done on the demo!<ul> <li>Sounds like it went very well!</li> <li>Lots of work has gone into this week to get things working and we have made great progress with getting the new updated models running</li> </ul> </li> <li>Model updates<ul> <li>We have a VM running the bot with llama-2-70b model (quantised to 4bit) using <code>all_data</code> (incorporated Rosie\u2019s work on the readers for Hut23 issues, wiki and all public data)<ul> <li>Currently available on Slack as <code>Reginald (llama-cpp)</code></li> </ul> </li> <li>We have an updated bot with Azure OpenAI endpoint using <code>public</code> (turing.ac.uk website, RSE course, RDS course, Turing Way, handbook)<ul> <li>Currently available on Slack as <code>Reginald (llama-index GPT)</code></li> </ul> </li> <li>Note that these are not always on: would cost approximately $90 to keep on for a full 24 hours</li> </ul> </li> <li>Rosie and Ryan met with James R this week to talk about the pulumi and Azure stuff in the repo<ul> <li>Have a hope that we can split up the slack bot into two separate components<ul> <li>The LLM which can be accessed via an API</li> <li>The Slack bot itself which can live in a container with very little compute</li> </ul> </li> <li>This would mean that we can change the LLM or VM whenever we want without having to tear down the entire system</li> <li>Also means we can turn off the VM on a timer so that its only on during working hours to save money<ul> <li>The bot can send an automatic message during those times to say that the LLM is off</li> </ul> </li> </ul> </li> <li>Rosie and Ryan met with James B and Iain S this week to talk about the queue and async programming<ul> <li>Rosie has made a few changes to make it work a lot better, but some small issues to iron out still<ul> <li>Does the queue refresh if a Slack session ends and restarts?</li> </ul> </li> <li>Now implemented multiple chat instances</li> </ul> </li> <li>Rosie has made several contributions to llama-hub (https://github.com/emptycrown/llama-hub/commits?author=rwood-97)<ul> <li>Currently the 13th most number of PRs in the repo!</li> <li>For urls metadata to be included in repo reader: https://github.com/emptycrown/llama-hub/pull/522</li> <li>For collaborators: https://github.com/emptycrown/llama-hub/pull/512</li> <li>For issue labels: https://github.com/emptycrown/llama-hub/pull/502</li> <li>For removing redundant if/else from imports: https://github.com/emptycrown/llama-hub/commit/2c80cdc1b496901e26ecaa05d69a4d351fd083ac</li> </ul> </li> <li>If we\u2019re happy with the data readers, we remove data from the repo (will need to keep turing.ac.uk csv)<ul> <li>Is it possible to hide the private data from being visible?</li> </ul> </li> <li>Discussion on repository layout<ul> <li>Do some cleaning up this week<ul> <li>Remove some data</li> <li>Rename models/ to notebooks/</li> <li>Reorganise repo by introducing src/ directory</li> <li>Maybe make this into package</li> <li>Clean up pyproject.toml</li> </ul> </li> <li>Should it be a package?<ul> <li>If so, what would be the parts that are reusable and useful for others?</li> <li>There are parts which will not run for others, e.g. document builder - they will not have the right GitHub API keys to access them, but there are parts which will be useful, e.g. spinning up a query engine that uses Llama-CPP easily</li> <li>Do we split up the query/chat engine set up more?</li> <li>At what point does it become too complicated?</li> </ul> </li> <li>If it is made public, will need better documentation and some tests</li> <li>How do we make it easy for people to come into the project (other REG people, Turing collaborators)?</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_09_22_23/#actions","title":"Actions","text":"<ul> <li>Ryan to write up a guide for getting quantised Llama-2 models running on Azure VMs for Tomas</li> <li>Rosie and Ryan to look into implementing API between VM and bot container<ul> <li>Set timers for VM to wake and sleep</li> <li>Implement automated Slack response for when VM is off or unavailable</li> </ul> </li> <li>Restore slack_bot/run.py</li> <li>Ryan to look removing sensitive data</li> <li>Think about repository layout - should it be in a package?</li> <li>Ryan set up wrap-up meeting on Monday 2nd October</li> </ul>"},{"location":"meeting_notes/Reginald_10_02_23/","title":"Reginald 02/10/23","text":""},{"location":"meeting_notes/Reginald_10_02_23/#notes","title":"Notes","text":"<ul> <li>Rosie summary of what we\u2019ve done in the FM<ul> <li>Got Llama-2 model running (with <code>llama-cpp-python</code> to run quantised versions)</li> <li>Implemented chat engine which can remember chat history instead of a query engine</li> <li>Implemented queue - add stuff to a (asyncio) queue and process queries once at a time<ul> <li>You get a clock emoji to say that it\u2019s been registered but not yet been processed - this gets changes to a llama when it is being processed</li> <li>Idea for future work to tell you the position in the queue if possible (using the various clock emojis as a count down)</li> </ul> </li> <li>Using readers from <code>llama-hub</code> to obtain data from different sources<ul> <li>Used to have all documents (from wikis, handbooks, turing website, a few Turing courses) inside the repo, but now able to remove the data - just need a <code>GITHUB_TOKEN</code> environment variable</li> <li>Can only access Hut23 (in <code>all_data</code> index) if you\u2019re invited to the repo</li> </ul> </li> <li>Almost finished implementing API for machine hosting LLM and the machine hosting the bot<ul> <li>Ryan to pick this up and try finish</li> </ul> </li> <li>Made several PRs to <code>llama-index</code> and <code>llama-hub</code> repos</li> <li>Updated README</li> <li>Updated Dockerfiles in the repo</li> </ul> </li> <li>Remaining tasks to do:<ul> <li>Pulumi scripts</li> <li>Testing the splitting of the model and the bot (API) properly<ul> <li>It\u2019s all been on a local machine at the moment</li> <li>Once we have that, we might be able to actually host the llama-cpp model properly and make it available for some light testing within REG</li> </ul> </li> <li>Need to update the blob storage indexes</li> <li>Tech talk to update progress and maybe try to get people to join the project</li> </ul> </li> <li>Future work<ul> <li>Turing slack?<ul> <li>Get feedback on team and try get people from REG to help out with specific engineering problems</li> </ul> </li> <li>Improve chat history management - can get errors after filling up the context. There is a temprorary fix to clear history using a slack command</li> <li>Evaluation and comparison of models</li> </ul> </li> </ul>"},{"location":"meeting_notes/Reginald_10_02_23/#actions","title":"Actions","text":"<ul> <li>Rosie to message Turing IT</li> <li>Ryan wrap up API stuff</li> <li>Get tech talk time in maybe end of November</li> <li>Get monthly date in the calendar for Reginald catch up and 22 days work</li> </ul>"},{"location":"notebooks/data_processing/scraped/","title":"Scraped data","text":"<p>Scraped data using code from Andy P (link TODO).</p> <p>Currently data generation run via pytest, pending development of CLI for tool.</p> <p>Hacky process I used for now:</p> <p>(TODOs mostly to link to Andy\u2019s code if he\u2019s happy to do so, otherwise ask James B or Andy)</p> <pre><code># clone scraping repo\ngit clone TODO\n\n# clone data sources\ngit clone --recurse-submodules https://github.com/alan-turing-institute/REG-handbook.git\ngit clone https://github.com/alan-turing-institute/research-engineering-group.wiki.git\n\n# build REG handbook\npushd REG-handbook\nhugo --minify # requires hugo\npopd\n\n# use scraping repo\ncd TODO\n\n# edit code to point to local copies\n# edit tests/interact_se_connector/test_scrapper.py::test_do_walk\nTODO\n\n\n\n# venv and install\npython -m venv env\nsource env/bin/activate\npip install -e .\npip install html5\n\n# generate data\n# note: will fail but will generate files in tests/interact_se_connector/output\npytest tests/interact_se_connector/test_scrapper.py::test_do_walk\n</code></pre> <p>scraped csvs have header: <code>,url,id,title,is_public,body,summary,author,keywords</code></p>"},{"location":"reference/DOC_STRINGS/","title":"DOC STRINGS","text":"<ul> <li>reginald<ul> <li>cli</li> <li>defaults</li> <li>models<ul> <li>app</li> <li>base</li> <li>chat_interact</li> <li>create_index</li> <li>download_from_fileshare</li> <li>llama_index<ul> <li>base</li> <li>data_index_creator</li> <li>llama_cpp_template</li> <li>llama_index</li> <li>llama_index_azure_openai</li> <li>llama_index_hf</li> <li>llama_index_llama_cpp</li> <li>llama_index_ollama</li> <li>llama_index_openai</li> <li>llama_utils</li> </ul> </li> <li>setup_llm</li> <li>simple<ul> <li>chat_completion</li> <li>hello</li> </ul> </li> </ul> </li> <li>run</li> <li>run_full_pipeline</li> <li>slack_bot<ul> <li>bot</li> <li>run_bot</li> <li>utils</li> </ul> </li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/reginald/","title":"reginald","text":""},{"location":"reference/reginald/cli/","title":"cli","text":""},{"location":"reference/reginald/cli/#reginald.cli.app","title":"app","text":"<pre><code>app(\n    model: Annotated[\n        str, Option(envvar=REGINALD_MODEL, help=HELP_TEXT[model])\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        Option(envvar=REGINALD_MODEL_NAME, help=HELP_TEXT[model_name]),\n    ] = None,\n    mode: Annotated[\n        str, Option(envvar=LLAMA_INDEX_MODE, help=HELP_TEXT[mode])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        Path, Option(envvar=LLAMA_INDEX_DATA_DIR, help=HELP_TEXT[data_dir])\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, Option(envvar=LLAMA_INDEX_WHICH_INDEX, help=HELP_TEXT[which_index])\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        Option(\n            envvar=LLAMA_INDEX_FORCE_NEW_INDEX, help=HELP_TEXT[force_new_index]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        Option(\n            envvar=LLAMA_INDEX_MAX_INPUT_SIZE, help=HELP_TEXT[max_input_size]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, Option(envvar=LLAMA_INDEX_K, help=HELP_TEXT[k])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        Option(envvar=LLAMA_INDEX_CHUNK_SIZE, help=HELP_TEXT[chunk_size]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        Option(\n            envvar=LLAMA_INDEX_CHUNK_OVERLAP_RATIO,\n            help=HELP_TEXT[chunk_overlap_ratio],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, Option(envvar=LLAMA_INDEX_NUM_OUTPUT, help=HELP_TEXT[num_output])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, Option(envvar=LLAMA_INDEX_IS_PATH, help=HELP_TEXT[is_path])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        Option(envvar=LLAMA_INDEX_N_GPU_LAYERS, help=HELP_TEXT[n_gpu_layers]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, Option(envvar=LLAMA_INDEX_DEVICE, help=HELP_TEXT[device])\n    ] = DEFAULT_ARGS[\"device\"],\n    host: Annotated[\n        str, Option(envvar=REGINALD_HOST, help=HELP_TEXT[host])\n    ] = DEFAULT_ARGS[\"host\"],\n    port: Annotated[\n        int, Option(envvar=REGINALD_PORT, help=HELP_TEXT[port])\n    ] = DEFAULT_ARGS[\"port\"],\n) -&gt; None\n</code></pre> <p>Sets up the response model and then creates a FastAPI app to serve the model.</p> <p>The app listens on port 8000 and has two endpoints: - /direct_message: for obtaining responses from direct messages - /channel_mention: for obtaining responses from channel mentions</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef app(\n    model: Annotated[\n        str,\n        typer.Option(\n            envvar=\"REGINALD_MODEL\",\n            help=HELP_TEXT[\"model\"],\n        ),\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        typer.Option(envvar=\"REGINALD_MODEL_NAME\", help=HELP_TEXT[\"model_name\"]),\n    ] = None,\n    mode: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_MODE\", help=HELP_TEXT[\"mode\"])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        pathlib.Path,\n        typer.Option(envvar=\"LLAMA_INDEX_DATA_DIR\", help=HELP_TEXT[\"data_dir\"]),\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str,\n        typer.Option(envvar=\"LLAMA_INDEX_WHICH_INDEX\", help=HELP_TEXT[\"which_index\"]),\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_FORCE_NEW_INDEX\", help=HELP_TEXT[\"force_new_index\"]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_MAX_INPUT_SIZE\", help=HELP_TEXT[\"max_input_size\"]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_K\", help=HELP_TEXT[\"k\"])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        typer.Option(envvar=\"LLAMA_INDEX_CHUNK_SIZE\", help=HELP_TEXT[\"chunk_size\"]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_CHUNK_OVERLAP_RATIO\",\n            help=HELP_TEXT[\"chunk_overlap_ratio\"],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_NUM_OUTPUT\", help=HELP_TEXT[\"num_output\"])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, typer.Option(envvar=\"LLAMA_INDEX_IS_PATH\", help=HELP_TEXT[\"is_path\"])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        typer.Option(envvar=\"LLAMA_INDEX_N_GPU_LAYERS\", help=HELP_TEXT[\"n_gpu_layers\"]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_DEVICE\", help=HELP_TEXT[\"device\"])\n    ] = DEFAULT_ARGS[\"device\"],\n    host: Annotated[\n        str, typer.Option(envvar=\"REGINALD_HOST\", help=HELP_TEXT[\"host\"])\n    ] = DEFAULT_ARGS[\"host\"],\n    port: Annotated[\n        int, typer.Option(envvar=\"REGINALD_PORT\", help=HELP_TEXT[\"port\"])\n    ] = DEFAULT_ARGS[\"port\"],\n) -&gt; None:\n    \"\"\"\n    Sets up the response model and then creates a\n    FastAPI app to serve the model.\n\n    The app listens on port 8000 and has two endpoints:\n    - /direct_message: for obtaining responses from direct messages\n    - /channel_mention: for obtaining responses from channel mentions\n    \"\"\"\n    set_up_logging_config(level=20)\n    main(\n        cli=\"app\",\n        host=host,\n        port=port,\n        model=model,\n        model_name=model_name,\n        mode=mode,\n        data_dir=data_dir,\n        which_index=which_index,\n        force_new_index=force_new_index,\n        max_input_size=max_input_size,\n        k=k,\n        chunk_size=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        num_output=num_output,\n        is_path=is_path,\n        n_gpu_layers=n_gpu_layers,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/reginald/cli/#reginald.cli.bot","title":"bot","text":"<pre><code>bot(\n    slack_app_token: Annotated[\n        str,\n        Option(\n            prompt=PROMPTS[slack_app_token],\n            envvar=SLACK_APP_TOKEN,\n            help=HELP_TEXT[slack_app_token],\n        ),\n    ],\n    slack_bot_token: Annotated[\n        str,\n        Option(\n            prompt=PROMPTS[slack_bot_token],\n            envvar=SLACK_BOT_TOKEN,\n            help=HELP_TEXT[slack_bot_token],\n        ),\n    ],\n    api_url: Annotated[\n        str,\n        Option(\n            prompt=PROMPTS[api_url],\n            envvar=REGINALD_API_URL,\n            help=HELP_TEXT[api_url],\n        ),\n    ],\n    emoji: Annotated[\n        str, Option(envvar=REGINALD_EMOJI, help=HELP_TEXT[emoji])\n    ] = EMOJI_DEFAULT,\n) -&gt; None\n</code></pre> <p>Run the Slack bot which sets up the bot (which uses an API for responding to messages) and then establishes a WebSocket connection to the Socket Mode servers and listens for events.</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef bot(\n    slack_app_token: Annotated[\n        str,\n        typer.Option(\n            prompt=PROMPTS[\"slack_app_token\"],\n            envvar=\"SLACK_APP_TOKEN\",\n            help=HELP_TEXT[\"slack_app_token\"],\n        ),\n    ],\n    slack_bot_token: Annotated[\n        str,\n        typer.Option(\n            prompt=PROMPTS[\"slack_bot_token\"],\n            envvar=\"SLACK_BOT_TOKEN\",\n            help=HELP_TEXT[\"slack_bot_token\"],\n        ),\n    ],\n    api_url: Annotated[\n        str,\n        typer.Option(\n            prompt=PROMPTS[\"api_url\"],\n            envvar=\"REGINALD_API_URL\",\n            help=HELP_TEXT[\"api_url\"],\n        ),\n    ],\n    emoji: Annotated[\n        str, typer.Option(envvar=\"REGINALD_EMOJI\", help=HELP_TEXT[\"emoji\"])\n    ] = EMOJI_DEFAULT,\n) -&gt; None:\n    \"\"\"\n    Run the Slack bot which sets up the bot\n    (which uses an API for responding to messages) and\n    then establishes a WebSocket connection to the\n    Socket Mode servers and listens for events.\n    \"\"\"\n    set_up_logging_config(level=20)\n    main(\n        cli=\"bot\",\n        slack_app_token=slack_app_token,\n        slack_bot_token=slack_bot_token,\n        api_url=api_url,\n        emoji=emoji,\n    )\n</code></pre>"},{"location":"reference/reginald/cli/#reginald.cli.chat","title":"chat","text":"<pre><code>chat(\n    model: Annotated[\n        str, Option(envvar=REGINALD_MODEL, help=HELP_TEXT[model])\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        Option(envvar=REGINALD_MODEL_NAME, help=HELP_TEXT[model_name]),\n    ] = None,\n    streaming: Annotated[bool, Option(help=HELP_TEXT[streaming])] = True,\n    mode: Annotated[\n        str, Option(envvar=LLAMA_INDEX_MODE, help=HELP_TEXT[mode])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        Path, Option(envvar=LLAMA_INDEX_DATA_DIR, help=HELP_TEXT[data_dir])\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, Option(envvar=LLAMA_INDEX_WHICH_INDEX, help=HELP_TEXT[which_index])\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        Option(\n            envvar=LLAMA_INDEX_FORCE_NEW_INDEX, help=HELP_TEXT[force_new_index]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        Option(\n            envvar=LLAMA_INDEX_MAX_INPUT_SIZE, help=HELP_TEXT[max_input_size]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, Option(envvar=LLAMA_INDEX_K, help=HELP_TEXT[k])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        Option(envvar=LLAMA_INDEX_CHUNK_SIZE, help=HELP_TEXT[chunk_size]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        Option(\n            envvar=LLAMA_INDEX_CHUNK_OVERLAP_RATIO,\n            help=HELP_TEXT[chunk_overlap_ratio],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, Option(envvar=LLAMA_INDEX_NUM_OUTPUT, help=HELP_TEXT[num_output])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, Option(envvar=LLAMA_INDEX_IS_PATH, help=HELP_TEXT[is_path])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        Option(envvar=LLAMA_INDEX_N_GPU_LAYERS, help=HELP_TEXT[n_gpu_layers]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, Option(envvar=LLAMA_INDEX_DEVICE, help=HELP_TEXT[device])\n    ] = DEFAULT_ARGS[\"device\"],\n) -&gt; None\n</code></pre> <p>Run the chat interaction with the Reginald model.</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef chat(\n    model: Annotated[\n        str,\n        typer.Option(\n            envvar=\"REGINALD_MODEL\",\n            help=HELP_TEXT[\"model\"],\n        ),\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        typer.Option(envvar=\"REGINALD_MODEL_NAME\", help=HELP_TEXT[\"model_name\"]),\n    ] = None,\n    streaming: Annotated[\n        bool,\n        typer.Option(\n            help=HELP_TEXT[\"streaming\"],\n        ),\n    ] = True,\n    mode: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_MODE\", help=HELP_TEXT[\"mode\"])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        pathlib.Path,\n        typer.Option(envvar=\"LLAMA_INDEX_DATA_DIR\", help=HELP_TEXT[\"data_dir\"]),\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str,\n        typer.Option(envvar=\"LLAMA_INDEX_WHICH_INDEX\", help=HELP_TEXT[\"which_index\"]),\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_FORCE_NEW_INDEX\", help=HELP_TEXT[\"force_new_index\"]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_MAX_INPUT_SIZE\", help=HELP_TEXT[\"max_input_size\"]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_K\", help=HELP_TEXT[\"k\"])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        typer.Option(envvar=\"LLAMA_INDEX_CHUNK_SIZE\", help=HELP_TEXT[\"chunk_size\"]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_CHUNK_OVERLAP_RATIO\",\n            help=HELP_TEXT[\"chunk_overlap_ratio\"],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_NUM_OUTPUT\", help=HELP_TEXT[\"num_output\"])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, typer.Option(envvar=\"LLAMA_INDEX_IS_PATH\", help=HELP_TEXT[\"is_path\"])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        typer.Option(envvar=\"LLAMA_INDEX_N_GPU_LAYERS\", help=HELP_TEXT[\"n_gpu_layers\"]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_DEVICE\", help=HELP_TEXT[\"device\"])\n    ] = DEFAULT_ARGS[\"device\"],\n) -&gt; None:\n    \"\"\"\n    Run the chat interaction with the Reginald model.\n    \"\"\"\n    set_up_logging_config(level=40)\n    main(\n        cli=\"chat\",\n        streaming=streaming,\n        model=model,\n        model_name=model_name,\n        mode=mode,\n        data_dir=data_dir,\n        which_index=which_index,\n        force_new_index=force_new_index,\n        max_input_size=max_input_size,\n        k=k,\n        chunk_size=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        num_output=num_output,\n        is_path=is_path,\n        n_gpu_layers=n_gpu_layers,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/reginald/cli/#reginald.cli.create_index","title":"create_index","text":"<pre><code>create_index(\n    data_dir: Annotated[\n        str, Option(envvar=LLAMA_INDEX_DATA_DIR)\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, Option(envvar=LLAMA_INDEX_WHICH_INDEX)\n    ] = DEFAULT_ARGS[\"which_index\"],\n    max_input_size: Annotated[\n        int, Option(envvar=LLAMA_INDEX_MAX_INPUT_SIZE)\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[int, Option(envvar=LLAMA_INDEX_K)] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int], Option(envvar=LLAMA_INDEX_CHUNK_SIZE)\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float, Option(envvar=LLAMA_INDEX_CHUNK_OVERLAP_RATIO)\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, Option(envvar=LLAMA_INDEX_NUM_OUTPUT)\n    ] = DEFAULT_ARGS[\"num_output\"],\n) -&gt; None\n</code></pre> <p>Create an index for the Reginald model.</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef create_index(\n    data_dir: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_DATA_DIR\")\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_WHICH_INDEX\")\n    ] = DEFAULT_ARGS[\"which_index\"],\n    max_input_size: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_MAX_INPUT_SIZE\")\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[int, typer.Option(envvar=\"LLAMA_INDEX_K\")] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int], typer.Option(envvar=\"LLAMA_INDEX_CHUNK_SIZE\")\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float, typer.Option(envvar=\"LLAMA_INDEX_CHUNK_OVERLAP_RATIO\")\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_NUM_OUTPUT\")\n    ] = DEFAULT_ARGS[\"num_output\"],\n) -&gt; None:\n    \"\"\"\n    Create an index for the Reginald model.\n    \"\"\"\n    set_up_logging_config(level=20)\n    main(\n        cli=\"create_index\",\n        data_dir=data_dir,\n        which_index=which_index,\n        max_input_size=max_input_size,\n        k=k,\n        chunk_size=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        num_output=num_output,\n    )\n</code></pre>"},{"location":"reference/reginald/cli/#reginald.cli.download","title":"download","text":"<pre><code>download(\n    data_dir: Annotated[\n        str, Option(envvar=LLAMA_INDEX_DATA_DIR)\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, Option(envvar=LLAMA_INDEX_WHICH_INDEX)\n    ] = DEFAULT_ARGS[\"which_index\"],\n    azure_storage_key: Annotated[\n        Optional[str], Option(envvar=AZURE_STORAGE_KEY)\n    ] = None,\n    connection_str: Annotated[\n        Optional[str], Option(envvar=AZURE_STORAGE_CONNECTION_STR)\n    ] = None,\n) -&gt; None\n</code></pre> <p>Download data from an Azure file share.</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef download(\n    data_dir: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_DATA_DIR\")\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_WHICH_INDEX\")\n    ] = DEFAULT_ARGS[\"which_index\"],\n    azure_storage_key: Annotated[\n        Optional[str], typer.Option(envvar=\"AZURE_STORAGE_KEY\")\n    ] = None,\n    connection_str: Annotated[\n        Optional[str], typer.Option(envvar=\"AZURE_STORAGE_CONNECTION_STR\")\n    ] = None,\n) -&gt; None:\n    \"\"\"\n    Download data from an Azure file share.\n    \"\"\"\n    set_up_logging_config(level=20)\n    main(\n        cli=\"download\",\n        data_dir=data_dir,\n        which_index=which_index,\n        azure_storage_key=azure_storage_key,\n        connection_str=connection_str,\n    )\n</code></pre>"},{"location":"reference/reginald/cli/#reginald.cli.run_all","title":"run_all","text":"<pre><code>run_all(\n    slack_app_token: Annotated[\n        str,\n        Option(\n            prompt=PROMPTS[slack_app_token],\n            envvar=SLACK_APP_TOKEN,\n            help=HELP_TEXT[slack_app_token],\n        ),\n    ],\n    slack_bot_token: Annotated[\n        str,\n        Option(\n            prompt=PROMPTS[slack_bot_token],\n            envvar=SLACK_BOT_TOKEN,\n            help=HELP_TEXT[slack_bot_token],\n        ),\n    ],\n    model: Annotated[\n        str, Option(envvar=REGINALD_MODEL, help=HELP_TEXT[model])\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        Option(envvar=REGINALD_MODEL_NAME, help=HELP_TEXT[model_name]),\n    ] = None,\n    mode: Annotated[\n        str, Option(envvar=LLAMA_INDEX_MODE, help=HELP_TEXT[mode])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        Path, Option(envvar=LLAMA_INDEX_DATA_DIR, help=HELP_TEXT[data_dir])\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str, Option(envvar=LLAMA_INDEX_WHICH_INDEX, help=HELP_TEXT[which_index])\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        Option(\n            envvar=LLAMA_INDEX_FORCE_NEW_INDEX, help=HELP_TEXT[force_new_index]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        Option(\n            envvar=LLAMA_INDEX_MAX_INPUT_SIZE, help=HELP_TEXT[max_input_size]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, Option(envvar=LLAMA_INDEX_K, help=HELP_TEXT[k])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        Option(envvar=LLAMA_INDEX_CHUNK_SIZE, help=HELP_TEXT[chunk_size]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        Option(\n            envvar=LLAMA_INDEX_CHUNK_OVERLAP_RATIO,\n            help=HELP_TEXT[chunk_overlap_ratio],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, Option(envvar=LLAMA_INDEX_NUM_OUTPUT, help=HELP_TEXT[num_output])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, Option(envvar=LLAMA_INDEX_IS_PATH, help=HELP_TEXT[is_path])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        Option(envvar=LLAMA_INDEX_N_GPU_LAYERS, help=HELP_TEXT[n_gpu_layers]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, Option(envvar=LLAMA_INDEX_DEVICE, help=HELP_TEXT[device])\n    ] = DEFAULT_ARGS[\"device\"],\n) -&gt; None\n</code></pre> <p>Run all the components of the Reginald slack bot. Establishes the connection to the Slack API, sets up the bot, and creates a Reginald model to query from.</p> Source code in <code>reginald/cli.py</code> <pre><code>@cli.command()\ndef run_all(\n    slack_app_token: Annotated[\n        str,\n        typer.Option(\n            prompt=PROMPTS[\"slack_app_token\"],\n            envvar=\"SLACK_APP_TOKEN\",\n            help=HELP_TEXT[\"slack_app_token\"],\n        ),\n    ],\n    slack_bot_token: Annotated[\n        str,\n        typer.Option(\n            prompt=PROMPTS[\"slack_bot_token\"],\n            envvar=\"SLACK_BOT_TOKEN\",\n            help=HELP_TEXT[\"slack_bot_token\"],\n        ),\n    ],\n    model: Annotated[\n        str,\n        typer.Option(\n            envvar=\"REGINALD_MODEL\",\n            help=HELP_TEXT[\"model\"],\n        ),\n    ] = DEFAULT_ARGS[\"model\"],\n    model_name: Annotated[\n        Optional[str],\n        typer.Option(envvar=\"REGINALD_MODEL_NAME\", help=HELP_TEXT[\"model_name\"]),\n    ] = None,\n    mode: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_MODE\", help=HELP_TEXT[\"mode\"])\n    ] = DEFAULT_ARGS[\"mode\"],\n    data_dir: Annotated[\n        pathlib.Path,\n        typer.Option(envvar=\"LLAMA_INDEX_DATA_DIR\", help=HELP_TEXT[\"data_dir\"]),\n    ] = DEFAULT_ARGS[\"data_dir\"],\n    which_index: Annotated[\n        str,\n        typer.Option(envvar=\"LLAMA_INDEX_WHICH_INDEX\", help=HELP_TEXT[\"which_index\"]),\n    ] = DEFAULT_ARGS[\"which_index\"],\n    force_new_index: Annotated[\n        bool,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_FORCE_NEW_INDEX\", help=HELP_TEXT[\"force_new_index\"]\n        ),\n    ] = DEFAULT_ARGS[\"force_new_index\"],\n    max_input_size: Annotated[\n        int,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_MAX_INPUT_SIZE\", help=HELP_TEXT[\"max_input_size\"]\n        ),\n    ] = DEFAULT_ARGS[\"max_input_size\"],\n    k: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_K\", help=HELP_TEXT[\"k\"])\n    ] = DEFAULT_ARGS[\"k\"],\n    chunk_size: Annotated[\n        Optional[int],\n        typer.Option(envvar=\"LLAMA_INDEX_CHUNK_SIZE\", help=HELP_TEXT[\"chunk_size\"]),\n    ] = DEFAULT_ARGS.get(\"chunk_size\"),\n    chunk_overlap_ratio: Annotated[\n        float,\n        typer.Option(\n            envvar=\"LLAMA_INDEX_CHUNK_OVERLAP_RATIO\",\n            help=HELP_TEXT[\"chunk_overlap_ratio\"],\n        ),\n    ] = DEFAULT_ARGS[\"chunk_overlap_ratio\"],\n    num_output: Annotated[\n        int, typer.Option(envvar=\"LLAMA_INDEX_NUM_OUTPUT\", help=HELP_TEXT[\"num_output\"])\n    ] = DEFAULT_ARGS[\"num_output\"],\n    is_path: Annotated[\n        bool, typer.Option(envvar=\"LLAMA_INDEX_IS_PATH\", help=HELP_TEXT[\"is_path\"])\n    ] = DEFAULT_ARGS[\"is_path\"],\n    n_gpu_layers: Annotated[\n        int,\n        typer.Option(envvar=\"LLAMA_INDEX_N_GPU_LAYERS\", help=HELP_TEXT[\"n_gpu_layers\"]),\n    ] = DEFAULT_ARGS[\"n_gpu_layers\"],\n    device: Annotated[\n        str, typer.Option(envvar=\"LLAMA_INDEX_DEVICE\", help=HELP_TEXT[\"device\"])\n    ] = DEFAULT_ARGS[\"device\"],\n) -&gt; None:\n    \"\"\"\n    Run all the components of the Reginald slack bot.\n    Establishes the connection to the Slack API, sets up the bot,\n    and creates a Reginald model to query from.\n    \"\"\"\n    set_up_logging_config(level=20)\n    main(\n        cli=\"run_all\",\n        slack_app_token=slack_app_token,\n        slack_bot_token=slack_bot_token,\n        model=model,\n        model_name=model_name,\n        mode=mode,\n        data_dir=data_dir,\n        which_index=which_index,\n        force_new_index=force_new_index,\n        max_input_size=max_input_size,\n        k=k,\n        chunk_size=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        num_output=num_output,\n        is_path=is_path,\n        n_gpu_layers=n_gpu_layers,\n        device=device,\n    )\n</code></pre>"},{"location":"reference/reginald/defaults/","title":"defaults","text":""},{"location":"reference/reginald/run/","title":"run","text":""},{"location":"reference/reginald/run_full_pipeline/","title":"run_full_pipeline","text":""},{"location":"reference/reginald/utils/","title":"utils","text":""},{"location":"reference/reginald/utils/#reginald.utils.create_folder","title":"create_folder","text":"<pre><code>create_folder(folder: str) -&gt; None\n</code></pre> <p>Function to create a folder if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Name of the folder to be created.</p> required Source code in <code>reginald/utils.py</code> <pre><code>def create_folder(folder: str) -&gt; None:\n    \"\"\"\n    Function to create a folder if it does not already exist.\n\n    Parameters\n    ----------\n    folder : str\n        Name of the folder to be created.\n    \"\"\"\n    if not os.path.exists(folder):\n        logging.info(f\"Creating folder '{folder}'\")\n        os.makedirs(folder)\n    else:\n        logging.info(f\"Folder '{folder}' already exists\")\n</code></pre>"},{"location":"reference/reginald/utils/#reginald.utils.get_env_var","title":"get_env_var","text":"<pre><code>get_env_var(\n    var: str, log: bool = True, secret_value: bool = True, default: str = None\n) -&gt; str | None\n</code></pre> <p>Get environment variable. Logs provided if log is True.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>Name of environment variable</p> required <code>log</code> <code>bool</code> <p>Whether or not to log if reading was successful, by default True</p> <code>True</code> <code>secret_value</code> <code>bool</code> <p>Whether or not the value is a secret, by default True. If True, the value will not be logged. Ignored if log is False.</p> <code>True</code> <code>default</code> <code>str</code> <p>Default value if environment variable is not found, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>Value of environment variable, or None if not found</p> Source code in <code>reginald/utils.py</code> <pre><code>def get_env_var(\n    var: str, log: bool = True, secret_value: bool = True, default: str = None\n) -&gt; str | None:\n    \"\"\"\n    Get environment variable. Logs provided if log is True.\n\n    Parameters\n    ----------\n    var : str\n        Name of environment variable\n    log : bool, optional\n        Whether or not to log if reading was successful, by default True\n    secret_value : bool, optional\n        Whether or not the value is a secret, by default True.\n        If True, the value will not be logged.\n        Ignored if log is False.\n    default : str, optional\n        Default value if environment variable is not found, by default None\n\n    Returns\n    -------\n    str | None\n        Value of environment variable, or None if not found\n    \"\"\"\n    if log:\n        logging.info(f\"Trying to get environment variable '{var}'\")\n    value = os.getenv(var, default=default)\n\n    if log:\n        if value is not None:\n            if secret_value:\n                logging.info(f\"Got environment variable '{var}' successfully\")\n            else:\n                logging.info(f\"Got environment variable '{var}' successfully: {value}\")\n        else:\n            logging.warn(f\"Environment variable '{var}' not found.\")\n\n    return value\n</code></pre>"},{"location":"reference/reginald/utils/#reginald.utils.stream_iter_progress_wrapper","title":"stream_iter_progress_wrapper","text":"<pre><code>stream_iter_progress_wrapper(\n    streamer: Iterable | Callable | chain,\n    task_str: str = REGINALD_PROMPT,\n    use_spinner: bool = True,\n    end: str = \"\",\n    *args,\n    **kwargs\n) -&gt; Iterable\n</code></pre> <p>Add a progress bar for iteration.</p> <p>Parameters:</p> Name Type Description Default <code>streamer</code> <code>Iterable | Callable | chain</code> <p><code>Iterable</code>, <code>Callable</code> or <code>chain</code> to add the <code>SpinnerColumn</code> while iteraing over. A <code>Callabe</code> will be converted to a <code>Generator</code>.</p> required <code>task_str</code> <code>str</code> <p>What to print whether <code>use_spinner</code> is <code>True</code> or not, and if <code>use_spinner</code> is <code>True</code> is printed prior to the <code>SpinningColumn</code>.</p> <code>REGINALD_PROMPT</code> <code>use_spinner</code> <code>bool</code> <p>Whether to print the <code>SpinnerColumn</code> or not.</p> <code>True</code> <code>end</code> <code>str</code> <p>What to pass to the <code>end</code> parameter of <code>print</code> calls.</p> <code>''</code> <code>args</code> <p>Any arguments to pass to <code>streamer</code></p> <code>()</code> <code>kwargs</code> <p>Any keyward arguments to pass to <code>streamer</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from time import sleep\n&gt;&gt;&gt; def sleeper(naps: int = 3) -&gt; Generator[str, None, None]:\n...     for nap in range(naps):\n...         sleep(1)\n...         yield f'nap: {nap}'\n&gt;&gt;&gt; tuple(stream_iter_progress_wrapper(streamer=sleeper))\n\nReginald: ('nap: 0', 'nap: 1', 'nap: 2')\n&gt;&gt;&gt; tuple(stream_iter_progress_wrapper(\n...     streamer=sleeper, use_spinner=False))\nReginald: ('nap: 0', 'nap: 1', 'nap: 2')\n</code></pre> Source code in <code>reginald/utils.py</code> <pre><code>def stream_iter_progress_wrapper(\n    streamer: Iterable | Callable | chain,\n    task_str: str = REGINALD_PROMPT,\n    use_spinner: bool = True,\n    end: str = \"\",\n    *args,\n    **kwargs,\n) -&gt; Iterable:\n    \"\"\"Add a progress bar for iteration.\n\n    Parameters\n    ----------\n    streamer\n        `Iterable`, `Callable` or `chain` to add the `SpinnerColumn`\n        while iteraing over. A `Callabe` will be converted to a\n        `Generator`.\n    task_str\n        What to print whether `use_spinner` is `True` or not,\n        and if `use_spinner` is `True` is printed prior to\n        the `SpinningColumn`.\n    use_spinner\n        Whether to print the `SpinnerColumn` or not.\n    end\n        What to pass to the `end` parameter of `print` calls.\n    args\n        Any arguments to pass to `streamer`\n    kwargs\n        Any keyward arguments to pass to `streamer`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from time import sleep\n    &gt;&gt;&gt; def sleeper(naps: int = 3) -&gt; Generator[str, None, None]:\n    ...     for nap in range(naps):\n    ...         sleep(1)\n    ...         yield f'nap: {nap}'\n    &gt;&gt;&gt; tuple(stream_iter_progress_wrapper(streamer=sleeper))\n    &lt;BLANKLINE&gt;\n    Reginald: ('nap: 0', 'nap: 1', 'nap: 2')\n    &gt;&gt;&gt; tuple(stream_iter_progress_wrapper(\n    ...     streamer=sleeper, use_spinner=False))\n    Reginald: ('nap: 0', 'nap: 1', 'nap: 2')\n    \"\"\"\n    if isinstance(streamer, Callable):\n        streamer = streamer(*args, **kwargs)\n    if use_spinner:\n        with Progress(\n            TextColumn(\"{task.description}[progress.description]\"),\n            SpinnerColumn(),\n            transient=True,\n        ) as progress:\n            if isinstance(streamer, list | tuple):\n                streamer = (item for item in streamer)\n            assert isinstance(streamer, Generator)\n            progress.add_task(task_str)\n            first_item = next(streamer)\n            streamer = chain((first_item,), streamer)\n    print(task_str, end=end)\n    return streamer\n</code></pre>"},{"location":"reference/reginald/utils/#reginald.utils.stream_progress_wrapper","title":"stream_progress_wrapper","text":"<pre><code>stream_progress_wrapper(\n    streamer: Callable,\n    task_str: str = REGINALD_PROMPT,\n    use_spinner: bool = True,\n    end: str = \"\\n\",\n    *args,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Add a progress bar for iteration.</p> <p>Parameters:</p> Name Type Description Default <code>streamer</code> <code>Callable</code> <p>Funciton to add the <code>SpinnerColumn</code> while running</p> required <code>task_str</code> <code>str</code> <p>What to print whether <code>use_spinner</code> is <code>True</code> or not, and if <code>use_spinner</code> is <code>True</code> is printed prior to the <code>SpinningColumn</code>.</p> <code>REGINALD_PROMPT</code> <code>use_spinner</code> <code>bool</code> <p>Whether to print the <code>SpinnerColumn</code> or not.</p> <code>True</code> <code>end</code> <code>str</code> <p>What to pass to the <code>end</code> parameter of <code>print</code> calls.</p> <code>'\\n'</code> <code>args</code> <p>Any arguments to pass to <code>streamer</code></p> <code>()</code> <code>kwargs</code> <p>Any keyward arguments to pass to <code>streamer</code>.</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from time import sleep\n&gt;&gt;&gt; def sleeper(seconds: int = 3) -&gt; str:\n...     sleep(seconds)\n...     return f'{seconds} seconds nap'\n&gt;&gt;&gt; stream_progress_wrapper(sleeper)\n\nReginald:\n'3 seconds nap'\n&gt;&gt;&gt; stream_progress_wrapper(sleeper, use_spinner=False, end='')\nReginald: '3 seconds nap'\n</code></pre> Source code in <code>reginald/utils.py</code> <pre><code>def stream_progress_wrapper(\n    streamer: Callable,\n    task_str: str = REGINALD_PROMPT,\n    use_spinner: bool = True,\n    end: str = \"\\n\",\n    *args,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Add a progress bar for iteration.\n\n    Parameters\n    ----------\n    streamer\n        Funciton to add the `SpinnerColumn` while running\n    task_str\n        What to print whether `use_spinner` is `True` or not,\n        and if `use_spinner` is `True` is printed prior to\n        the `SpinningColumn`.\n    use_spinner\n        Whether to print the `SpinnerColumn` or not.\n    end\n        What to pass to the `end` parameter of `print` calls.\n    args\n        Any arguments to pass to `streamer`\n    kwargs\n        Any keyward arguments to pass to `streamer`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from time import sleep\n    &gt;&gt;&gt; def sleeper(seconds: int = 3) -&gt; str:\n    ...     sleep(seconds)\n    ...     return f'{seconds} seconds nap'\n    &gt;&gt;&gt; stream_progress_wrapper(sleeper)\n    &lt;BLANKLINE&gt;\n    Reginald:\n    '3 seconds nap'\n    &gt;&gt;&gt; stream_progress_wrapper(sleeper, use_spinner=False, end='')\n    Reginald: '3 seconds nap'\n    \"\"\"\n    if use_spinner:\n        with Progress(\n            TextColumn(\"{task.description}[progress.description]\"),\n            SpinnerColumn(),\n            transient=True,\n        ) as progress:\n            progress.add_task(task_str)\n            results: Any = streamer(*args, **kwargs)\n        print(task_str, end=end)\n        return results\n    else:\n        print(task_str, end=end)\n        return streamer(*args, **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/","title":"models","text":""},{"location":"reference/reginald/models/app/","title":"app","text":""},{"location":"reference/reginald/models/base/","title":"base","text":""},{"location":"reference/reginald/models/base/#reginald.models.base.MessageResponse","title":"MessageResponse","text":"Source code in <code>reginald/models/base.py</code> <pre><code>class MessageResponse:\n    def __init__(self, message: Optional[str]) -&gt; None:\n        \"\"\"\n        Class for holding the response message.\n\n        Parameters\n        ----------\n        message : Optional[str]\n            Message to send back to Slack\n        \"\"\"\n        self.message = message\n</code></pre>"},{"location":"reference/reginald/models/base/#reginald.models.base.MessageResponse.__init__","title":"__init__","text":"<pre><code>__init__(message: Optional[str]) -&gt; None\n</code></pre> <p>Class for holding the response message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[str]</code> <p>Message to send back to Slack</p> required Source code in <code>reginald/models/base.py</code> <pre><code>def __init__(self, message: Optional[str]) -&gt; None:\n    \"\"\"\n    Class for holding the response message.\n\n    Parameters\n    ----------\n    message : Optional[str]\n        Message to send back to Slack\n    \"\"\"\n    self.message = message\n</code></pre>"},{"location":"reference/reginald/models/base/#reginald.models.base.ResponseModel","title":"ResponseModel","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>reginald/models/base.py</code> <pre><code>class ResponseModel(ABC):\n    def __init__(self, emoji: Optional[str], *args: Any, **kwargs: Any):\n        \"\"\"\n        When the strategy receives a message it should\n        return a `MessageResponse` where both are optional.\n\n        Parameters\n        ----------\n        emoji : Optional[str]\n            Emoji to use for the bot's response\n        \"\"\"\n        self.emoji = emoji\n        self.mode = \"NA\"\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        raise NotImplementedError\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        raise NotImplementedError\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/reginald/models/base/#reginald.models.base.ResponseModel.__init__","title":"__init__","text":"<pre><code>__init__(emoji: Optional[str], *args: Any, **kwargs: Any)\n</code></pre> <p>When the strategy receives a message it should return a <code>MessageResponse</code> where both are optional.</p> <p>Parameters:</p> Name Type Description Default <code>emoji</code> <code>Optional[str]</code> <p>Emoji to use for the bot\u2019s response</p> required Source code in <code>reginald/models/base.py</code> <pre><code>def __init__(self, emoji: Optional[str], *args: Any, **kwargs: Any):\n    \"\"\"\n    When the strategy receives a message it should\n    return a `MessageResponse` where both are optional.\n\n    Parameters\n    ----------\n    emoji : Optional[str]\n        Emoji to use for the bot's response\n    \"\"\"\n    self.emoji = emoji\n    self.mode = \"NA\"\n</code></pre>"},{"location":"reference/reginald/models/chat_interact/","title":"chat_interact","text":""},{"location":"reference/reginald/models/create_index/","title":"create_index","text":""},{"location":"reference/reginald/models/create_index/#reginald.models.create_index.DummyLLM","title":"DummyLLM","text":"<p>               Bases: <code>CustomLLM</code></p> <p>Dummy LLM for passing into the Settings below to create the index. The minimum required attributes are set here, but this LLM is not used anywhere else.</p> Source code in <code>reginald/models/create_index.py</code> <pre><code>class DummyLLM(CustomLLM):\n    \"\"\"\n    Dummy LLM for passing into the Settings below to create the index.\n    The minimum required attributes are set here, but this LLM is not used anywhere else.\n    \"\"\"\n\n    context_window: int = 1024\n    num_output: int = 256\n    model_name: str = \"dummy\"\n    dummy_response: str = \"This is a dummy model\"\n\n    @property\n    def metadata(self) -&gt; LLMMetadata:\n        return LLMMetadata(\n            context_window=self.context_window,\n            num_output=self.num_output,\n            model_name=self.model_name,\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponse:\n        return CompletionResponse(text=self.dummy_response)\n\n    @llm_completion_callback()\n    def stream_complete(self, prompt: str, **kwargs: Any) -&gt; CompletionResponseGen:\n        response = \"\"\n        for token in self.dummy_response:\n            response += token\n            yield CompletionResponse(text=response, delta=token)\n</code></pre>"},{"location":"reference/reginald/models/download_from_fileshare/","title":"download_from_fileshare","text":""},{"location":"reference/reginald/models/setup_llm/","title":"setup_llm","text":""},{"location":"reference/reginald/models/setup_llm/#reginald.models.setup_llm.setup_llm","title":"setup_llm","text":"<pre><code>setup_llm(\n    model: str | None = None,\n    model_name: str | None = None,\n    mode: str | None = None,\n    data_dir: Path | str | None = None,\n    which_index: str | None = None,\n    force_new_index: bool | str | None = None,\n    max_input_size: int | str | None = None,\n    k: int | str | None = None,\n    chunk_size: int | str | None = None,\n    chunk_overlap_ratio: float | str | None = None,\n    num_output: int | str | None = None,\n    is_path: bool | str | None = None,\n    n_gpu_layers: int | str | None = None,\n    device: str | None = None,\n) -&gt; ResponseModel\n</code></pre> <p>Set up a query or chat engine with an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | None</code> <p>Which type of model to use, by default None (uses \u201chello\u201d model)</p> <code>None</code> <code>model_name</code> <code>str | None</code> <p>Select which sub-model to use within the model requested. For example, if by model is \u201cllama-index-hf\u201d, this specifies the Huggingface model which we would like to use, default None. This is ignored if using \u2018hello\u2019 model. Otherwise, the defaults are set in <code>reginald/models/models/__init__.py</code></p> <code>None</code> <code>mode</code> <code>str | None</code> <p>Select which mode to use between \u201cchat\u201d and \u201cquery\u201d, by default None (uses \u201cchat\u201d). This is ignored if not using llama-index</p> <code>None</code> <code>data_dir</code> <code>Path | str | None</code> <p>Location of the data, by default None (uses the data directory in the root of the repo). If this is a string, it is converted to a pathlib.Path</p> <code>None</code> <code>which_index</code> <code>str | None</code> <p>Specifies the directory name for looking up/writing indices. Currently supports \u2018handbook\u2019, \u2018wikis\u2019, \u2018public\u2019, or \u2018all_data\u2019. By default None (uses \u2018all_data\u2019)</p> <code>None</code> <code>force_new_index</code> <code>bool | str | None</code> <p>Whether to recreate the index vector store or not, by default None (uses False). If this is a string, it is converted to a boolean using <code>force_new_index.lower() == \"true\"</code>.</p> <code>None</code> <code>max_input_size</code> <code>int | str | None</code> <p>Select the maximum input size for the model, by default None (uses 4096). Ignored if not using \u201cllama-index-llama-cpp\u201d or \u201cllama-index-hf\u201d models, If this is a string, it is converted to an integer</p> <code>None</code> <code>k</code> <code>int | str | None</code> <p><code>similarity_top_k</code> to use in chat or query engine, by default None (uses 3). If this is a string, it is converted to an integer</p> <code>None</code> <code>chunk_size</code> <code>int | str | None</code> <p>Maximum size of chunks to use, by default None. If None, this is computed as <code>ceil(max_input_size / k)</code>. If this is a string, it is converted to an integer</p> <code>None</code> <code>chunk_overlap_ratio</code> <code>float | str | None</code> <p>Chunk overlap as a ratio of chunk size, by default None (uses 0.1). If this is a string, it is converted to a float</p> <code>None</code> <code>num_output</code> <code>int | str | None</code> <p>Number of outputs for the LLM, by default None (uses 512). If this is a string, it is converted to an integer</p> <code>None</code> <code>is_path</code> <code>bool | str | None</code> <p>Whether or not model_name is used as a path to the model file, otherwise it should be the URL to download the model, by default None (uses False). If this is a string, it is converted to a boolean using <code>is_path.lower() == \"true\"</code>. Ignored if not using \u201cllama-index-llama-cpp\u201d model</p> <code>None</code> <code>n_gpu_layers</code> <code>int | str | None</code> <p>Select the number of GPU layers to use. If -1, all layers are offloaded to the GPU. If 0, no layers are offloaded to the GPU, by default None (uses 0). Ignored if not using \u201cllama-index-llama-cpp\u201d model. If this is a string, it is converted to an integer</p> <code>None</code> <code>device</code> <code>str | None</code> <p>Select which device to use, by default None (uses \u201cauto\u201d). Ignored if not using \u201cllama-index-llama-cpp\u201d or \u201cllama-index-hf\u201d models</p> <code>None</code> <p>Returns:</p> Type Description <code>ResponseModel</code> <p>Sets up query or chat engine with an LLM that has methods <code>direct_message</code> and <code>channel_mention</code>, which takes a message and user_id and returns a <code>MessageResponse</code> object with the response message.</p> Source code in <code>reginald/models/setup_llm.py</code> <pre><code>def setup_llm(\n    model: str | None = None,\n    model_name: str | None = None,\n    mode: str | None = None,\n    data_dir: pathlib.Path | str | None = None,\n    which_index: str | None = None,\n    force_new_index: bool | str | None = None,\n    max_input_size: int | str | None = None,\n    k: int | str | None = None,\n    chunk_size: int | str | None = None,\n    chunk_overlap_ratio: float | str | None = None,\n    num_output: int | str | None = None,\n    is_path: bool | str | None = None,\n    n_gpu_layers: int | str | None = None,\n    device: str | None = None,\n) -&gt; ResponseModel:\n    \"\"\"\n    Set up a query or chat engine with an LLM.\n\n    Parameters\n    ----------\n    model : str | None, optional\n        Which type of model to use, by default None (uses \"hello\" model)\n    model_name : str | None, optional\n        Select which sub-model to use within the model requested.\n        For example, if by model is \"llama-index-hf\", this specifies\n        the Huggingface model which we would like to use, default None.\n        This is ignored if using 'hello' model.\n        Otherwise, the defaults are set in `reginald/models/models/__init__.py`\n    mode : str | None, optional\n        Select which mode to use between \"chat\" and \"query\",\n        by default None (uses \"chat\"). This is ignored if not using\n        llama-index\n    data_dir : pathlib.Path | str | None, optional\n        Location of the data, by default None\n        (uses the data directory in the root of the repo).\n        If this is a string, it is converted to a pathlib.Path\n    which_index : str | None, optional\n        Specifies the directory name for looking up/writing indices.\n        Currently supports 'handbook', 'wikis', 'public', or 'all_data'.\n        By default None (uses 'all_data')\n    force_new_index : bool | str | None, optional\n        Whether to recreate the index vector store or not, by default None\n        (uses False). If this is a string, it is converted to a boolean\n        using `force_new_index.lower() == \"true\"`.\n    max_input_size : int | str | None, optional\n        Select the maximum input size for the model, by default None\n        (uses 4096). Ignored if not using \"llama-index-llama-cpp\" or\n        \"llama-index-hf\" models, If this is a string, it is converted\n        to an integer\n    k : int | str | None, optional\n        `similarity_top_k` to use in chat or query engine,\n        by default None (uses 3). If this is a string, it is converted\n        to an integer\n    chunk_size : int | str | None, optional\n        Maximum size of chunks to use, by default None.\n        If None, this is computed as `ceil(max_input_size / k)`.\n        If this is a string, it is converted to an integer\n    chunk_overlap_ratio : float | str | None, optional\n        Chunk overlap as a ratio of chunk size, by default None (uses 0.1).\n        If this is a string, it is converted to a float\n    num_output : int | str | None, optional\n        Number of outputs for the LLM, by default None (uses 512).\n        If this is a string, it is converted to an integer\n    is_path : bool | str | None, optional\n        Whether or not model_name is used as a path to the model file,\n        otherwise it should be the URL to download the model,\n        by default None (uses False). If this is a string, it is\n        converted to a boolean using `is_path.lower() == \"true\"`.\n        Ignored if not using \"llama-index-llama-cpp\" model\n    n_gpu_layers : int | str | None, optional\n        Select the number of GPU layers to use. If -1, all layers are\n        offloaded to the GPU. If 0, no layers are offloaded to the GPU,\n        by default None (uses 0). Ignored if not using\n        \"llama-index-llama-cpp\" model. If this is a string, it is\n        converted to an integer\n    device : str | None, optional\n        Select which device to use, by default None (uses \"auto\").\n        Ignored if not using \"llama-index-llama-cpp\" or \"llama-index-hf\" models\n\n    Returns\n    -------\n    ResponseModel\n        Sets up query or chat engine with an LLM that has\n        methods `direct_message` and `channel_mention`, which takes\n        a message and user_id and returns a `MessageResponse` object\n        with the response message.\n    \"\"\"\n    # default for model\n    if model is None:\n        model = DEFAULT_ARGS[\"model\"]\n    model = model.lower()\n    if model not in ModelMapper.available_models():\n        logging.error(f\"Model '{model}' not recognised.\")\n        sys.exit(1)\n    logging.info(f\"Setting up '{model}' model.\")\n\n    # defaulf for model_name\n    if model_name is None:\n        model_name = DEFAULTS[model]\n\n    # default for mode\n    if mode is None:\n        mode = DEFAULT_ARGS[\"mode\"]\n\n    # default for data_dir\n    if data_dir is None:\n        # data_dir by default is the data directory in the root of the repo\n        data_dir = DEFAULT_ARGS[\"data_dir\"]\n    data_dir = pathlib.Path(data_dir).resolve()\n\n    # default for which_index\n    if which_index is None:\n        which_index = DEFAULT_ARGS[\"which_index\"]\n\n    # default for force_new_index\n    if force_new_index is None:\n        force_new_index = DEFAULT_ARGS[\"force_new_index\"]\n    if isinstance(force_new_index, str):\n        force_new_index = force_new_index.lower() == \"true\"\n\n    # default for max_input_size\n    if max_input_size is None:\n        max_input_size = DEFAULT_ARGS[\"max_input_size\"]\n    if isinstance(max_input_size, str):\n        max_input_size = int(max_input_size)\n\n    # default for k\n    if k is None:\n        k = DEFAULT_ARGS[\"k\"]\n    if isinstance(k, str):\n        k = int(k)\n\n    # convert chunk_size if provided as str\n    # default is computed later using values of max_input_size and k\n    if isinstance(chunk_size, str):\n        chunk_size = int(chunk_size)\n\n    # default for chunk_overlap_ratio\n    if chunk_overlap_ratio is None:\n        chunk_overlap_ratio = DEFAULT_ARGS[\"chunk_overlap_ratio\"]\n    if isinstance(chunk_overlap_ratio, str):\n        chunk_overlap_ratio = float(chunk_overlap_ratio)\n\n    # default for num_output\n    if num_output is None:\n        num_output = DEFAULT_ARGS[\"num_output\"]\n    if isinstance(num_output, str):\n        num_output = int(num_output)\n\n    # default for is_path\n    if is_path is None:\n        is_path = DEFAULT_ARGS[\"is_path\"]\n    if isinstance(is_path, str):\n        is_path = is_path.lower() == \"true\"\n\n    # default for n_gpu_layers\n    if n_gpu_layers is None:\n        n_gpu_layers = DEFAULT_ARGS[\"n_gpu_layers\"]\n    if isinstance(n_gpu_layers, str):\n        n_gpu_layers = int(n_gpu_layers)\n\n    # default for device\n    if device is None:\n        device = DEFAULT_ARGS[\"device\"]\n\n    # set up response model\n    model = ModelMapper.get_model(model)\n    response_model = model(\n        model_name=model_name,\n        max_input_size=max_input_size,\n        data_dir=data_dir,\n        which_index=which_index,\n        mode=mode,\n        k=k,\n        chunk_size=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        num_output=num_output,\n        force_new_index=force_new_index,\n        is_path=is_path,\n        n_gpu_layers=n_gpu_layers,\n        device=device,\n    )\n\n    return response_model\n</code></pre>"},{"location":"reference/reginald/models/llama_index/","title":"llama_index","text":""},{"location":"reference/reginald/models/llama_index/base/","title":"base","text":""},{"location":"reference/reginald/models/llama_index/base/#reginald.models.llama_index.base.LlamaIndex","title":"LlamaIndex","text":"<p>               Bases: <code>ResponseModel</code></p> Source code in <code>reginald/models/llama_index/base.py</code> <pre><code>class LlamaIndex(ResponseModel):\n    def __init__(\n        self,\n        model_name: str,\n        max_input_size: int,\n        data_dir: pathlib.Path | str,\n        which_index: str,\n        mode: str = \"chat\",\n        k: int = 3,\n        chunk_size: int | None = None,\n        chunk_overlap_ratio: float = 0.1,\n        num_output: int = 512,\n        force_new_index: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Base class for models using llama-index.\n        This class is not intended to be used directly, but rather subclassed\n        to implement the `_prep_llm` method which constructs the LLM to be used.\n\n        Parameters\n        ----------\n        model_name : str\n            Model name to specify which LLM to use.\n        max_input_size : int\n            Context window size for the LLM.\n        data_dir : pathlib.Path | str\n            Path to the data directory.\n        which_index : str\n            Which index to construct (if force_new_index is True) or use.\n            Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n        mode : Optional[str], optional\n            The type of engine to use when interacting with the data, options of \"chat\" or \"query\".\n            Default is \"chat\".\n        k : int, optional\n            `similarity_top_k` to use in chat or query engine, by default 3\n        chunk_size : int | None, optional\n            Maximum size of chunks to use, by default None.\n            If None, this is computed as `ceil(max_input_size / k)`.\n        chunk_overlap_ratio : float, optional\n            Chunk overlap as a ratio of chunk size, by default 0.1\n        num_output : int, optional\n            Number of outputs for the LLM, by default 512\n        force_new_index : bool, optional\n            Whether or not to recreate the index vector store,\n            by default False\n        \"\"\"\n        super().__init__(*args, emoji=\"llama\", **kwargs)\n        logging.info(\"Setting up Huggingface backend.\")\n        if mode == \"chat\":\n            logging.info(\"Setting up chat engine.\")\n        elif mode == \"query\":\n            logging.info(\"Setting up query engine.\")\n        else:\n            logging.error(\"Mode must either be 'query' or 'chat'.\")\n            sys.exit(1)\n\n        self.max_input_size = max_input_size\n        self.model_name = model_name\n        self.num_output = num_output\n        self.mode = mode\n        self.k = k\n        self.chunk_size = chunk_size or compute_default_chunk_size(\n            max_input_size=max_input_size, k=k\n        )\n        self.chunk_overlap_ratio = chunk_overlap_ratio\n        self.data_dir = pathlib.Path(data_dir)\n        self.which_index = which_index\n        self.documents = []\n\n        # set up LLM\n        llm = self._prep_llm()\n\n        # set up settings\n        settings = setup_settings(\n            llm=llm,\n            max_input_size=self.max_input_size,\n            num_output=self.num_output,\n            chunk_size=self.chunk_size,\n            chunk_overlap_ratio=self.chunk_overlap_ratio,\n            k=self.k,\n            tokenizer=self._prep_tokenizer(),\n        )\n\n        if force_new_index:\n            logging.info(\"Generating the index from scratch...\")\n            data_creator = DataIndexCreator(\n                which_index=self.which_index,\n                data_dir=self.data_dir,\n                settings=settings,\n            )\n            self.index: VectorStoreIndex = stream_progress_wrapper(\n                data_creator.create_index,\n                task_str=\"Generating the index from scratch...\",\n            )\n            stream_progress_wrapper(\n                data_creator.save_index,\n                task_str=\"Saving the index...\",\n            )\n\n        else:\n            logging.info(\"Loading the storage context\")\n            storage_context = stream_progress_wrapper(\n                StorageContext.from_defaults,\n                task_str=\"Loading the storage context...\",\n                persist_dir=self.data_dir / LLAMA_INDEX_DIR / self.which_index,\n            )\n\n            logging.info(\"Loading the pre-processed index\")\n            self.index = stream_progress_wrapper(\n                load_index_from_storage,\n                task_str=\"Loading the pre-processed index...\",\n                storage_context=storage_context,\n                settings=settings,\n            )\n\n        self.response_mode = \"simple_summarize\"\n        if self.mode == \"chat\":\n            self.chat_engine = {}\n            logging.info(\"Done setting up Huggingface backend for chat engine.\")\n        elif self.mode == \"query\":\n            self.query_engine = self.index.as_query_engine(\n                response_mode=self.response_mode,\n                similarity_top_k=k,\n            )\n            logging.info(\"Done setting up Huggingface backend for query engine.\")\n\n        self.error_response_template = (\n            \"Oh no! When I tried to get a response to your prompt, \"\n            \"I got the following error:\"\n            \"\\n```\\n{}\\n```\"\n        )\n\n    @staticmethod\n    def _format_sources(response: RESPONSE_TYPE) -&gt; str:\n        \"\"\"\n        Method to format the sources used to compose the response.\n\n        Parameters\n        ----------\n        response : RESPONSE_TYPE\n            response object from the query engine\n\n        Returns\n        -------\n        str\n            String containing the formatted sources that\n            were used to compose the response\n        \"\"\"\n        texts = []\n        for source_node in response.source_nodes:\n            # obtain the URL for source\n            try:\n                node_url = source_node.node.extra_info[\"url\"]\n            except KeyError:\n                node_url = source_node.node.extra_info[\"filename\"]\n\n            # add its similarity score and append to texts\n            source_text = node_url + f\" (similarity: {round(source_node.score, 2)})\"\n            texts.append(source_text)\n\n        result = \"I read the following documents to compose this answer:\\n\"\n        result += \"\\n\\n\".join(texts)\n\n        return result\n\n    def _prep_llm(self) -&gt; BaseLLM:\n        \"\"\"\n        Method to prepare the LLM to be used.\n\n        Returns\n        -------\n        BaseLLM\n            LLM to be used.\n\n        Raises\n        ------\n        NotImplemented\n            This must be implemented by a subclass of LlamaIndex.\n        \"\"\"\n        raise NotImplementedError(\n            \"_prep_llm needs to be implemented by a subclass of LlamaIndex.\"\n        )\n\n    def _prep_tokenizer(self) -&gt; Callable[[str], int] | None:\n        \"\"\"\n        Method to prepare the Tokenizer to be used.\n\n        Returns\n        -------\n        Callable[[str], int] | None\n            Tokenizer to use. A callable function on a string.\n            Can also be None if using the default set by LlamaIndex.\n\n        Raises\n        ------\n        NotImplemented\n        \"\"\"\n        raise NotImplementedError(\n            \"_prep_tokenizer needs to be implemented by a subclass of LlamaIndex.\"\n        )\n\n    def _get_response(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        try:\n            if self.mode == \"chat\":\n                # create chat engine for user if does not exist\n                if self.chat_engine.get(user_id) is None:\n                    self.chat_engine[user_id] = self.index.as_chat_engine(\n                        chat_mode=\"context\",\n                        response_mode=self.response_mode,\n                        similarity_top_k=self.k,\n                    )\n\n                # obtain chat engine for particular user\n                chat_engine = self.chat_engine[user_id]\n                response = chat_engine.chat(message)\n            elif self.mode == \"query\":\n                self.query_engine._response_synthesizer._streaming = False\n                response = self.query_engine.query(message)\n\n            # concatenate the response with the resources that it used\n            formatted_response = (\n                response.response + \"\\n\\n\\n\" + self._format_sources(response)\n            )\n        except Exception as e:  # ignore: broad-except\n            formatted_response = self.error_response_template.format(repr(e))\n\n        pattern = (\n            r\"(?s)^Context information is\"\n            r\".*\"\n            r\"Given the context information and not prior knowledge, answer the question: \"\n            rf\"{message}\"\n            r\"\\n(.*)\"\n        )\n        m = re.search(pattern, formatted_response)\n\n        if m:\n            answer = m.group(1)\n        else:\n            logging.warning(\n                \"Was expecting a backend response with a regular expression but couldn't find a match.\"\n            )\n            answer = formatted_response\n\n        return MessageResponse(answer)\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a direct message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._get_response(message=message, user_id=user_id)\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a channel mention in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._get_response(message=message, user_id=user_id)\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        \"\"\"\n        Method to respond to a stream message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        try:\n            if self.mode == \"chat\":\n                # create chat engine for user if does not exist\n                if self.chat_engine.get(user_id) is None:\n                    self.chat_engine[user_id] = self.index.as_chat_engine(\n                        chat_mode=\"context\",\n                        response_mode=self.response_mode,\n                        similarity_top_k=self.k,\n                        streaming=True,\n                    )\n\n                # obtain chat engine for particular user\n                chat_engine = self.chat_engine[user_id]\n                response_stream = chat_engine.stream_chat(message)\n            elif self.mode == \"query\":\n                self.query_engine._response_synthesizer._streaming = True\n                response_stream = self.query_engine.query(message)\n\n            for token in stream_iter_progress_wrapper(response_stream.response_gen):\n                print(token, end=\"\", flush=True)\n\n            formatted_response = \"\\n\\n\\n\" + self._format_sources(response_stream)\n\n            for token in re.split(r\"(\\s+)\", formatted_response):\n                print(token, end=\"\", flush=True)\n        except Exception as e:  # ignore: broad-except\n            for token in re.split(\n                r\"(\\s+)\", self.error_response_template.format(repr(e))\n            ):\n                print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/base/#reginald.models.llama_index.base.LlamaIndex.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name: str,\n    max_input_size: int,\n    data_dir: Path | str,\n    which_index: str,\n    mode: str = \"chat\",\n    k: int = 3,\n    chunk_size: int | None = None,\n    chunk_overlap_ratio: float = 0.1,\n    num_output: int = 512,\n    force_new_index: bool = False,\n    *args,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Base class for models using llama-index. This class is not intended to be used directly, but rather subclassed to implement the <code>_prep_llm</code> method which constructs the LLM to be used.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name to specify which LLM to use.</p> required <code>max_input_size</code> <code>int</code> <p>Context window size for the LLM.</p> required <code>data_dir</code> <code>Path | str</code> <p>Path to the data directory.</p> required <code>which_index</code> <code>str</code> <p>Which index to construct (if force_new_index is True) or use. Options are \u201chandbook\u201d, \u201cwikis\u201d,  \u201cpublic\u201d, \u201creg\u201d or \u201call_data\u201d.</p> required <code>mode</code> <code>Optional[str]</code> <p>The type of engine to use when interacting with the data, options of \u201cchat\u201d or \u201cquery\u201d. Default is \u201cchat\u201d.</p> <code>'chat'</code> <code>k</code> <code>int</code> <p><code>similarity_top_k</code> to use in chat or query engine, by default 3</p> <code>3</code> <code>chunk_size</code> <code>int | None</code> <p>Maximum size of chunks to use, by default None. If None, this is computed as <code>ceil(max_input_size / k)</code>.</p> <code>None</code> <code>chunk_overlap_ratio</code> <code>float</code> <p>Chunk overlap as a ratio of chunk size, by default 0.1</p> <code>0.1</code> <code>num_output</code> <code>int</code> <p>Number of outputs for the LLM, by default 512</p> <code>512</code> <code>force_new_index</code> <code>bool</code> <p>Whether or not to recreate the index vector store, by default False</p> <code>False</code> Source code in <code>reginald/models/llama_index/base.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    max_input_size: int,\n    data_dir: pathlib.Path | str,\n    which_index: str,\n    mode: str = \"chat\",\n    k: int = 3,\n    chunk_size: int | None = None,\n    chunk_overlap_ratio: float = 0.1,\n    num_output: int = 512,\n    force_new_index: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Base class for models using llama-index.\n    This class is not intended to be used directly, but rather subclassed\n    to implement the `_prep_llm` method which constructs the LLM to be used.\n\n    Parameters\n    ----------\n    model_name : str\n        Model name to specify which LLM to use.\n    max_input_size : int\n        Context window size for the LLM.\n    data_dir : pathlib.Path | str\n        Path to the data directory.\n    which_index : str\n        Which index to construct (if force_new_index is True) or use.\n        Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n    mode : Optional[str], optional\n        The type of engine to use when interacting with the data, options of \"chat\" or \"query\".\n        Default is \"chat\".\n    k : int, optional\n        `similarity_top_k` to use in chat or query engine, by default 3\n    chunk_size : int | None, optional\n        Maximum size of chunks to use, by default None.\n        If None, this is computed as `ceil(max_input_size / k)`.\n    chunk_overlap_ratio : float, optional\n        Chunk overlap as a ratio of chunk size, by default 0.1\n    num_output : int, optional\n        Number of outputs for the LLM, by default 512\n    force_new_index : bool, optional\n        Whether or not to recreate the index vector store,\n        by default False\n    \"\"\"\n    super().__init__(*args, emoji=\"llama\", **kwargs)\n    logging.info(\"Setting up Huggingface backend.\")\n    if mode == \"chat\":\n        logging.info(\"Setting up chat engine.\")\n    elif mode == \"query\":\n        logging.info(\"Setting up query engine.\")\n    else:\n        logging.error(\"Mode must either be 'query' or 'chat'.\")\n        sys.exit(1)\n\n    self.max_input_size = max_input_size\n    self.model_name = model_name\n    self.num_output = num_output\n    self.mode = mode\n    self.k = k\n    self.chunk_size = chunk_size or compute_default_chunk_size(\n        max_input_size=max_input_size, k=k\n    )\n    self.chunk_overlap_ratio = chunk_overlap_ratio\n    self.data_dir = pathlib.Path(data_dir)\n    self.which_index = which_index\n    self.documents = []\n\n    # set up LLM\n    llm = self._prep_llm()\n\n    # set up settings\n    settings = setup_settings(\n        llm=llm,\n        max_input_size=self.max_input_size,\n        num_output=self.num_output,\n        chunk_size=self.chunk_size,\n        chunk_overlap_ratio=self.chunk_overlap_ratio,\n        k=self.k,\n        tokenizer=self._prep_tokenizer(),\n    )\n\n    if force_new_index:\n        logging.info(\"Generating the index from scratch...\")\n        data_creator = DataIndexCreator(\n            which_index=self.which_index,\n            data_dir=self.data_dir,\n            settings=settings,\n        )\n        self.index: VectorStoreIndex = stream_progress_wrapper(\n            data_creator.create_index,\n            task_str=\"Generating the index from scratch...\",\n        )\n        stream_progress_wrapper(\n            data_creator.save_index,\n            task_str=\"Saving the index...\",\n        )\n\n    else:\n        logging.info(\"Loading the storage context\")\n        storage_context = stream_progress_wrapper(\n            StorageContext.from_defaults,\n            task_str=\"Loading the storage context...\",\n            persist_dir=self.data_dir / LLAMA_INDEX_DIR / self.which_index,\n        )\n\n        logging.info(\"Loading the pre-processed index\")\n        self.index = stream_progress_wrapper(\n            load_index_from_storage,\n            task_str=\"Loading the pre-processed index...\",\n            storage_context=storage_context,\n            settings=settings,\n        )\n\n    self.response_mode = \"simple_summarize\"\n    if self.mode == \"chat\":\n        self.chat_engine = {}\n        logging.info(\"Done setting up Huggingface backend for chat engine.\")\n    elif self.mode == \"query\":\n        self.query_engine = self.index.as_query_engine(\n            response_mode=self.response_mode,\n            similarity_top_k=k,\n        )\n        logging.info(\"Done setting up Huggingface backend for query engine.\")\n\n    self.error_response_template = (\n        \"Oh no! When I tried to get a response to your prompt, \"\n        \"I got the following error:\"\n        \"\\n```\\n{}\\n```\"\n    )\n</code></pre>"},{"location":"reference/reginald/models/llama_index/base/#reginald.models.llama_index.base.LlamaIndex.channel_mention","title":"channel_mention","text":"<pre><code>channel_mention(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a channel mention in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/base.py</code> <pre><code>def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a channel mention in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._get_response(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/base/#reginald.models.llama_index.base.LlamaIndex.direct_message","title":"direct_message","text":"<pre><code>direct_message(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a direct message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/base.py</code> <pre><code>def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a direct message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._get_response(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/base/#reginald.models.llama_index.base.LlamaIndex.stream_message","title":"stream_message","text":"<pre><code>stream_message(message: str, user_id: str) -&gt; None\n</code></pre> <p>Method to respond to a stream message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/base.py</code> <pre><code>def stream_message(self, message: str, user_id: str) -&gt; None:\n    \"\"\"\n    Method to respond to a stream message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    try:\n        if self.mode == \"chat\":\n            # create chat engine for user if does not exist\n            if self.chat_engine.get(user_id) is None:\n                self.chat_engine[user_id] = self.index.as_chat_engine(\n                    chat_mode=\"context\",\n                    response_mode=self.response_mode,\n                    similarity_top_k=self.k,\n                    streaming=True,\n                )\n\n            # obtain chat engine for particular user\n            chat_engine = self.chat_engine[user_id]\n            response_stream = chat_engine.stream_chat(message)\n        elif self.mode == \"query\":\n            self.query_engine._response_synthesizer._streaming = True\n            response_stream = self.query_engine.query(message)\n\n        for token in stream_iter_progress_wrapper(response_stream.response_gen):\n            print(token, end=\"\", flush=True)\n\n        formatted_response = \"\\n\\n\\n\" + self._format_sources(response_stream)\n\n        for token in re.split(r\"(\\s+)\", formatted_response):\n            print(token, end=\"\", flush=True)\n    except Exception as e:  # ignore: broad-except\n        for token in re.split(\n            r\"(\\s+)\", self.error_response_template.format(repr(e))\n        ):\n            print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/data_index_creator/","title":"data_index_creator","text":""},{"location":"reference/reginald/models/llama_index/data_index_creator/#reginald.models.llama_index.data_index_creator.DataIndexCreator","title":"DataIndexCreator","text":"Source code in <code>reginald/models/llama_index/data_index_creator.py</code> <pre><code>class DataIndexCreator:\n    def __init__(\n        self,\n        data_dir: pathlib.Path | str,\n        which_index: str,\n        settings: _Settings,\n    ) -&gt; None:\n        \"\"\"\n        Class for creating the data index.\n\n        Parameters\n        ----------\n        data_dir : pathlib.Path | str\n            Path to the data directory.\n        which_index : str\n            Which index to construct (if force_new_index is True) or use.\n            Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n        settings : _Settings\n            llama_index.core.settings._Settings object to use to create the index.\n        \"\"\"\n        self.data_dir: pathlib.Path = pathlib.Path(data_dir)\n        self.which_index: str = which_index\n        self.settings: _Settings = settings\n        self.documents: list[str] = []\n        self.index: VectorStoreIndex | None = None\n\n    def prep_documents(self) -&gt; None:\n        \"\"\"\n        Method to prepare the documents for the index vector store.\n        \"\"\"\n        # prep the contextual documents\n        gh_token = get_env_var(\"GITHUB_TOKEN\")\n\n        if gh_token is None:\n            raise ValueError(\n                \"Please export your github personal access token as 'GITHUB_TOKEN'.\"\n            )\n\n        if self.which_index == \"handbook\":\n            logging.info(\"Regenerating index only for the handbook\")\n\n            # load handbook from repo\n            self._load_handbook(gh_token)\n\n        elif self.which_index == \"wikis\":\n            logging.info(\"Regenerating index only for the wikis\")\n\n            # load wikis\n            self._load_wikis(gh_token)\n\n        elif self.which_index == \"public\":\n            logging.info(\"Regenerating index for all PUBLIC. Will take a long time...\")\n\n            # load in scraped turing.ac.uk website\n            self._load_turing_ac_uk()\n\n            # load public data from repos\n            self._load_handbook(gh_token)\n            self._load_rse_course(gh_token)\n            self._load_rds_course(gh_token)\n            self._load_turing_way(gh_token)\n\n        elif self.which_index == \"reg\":\n            logging.info(\"Regenerating index for REG. Will take a long time...\")\n\n            # load in scraped turing.ac.uk website\n            self._load_turing_ac_uk()\n\n            # load public data from repos\n            self._load_handbook(gh_token)\n\n            # load hut23 data\n            self._load_hut23(gh_token)\n\n            # load wikis\n            self._load_wikis(gh_token)\n\n        elif self.which_index == \"all_data\":\n            logging.info(\"Regenerating index for ALL DATA. Will take a long time...\")\n\n            # load in scraped turing.ac.uk website\n            self._load_turing_ac_uk()\n\n            # load public data from repos\n            self._load_handbook(gh_token)\n            self._load_rse_course(gh_token)\n            self._load_rds_course(gh_token)\n            self._load_turing_way(gh_token)\n\n            # load hut23 data\n            self._load_hut23(gh_token)\n\n            # load wikis\n            self._load_wikis(gh_token)\n\n        else:\n            logging.info(\"The which_index provided is unrecognized\")\n\n    def _load_turing_ac_uk(self) -&gt; None:\n        \"\"\"\n        Load in the scraped turing.ac.uk website.\n\n        For 'public' index and 'all_data' index.\n        \"\"\"\n        data_file = f\"{self.data_dir}/public/turingacuk-no-boilerplate.csv\"\n        turing_df = pd.read_csv(data_file)\n        turing_df = turing_df[~turing_df.loc[:, \"body\"].isna()]\n        self.documents += [\n            Document(text=row[1][\"body\"], extra_info={\"url\": row[1][\"url\"]})\n            for row in turing_df.iterrows()\n        ]\n\n    def _load_handbook(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in the REG handbook.\n\n        For 'handbook' index, 'public' index, and 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the handbook repo.\n        \"\"\"\n        owner = \"alan-turing-institute\"\n        repo = \"REG-handbook\"\n\n        handbook_loader = GithubRepositoryReader(\n            GithubClient(gh_token, fail_on_http_error=False),\n            owner=owner,\n            repo=repo,\n            verbose=False,\n            concurrent_requests=1,\n            timeout=60,\n            retries=3,\n            filter_file_extensions=(\n                [\".md\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n            filter_directories=(\n                [\"content\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n        )\n        self.documents.extend(handbook_loader.load_data(branch=\"main\"))\n\n    def _load_rse_course(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in the REG RSE course.\n\n        For 'public' index and 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the RSE course repo.\n        \"\"\"\n        owner = \"alan-turing-institute\"\n        repo = \"rse-course\"\n\n        rse_course_loader = GithubRepositoryReader(\n            GithubClient(gh_token, fail_on_http_error=False),\n            owner=owner,\n            repo=repo,\n            verbose=False,\n            concurrent_requests=1,\n            timeout=60,\n            retries=3,\n            filter_file_extensions=(\n                [\".md\", \".ipynb\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n        )\n        self.documents.extend(rse_course_loader.load_data(branch=\"main\"))\n\n    def _load_rds_course(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in REG RDS course.\n\n        For 'public' index and 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the RDS course repo.\n        \"\"\"\n        owner = \"alan-turing-institute\"\n        repo = \"rds-course\"\n\n        rds_course_loader = GithubRepositoryReader(\n            GithubClient(gh_token, fail_on_http_error=False),\n            owner=owner,\n            repo=repo,\n            verbose=False,\n            concurrent_requests=1,\n            timeout=60,\n            retries=3,\n            filter_file_extensions=(\n                [\".md\", \".ipynb\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n        )\n        self.documents.extend(rds_course_loader.load_data(branch=\"develop\"))\n\n    def _load_turing_way(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in the Turing Way.\n\n        For 'public' index and 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the Turing Way repo.\n        \"\"\"\n        owner = \"the-turing-way\"\n        repo = \"the-turing-way\"\n\n        turing_way_loader = GithubRepositoryReader(\n            GithubClient(gh_token, fail_on_http_error=False),\n            owner=owner,\n            repo=repo,\n            verbose=False,\n            concurrent_requests=1,\n            timeout=60,\n            retries=3,\n            filter_file_extensions=(\n                [\".md\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n        )\n        self.documents.extend(turing_way_loader.load_data(branch=\"main\"))\n\n    def _load_hut23(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in documents from the Hut23 repo.\n\n        For 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the Hut23 repo.\n        \"\"\"\n        owner = \"alan-turing-institute\"\n        repo = \"Hut23\"\n\n        # load repo\n        hut23_repo_loader = GithubRepositoryReader(\n            GithubClient(gh_token, fail_on_http_error=False),\n            owner=owner,\n            repo=repo,\n            verbose=False,\n            concurrent_requests=1,\n            timeout=60,\n            retries=3,\n            filter_file_extensions=(\n                [\".md\", \".ipynb\"],\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n            filter_directories=(\n                [\n                    \"JDs\",\n                    \"development\",\n                    \"newsletters\",\n                    \"objectives\",\n                    \"project-appraisal\",\n                    \"rfc\",\n                    \"team-meetings\",\n                ],  # we can adjust these\n                GithubRepositoryReader.FilterType.INCLUDE,\n            ),\n        )\n        self.documents.extend(hut23_repo_loader.load_data(branch=\"main\"))\n\n        try:\n            # load_issues\n            hut23_issues_loader = GitHubRepositoryIssuesReader(\n                GitHubIssuesClient(gh_token),\n                owner=owner,\n                repo=repo,\n                verbose=True,\n            )\n\n            issue_docs = hut23_issues_loader.load_data()\n            for doc in issue_docs:\n                doc.metadata[\"api_url\"] = str(doc.metadata[\"url\"])\n                doc.metadata[\"url\"] = doc.metadata[\"source\"]\n            self.documents.extend(issue_docs)\n\n        except HTTPError as e:\n            logging.error(f\"Failed to load Hut23 issues: {e}\")\n\n        # load collaborators\n        # hut23_collaborators_loader = GitHubRepositoryCollaboratorsReader(\n        #     GitHubCollaboratorsClient(gh_token),\n        #     owner=owner,\n        #     repo=repo,\n        #     verbose=True,\n        # )\n        # self.documents.extend(hut23_collaborators_loader.load_data())\n\n    def _load_wikis(self, gh_token: str) -&gt; None:\n        \"\"\"\n        Load in documents from the wikis.\n\n        For 'wikis' index and 'all_data' index.\n\n        Parameters\n        ----------\n        gh_token : str\n            Github token to use to access the research-engineering-group\n            and Hut23 repo wikis.\n        \"\"\"\n        wiki_urls = [\n            f\"https://oauth2:{gh_token}@github.com/alan-turing-institute/research-engineering-group.wiki.git\",\n            f\"https://oauth2:{gh_token}@github.com/alan-turing-institute/Hut23.wiki.git\",\n        ]\n\n        for url in wiki_urls:\n            temp_dir = TemporaryDirectory()\n            wiki_path = os.path.join(temp_dir.name, url.split(\"/\")[-1])\n\n            _ = Repo.clone_from(url, wiki_path)\n\n            reader = SimpleDirectoryReader(\n                input_dir=wiki_path,\n                required_exts=[\".md\"],\n                recursive=True,\n                filename_as_id=True,\n            )\n\n            # get base url and file names\n            base_url = url.removesuffix(\".wiki.git\")\n            fnames = [str(file) for file in reader.input_files]\n\n            # get file urls and create dictionary to map fname to url\n            file_urls = [\n                os.path.join(base_url, \"wiki\", fname.split(\"/\")[-1].removesuffix(\".md\"))\n                for fname in fnames\n            ]\n            file_urls_dict = {\n                fname: file_url for fname, file_url in zip(fnames, file_urls)\n            }\n\n            def get_urls(fname):\n                return {\"url\": file_urls_dict.get(fname)}\n\n            # add `get_urls` function to reader\n            reader.file_metadata = get_urls\n\n            self.documents.extend(reader.load_data())\n\n    def create_index(self) -&gt; VectorStoreIndex:\n        \"\"\"\n        Create the index vector store.\n        \"\"\"\n        # obtain documents\n        logging.info(f\"Preparing documents for {self.which_index} index...\")\n        self.prep_documents()\n\n        # create index\n        logging.info(\"Creating index...\")\n        self.index = VectorStoreIndex.from_documents(\n            self.documents, settings=self.settings\n        )\n\n        return self.index\n\n    def save_index(self, directory: pathlib.Path | None = None) -&gt; None:\n        if directory is None:\n            directory = self.data_dir / LLAMA_INDEX_DIR / self.which_index\n\n        # save the settings and persist the index\n        logging.info(f\"Saving the index in {directory}...\")\n        self.index.storage_context.persist(persist_dir=directory)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/data_index_creator/#reginald.models.llama_index.data_index_creator.DataIndexCreator.__init__","title":"__init__","text":"<pre><code>__init__(data_dir: Path | str, which_index: str, settings: _Settings) -&gt; None\n</code></pre> <p>Class for creating the data index.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path | str</code> <p>Path to the data directory.</p> required <code>which_index</code> <code>str</code> <p>Which index to construct (if force_new_index is True) or use. Options are \u201chandbook\u201d, \u201cwikis\u201d,  \u201cpublic\u201d, \u201creg\u201d or \u201call_data\u201d.</p> required <code>settings</code> <code>_Settings</code> <p>llama_index.core.settings._Settings object to use to create the index.</p> required Source code in <code>reginald/models/llama_index/data_index_creator.py</code> <pre><code>def __init__(\n    self,\n    data_dir: pathlib.Path | str,\n    which_index: str,\n    settings: _Settings,\n) -&gt; None:\n    \"\"\"\n    Class for creating the data index.\n\n    Parameters\n    ----------\n    data_dir : pathlib.Path | str\n        Path to the data directory.\n    which_index : str\n        Which index to construct (if force_new_index is True) or use.\n        Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n    settings : _Settings\n        llama_index.core.settings._Settings object to use to create the index.\n    \"\"\"\n    self.data_dir: pathlib.Path = pathlib.Path(data_dir)\n    self.which_index: str = which_index\n    self.settings: _Settings = settings\n    self.documents: list[str] = []\n    self.index: VectorStoreIndex | None = None\n</code></pre>"},{"location":"reference/reginald/models/llama_index/data_index_creator/#reginald.models.llama_index.data_index_creator.DataIndexCreator.create_index","title":"create_index","text":"<pre><code>create_index() -&gt; VectorStoreIndex\n</code></pre> <p>Create the index vector store.</p> Source code in <code>reginald/models/llama_index/data_index_creator.py</code> <pre><code>def create_index(self) -&gt; VectorStoreIndex:\n    \"\"\"\n    Create the index vector store.\n    \"\"\"\n    # obtain documents\n    logging.info(f\"Preparing documents for {self.which_index} index...\")\n    self.prep_documents()\n\n    # create index\n    logging.info(\"Creating index...\")\n    self.index = VectorStoreIndex.from_documents(\n        self.documents, settings=self.settings\n    )\n\n    return self.index\n</code></pre>"},{"location":"reference/reginald/models/llama_index/data_index_creator/#reginald.models.llama_index.data_index_creator.DataIndexCreator.prep_documents","title":"prep_documents","text":"<pre><code>prep_documents() -&gt; None\n</code></pre> <p>Method to prepare the documents for the index vector store.</p> Source code in <code>reginald/models/llama_index/data_index_creator.py</code> <pre><code>def prep_documents(self) -&gt; None:\n    \"\"\"\n    Method to prepare the documents for the index vector store.\n    \"\"\"\n    # prep the contextual documents\n    gh_token = get_env_var(\"GITHUB_TOKEN\")\n\n    if gh_token is None:\n        raise ValueError(\n            \"Please export your github personal access token as 'GITHUB_TOKEN'.\"\n        )\n\n    if self.which_index == \"handbook\":\n        logging.info(\"Regenerating index only for the handbook\")\n\n        # load handbook from repo\n        self._load_handbook(gh_token)\n\n    elif self.which_index == \"wikis\":\n        logging.info(\"Regenerating index only for the wikis\")\n\n        # load wikis\n        self._load_wikis(gh_token)\n\n    elif self.which_index == \"public\":\n        logging.info(\"Regenerating index for all PUBLIC. Will take a long time...\")\n\n        # load in scraped turing.ac.uk website\n        self._load_turing_ac_uk()\n\n        # load public data from repos\n        self._load_handbook(gh_token)\n        self._load_rse_course(gh_token)\n        self._load_rds_course(gh_token)\n        self._load_turing_way(gh_token)\n\n    elif self.which_index == \"reg\":\n        logging.info(\"Regenerating index for REG. Will take a long time...\")\n\n        # load in scraped turing.ac.uk website\n        self._load_turing_ac_uk()\n\n        # load public data from repos\n        self._load_handbook(gh_token)\n\n        # load hut23 data\n        self._load_hut23(gh_token)\n\n        # load wikis\n        self._load_wikis(gh_token)\n\n    elif self.which_index == \"all_data\":\n        logging.info(\"Regenerating index for ALL DATA. Will take a long time...\")\n\n        # load in scraped turing.ac.uk website\n        self._load_turing_ac_uk()\n\n        # load public data from repos\n        self._load_handbook(gh_token)\n        self._load_rse_course(gh_token)\n        self._load_rds_course(gh_token)\n        self._load_turing_way(gh_token)\n\n        # load hut23 data\n        self._load_hut23(gh_token)\n\n        # load wikis\n        self._load_wikis(gh_token)\n\n    else:\n        logging.info(\"The which_index provided is unrecognized\")\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_cpp_template/","title":"llama_cpp_template","text":""},{"location":"reference/reginald/models/llama_index/llama_index/","title":"llama_index","text":""},{"location":"reference/reginald/models/llama_index/llama_index/#reginald.models.llama_index.llama_index.LlamaIndex","title":"LlamaIndex","text":"<p>               Bases: <code>ResponseModel</code></p> Source code in <code>reginald/models/llama_index/llama_index.py</code> <pre><code>class LlamaIndex(ResponseModel):\n    def __init__(\n        self,\n        model_name: str,\n        max_input_size: int,\n        data_dir: pathlib.Path | str,\n        which_index: str,\n        mode: str = \"chat\",\n        k: int = 3,\n        chunk_size: int | None = None,\n        chunk_overlap_ratio: float = 0.1,\n        num_output: int = 512,\n        force_new_index: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Base class for models using llama-index.\n        This class is not intended to be used directly, but rather subclassed\n        to implement the `_prep_llm` method which constructs the LLM to be used.\n\n        Parameters\n        ----------\n        model_name : str\n            Model name to specify which LLM to use.\n        max_input_size : int\n            Context window size for the LLM.\n        data_dir : pathlib.Path | str\n            Path to the data directory.\n        which_index : str\n            Which index to construct (if force_new_index is True) or use.\n            Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n        mode : Optional[str], optional\n            The type of engine to use when interacting with the data, options of \"chat\" or \"query\".\n            Default is \"chat\".\n        k : int, optional\n            `similarity_top_k` to use in chat or query engine, by default 3\n        chunk_size : int | None, optional\n            Maximum size of chunks to use, by default None.\n            If None, this is computed as `ceil(max_input_size / k)`.\n        chunk_overlap_ratio : float, optional\n            Chunk overlap as a ratio of chunk size, by default 0.1\n        num_output : int, optional\n            Number of outputs for the LLM, by default 512\n        force_new_index : bool, optional\n            Whether or not to recreate the index vector store,\n            by default False\n        \"\"\"\n        super().__init__(*args, emoji=\"llama\", **kwargs)\n        logging.info(\"Setting up Huggingface backend.\")\n        if mode == \"chat\":\n            logging.info(\"Setting up chat engine.\")\n        elif mode == \"query\":\n            logging.info(\"Setting up query engine.\")\n        else:\n            logging.error(\"Mode must either be 'query' or 'chat'.\")\n            sys.exit(1)\n\n        self.max_input_size = max_input_size\n        self.model_name = model_name\n        self.num_output = num_output\n        self.mode = mode\n        self.k = k\n        self.chunk_size = chunk_size or compute_default_chunk_size(\n            max_input_size=max_input_size, k=k\n        )\n        self.chunk_overlap_ratio = chunk_overlap_ratio\n        self.data_dir = pathlib.Path(data_dir)\n        self.which_index = which_index\n        self.documents = []\n\n        # set up LLM\n        llm = self._prep_llm()\n\n        # set up settings\n        settings = setup_settings(\n            llm=llm,\n            max_input_size=self.max_input_size,\n            num_output=self.num_output,\n            chunk_size=self.chunk_size,\n            chunk_overlap_ratio=self.chunk_overlap_ratio,\n            k=self.k,\n            tokenizer=self._prep_tokenizer(),\n        )\n\n        if force_new_index:\n            logging.info(\"Generating the index from scratch...\")\n            data_creator = DataIndexCreator(\n                which_index=self.which_index,\n                data_dir=self.data_dir,\n                settings=settings,\n            )\n            self.index: VectorStoreIndex = stream_progress_wrapper(\n                data_creator.create_index,\n                task_str=\"Generating the index from scratch...\",\n            )\n            stream_progress_wrapper(\n                data_creator.save_index,\n                task_str=\"Saving the index...\",\n            )\n\n        else:\n            logging.info(\"Loading the storage context\")\n            storage_context = stream_progress_wrapper(\n                StorageContext.from_defaults,\n                task_str=\"Loading the storage context...\",\n                persist_dir=self.data_dir / LLAMA_INDEX_DIR / self.which_index,\n            )\n\n            logging.info(\"Loading the pre-processed index\")\n            self.index = stream_progress_wrapper(\n                load_index_from_storage,\n                task_str=\"Loading the pre-processed index...\",\n                storage_context=storage_context,\n                settings=settings,\n            )\n\n        self.response_mode = \"simple_summarize\"\n        if self.mode == \"chat\":\n            self.chat_engine = {}\n            logging.info(\"Done setting up Huggingface backend for chat engine.\")\n        elif self.mode == \"query\":\n            self.query_engine = self.index.as_query_engine(\n                response_mode=self.response_mode,\n                similarity_top_k=k,\n            )\n            logging.info(\"Done setting up Huggingface backend for query engine.\")\n\n        self.error_response_template = (\n            \"Oh no! When I tried to get a response to your prompt, \"\n            \"I got the following error:\"\n            \"\\n```\\n{}\\n```\"\n        )\n\n    @staticmethod\n    def _format_sources(response: RESPONSE_TYPE) -&gt; str:\n        \"\"\"\n        Method to format the sources used to compose the response.\n\n        Parameters\n        ----------\n        response : RESPONSE_TYPE\n            response object from the query engine\n\n        Returns\n        -------\n        str\n            String containing the formatted sources that\n            were used to compose the response\n        \"\"\"\n        texts = []\n        for source_node in response.source_nodes:\n            # obtain the URL for source\n            try:\n                node_url = source_node.node.extra_info[\"url\"]\n            except KeyError:\n                node_url = source_node.node.extra_info[\"filename\"]\n\n            # add its similarity score and append to texts\n            source_text = node_url + f\" (similarity: {round(source_node.score, 2)})\"\n            texts.append(source_text)\n\n        result = \"I read the following documents to compose this answer:\\n\"\n        result += \"\\n\\n\".join(texts)\n\n        return result\n\n    def _prep_llm(self) -&gt; BaseLLM:\n        \"\"\"\n        Method to prepare the LLM to be used.\n\n        Returns\n        -------\n        BaseLLM\n            LLM to be used.\n\n        Raises\n        ------\n        NotImplemented\n            This must be implemented by a subclass of LlamaIndex.\n        \"\"\"\n        raise NotImplementedError(\n            \"_prep_llm needs to be implemented by a subclass of LlamaIndex.\"\n        )\n\n    def _prep_tokenizer(self) -&gt; callable[str] | None:\n        \"\"\"\n        Method to prepare the Tokenizer to be used.\n\n        Returns\n        -------\n        callable[str] | None\n            Tokenizer to use. A callable function on a string.\n            Can also be None if using the default set by LlamaIndex.\n\n        Raises\n        ------\n        NotImplemented\n        \"\"\"\n        raise NotImplementedError(\n            \"_prep_tokenizer needs to be implemented by a subclass of LlamaIndex.\"\n        )\n\n    def _get_response(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        try:\n            if self.mode == \"chat\":\n                # create chat engine for user if does not exist\n                if self.chat_engine.get(user_id) is None:\n                    self.chat_engine[user_id] = self.index.as_chat_engine(\n                        chat_mode=\"context\",\n                        response_mode=self.response_mode,\n                        similarity_top_k=self.k,\n                    )\n\n                # obtain chat engine for particular user\n                chat_engine = self.chat_engine[user_id]\n                response = chat_engine.chat(message)\n            elif self.mode == \"query\":\n                self.query_engine._response_synthesizer._streaming = False\n                response = self.query_engine.query(message)\n\n            # concatenate the response with the resources that it used\n            formatted_response = (\n                response.response + \"\\n\\n\\n\" + self._format_sources(response)\n            )\n        except Exception as e:  # ignore: broad-except\n            formatted_response = self.error_response_template.format(repr(e))\n\n        pattern = (\n            r\"(?s)^Context information is\"\n            r\".*\"\n            r\"Given the context information and not prior knowledge, answer the question: \"\n            rf\"{message}\"\n            r\"\\n(.*)\"\n        )\n        m = re.search(pattern, formatted_response)\n\n        if m:\n            answer = m.group(1)\n        else:\n            logging.warning(\n                \"Was expecting a backend response with a regular expression but couldn't find a match.\"\n            )\n            answer = formatted_response\n\n        return MessageResponse(answer)\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a direct message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._get_response(message=message, user_id=user_id)\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a channel mention in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._get_response(message=message, user_id=user_id)\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        \"\"\"\n        Method to respond to a stream message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        try:\n            if self.mode == \"chat\":\n                # create chat engine for user if does not exist\n                if self.chat_engine.get(user_id) is None:\n                    self.chat_engine[user_id] = self.index.as_chat_engine(\n                        chat_mode=\"context\",\n                        response_mode=self.response_mode,\n                        similarity_top_k=self.k,\n                        streaming=True,\n                    )\n\n                # obtain chat engine for particular user\n                chat_engine = self.chat_engine[user_id]\n                response_stream = chat_engine.stream_chat(message)\n            elif self.mode == \"query\":\n                self.query_engine._response_synthesizer._streaming = True\n                response_stream = self.query_engine.query(message)\n\n            for token in stream_iter_progress_wrapper(response_stream.response_gen):\n                print(token, end=\"\", flush=True)\n\n            formatted_response = \"\\n\\n\\n\" + self._format_sources(response_stream)\n\n            for token in re.split(r\"(\\s+)\", formatted_response):\n                print(token, end=\"\", flush=True)\n        except Exception as e:  # ignore: broad-except\n            for token in re.split(\n                r\"(\\s+)\", self.error_response_template.format(repr(e))\n            ):\n                print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index/#reginald.models.llama_index.llama_index.LlamaIndex.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name: str,\n    max_input_size: int,\n    data_dir: Path | str,\n    which_index: str,\n    mode: str = \"chat\",\n    k: int = 3,\n    chunk_size: int | None = None,\n    chunk_overlap_ratio: float = 0.1,\n    num_output: int = 512,\n    force_new_index: bool = False,\n    *args,\n    **kwargs\n) -&gt; None\n</code></pre> <p>Base class for models using llama-index. This class is not intended to be used directly, but rather subclassed to implement the <code>_prep_llm</code> method which constructs the LLM to be used.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name to specify which LLM to use.</p> required <code>max_input_size</code> <code>int</code> <p>Context window size for the LLM.</p> required <code>data_dir</code> <code>Path | str</code> <p>Path to the data directory.</p> required <code>which_index</code> <code>str</code> <p>Which index to construct (if force_new_index is True) or use. Options are \u201chandbook\u201d, \u201cwikis\u201d,  \u201cpublic\u201d, \u201creg\u201d or \u201call_data\u201d.</p> required <code>mode</code> <code>Optional[str]</code> <p>The type of engine to use when interacting with the data, options of \u201cchat\u201d or \u201cquery\u201d. Default is \u201cchat\u201d.</p> <code>'chat'</code> <code>k</code> <code>int</code> <p><code>similarity_top_k</code> to use in chat or query engine, by default 3</p> <code>3</code> <code>chunk_size</code> <code>int | None</code> <p>Maximum size of chunks to use, by default None. If None, this is computed as <code>ceil(max_input_size / k)</code>.</p> <code>None</code> <code>chunk_overlap_ratio</code> <code>float</code> <p>Chunk overlap as a ratio of chunk size, by default 0.1</p> <code>0.1</code> <code>num_output</code> <code>int</code> <p>Number of outputs for the LLM, by default 512</p> <code>512</code> <code>force_new_index</code> <code>bool</code> <p>Whether or not to recreate the index vector store, by default False</p> <code>False</code> Source code in <code>reginald/models/llama_index/llama_index.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    max_input_size: int,\n    data_dir: pathlib.Path | str,\n    which_index: str,\n    mode: str = \"chat\",\n    k: int = 3,\n    chunk_size: int | None = None,\n    chunk_overlap_ratio: float = 0.1,\n    num_output: int = 512,\n    force_new_index: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Base class for models using llama-index.\n    This class is not intended to be used directly, but rather subclassed\n    to implement the `_prep_llm` method which constructs the LLM to be used.\n\n    Parameters\n    ----------\n    model_name : str\n        Model name to specify which LLM to use.\n    max_input_size : int\n        Context window size for the LLM.\n    data_dir : pathlib.Path | str\n        Path to the data directory.\n    which_index : str\n        Which index to construct (if force_new_index is True) or use.\n        Options are \"handbook\", \"wikis\",  \"public\", \"reg\" or \"all_data\".\n    mode : Optional[str], optional\n        The type of engine to use when interacting with the data, options of \"chat\" or \"query\".\n        Default is \"chat\".\n    k : int, optional\n        `similarity_top_k` to use in chat or query engine, by default 3\n    chunk_size : int | None, optional\n        Maximum size of chunks to use, by default None.\n        If None, this is computed as `ceil(max_input_size / k)`.\n    chunk_overlap_ratio : float, optional\n        Chunk overlap as a ratio of chunk size, by default 0.1\n    num_output : int, optional\n        Number of outputs for the LLM, by default 512\n    force_new_index : bool, optional\n        Whether or not to recreate the index vector store,\n        by default False\n    \"\"\"\n    super().__init__(*args, emoji=\"llama\", **kwargs)\n    logging.info(\"Setting up Huggingface backend.\")\n    if mode == \"chat\":\n        logging.info(\"Setting up chat engine.\")\n    elif mode == \"query\":\n        logging.info(\"Setting up query engine.\")\n    else:\n        logging.error(\"Mode must either be 'query' or 'chat'.\")\n        sys.exit(1)\n\n    self.max_input_size = max_input_size\n    self.model_name = model_name\n    self.num_output = num_output\n    self.mode = mode\n    self.k = k\n    self.chunk_size = chunk_size or compute_default_chunk_size(\n        max_input_size=max_input_size, k=k\n    )\n    self.chunk_overlap_ratio = chunk_overlap_ratio\n    self.data_dir = pathlib.Path(data_dir)\n    self.which_index = which_index\n    self.documents = []\n\n    # set up LLM\n    llm = self._prep_llm()\n\n    # set up settings\n    settings = setup_settings(\n        llm=llm,\n        max_input_size=self.max_input_size,\n        num_output=self.num_output,\n        chunk_size=self.chunk_size,\n        chunk_overlap_ratio=self.chunk_overlap_ratio,\n        k=self.k,\n        tokenizer=self._prep_tokenizer(),\n    )\n\n    if force_new_index:\n        logging.info(\"Generating the index from scratch...\")\n        data_creator = DataIndexCreator(\n            which_index=self.which_index,\n            data_dir=self.data_dir,\n            settings=settings,\n        )\n        self.index: VectorStoreIndex = stream_progress_wrapper(\n            data_creator.create_index,\n            task_str=\"Generating the index from scratch...\",\n        )\n        stream_progress_wrapper(\n            data_creator.save_index,\n            task_str=\"Saving the index...\",\n        )\n\n    else:\n        logging.info(\"Loading the storage context\")\n        storage_context = stream_progress_wrapper(\n            StorageContext.from_defaults,\n            task_str=\"Loading the storage context...\",\n            persist_dir=self.data_dir / LLAMA_INDEX_DIR / self.which_index,\n        )\n\n        logging.info(\"Loading the pre-processed index\")\n        self.index = stream_progress_wrapper(\n            load_index_from_storage,\n            task_str=\"Loading the pre-processed index...\",\n            storage_context=storage_context,\n            settings=settings,\n        )\n\n    self.response_mode = \"simple_summarize\"\n    if self.mode == \"chat\":\n        self.chat_engine = {}\n        logging.info(\"Done setting up Huggingface backend for chat engine.\")\n    elif self.mode == \"query\":\n        self.query_engine = self.index.as_query_engine(\n            response_mode=self.response_mode,\n            similarity_top_k=k,\n        )\n        logging.info(\"Done setting up Huggingface backend for query engine.\")\n\n    self.error_response_template = (\n        \"Oh no! When I tried to get a response to your prompt, \"\n        \"I got the following error:\"\n        \"\\n```\\n{}\\n```\"\n    )\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index/#reginald.models.llama_index.llama_index.LlamaIndex.channel_mention","title":"channel_mention","text":"<pre><code>channel_mention(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a channel mention in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/llama_index.py</code> <pre><code>def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a channel mention in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._get_response(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index/#reginald.models.llama_index.llama_index.LlamaIndex.direct_message","title":"direct_message","text":"<pre><code>direct_message(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a direct message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/llama_index.py</code> <pre><code>def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a direct message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._get_response(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index/#reginald.models.llama_index.llama_index.LlamaIndex.stream_message","title":"stream_message","text":"<pre><code>stream_message(message: str, user_id: str) -&gt; None\n</code></pre> <p>Method to respond to a stream message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/llama_index/llama_index.py</code> <pre><code>def stream_message(self, message: str, user_id: str) -&gt; None:\n    \"\"\"\n    Method to respond to a stream message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    try:\n        if self.mode == \"chat\":\n            # create chat engine for user if does not exist\n            if self.chat_engine.get(user_id) is None:\n                self.chat_engine[user_id] = self.index.as_chat_engine(\n                    chat_mode=\"context\",\n                    response_mode=self.response_mode,\n                    similarity_top_k=self.k,\n                    streaming=True,\n                )\n\n            # obtain chat engine for particular user\n            chat_engine = self.chat_engine[user_id]\n            response_stream = chat_engine.stream_chat(message)\n        elif self.mode == \"query\":\n            self.query_engine._response_synthesizer._streaming = True\n            response_stream = self.query_engine.query(message)\n\n        for token in stream_iter_progress_wrapper(response_stream.response_gen):\n            print(token, end=\"\", flush=True)\n\n        formatted_response = \"\\n\\n\\n\" + self._format_sources(response_stream)\n\n        for token in re.split(r\"(\\s+)\", formatted_response):\n            print(token, end=\"\", flush=True)\n    except Exception as e:  # ignore: broad-except\n        for token in re.split(\n            r\"(\\s+)\", self.error_response_template.format(repr(e))\n        ):\n            print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_azure_openai/","title":"llama_index_azure_openai","text":""},{"location":"reference/reginald/models/llama_index/llama_index_azure_openai/#reginald.models.llama_index.llama_index_azure_openai.LlamaIndexGPTAzure","title":"LlamaIndexGPTAzure","text":"<p>               Bases: <code>LlamaIndex</code></p> Source code in <code>reginald/models/llama_index/llama_index_azure_openai.py</code> <pre><code>class LlamaIndexGPTAzure(LlamaIndex):\n    def __init__(\n        self, model_name: str = \"reginald-gpt4\", *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n         `LlamaIndexGPTAzure` is a subclass of `LlamaIndex` that uses Azure's\n        instance of OpenAI's LLMs to implement the LLM.\n\n        Must have the following environment variables set:\n        - `OPENAI_API_BASE`: Azure endpoint which looks\n          like https://YOUR_RESOURCE_NAME.openai.azure.com/\n        - `OPENAI_API_KEY`: Azure API key\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The deployment name of the model, by default \"reginald-gpt4\"\n        \"\"\"\n        openai_azure_api_base = get_env_var(\"OPENAI_AZURE_API_BASE\", secret_value=False)\n        if openai_azure_api_base is None:\n            raise ValueError(\n                \"You must set OPENAI_AZURE_API_BASE to your Azure endpoint. \"\n                \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"\n            )\n\n        openai_azure_api_key = get_env_var(\"OPENAI_AZURE_API_KEY\")\n        if openai_azure_api_key is None:\n            raise ValueError(\"You must set OPENAI_AZURE_API_KEY for Azure OpenAI.\")\n\n        # deployment name can be found in the Azure AI Studio portal\n        self.deployment_name = model_name\n        self.openai_api_base = openai_azure_api_base\n        self.openai_api_key = openai_azure_api_key\n        self.openai_api_version = \"2023-09-15-preview\"\n        self.temperature = 0.7\n        super().__init__(*args, model_name=\"gpt-4\", **kwargs)\n\n    def _prep_llm(self) -&gt; AzureOpenAI:\n        logging.info(f\"Setting up AzureOpenAI LLM (model {self.deployment_name})\")\n        return AzureOpenAI(\n            model=self.model_name,\n            engine=self.deployment_name,\n            temperature=self.temperature,\n            max_tokens=self.num_output,\n            api_key=self.openai_api_key,\n            api_base=self.openai_api_base,\n            api_type=\"azure\",\n            azure_endpoint=self.openai_api_base,\n            api_version=self.openai_api_version,\n        )\n\n    def _prep_tokenizer(self) -&gt; None:\n        import tiktoken\n\n        logging.info(f\"Setting up tiktoken tokenizer for model {self.model_name}\")\n        tokenizer = tiktoken.encoding_for_model(\"gpt-4\").encode\n        set_global_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_azure_openai/#reginald.models.llama_index.llama_index_azure_openai.LlamaIndexGPTAzure.__init__","title":"__init__","text":"<pre><code>__init__(model_name: str = 'reginald-gpt4', *args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p><code>LlamaIndexGPTAzure</code> is a subclass of <code>LlamaIndex</code> that uses Azure\u2019s instance of OpenAI\u2019s LLMs to implement the LLM.</p> <p>Must have the following environment variables set: - <code>OPENAI_API_BASE</code>: Azure endpoint which looks   like https://YOUR_RESOURCE_NAME.openai.azure.com/ - <code>OPENAI_API_KEY</code>: Azure API key</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The deployment name of the model, by default \u201creginald-gpt4\u201d</p> <code>'reginald-gpt4'</code> Source code in <code>reginald/models/llama_index/llama_index_azure_openai.py</code> <pre><code>def __init__(\n    self, model_name: str = \"reginald-gpt4\", *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n     `LlamaIndexGPTAzure` is a subclass of `LlamaIndex` that uses Azure's\n    instance of OpenAI's LLMs to implement the LLM.\n\n    Must have the following environment variables set:\n    - `OPENAI_API_BASE`: Azure endpoint which looks\n      like https://YOUR_RESOURCE_NAME.openai.azure.com/\n    - `OPENAI_API_KEY`: Azure API key\n\n    Parameters\n    ----------\n    model_name : str, optional\n        The deployment name of the model, by default \"reginald-gpt4\"\n    \"\"\"\n    openai_azure_api_base = get_env_var(\"OPENAI_AZURE_API_BASE\", secret_value=False)\n    if openai_azure_api_base is None:\n        raise ValueError(\n            \"You must set OPENAI_AZURE_API_BASE to your Azure endpoint. \"\n            \"It should look like https://YOUR_RESOURCE_NAME.openai.azure.com/\"\n        )\n\n    openai_azure_api_key = get_env_var(\"OPENAI_AZURE_API_KEY\")\n    if openai_azure_api_key is None:\n        raise ValueError(\"You must set OPENAI_AZURE_API_KEY for Azure OpenAI.\")\n\n    # deployment name can be found in the Azure AI Studio portal\n    self.deployment_name = model_name\n    self.openai_api_base = openai_azure_api_base\n    self.openai_api_key = openai_azure_api_key\n    self.openai_api_version = \"2023-09-15-preview\"\n    self.temperature = 0.7\n    super().__init__(*args, model_name=\"gpt-4\", **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_hf/","title":"llama_index_hf","text":""},{"location":"reference/reginald/models/llama_index/llama_index_hf/#reginald.models.llama_index.llama_index_hf.LlamaIndexHF","title":"LlamaIndexHF","text":"<p>               Bases: <code>LlamaIndex</code></p> Source code in <code>reginald/models/llama_index/llama_index_hf.py</code> <pre><code>class LlamaIndexHF(LlamaIndex):\n    def __init__(\n        self,\n        model_name: str = \"google/gemma-2b-it\",\n        device: str = \"auto\",\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        `LlamaIndexHF` is a subclass of `LlamaIndex` that uses HuggingFace's\n        `transformers` library to implement the LLM.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            Model name from Huggingface's model hub,\n            by default \"google/gemma-2b-it\".\n        device : str, optional\n            Device map to use for the LLM, by default \"auto\".\n        \"\"\"\n        self.device = device\n        super().__init__(*args, model_name=model_name, **kwargs)\n\n    def _prep_llm(self) -&gt; HuggingFaceLLM:\n        logging.info(\n            f\"Setting up Huggingface LLM (model {self.model_name}) on device {self.device}\"\n        )\n        logging.info(\n            f\"HF-args: (context_window: {self.max_input_size}, num_output: {self.num_output})\"\n        )\n\n        return HuggingFaceLLM(\n            context_window=self.max_input_size,\n            max_new_tokens=self.num_output,\n            generate_kwargs={\"temperature\": 0.1, \"do_sample\": False},\n            tokenizer_name=self.model_name,\n            model_name=self.model_name,\n            device_map=self.device or \"auto\",\n        )\n\n    def _prep_tokenizer(self) -&gt; Callable[[str], int]:\n        logging.info(f\"Setting up Huggingface tokenizer for model {self.model_name}\")\n        tokenizer = AutoTokenizer.from_pretrained(self.model_name).encode\n        set_global_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_hf/#reginald.models.llama_index.llama_index_hf.LlamaIndexHF.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name: str = \"google/gemma-2b-it\",\n    device: str = \"auto\",\n    *args: Any,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p><code>LlamaIndexHF</code> is a subclass of <code>LlamaIndex</code> that uses HuggingFace\u2019s <code>transformers</code> library to implement the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name from Huggingface\u2019s model hub, by default \u201cgoogle/gemma-2b-it\u201d.</p> <code>'google/gemma-2b-it'</code> <code>device</code> <code>str</code> <p>Device map to use for the LLM, by default \u201cauto\u201d.</p> <code>'auto'</code> Source code in <code>reginald/models/llama_index/llama_index_hf.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"google/gemma-2b-it\",\n    device: str = \"auto\",\n    *args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    `LlamaIndexHF` is a subclass of `LlamaIndex` that uses HuggingFace's\n    `transformers` library to implement the LLM.\n\n    Parameters\n    ----------\n    model_name : str, optional\n        Model name from Huggingface's model hub,\n        by default \"google/gemma-2b-it\".\n    device : str, optional\n        Device map to use for the LLM, by default \"auto\".\n    \"\"\"\n    self.device = device\n    super().__init__(*args, model_name=model_name, **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_llama_cpp/","title":"llama_index_llama_cpp","text":""},{"location":"reference/reginald/models/llama_index/llama_index_llama_cpp/#reginald.models.llama_index.llama_index_llama_cpp.LlamaIndexLlamaCPP","title":"LlamaIndexLlamaCPP","text":"<p>               Bases: <code>LlamaIndex</code></p> Source code in <code>reginald/models/llama_index/llama_index_llama_cpp.py</code> <pre><code>class LlamaIndexLlamaCPP(LlamaIndex):\n    def __init__(\n        self,\n        model_name: str,\n        is_path: bool,\n        n_gpu_layers: int = 0,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        `LlamaIndexLlamaCPP` is a subclass of `LlamaIndex` that uses\n        llama-cpp to implement the LLM.\n\n        Parameters\n        ----------\n        model_name : str\n            Either the path to the model or the URL to download the model from\n        is_path : bool, optional\n            If True, model_name is used as a path to the model file,\n            otherwise it should be the URL to download the model\n        n_gpu_layers : int, optional\n            Number of layers to offload to GPU.\n            If -1, all layers are offloaded, by default 0\n        \"\"\"\n        self.is_path = is_path\n        self.n_gpu_layers = n_gpu_layers\n        super().__init__(*args, model_name=model_name, **kwargs)\n\n    def _prep_llm(self) -&gt; LlamaCPP:\n        logging.info(\n            f\"Setting up LlamaCPP LLM (model {self.model_name}) on {self.n_gpu_layers} GPU layers\"\n        )\n        logging.info(\n            f\"LlamaCPP-args: (context_window: {self.max_input_size}, num_output: {self.num_output})\"\n        )\n\n        return LlamaCPP(\n            model_url=self.model_name if not self.is_path else None,\n            model_path=self.model_name if self.is_path else None,\n            temperature=0.1,\n            max_new_tokens=self.num_output,\n            context_window=self.max_input_size,\n            # kwargs to pass to __call__()\n            generate_kwargs={},\n            # kwargs to pass to __init__()\n            model_kwargs={\"n_gpu_layers\": self.n_gpu_layers},\n            messages_to_prompt=messages_to_prompt,\n            completion_to_prompt=completion_to_prompt,\n            verbose=True,\n        )\n\n    def _prep_tokenizer(self) -&gt; Callable[[str], int]:\n        # NOTE: this should depend on the model used, but hard coding tiktoken for now\n        logging.info(\"Setting up tiktoken gpt-4 tokenizer\")\n        tokenizer = encoding_for_model(\"gpt-4\").encode\n        set_global_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_llama_cpp/#reginald.models.llama_index.llama_index_llama_cpp.LlamaIndexLlamaCPP.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name: str,\n    is_path: bool,\n    n_gpu_layers: int = 0,\n    *args: Any,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p><code>LlamaIndexLlamaCPP</code> is a subclass of <code>LlamaIndex</code> that uses llama-cpp to implement the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Either the path to the model or the URL to download the model from</p> required <code>is_path</code> <code>bool</code> <p>If True, model_name is used as a path to the model file, otherwise it should be the URL to download the model</p> required <code>n_gpu_layers</code> <code>int</code> <p>Number of layers to offload to GPU. If -1, all layers are offloaded, by default 0</p> <code>0</code> Source code in <code>reginald/models/llama_index/llama_index_llama_cpp.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    is_path: bool,\n    n_gpu_layers: int = 0,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    `LlamaIndexLlamaCPP` is a subclass of `LlamaIndex` that uses\n    llama-cpp to implement the LLM.\n\n    Parameters\n    ----------\n    model_name : str\n        Either the path to the model or the URL to download the model from\n    is_path : bool, optional\n        If True, model_name is used as a path to the model file,\n        otherwise it should be the URL to download the model\n    n_gpu_layers : int, optional\n        Number of layers to offload to GPU.\n        If -1, all layers are offloaded, by default 0\n    \"\"\"\n    self.is_path = is_path\n    self.n_gpu_layers = n_gpu_layers\n    super().__init__(*args, model_name=model_name, **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_ollama/","title":"llama_index_ollama","text":""},{"location":"reference/reginald/models/llama_index/llama_index_ollama/#reginald.models.llama_index.llama_index_ollama.LlamaIndexOllama","title":"LlamaIndexOllama","text":"<p>               Bases: <code>LlamaIndex</code></p> Source code in <code>reginald/models/llama_index/llama_index_ollama.py</code> <pre><code>class LlamaIndexOllama(LlamaIndex):\n    def __init__(\n        self,\n        model_name: str,\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        `LlamaIndexOllama` is a subclass of `LlamaIndex` that uses\n        ollama to run inference on the LLM.\n\n        Parameters\n        ----------\n        model_name : str\n            The Ollama model to use\n        \"\"\"\n        ollama_api_endpoint = get_env_var(\"OLLAMA_API_ENDPOINT\")\n        if ollama_api_endpoint is None:\n            raise ValueError(\"You must set OLLAMA_API_ENDPOINT for Ollama.\")\n        self.ollama_api_endpoint = ollama_api_endpoint\n        super().__init__(*args, model_name=model_name, **kwargs)\n\n    def _prep_llm(self) -&gt; Ollama:\n        logging.info(f\"Setting up Ollama (model {self.model_name})\")\n        return Ollama(\n            base_url=self.ollama_api_endpoint,\n            model=self.model_name,\n            request_timeout=60,\n        )\n\n    def _prep_tokenizer(self) -&gt; Callable[[str], int]:\n        # NOTE: this should depend on the model used, but hard coding tiktoken for now\n        logging.info(\"Setting up tiktoken gpt-4 tokenizer\")\n        tokenizer = encoding_for_model(\"gpt-4\").encode\n        set_global_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_ollama/#reginald.models.llama_index.llama_index_ollama.LlamaIndexOllama.__init__","title":"__init__","text":"<pre><code>__init__(model_name: str, *args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p><code>LlamaIndexOllama</code> is a subclass of <code>LlamaIndex</code> that uses ollama to run inference on the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The Ollama model to use</p> required Source code in <code>reginald/models/llama_index/llama_index_ollama.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    `LlamaIndexOllama` is a subclass of `LlamaIndex` that uses\n    ollama to run inference on the LLM.\n\n    Parameters\n    ----------\n    model_name : str\n        The Ollama model to use\n    \"\"\"\n    ollama_api_endpoint = get_env_var(\"OLLAMA_API_ENDPOINT\")\n    if ollama_api_endpoint is None:\n        raise ValueError(\"You must set OLLAMA_API_ENDPOINT for Ollama.\")\n    self.ollama_api_endpoint = ollama_api_endpoint\n    super().__init__(*args, model_name=model_name, **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_openai/","title":"llama_index_openai","text":""},{"location":"reference/reginald/models/llama_index/llama_index_openai/#reginald.models.llama_index.llama_index_openai.LlamaIndexGPTOpenAI","title":"LlamaIndexGPTOpenAI","text":"<p>               Bases: <code>LlamaIndex</code></p> Source code in <code>reginald/models/llama_index/llama_index_openai.py</code> <pre><code>class LlamaIndexGPTOpenAI(LlamaIndex):\n    def __init__(\n        self, model_name: str = \"gpt-3.5-turbo\", *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        `LlamaIndexGPTOpenAI` is a subclass of `LlamaIndex` that uses OpenAI's\n        API to implement the LLM.\n\n        Must have `OPENAI_API_KEY` set as an environment variable.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The model to use from the OpenAI API, by default \"gpt-3.5-turbo\"\n        \"\"\"\n        openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n        if openai_api_key is None:\n            raise ValueError(\"You must set OPENAI_API_KEY for OpenAI.\")\n\n        self.model_name = model_name\n        self.openai_api_key = openai_api_key\n        self.temperature = 0.7\n        super().__init__(*args, model_name=self.model_name, **kwargs)\n\n    def _prep_llm(self) -&gt; OpenAI:\n        logging.info(f\"Setting up OpenAI LLM (model {self.model_name})\")\n        return OpenAI(\n            model=self.model_name,\n            temperature=self.temperature,\n            max_tokens=self.num_output,\n            api_key=self.openai_api_key,\n        )\n\n    def _prep_tokenizer(self) -&gt; None:\n        import tiktoken\n\n        logging.info(f\"Setting up tiktoken tokenizer for model {self.model_name}\")\n        tokenizer = tiktoken.encoding_for_model(self.model_name).encode\n        set_global_tokenizer(tokenizer)\n        return tokenizer\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_index_openai/#reginald.models.llama_index.llama_index_openai.LlamaIndexGPTOpenAI.__init__","title":"__init__","text":"<pre><code>__init__(model_name: str = 'gpt-3.5-turbo', *args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p><code>LlamaIndexGPTOpenAI</code> is a subclass of <code>LlamaIndex</code> that uses OpenAI\u2019s API to implement the LLM.</p> <p>Must have <code>OPENAI_API_KEY</code> set as an environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model to use from the OpenAI API, by default \u201cgpt-3.5-turbo\u201d</p> <code>'gpt-3.5-turbo'</code> Source code in <code>reginald/models/llama_index/llama_index_openai.py</code> <pre><code>def __init__(\n    self, model_name: str = \"gpt-3.5-turbo\", *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    `LlamaIndexGPTOpenAI` is a subclass of `LlamaIndex` that uses OpenAI's\n    API to implement the LLM.\n\n    Must have `OPENAI_API_KEY` set as an environment variable.\n\n    Parameters\n    ----------\n    model_name : str, optional\n        The model to use from the OpenAI API, by default \"gpt-3.5-turbo\"\n    \"\"\"\n    openai_api_key = get_env_var(\"OPENAI_API_KEY\")\n    if openai_api_key is None:\n        raise ValueError(\"You must set OPENAI_API_KEY for OpenAI.\")\n\n    self.model_name = model_name\n    self.openai_api_key = openai_api_key\n    self.temperature = 0.7\n    super().__init__(*args, model_name=self.model_name, **kwargs)\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_utils/","title":"llama_utils","text":""},{"location":"reference/reginald/models/llama_index/llama_utils/#reginald.models.llama_index.llama_utils.compute_default_chunk_size","title":"compute_default_chunk_size","text":"<pre><code>compute_default_chunk_size(max_input_size: int, k: int) -&gt; int\n</code></pre> <p>Compute the default chunk size to use for the index vector store.</p> <p>Parameters:</p> Name Type Description Default <code>max_input_size</code> <code>int</code> <p>Maximum input size for the LLM.</p> required <code>k</code> <code>int</code> <p><code>similarity_top_k</code> to use in chat or query engine.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Default chunk size to use for the index vector store.</p> Source code in <code>reginald/models/llama_index/llama_utils.py</code> <pre><code>def compute_default_chunk_size(max_input_size: int, k: int) -&gt; int:\n    \"\"\"\n    Compute the default chunk size to use for the index vector store.\n\n    Parameters\n    ----------\n    max_input_size : int\n        Maximum input size for the LLM.\n    k : int\n        `similarity_top_k` to use in chat or query engine.\n\n    Returns\n    -------\n    int\n        Default chunk size to use for the index vector store.\n    \"\"\"\n    return ceil(max_input_size / (k + 1))\n</code></pre>"},{"location":"reference/reginald/models/llama_index/llama_utils/#reginald.models.llama_index.llama_utils.setup_settings","title":"setup_settings","text":"<pre><code>setup_settings(\n    llm: BaseLLM,\n    max_input_size: int | str,\n    num_output: int | str,\n    chunk_overlap_ratio: float | str,\n    chunk_size: int | str | None = None,\n    k: int | str | None = None,\n    tokenizer: Callable[[str], int] | None = None,\n) -&gt; _Settings\n</code></pre> <p>Helper function to set up the settings. Can pass in either chunk_size or k. If chunk_size is not provided, it is computed as <code>ceil(max_input_size / k)</code>. If chunk_size is provided, k is ignored.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>LLM to use to create the index vectors.</p> required <code>max_input_size</code> <code>int | str</code> <p>Context window size for the LLM.</p> required <code>num_output</code> <code>int</code> <p>Number of outputs for the LLM.</p> required <code>chunk_overlap_ratio</code> <code>float</code> <p>Chunk overlap as a ratio of chunk size._</p> required <code>chunk_size</code> <code>int | None</code> <p>Maximum size of chunks to use, by default None. If None, this is computed as <code>ceil(max_input_size / k)</code>.</p> <code>None</code> <code>k</code> <code>int | str | None</code> <p><code>similarity_top_k</code> to use in chat or query engine, by default None</p> <code>None</code> <code>tokenizer</code> <code>Callable[[str], int] | None</code> <p>Tokenizer to use. A callable function on a string. Can also be None if using the default set by LlamaIndex.</p> <code>None</code> <p>Returns:</p> Type Description <code>Settings</code> <p>_Settings object to use to create the index vectors.</p> Source code in <code>reginald/models/llama_index/llama_utils.py</code> <pre><code>def setup_settings(\n    llm: BaseLLM,\n    max_input_size: int | str,\n    num_output: int | str,\n    chunk_overlap_ratio: float | str,\n    chunk_size: int | str | None = None,\n    k: int | str | None = None,\n    tokenizer: Callable[[str], int] | None = None,\n) -&gt; _Settings:\n    \"\"\"\n    Helper function to set up the settings.\n    Can pass in either chunk_size or k.\n    If chunk_size is not provided, it is computed as\n    `ceil(max_input_size / k)`.\n    If chunk_size is provided, k is ignored.\n\n    Parameters\n    ----------\n    llm : BaseLLM\n        LLM to use to create the index vectors.\n    max_input_size : int | str\n        Context window size for the LLM.\n    num_output : int, optional\n        Number of outputs for the LLM.\n    chunk_overlap_ratio : float, optional\n        Chunk overlap as a ratio of chunk size._\n    chunk_size : int | None, optional\n        Maximum size of chunks to use, by default None.\n        If None, this is computed as `ceil(max_input_size / k)`.\n    k : int | str | None, optional\n        `similarity_top_k` to use in chat or query engine,\n        by default None\n    tokenizer: Callable[[str], int] | None, optional\n        Tokenizer to use. A callable function on a string.\n        Can also be None if using the default set by LlamaIndex.\n\n    Returns\n    -------\n    Settings\n        _Settings object to use to create the index vectors.\n    \"\"\"\n    if chunk_size is None and k is None:\n        raise ValueError(\"Either chunk_size or k must be provided.\")\n\n    # convert to int or float if necessary\n    if isinstance(max_input_size, str):\n        max_input_size = int(max_input_size)\n    if isinstance(num_output, str):\n        num_output = int(num_output)\n    if isinstance(chunk_overlap_ratio, str):\n        chunk_overlap_ratio = float(chunk_overlap_ratio)\n    if isinstance(chunk_size, str):\n        chunk_size = int(chunk_size)\n    if isinstance(k, str):\n        k = int(k)\n\n    # if chunk_size is not provided, compute a default value\n    chunk_size = chunk_size or compute_default_chunk_size(\n        max_input_size=max_input_size, k=k\n    )\n\n    # initialise embedding model to use to create the index vectors\n    embed_model = HuggingFaceEmbedding(\n        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n        embed_batch_size=64,\n    )\n\n    # construct the prompt helper\n    prompt_helper = PromptHelper(\n        context_window=max_input_size,\n        num_output=num_output,\n        chunk_size_limit=chunk_size,\n        chunk_overlap_ratio=chunk_overlap_ratio,\n        tokenizer=tokenizer,\n    )\n\n    # construct the settings (and logging the settings set)\n    Settings.llm = llm\n    logging.info(f\"Settings llm: {llm}\")\n    Settings.embed_model = embed_model\n    logging.info(f\"Settings embed_model: {embed_model}\")\n    logging.info(f\"Embedding model initialised on device {embed_model._device}\")\n    Settings.prompt_helper = prompt_helper\n    logging.info(f\"Settings prompt_helper: {prompt_helper}\")\n    Settings.chunk_size = chunk_size\n    logging.info(f\"Settings chunk_size: {chunk_size}\")\n    Settings.tokenizer = tokenizer\n    logging.info(f\"Settings tokenizer: {tokenizer}\")\n\n    return Settings\n</code></pre>"},{"location":"reference/reginald/models/simple/","title":"simple","text":""},{"location":"reference/reginald/models/simple/chat_completion/","title":"chat_completion","text":""},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionAzure","title":"ChatCompletionAzure","text":"<p>               Bases: <code>ChatCompletionBase</code></p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>class ChatCompletionAzure(ChatCompletionBase):\n    def __init__(\n        self,\n        model_name: str = \"reginald-gpt4\",\n        mode: str = \"chat\",\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Simple chat completion model using Azure's\n        instance of OpenAI's LLMs to implement the LLM.\n\n        Must have the following environment variables set:\n        - `OPENAI_API_BASE`: Azure endpoint which looks\n          like https://YOUR_RESOURCE_NAME.openai.azure.com/\n        - `OPENAI_API_KEY`: Azure API key\n\n        Parameters\n        ----------\n        model_name : str, optional\n            Deployment name of the model on Azure, by default \"reginald-gpt4\"\n        mode : Optional[str], optional\n            The type of engine to use when interacting with the model,\n            options of \"chat\" (where a chat completion is requested)\n            or \"query\" (where a completion in requested). Default is \"chat\".\n        \"\"\"\n        logging.info(f\"Setting up AzureOpenAI LLM (model {model_name})\")\n        if mode == \"chat\":\n            logging.info(\"Setting up chat engine.\")\n        elif mode == \"query\":\n            logging.info(\"Setting up query engine.\")\n        else:\n            logging.error(\"Mode must either be 'query' or 'chat'.\")\n            sys.exit(1)\n\n        super().__init__(*args, **kwargs)\n        self.api_base = get_env_var(\"OPENAI_AZURE_API_BASE\", secret_value=False)\n        self.api_key = get_env_var(\"OPENAI_AZURE_API_KEY\")\n        self.api_type = \"azure\"\n        self.api_version = \"2023-09-15-preview\"\n        self.best_of = 1\n        self.engine = model_name  # the deployment name\n        self.frequency_penalty = 0\n        self.max_tokens = 512\n        self.presence_penalty = 0\n        self.temperature = 0.2\n        self.top_p = 0.95\n        self.client = AzureOpenAI(\n            api_key=self.api_key,\n            azure_endpoint=self.api_base,\n            api_version=self.api_version,\n        )\n        self.mode = mode\n\n    def _respond(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        openai.api_base = self.api_base\n        openai.api_type = self.api_type\n        openai.api_version = self.api_version\n        openai.api_key = self.api_key\n        if self.mode == \"chat\":\n            response = self.client.chat.completions.create(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": message}],\n                frequency_penalty=self.frequency_penalty,\n                max_tokens=self.max_tokens,\n                presence_penalty=self.presence_penalty,\n                stop=None,\n                temperature=self.temperature,\n                top_p=self.top_p,\n            )\n\n            return MessageResponse(response.choices[0].message.content)\n        elif self.mode == \"query\":\n            response = self.client.completions.create(\n                model=self.engine,\n                frequency_penalty=self.frequency_penalty,\n                max_tokens=self.max_tokens,\n                presence_penalty=self.presence_penalty,\n                prompt=message,\n                stop=None,\n                temperature=self.temperature,\n                top_p=self.top_p,\n            )\n\n            return MessageResponse(response.choices[0].text)\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a direct message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._respond(message=message, user_id=user_id)\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a channel mention in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._respond(message=message, user_id=user_id)\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        if self.mode == \"chat\":\n            response = self.client.chat.completions.create(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": message}],\n                frequency_penalty=self.frequency_penalty,\n                max_tokens=self.max_tokens,\n                presence_penalty=self.presence_penalty,\n                stop=None,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                stream=True,\n            )\n        elif self.mode == \"query\":\n            response = self.client.completions.create(\n                model=self.engine,\n                frequency_penalty=self.frequency_penalty,\n                max_tokens=self.max_tokens,\n                presence_penalty=self.presence_penalty,\n                prompt=message,\n                stop=None,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                stream=True,\n            )\n\n        for chunk in stream_iter_progress_wrapper(response):\n            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionAzure.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_name: str = \"reginald-gpt4\",\n    mode: str = \"chat\",\n    *args: Any,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Simple chat completion model using Azure\u2019s instance of OpenAI\u2019s LLMs to implement the LLM.</p> <p>Must have the following environment variables set: - <code>OPENAI_API_BASE</code>: Azure endpoint which looks   like https://YOUR_RESOURCE_NAME.openai.azure.com/ - <code>OPENAI_API_KEY</code>: Azure API key</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Deployment name of the model on Azure, by default \u201creginald-gpt4\u201d</p> <code>'reginald-gpt4'</code> <code>mode</code> <code>Optional[str]</code> <p>The type of engine to use when interacting with the model, options of \u201cchat\u201d (where a chat completion is requested) or \u201cquery\u201d (where a completion in requested). Default is \u201cchat\u201d.</p> <code>'chat'</code> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"reginald-gpt4\",\n    mode: str = \"chat\",\n    *args: Any,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Simple chat completion model using Azure's\n    instance of OpenAI's LLMs to implement the LLM.\n\n    Must have the following environment variables set:\n    - `OPENAI_API_BASE`: Azure endpoint which looks\n      like https://YOUR_RESOURCE_NAME.openai.azure.com/\n    - `OPENAI_API_KEY`: Azure API key\n\n    Parameters\n    ----------\n    model_name : str, optional\n        Deployment name of the model on Azure, by default \"reginald-gpt4\"\n    mode : Optional[str], optional\n        The type of engine to use when interacting with the model,\n        options of \"chat\" (where a chat completion is requested)\n        or \"query\" (where a completion in requested). Default is \"chat\".\n    \"\"\"\n    logging.info(f\"Setting up AzureOpenAI LLM (model {model_name})\")\n    if mode == \"chat\":\n        logging.info(\"Setting up chat engine.\")\n    elif mode == \"query\":\n        logging.info(\"Setting up query engine.\")\n    else:\n        logging.error(\"Mode must either be 'query' or 'chat'.\")\n        sys.exit(1)\n\n    super().__init__(*args, **kwargs)\n    self.api_base = get_env_var(\"OPENAI_AZURE_API_BASE\", secret_value=False)\n    self.api_key = get_env_var(\"OPENAI_AZURE_API_KEY\")\n    self.api_type = \"azure\"\n    self.api_version = \"2023-09-15-preview\"\n    self.best_of = 1\n    self.engine = model_name  # the deployment name\n    self.frequency_penalty = 0\n    self.max_tokens = 512\n    self.presence_penalty = 0\n    self.temperature = 0.2\n    self.top_p = 0.95\n    self.client = AzureOpenAI(\n        api_key=self.api_key,\n        azure_endpoint=self.api_base,\n        api_version=self.api_version,\n    )\n    self.mode = mode\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionAzure.channel_mention","title":"channel_mention","text":"<pre><code>channel_mention(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a channel mention in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a channel mention in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._respond(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionAzure.direct_message","title":"direct_message","text":"<pre><code>direct_message(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a direct message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a direct message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._respond(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionBase","title":"ChatCompletionBase","text":"<p>               Bases: <code>ResponseModel</code></p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>class ChatCompletionBase(ResponseModel):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Base class for chat completion models.\n        \"\"\"\n        super().__init__(emoji=\"books\")\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionBase.__init__","title":"__init__","text":"<pre><code>__init__(*args, **kwargs) -&gt; None\n</code></pre> <p>Base class for chat completion models.</p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Base class for chat completion models.\n    \"\"\"\n    super().__init__(emoji=\"books\")\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionOpenAI","title":"ChatCompletionOpenAI","text":"<p>               Bases: <code>ChatCompletionBase</code></p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>class ChatCompletionOpenAI(ChatCompletionBase):\n    def __init__(\n        self, model_name: str = \"gpt-3.5-turbo\", *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Simple chat completion model using OpenAI's API.\n\n        Must have `OPENAI_API_KEY` set as an environment variable.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            Model name on OpenAI, by default \"gpt-3.5-turbo\"\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.model_name = model_name\n        self.api_key = get_env_var(\"OPENAI_API_KEY\")\n        self.client = OpenAI(api_key=self.api_key)\n\n    def _respond(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        openai.api_key = self.api_key\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[{\"role\": \"user\", \"content\": message}],\n        )\n        return MessageResponse(response[\"choices\"][0][\"message\"][\"content\"])\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a direct message in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._respond(message=message, user_id=user_id)\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        \"\"\"\n        Method to respond to a channel mention in Slack.\n\n        Parameters\n        ----------\n        msg_in : str\n            Message from user\n        user_id : str\n            User ID\n\n        Returns\n        -------\n        MessageResponse\n            Response from the query engine.\n        \"\"\"\n        return self._respond(message=message, user_id=user_id)\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=[{\"role\": \"user\", \"content\": message}],\n            stream=True,\n        )\n\n        for chunk in stream_iter_progress_wrapper(response):\n            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionOpenAI.__init__","title":"__init__","text":"<pre><code>__init__(model_name: str = 'gpt-3.5-turbo', *args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Simple chat completion model using OpenAI\u2019s API.</p> <p>Must have <code>OPENAI_API_KEY</code> set as an environment variable.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name on OpenAI, by default \u201cgpt-3.5-turbo\u201d</p> <code>'gpt-3.5-turbo'</code> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def __init__(\n    self, model_name: str = \"gpt-3.5-turbo\", *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Simple chat completion model using OpenAI's API.\n\n    Must have `OPENAI_API_KEY` set as an environment variable.\n\n    Parameters\n    ----------\n    model_name : str, optional\n        Model name on OpenAI, by default \"gpt-3.5-turbo\"\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.model_name = model_name\n    self.api_key = get_env_var(\"OPENAI_API_KEY\")\n    self.client = OpenAI(api_key=self.api_key)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionOpenAI.channel_mention","title":"channel_mention","text":"<pre><code>channel_mention(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a channel mention in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a channel mention in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._respond(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/simple/chat_completion/#reginald.models.simple.chat_completion.ChatCompletionOpenAI.direct_message","title":"direct_message","text":"<pre><code>direct_message(message: str, user_id: str) -&gt; MessageResponse\n</code></pre> <p>Method to respond to a direct message in Slack.</p> <p>Parameters:</p> Name Type Description Default <code>msg_in</code> <code>str</code> <p>Message from user</p> required <code>user_id</code> <code>str</code> <p>User ID</p> required <p>Returns:</p> Type Description <code>MessageResponse</code> <p>Response from the query engine.</p> Source code in <code>reginald/models/simple/chat_completion.py</code> <pre><code>def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n    \"\"\"\n    Method to respond to a direct message in Slack.\n\n    Parameters\n    ----------\n    msg_in : str\n        Message from user\n    user_id : str\n        User ID\n\n    Returns\n    -------\n    MessageResponse\n        Response from the query engine.\n    \"\"\"\n    return self._respond(message=message, user_id=user_id)\n</code></pre>"},{"location":"reference/reginald/models/simple/hello/","title":"hello","text":""},{"location":"reference/reginald/models/simple/hello/#reginald.models.simple.hello.Hello","title":"Hello","text":"<p>               Bases: <code>ResponseModel</code></p> Source code in <code>reginald/models/simple/hello.py</code> <pre><code>class Hello(ResponseModel):\n    def __init__(self, *args: Any, **kwargs: Any):\n        \"\"\"\n        Basic response model that has set response to\n        direct messagesa and channel mentions.\n        \"\"\"\n        super().__init__(*args, emoji=\"wave\", **kwargs)\n\n    def direct_message(self, message: str, user_id: str) -&gt; MessageResponse:\n        return MessageResponse(\"Let's discuss this in a channel!\")\n\n    def channel_mention(self, message: str, user_id: str) -&gt; MessageResponse:\n        return MessageResponse(f\"Hello &lt;@{user_id}&gt;\")\n\n    def stream_message(self, message: str, user_id: str) -&gt; None:\n        # print(\"\\nReginald: \", end=\"\")\n        token_list: tuple[str, ...] = (\"Hello\", \"!\", \" How\", \" are\", \" you\", \"?\")\n        for token in stream_iter_progress_wrapper(token_list):\n            print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/reginald/models/simple/hello/#reginald.models.simple.hello.Hello.__init__","title":"__init__","text":"<pre><code>__init__(*args: Any, **kwargs: Any)\n</code></pre> <p>Basic response model that has set response to direct messagesa and channel mentions.</p> Source code in <code>reginald/models/simple/hello.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"\n    Basic response model that has set response to\n    direct messagesa and channel mentions.\n    \"\"\"\n    super().__init__(*args, emoji=\"wave\", **kwargs)\n</code></pre>"},{"location":"reference/reginald/slack_bot/","title":"slack_bot","text":""},{"location":"reference/reginald/slack_bot/bot/","title":"bot","text":""},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.ApiBot","title":"ApiBot","text":"<p>               Bases: <code>AsyncSocketModeRequestListener</code></p> Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>class ApiBot(AsyncSocketModeRequestListener):\n    def __init__(self, api_url: str, emoji: str) -&gt; None:\n        \"\"\"\n        Bot class for responding to Slack messages using an API to a response model.\n\n        Parameters\n        ----------\n        api_url : str\n            The api url of the model\n        emoji : str\n            The emoji to use when responding to a direct message/channel mention\n        \"\"\"\n        self.api_url = api_url\n        self.emoji = emoji\n\n        # set up queue and task\n        self.queue = asyncio.Queue(maxsize=12)\n        _ = asyncio.create_task(self.worker(self.queue))\n\n    async def __call__(self, client: SocketModeClient, req: SocketModeRequest) -&gt; None:\n        \"\"\"\n        Callback function for handling Slack requests.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        req : SocketModeRequest\n            Slack request\n        \"\"\"\n        if req.type == \"events_api\":\n            # acknowledge the request\n            logging.info(\"Received an events_api request\")\n            response = SocketModeResponse(envelope_id=req.envelope_id)\n            await client.send_socket_mode_response(response)\n\n            try:\n                # extract event from payload\n                event = req.payload[\"event\"]\n\n                # ignore messages from bots\n                if event.get(\"bot_id\") is not None:\n                    logging.info(\"Ignoring an event triggered by a bot.\")\n                    return\n                if event.get(\"hidden\") is not None:\n                    logging.info(\"Ignoring hidden message.\")\n                    return\n\n                # add clock emoji\n                logging.info(\"Reacting with clock emoji.\")\n                await client.web_client.reactions_add(\n                    name=\"clock2\",\n                    channel=event[\"channel\"],\n                    timestamp=event[\"ts\"],\n                )\n\n                self.queue.put_nowait((client, event))\n                logging.info(\n                    f\"There are currently {self.queue.qsize()} items in the queue.\"\n                )\n\n            except KeyError as exc:\n                logging.warning(\n                    f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n                )\n\n            except Exception as exc:\n                logging.error(\n                    f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n                )\n                raise\n\n        elif req.type == \"slash_commands\":\n            # acknowledge the request\n            logging.info(\"Received an slash_commands request\")\n            response = SocketModeResponse(envelope_id=req.envelope_id)\n            await client.send_socket_mode_response(response)\n\n            try:\n                # extract command, user, etc from payload\n                command = req.payload[\"command\"]\n                user_id = req.payload[\"user_id\"]\n\n                if command.startswith(\"/clear_history\"):\n                    if self.model.mode == \"chat\":\n                        logging.info(f\"Clearing {user_id}'s history\")\n                        if self.model.chat_engine.get(user_id) is not None:\n                            self.model.chat_engine[user_id].reset()\n                            message = \"All done! Chat history is cleared.\"\n                            logging.info(f\"Done clearing {user_id}'s history\")\n                        else:\n                            logging.info(f\"{user_id} has no history to be cleared.\")\n                            message = \"No history to clear\"\n                    else:\n                        logging.info(\"Using query engine, no history to be cleared.\")\n                        message = \"No history to clear\"\n\n                    logging.info(\"Posting clear_history message.\")\n                    await client.web_client.chat_postMessage(\n                        channel=req.payload[\"channel_id\"],\n                        text=message,\n                    )\n\n            except KeyError as exc:\n                logging.warning(\n                    f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n                )\n\n            except Exception as exc:\n                logging.error(\n                    f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n                )\n                raise\n\n        else:\n            logging.info(f\"Received unexpected request of type '{req.type}'\")\n            return\n\n    async def worker(self, queue: asyncio.Queue) -&gt; None:\n        \"\"\"\n        Async worker to process Slack requests.\n\n        Parameters\n        ----------\n        queue : asyncio.Queue\n            Queue of Slack requests to process\n        \"\"\"\n        while True:\n            (client, event) = await queue.get()\n            await self._process_request(client, event)\n            # notify the queue that the \"work item\" has been processed.\n            queue.task_done()\n\n    async def _process_request(\n        self,\n        client: SocketModeClient,\n        event: str,\n    ) -&gt; None:\n        \"\"\"\n        Method to process a Slack request and respond with a message.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        req : SocketModeRequest\n            Slack request\n        \"\"\"\n        # extract user and message information\n        message = event[\"text\"]\n        user_id = event[\"user\"]\n        event_type = event[\"type\"]\n        event_subtype = event.get(\"subtype\", None)\n\n        # start processing the message\n        logging.info(f\"Processing message '{message}' from user '{user_id}'.\")\n\n        # remove clock as we're about to process the message\n        await client.web_client.reactions_remove(\n            name=\"clock2\",\n            channel=event[\"channel\"],\n            timestamp=event[\"ts\"],\n        )\n\n        try:\n            # try to get a response from the model api\n            # if this is a direct message to Reginald...\n            if event_type == \"message\" and event_subtype is None:\n                await self.react(client, event[\"channel\"], event[\"ts\"])\n                # check VM is reachable\n                await self.ping()\n                model_response = await asyncio.get_running_loop().run_in_executor(\n                    None,\n                    lambda: requests.get(\n                        f\"{self.api_url}/direct_message\",\n                        json={\"message\": message, \"user_id\": user_id},\n                    ),\n                )\n\n            # if @Reginald is mentioned in a channel\n            elif event_type == \"app_mention\":\n                await self.react(client, event[\"channel\"], event[\"ts\"])\n                # check VM is reachable\n                await self.ping()\n                model_response = await asyncio.get_running_loop().run_in_executor(\n                    None,\n                    lambda: requests.get(\n                        f\"{self.api_url}/channel_mention\",\n                        json={\"message\": message, \"user_id\": user_id},\n                    ),\n                )\n\n            # otherwise\n            else:\n                logging.info(f\"Received unexpected event of type '{event['type']}'.\")\n                return\n\n            model_response.raise_for_status()\n            model_response = model_response.json()\n        except requests.exceptions.ConnectTimeout:\n            # if the model api times out, return a out of office message\n            logging.info(\"Model API timed out.\")\n            # add a error flag to the response so we send an error message\n            model_response = {\n                \"message\": \"Reginald is out of office\",\n                \"user_id\": user_id,\n                \"error\": True,\n                \"timeout\": True,\n            }\n        except Exception as err:\n            logging.error(\n                f\"Something went wrong in processing a Slack request.\\n{str(err)}\"\n            )\n            # add a error flag to the response so we send an error message\n            model_response = {\n                \"message\": f\"while processing your request, something went wrong: {type(err).__name__} - {err}\",\n                \"user_id\": user_id,\n                \"error\": True,\n                \"timeout\": False,\n            }\n\n        # add a reply as required\n        if model_response and model_response[\"message\"]:\n            logging.info(f\"Posting reply: {model_response['message']}.\")\n            if model_response.get(\"error\"):\n                if model_response[\"timeout\"]:\n                    # react with a sleeping emoji if timeout\n                    await client.web_client.reactions_add(\n                        name=\"sleeping\",\n                        channel=event[\"channel\"],\n                        timestamp=event[\"ts\"],\n                    )\n                else:\n                    # react with a general error emoji\n                    await client.web_client.reactions_add(\n                        name=\"x\",\n                        channel=event[\"channel\"],\n                        timestamp=event[\"ts\"],\n                    )\n                # if timeout, we send an out of office message\n                await client.web_client.chat_postMessage(\n                    channel=event[\"channel\"],\n                    text=f\"&lt;@{user_id}&gt;, {model_response['message']}.\",\n                )\n            else:\n                await client.web_client.chat_postMessage(\n                    channel=event[\"channel\"],\n                    text=f\"&lt;@{user_id}&gt;, you asked me: '{message}'.\\n{model_response['message']}\",\n                )\n        else:\n            logging.info(\"No reply was generated.\")\n\n    async def ping(self):\n        pong = await asyncio.get_running_loop().run_in_executor(\n            None,\n            lambda: requests.get(\n                f\"{self.api_url}/\",\n                timeout=5,\n            ),\n        )\n        pong.raise_for_status()\n\n    async def react(\n        self, client: SocketModeClient, channel: str, timestamp: str\n    ) -&gt; None:\n        \"\"\"\n        Emoji react to the input message.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        channel : str\n            Channel ID\n        timestamp : str\n            Timestamp of message\n        \"\"\"\n        if self.emoji:\n            logging.info(f\"Reacting with emoji {self.emoji}.\")\n            await client.web_client.reactions_add(\n                name=self.emoji,\n                channel=channel,\n                timestamp=timestamp,\n            )\n        else:\n            logging.info(\"No emoji defined for this bot.\")\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.ApiBot.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(client: SocketModeClient, req: SocketModeRequest) -&gt; None\n</code></pre> <p>Callback function for handling Slack requests.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>SocketModeClient</code> <p>Slack client</p> required <code>req</code> <code>SocketModeRequest</code> <p>Slack request</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def __call__(self, client: SocketModeClient, req: SocketModeRequest) -&gt; None:\n    \"\"\"\n    Callback function for handling Slack requests.\n\n    Parameters\n    ----------\n    client : SocketModeClient\n        Slack client\n    req : SocketModeRequest\n        Slack request\n    \"\"\"\n    if req.type == \"events_api\":\n        # acknowledge the request\n        logging.info(\"Received an events_api request\")\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        await client.send_socket_mode_response(response)\n\n        try:\n            # extract event from payload\n            event = req.payload[\"event\"]\n\n            # ignore messages from bots\n            if event.get(\"bot_id\") is not None:\n                logging.info(\"Ignoring an event triggered by a bot.\")\n                return\n            if event.get(\"hidden\") is not None:\n                logging.info(\"Ignoring hidden message.\")\n                return\n\n            # add clock emoji\n            logging.info(\"Reacting with clock emoji.\")\n            await client.web_client.reactions_add(\n                name=\"clock2\",\n                channel=event[\"channel\"],\n                timestamp=event[\"ts\"],\n            )\n\n            self.queue.put_nowait((client, event))\n            logging.info(\n                f\"There are currently {self.queue.qsize()} items in the queue.\"\n            )\n\n        except KeyError as exc:\n            logging.warning(\n                f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n            )\n\n        except Exception as exc:\n            logging.error(\n                f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n            )\n            raise\n\n    elif req.type == \"slash_commands\":\n        # acknowledge the request\n        logging.info(\"Received an slash_commands request\")\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        await client.send_socket_mode_response(response)\n\n        try:\n            # extract command, user, etc from payload\n            command = req.payload[\"command\"]\n            user_id = req.payload[\"user_id\"]\n\n            if command.startswith(\"/clear_history\"):\n                if self.model.mode == \"chat\":\n                    logging.info(f\"Clearing {user_id}'s history\")\n                    if self.model.chat_engine.get(user_id) is not None:\n                        self.model.chat_engine[user_id].reset()\n                        message = \"All done! Chat history is cleared.\"\n                        logging.info(f\"Done clearing {user_id}'s history\")\n                    else:\n                        logging.info(f\"{user_id} has no history to be cleared.\")\n                        message = \"No history to clear\"\n                else:\n                    logging.info(\"Using query engine, no history to be cleared.\")\n                    message = \"No history to clear\"\n\n                logging.info(\"Posting clear_history message.\")\n                await client.web_client.chat_postMessage(\n                    channel=req.payload[\"channel_id\"],\n                    text=message,\n                )\n\n        except KeyError as exc:\n            logging.warning(\n                f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n            )\n\n        except Exception as exc:\n            logging.error(\n                f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n            )\n            raise\n\n    else:\n        logging.info(f\"Received unexpected request of type '{req.type}'\")\n        return\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.ApiBot.__init__","title":"__init__","text":"<pre><code>__init__(api_url: str, emoji: str) -&gt; None\n</code></pre> <p>Bot class for responding to Slack messages using an API to a response model.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>The api url of the model</p> required <code>emoji</code> <code>str</code> <p>The emoji to use when responding to a direct message/channel mention</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>def __init__(self, api_url: str, emoji: str) -&gt; None:\n    \"\"\"\n    Bot class for responding to Slack messages using an API to a response model.\n\n    Parameters\n    ----------\n    api_url : str\n        The api url of the model\n    emoji : str\n        The emoji to use when responding to a direct message/channel mention\n    \"\"\"\n    self.api_url = api_url\n    self.emoji = emoji\n\n    # set up queue and task\n    self.queue = asyncio.Queue(maxsize=12)\n    _ = asyncio.create_task(self.worker(self.queue))\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.ApiBot.react","title":"react  <code>async</code>","text":"<pre><code>react(client: SocketModeClient, channel: str, timestamp: str) -&gt; None\n</code></pre> <p>Emoji react to the input message.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>SocketModeClient</code> <p>Slack client</p> required <code>channel</code> <code>str</code> <p>Channel ID</p> required <code>timestamp</code> <code>str</code> <p>Timestamp of message</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def react(\n    self, client: SocketModeClient, channel: str, timestamp: str\n) -&gt; None:\n    \"\"\"\n    Emoji react to the input message.\n\n    Parameters\n    ----------\n    client : SocketModeClient\n        Slack client\n    channel : str\n        Channel ID\n    timestamp : str\n        Timestamp of message\n    \"\"\"\n    if self.emoji:\n        logging.info(f\"Reacting with emoji {self.emoji}.\")\n        await client.web_client.reactions_add(\n            name=self.emoji,\n            channel=channel,\n            timestamp=timestamp,\n        )\n    else:\n        logging.info(\"No emoji defined for this bot.\")\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.ApiBot.worker","title":"worker  <code>async</code>","text":"<pre><code>worker(queue: Queue) -&gt; None\n</code></pre> <p>Async worker to process Slack requests.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue</code> <p>Queue of Slack requests to process</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def worker(self, queue: asyncio.Queue) -&gt; None:\n    \"\"\"\n    Async worker to process Slack requests.\n\n    Parameters\n    ----------\n    queue : asyncio.Queue\n        Queue of Slack requests to process\n    \"\"\"\n    while True:\n        (client, event) = await queue.get()\n        await self._process_request(client, event)\n        # notify the queue that the \"work item\" has been processed.\n        queue.task_done()\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.Bot","title":"Bot","text":"<p>               Bases: <code>AsyncSocketModeRequestListener</code></p> Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>class Bot(AsyncSocketModeRequestListener):\n    def __init__(self, model: ResponseModel) -&gt; None:\n        \"\"\"\n        Bot class for responding to Slack messages using a response model.\n\n        Parameters\n        ----------\n        model : ResponseModel\n            Response model to use for the bot\n        \"\"\"\n        self.model = model\n\n        # set up queue and task to run worker\n        self.queue = asyncio.Queue(maxsize=12)\n        _ = asyncio.create_task(self.worker(self.queue))\n\n    async def __call__(self, client: SocketModeClient, req: SocketModeRequest) -&gt; None:\n        \"\"\"\n        Callback function for handling Slack requests.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        req : SocketModeRequest\n            Slack request\n        \"\"\"\n        if req.type == \"events_api\":\n            # acknowledge the request\n            logging.info(\"Received an events_api request\")\n            response = SocketModeResponse(envelope_id=req.envelope_id)\n            await client.send_socket_mode_response(response)\n\n            try:\n                # extract event from payload\n                event = req.payload[\"event\"]\n\n                # ignore messages from bots\n                if event.get(\"bot_id\") is not None:\n                    logging.info(\"Ignoring an event triggered by a bot.\")\n                    return\n                if event.get(\"hidden\") is not None:\n                    logging.info(\"Ignoring hidden message.\")\n                    return\n\n                # add clock emoji\n                logging.info(\"Reacting with clock emoji.\")\n                await client.web_client.reactions_add(\n                    name=\"clock2\",\n                    channel=event[\"channel\"],\n                    timestamp=event[\"ts\"],\n                )\n\n                self.queue.put_nowait((client, event))\n                logging.info(\n                    f\"There are currently {self.queue.qsize()} items in the queue.\"\n                )\n\n            except KeyError as exc:\n                logging.warning(\n                    f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n                )\n\n            except Exception as exc:\n                logging.error(\n                    f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n                )\n                raise\n\n        elif req.type == \"slash_commands\":\n            # acknowledge the request\n            logging.info(\"Received an slash_commands request\")\n            response = SocketModeResponse(envelope_id=req.envelope_id)\n            await client.send_socket_mode_response(response)\n\n            try:\n                # extract command, user, etc from payload\n                command = req.payload[\"command\"]\n                user_id = req.payload[\"user_id\"]\n\n                if command.startswith(\"/clear_history\"):\n                    if self.model.mode == \"chat\":\n                        logging.info(f\"Clearing {user_id}'s history\")\n                        if self.model.chat_engine.get(user_id) is not None:\n                            self.model.chat_engine[user_id].reset()\n                            message = \"All done! Chat history is cleared.\"\n                            logging.info(f\"Done clearing {user_id}'s history\")\n                        else:\n                            logging.info(f\"{user_id} has no history to be cleared.\")\n                            message = \"No history to clear\"\n                    else:\n                        logging.info(\"Using query engine, no history to be cleared.\")\n                        message = \"No history to clear\"\n\n                    logging.info(\"Posting clear_history message.\")\n                    await client.web_client.chat_postMessage(\n                        channel=req.payload[\"channel_id\"],\n                        text=message,\n                    )\n\n            except KeyError as exc:\n                logging.warning(\n                    f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n                )\n\n            except Exception as exc:\n                logging.error(\n                    f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n                )\n                raise\n\n        else:\n            logging.info(f\"Received unexpected request of type '{req.type}'\")\n            return\n\n    async def worker(self, queue: asyncio.Queue) -&gt; None:\n        \"\"\"\n        Async worker to process Slack requests.\n\n        Parameters\n        ----------\n        queue : asyncio.Queue\n            Queue of Slack requests to process\n        \"\"\"\n        while True:\n            (client, event) = await queue.get()\n            await self._process_request(client, event)\n            # notify the queue that the \"work item\" has been processed.\n            queue.task_done()\n\n    async def _process_request(\n        self,\n        client: SocketModeClient,\n        event: str,\n    ) -&gt; None:\n        \"\"\"\n        Method to process a Slack request and respond with a message.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        req : SocketModeRequest\n            Slack request\n        \"\"\"\n        # extract user and message information\n        message = event[\"text\"]\n        user_id = event[\"user\"]\n        event_type = event[\"type\"]\n        event_subtype = event.get(\"subtype\", None)\n\n        # start processing the message\n        logging.info(f\"Processing message '{message}' from user '{user_id}'.\")\n\n        await client.web_client.reactions_remove(\n            name=\"clock2\",\n            channel=event[\"channel\"],\n            timestamp=event[\"ts\"],\n        )\n\n        # if this is a direct message to Reginald...\n        if event_type == \"message\" and event_subtype is None:\n            await self.react(client, event[\"channel\"], event[\"ts\"])\n            model_response = await asyncio.get_running_loop().run_in_executor(\n                None, self.model.direct_message, message, user_id\n            )\n\n        # if @Reginald is mentioned in a channel\n        elif event_type == \"app_mention\":\n            await self.react(client, event[\"channel\"], event[\"ts\"])\n            model_response = await asyncio.get_running_loop().run_in_executor(\n                None, self.model.channel_mention, message, user_id\n            )\n\n        # otherwise\n        else:\n            logging.info(f\"Received unexpected event of type '{event['type']}'.\")\n            return\n\n        # add a reply as required\n        if model_response and model_response.message:\n            logging.info(f\"Posting reply {model_response.message}.\")\n            await client.web_client.chat_postMessage(\n                channel=event[\"channel\"],\n                text=f\"&lt;@{user_id}&gt;, you asked me: '{message}'.\\n{model_response.message}\",\n            )\n        else:\n            logging.info(\"No reply was generated.\")\n\n    async def react(\n        self, client: SocketModeClient, channel: str, timestamp: str\n    ) -&gt; None:\n        \"\"\"\n        Emoji react to the input message.\n\n        Parameters\n        ----------\n        client : SocketModeClient\n            Slack client\n        channel : str\n            Channel ID\n        timestamp : str\n            Timestamp of message\n        \"\"\"\n        if self.model.emoji:\n            logging.info(f\"Reacting with emoji {self.model.emoji}.\")\n            await client.web_client.reactions_add(\n                name=self.model.emoji,\n                channel=channel,\n                timestamp=timestamp,\n            )\n        else:\n            logging.info(\"No emoji defined for this model.\")\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.Bot.__call__","title":"__call__  <code>async</code>","text":"<pre><code>__call__(client: SocketModeClient, req: SocketModeRequest) -&gt; None\n</code></pre> <p>Callback function for handling Slack requests.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>SocketModeClient</code> <p>Slack client</p> required <code>req</code> <code>SocketModeRequest</code> <p>Slack request</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def __call__(self, client: SocketModeClient, req: SocketModeRequest) -&gt; None:\n    \"\"\"\n    Callback function for handling Slack requests.\n\n    Parameters\n    ----------\n    client : SocketModeClient\n        Slack client\n    req : SocketModeRequest\n        Slack request\n    \"\"\"\n    if req.type == \"events_api\":\n        # acknowledge the request\n        logging.info(\"Received an events_api request\")\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        await client.send_socket_mode_response(response)\n\n        try:\n            # extract event from payload\n            event = req.payload[\"event\"]\n\n            # ignore messages from bots\n            if event.get(\"bot_id\") is not None:\n                logging.info(\"Ignoring an event triggered by a bot.\")\n                return\n            if event.get(\"hidden\") is not None:\n                logging.info(\"Ignoring hidden message.\")\n                return\n\n            # add clock emoji\n            logging.info(\"Reacting with clock emoji.\")\n            await client.web_client.reactions_add(\n                name=\"clock2\",\n                channel=event[\"channel\"],\n                timestamp=event[\"ts\"],\n            )\n\n            self.queue.put_nowait((client, event))\n            logging.info(\n                f\"There are currently {self.queue.qsize()} items in the queue.\"\n            )\n\n        except KeyError as exc:\n            logging.warning(\n                f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n            )\n\n        except Exception as exc:\n            logging.error(\n                f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n            )\n            raise\n\n    elif req.type == \"slash_commands\":\n        # acknowledge the request\n        logging.info(\"Received an slash_commands request\")\n        response = SocketModeResponse(envelope_id=req.envelope_id)\n        await client.send_socket_mode_response(response)\n\n        try:\n            # extract command, user, etc from payload\n            command = req.payload[\"command\"]\n            user_id = req.payload[\"user_id\"]\n\n            if command.startswith(\"/clear_history\"):\n                if self.model.mode == \"chat\":\n                    logging.info(f\"Clearing {user_id}'s history\")\n                    if self.model.chat_engine.get(user_id) is not None:\n                        self.model.chat_engine[user_id].reset()\n                        message = \"All done! Chat history is cleared.\"\n                        logging.info(f\"Done clearing {user_id}'s history\")\n                    else:\n                        logging.info(f\"{user_id} has no history to be cleared.\")\n                        message = \"No history to clear\"\n                else:\n                    logging.info(\"Using query engine, no history to be cleared.\")\n                    message = \"No history to clear\"\n\n                logging.info(\"Posting clear_history message.\")\n                await client.web_client.chat_postMessage(\n                    channel=req.payload[\"channel_id\"],\n                    text=message,\n                )\n\n        except KeyError as exc:\n            logging.warning(\n                f\"Attempted to access key that does not exist.\\n{str(exc)}\"\n            )\n\n        except Exception as exc:\n            logging.error(\n                f\"Something went wrong in processing a Slack request.\\nPayload: {req.payload}.\\n{str(exc)}\"\n            )\n            raise\n\n    else:\n        logging.info(f\"Received unexpected request of type '{req.type}'\")\n        return\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.Bot.__init__","title":"__init__","text":"<pre><code>__init__(model: ResponseModel) -&gt; None\n</code></pre> <p>Bot class for responding to Slack messages using a response model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ResponseModel</code> <p>Response model to use for the bot</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>def __init__(self, model: ResponseModel) -&gt; None:\n    \"\"\"\n    Bot class for responding to Slack messages using a response model.\n\n    Parameters\n    ----------\n    model : ResponseModel\n        Response model to use for the bot\n    \"\"\"\n    self.model = model\n\n    # set up queue and task to run worker\n    self.queue = asyncio.Queue(maxsize=12)\n    _ = asyncio.create_task(self.worker(self.queue))\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.Bot.react","title":"react  <code>async</code>","text":"<pre><code>react(client: SocketModeClient, channel: str, timestamp: str) -&gt; None\n</code></pre> <p>Emoji react to the input message.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>SocketModeClient</code> <p>Slack client</p> required <code>channel</code> <code>str</code> <p>Channel ID</p> required <code>timestamp</code> <code>str</code> <p>Timestamp of message</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def react(\n    self, client: SocketModeClient, channel: str, timestamp: str\n) -&gt; None:\n    \"\"\"\n    Emoji react to the input message.\n\n    Parameters\n    ----------\n    client : SocketModeClient\n        Slack client\n    channel : str\n        Channel ID\n    timestamp : str\n        Timestamp of message\n    \"\"\"\n    if self.model.emoji:\n        logging.info(f\"Reacting with emoji {self.model.emoji}.\")\n        await client.web_client.reactions_add(\n            name=self.model.emoji,\n            channel=channel,\n            timestamp=timestamp,\n        )\n    else:\n        logging.info(\"No emoji defined for this model.\")\n</code></pre>"},{"location":"reference/reginald/slack_bot/bot/#reginald.slack_bot.bot.Bot.worker","title":"worker  <code>async</code>","text":"<pre><code>worker(queue: Queue) -&gt; None\n</code></pre> <p>Async worker to process Slack requests.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue</code> <p>Queue of Slack requests to process</p> required Source code in <code>reginald/slack_bot/bot.py</code> <pre><code>async def worker(self, queue: asyncio.Queue) -&gt; None:\n    \"\"\"\n    Async worker to process Slack requests.\n\n    Parameters\n    ----------\n    queue : asyncio.Queue\n        Queue of Slack requests to process\n    \"\"\"\n    while True:\n        (client, event) = await queue.get()\n        await self._process_request(client, event)\n        # notify the queue that the \"work item\" has been processed.\n        queue.task_done()\n</code></pre>"},{"location":"reference/reginald/slack_bot/run_bot/","title":"run_bot","text":""},{"location":"reference/reginald/slack_bot/utils/","title":"utils","text":""},{"location":"reference/reginald/slack_bot/utils/#reginald.slack_bot.utils.setup_api_slack_bot","title":"setup_api_slack_bot","text":"<pre><code>setup_api_slack_bot(api_url: str, emoji: str | None) -&gt; ApiBot\n</code></pre> <p>Initialise <code>ApiBot</code> with response model.</p> <p>Parameters:</p> Name Type Description Default <code>emoji</code> <code>str</code> <p>Emoji to use for the bot for responding to messages</p> required <p>Returns:</p> Type Description <code>ApiBot</code> <p>Bot which uses an API for responding to messages</p> Source code in <code>reginald/slack_bot/utils.py</code> <pre><code>def setup_api_slack_bot(api_url: str, emoji: str | None) -&gt; ApiBot:\n    \"\"\"\n    Initialise `ApiBot` with response model.\n\n    Parameters\n    ----------\n    emoji : str\n        Emoji to use for the bot for responding to messages\n\n    Returns\n    -------\n    ApiBot\n        Bot which uses an API for responding to messages\n    \"\"\"\n    logging.info(f\"Initalising bot at {api_url}\")\n\n    if emoji is None:\n        emoji = EMOJI_DEFAULT\n    logging.info(f\"Initalising bot with {emoji} emoji\")\n\n    # set up bot with the api_url and emoji\n    slack_bot = ApiBot(api_url=api_url, emoji=emoji)\n\n    return slack_bot\n</code></pre>"},{"location":"reference/reginald/slack_bot/utils/#reginald.slack_bot.utils.setup_slack_bot","title":"setup_slack_bot","text":"<pre><code>setup_slack_bot(model: ResponseModel) -&gt; Bot\n</code></pre> <p>Initialise <code>Bot</code> with response model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ResponseModel</code> <p>Response model to use for the bot</p> required <p>Returns:</p> Type Description <code>Bot</code> <p>Bot with response model</p> Source code in <code>reginald/slack_bot/utils.py</code> <pre><code>def setup_slack_bot(model: ResponseModel) -&gt; Bot:\n    \"\"\"\n    Initialise `Bot` with response model.\n\n    Parameters\n    ----------\n    model : ResponseModel\n        Response model to use for the bot\n\n    Returns\n    -------\n    Bot\n        Bot with response model\n    \"\"\"\n    logging.info(f\"Initalising bot with model: {model}\")\n\n    slack_bot = Bot(model=model)\n\n    return slack_bot\n</code></pre>"},{"location":"reference/reginald/slack_bot/utils/#reginald.slack_bot.utils.setup_slack_client","title":"setup_slack_client","text":"<pre><code>setup_slack_client(\n    slack_bot: ApiBot | Bot, slack_app_token: str, slack_bot_token: str\n) -&gt; SocketModeClient\n</code></pre> <p>Initialise Slack client with bot.</p> <p>Parameters:</p> Name Type Description Default <code>slack_bot</code> <code>ApiBot | Bot</code> <p>Bot to use for responding to messages. This can be either an ApiBot object (a bot which uses an API for responding) or a Bot object (a bot which uses a response model object directly for responding)</p> required <p>Returns:</p> Type Description <code>SocketModeClient</code> <p>Slack client with bot</p> Source code in <code>reginald/slack_bot/utils.py</code> <pre><code>def setup_slack_client(\n    slack_bot: ApiBot | Bot, slack_app_token: str, slack_bot_token: str\n) -&gt; SocketModeClient:\n    \"\"\"\n    Initialise Slack client with bot.\n\n    Parameters\n    ----------\n    slack_bot : ApiBot | Bot\n        Bot to use for responding to messages.\n        This can be either an ApiBot object\n        (a bot which uses an API for responding)\n        or a Bot object (a bot which uses a response\n        model object directly for responding)\n\n    Returns\n    -------\n    SocketModeClient\n        Slack client with bot\n    \"\"\"\n    logging.info(\"Connecting to Slack...\")\n\n    # initialize SocketModeClient with an app-level token + AsyncWebClient\n    client = SocketModeClient(\n        # this app-level token will be used only for establishing a connection\n        app_token=slack_app_token,\n        # you will be using this AsyncWebClient for performing Web API calls in listeners\n        web_client=AsyncWebClient(token=slack_bot_token),\n        # to ensure connection doesn't go stale - we can adjust as needed.\n        ping_interval=60,\n    )\n\n    # add a new listener to receive messages from Slack\n    client.socket_mode_request_listeners.append(slack_bot)\n\n    return client\n</code></pre>"}]}