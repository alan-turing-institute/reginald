url,body
https://www.turing.ac.uk/about-us/impact/supporting-innovation-fintech-sector,"
The Turing was a key academic partner in the Digital Sandbox Pilot – an initiative from the Financial Conduct Authority (FCA) and the City of London Corporation that ran in winter 2020-21 and provided 28 financial technology (fintech) firms with tools to test, develop and showcase their technologies. The aim was to develop products and services to tackle financial challenges brought about by the COVID-19 pandemic, including supporting people who have become financially vulnerable, and preventing fraud and scams.
A crucial aspect of this pilot was synthetic data – artificially generated data that has realistic statistical properties but no identifying information, making it suited for training and validating computer models in areas where privacy is key. The Turing was the lead evaluator of the synthetic data used in the pilot, with researchers in the finance and economics programme and Research Engineering team analysing the data to ensure that it was good quality. The synthetic data made available to participants included details of seven million fictional individuals, 400 million banking transactions made through five fictional banks, and five million devices used for electronic payments.
“Our project wouldn’t have been possible without the synthetic data that was evaluated by the Turing’s experts. They helped us to make sure that the data matched up as closely as possible to real-world data.”
Matt Lowe, Senior Technical Specialist at the FCA and technical lead on the Digital Sandbox Pilot
The pilot ran from November 2020 to February 2021, and the FCA hopes to make the synthetic data more widely available, so that other fintech firms can benefit. The FCA is now planning a second pilot, this time focusing on financial challenges related to sustainability, while also exploring operating and governance models for a permanent version of the digital sandbox.
 
This piece first appeared in The Alan Turing Institute’s Annual Report 2020-21
Top image: alevision / Unsplash
"
https://www.turing.ac.uk/research/research-projects/elsa-eu-project,"
A large network of top European experts in artificial intelligence (AI) and machine learning (ML) is to promote the development and deployment of cutting-edge AI solutions in the future and make Europe the world's lighthouse of AI. The EU is funding the project with around €7.5 million, with a further €2.5 million coming from the United Kingdom and Switzerland. The ""European Lighthouse on Secure and Safe AI"" (ELSA) project networks excellent researchers from 26 top research institutions and companies in Europe to pool their expertise in the field of AI and machine learning. The growing network builds upon the European Laboratory for Learning and Intelligent Systems (ELLIS) and is coordinated by the CISPA Helmholtz Center for Information Security in Saarbrücken which is member of the ELLIS Unit Saarbrücken.
""Artificial intelligence has the potential to improve all our lives enormously - be it through better healthcare or completely new possibilities for mobility. But a blessing can quickly turn into a curse if the technology is not based on a secure foundation. I see enormous potential in bringing together Europe's top researchers through ELSA so that we can face the great challenges of AI and ML together. I am sure we can make a lot of difference together,"" says Mario Fritz, faculty at CISPA.
The new virtual centre of excellence will focus on the further development of ML methods, in particular so-called deep learning, as these methods form the basis for most modern AI applications. The initial three-year project will focus on developing robust technical approaches that are compatible with legal and ethical principles. At the same time, the network of excellence aims to create the necessary structures to promote the development and use of AI technology and to break down barriers. ELSA builds on and extends ELLIS. Founded in 2018,  ELLIS is an internationally recognized pan-European AI network of excellence which wants to ensure that the highest level of modern AI research can be performed in the open societies of Europe.
“ELLIS has set out to build a European brand of AI. For this it is crucial not only to have state-of-the-art performance, but also a high level of transparency, accountability, and trust. With its focus on security and safety, ELSA tackles some of the core issues, and with its outstanding team drawing from the extended ELLIS community, it has the technical power to make progress on some of the hard open problems"", says ELLIS President Bernhard Schölkopf.
ELLIS and its partner network ELISE (European Network of AI Excellence Centres) have already demonstrated in the past what Europe is capable of in the field of AI and ML. Following their success and structure, ELSA also rests on three pillars: selected research programs, local research units, and a PhD and postdoc program.
The ELSA research programs will underpin the research agenda of the 26 partnering institutions and will focus on technical robustness and security, privacy protection techniques and infrastructures, and human agency and oversight. One of the primary goals of the network of excellence is to detect threats early and mitigate damage. This is important because areas for AI solutions are such sensitive as the healthcare sector or autonomous driving. Artificial intelligence is also used in robotics, cybersecurity, media and document security. All of these application areas are the focus of the network.
To achieve its goals, the network is addressing three major challenges: The development of robustness guarantees and certificates, data-secure and robust collaborative learning, and the development of human control mechanisms for the ethical and secure use of AI. The local research units are designed to bring together the best AI researchers in Europe not only virtually, but also at their sites. In addition, excellent researchers from other institutions can be connected to ELSA units through fellowships. Above all, the network will be open to contributions from industry partners and collaborations in order to significantly advance AI solutions and remain internationally competitive.
This is where the ELSA Innovation Lab will make a decisive contribution. Building on the already existing and highly successful PhD and postdoc programs of ELLIS and ELISE, ELSA will support young researchers from all over the world. They will benefit from the knowledge and shared mentorship of leading scientists in the network, be able to participate in events to network, and have the opportunity to further their education through workshops. ELSA is funded under Horizon Europe, one of the largest research and innovation funding programs in the world. Its content is geared toward important societal issues such as health, sustainable development, and digitalisation. A central goal of the EU framework program is to promote excellence in European science. For more information please visit here.
The Turing is working primarily on WP3 'Human Agency and Oversight' T3.3, T3.4 with University of Birmingham in order to deliver T3.5 which the Institute is leading on. The Turing is responsible for delivering D3.6 Papers and accompanying code from Task 3.5 with new methods and open source tools.
"
https://www.turing.ac.uk/events/citizen-participation-and-machine-learning-better-democracy,"
131.2 KB
View YouTube playlist

This event is going to be recorded and live streamed, this can viewed via our YouTube. 
Around the world, digital participation platforms are being used as a tool for direct democracy, aiming to empower citizens to contribute to policy making. As trust in traditional democratic institutions declines, these deliberative platforms offer a way to build new relationships and trust between citizens and policy-makers.
However, experience suggests that the popularity of these direct democracy initiatives and the sheer volume of proposals can make it difficult for citizens and policy-makers to make sense of them. Hence, common objectives may prove difficult to achieve; paradoxically, the consequence may be to reduce the willingness of citizens to get involved in direct democracy initiatives.
Understanding this challenge as a unique opportunity, researchers at the Turing are joining forces with Consul, the world's most popular digital participation platform, to create new opportunities for citizen engagement in policy-making by combining human collective intelligence with machine learning.
At this event, speakers will present on how machine learning techniques can be applied to digital democracy platforms to enhance citizen engagement in policy making.
Speakers will present on the Consul project, on how machine learning is being used to enhance citizen’s experiences of direct democracy, new cases of direct democracy in Scotland and the Netherlands, and more advanced applications of machine learning in this critically important domain.
The event is open to members of the general public interested in digital direct democracy, experts in machine learning and natural language processing, and policy-makers.
Draft agenda for ""Citizen participation and machine learning for a better democracy""

"
https://www.turing.ac.uk/events/rsf-seminar-series-applying-learning-patient-experience-research-practice-change,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility project. 
Speaker: Michelle Roberts & Eve Riley, The Richmond Group of Charities
Event title: Applying learning from patient experience research into practice change
In this session, we will explore learning from our programmatic work around the topic of supporting people to live well with multiple long-term conditions. The session will focus on key insights from the qualitative research reports undertaken by the Taskforce on Multiple Conditions, with a specific focus on how multiple conditions intersect with issues of health equity. We will also look at how research informs the charities’ own services and support offers through the Movement for All programme, and how research can be used to advocate for change in practice.
Everyone is welcome, however, the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn about research outputs from the Taskforce on Multiple Conditions and how this learning can be applied to their respective areas of work.
"
https://www.turing.ac.uk/news/publications/building-resilience-government-using-data-science,"
In the aftermath of crises, from world wars to global pandemics, governments often seek resilience in their rebuilding efforts. Prioritising resilience as an organisational value entails developing decision-making processes that are stable and adaptable.
Technology can help, but in recent decades governments have tended to use technology to further other values, such as economic efficiency. Modern data-intensive technologies, such as data science and artificial intelligence (AI), hold tremendous potential to rebuild resilience in government. However, changes are needed in order to realise this potential.
Researchers from the AI for science & government (ASG) programme at The Alan Turing Institute have been addressing these challenges under the multidisciplinary theme of ‘Shocks and resilience’ (S&R). In this white paper, we argue that developing resilience requires a new and distinctly public sector approach to data science, in which data-intensive technologies do not just automate or replicate what humans can already do well, but rather do things which people cannot – such as tackling difficult, multi-sector problems that have no 'right' solution. We illustrate our argument with selected case studies based on projects involving ASG researchers.
Based on our experiences under the S&R theme and within the broader ASG programme, we propose five recommendations for building resilience into government using data science:
Following these recommendations will place governments in a better position to tackle the growing list of existential problems that loom, from the next pandemic to global environmental collapse
White paper coming soon. 
"
https://www.turing.ac.uk/people/external-researchers/nunzio-alberto-borghese,"

"
https://www.turing.ac.uk/events/market-simulators,"
Regulators, researchers, and professional investors are interested in the impact that proposed financial policies, trading algorithms, and agent behaviour have on financial systems. This interest follows from an onslaught of flash-crashes that has led to a call for improved market stability. The problem is that purely studying the record of the past only gives us a glimpse of a single world view. To study a true counterfactual there is a need to model the financial system and its interacting parts in all its complexity. To do this we have to, at first, set aside reductive economic theories and focus on modelling complex adaptive systems. In recent years multi-agent based systems have been the prevailing paradigm to model realistic financial environments. Current machine learning innovations like deep supervised and reinforcement learning have also played an important role in accurately modelling advanced algorithmic methods. In this workshop, we bring together the world’s thought leaders on this subject.
Derek Snow, Stefan Zohren, and Sam Cohen.
10:00-10:15 – Sam Cohen – Oxford, Turing
 
10:15-10:45 – Steve Roberts – Oxford Man Institute
10:45-11:00 – Discussion
 
11:00-11:30 – James Paulin – CS Oxford University
11:30-11:45 – Discussion  
 
11:45-12:15 – Maarten Scholl – INET Oxford
12:15-12:30 – Discussion
 
12:30-13:00 – Zihao Zhang - Oxford Man Institute
13:00-13:15 – Discussion
 
13:15-13:45 – Michael Wellman – University of Michigan
13:45-14:00 – Discussion
 
14:00-14:30 – David Byrd - Georgia Institute of Technology
14:30-14:45 – Discussion
 
14:45-15:15 – Sumitra Ganesh – JP Morgan AI Research
15:15-15:30 – Discussion
 
15:30-15:45 – Stefan Zohren – Oxford Man Institute
 
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-warwick,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/airbus,"
www.airbus.com
"
https://www.turing.ac.uk/people/researchers/katherine-mcdonough,"
Katherine McDonough is Lecturer in Digital Humanities in the Department of History at Lancaster University and Senior Research Fellow at The Alan Turing Institute.
Her digital work uses computational image, text, and spatial analysis to create new ways of doing historical research with large cultural heritage collections as data. With colleagues from Living with Machines, she co-developed the MapReader software library. She is the UK PI on Machines Reading Maps, a project that creates, curates, and analyses datasets of text on maps for research and discovery applications. In addition to her Turing projects she works on GEODE with colleagues in France.
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/operating-systems,"
"
https://www.turing.ac.uk/events/ima-leslie-fox-prize-2023,"
Photo by Pawel Czerwinski on Unsplash
Partnering with the Institute of Mathematics and its Applications (IMA) in this year’s Leslie Fox Prize for the first time, The Alan Turing Institute is pleased to invite numerical analysts to attend the 21st Leslie Fox Prize presentation day. The exclusively in-person event takes place between 09:00 and 17:00 on Monday 26 June at the University of Strathclyde.
Featuring the next generation of worldwide leaders in numerical analysis, the presentation day will showcase a great range of important research topics, as well as current research developments in numerical analysis. The day-long event includes presentations from this year’s six shortlisted candidates, as well as outstanding networking opportunities with a brilliant selection of leaders and researchers in the field.
The Leslie Fox Prize presentation event takes place the day before the 29th Biennial Numerical Analysis Conference 2023, which is hosted by the Department of Mathematics and Statistics at the University of Strathclyde from Tuesday 27 to Friday 30 June 2023. 
The event will not be live streamed but the recordings of the talks will be available on The Alan Turing Institute's YouTube channel afterwards. 
The biennial Leslie Fox Prize was first established in 1985, in honour of the eponymous researcher and distinguished mathematician, Leslie Fox (1918-1992).
Awarded every two years, the prize honours leading early career researchers in the field across the globe. Numerical analysts under the age of 31 submit papers for review with shortlisted candidates invited to give lectures at the Leslie Fox Prize meeting.
First and second places are awarded on the day by an expert adjudicating committee, based on mathematical and algorithmic brilliance in tandem with presentational skills.
The panel comprises sector-leading professionals, this year including:
Set to be an outstanding opportunity for numerical analysts to catch up with one another while expanding their networks and discovering the details of shortlisted candidates’ papers, mathematics doctoral candidates are invited to register attendance for this stimulating day of research and networking.
Lunch and refreshments will be provided. 
 
This event is organised in collaboration with the Institute of Mathematics and its Applications (IMA)

For more information on the agenda and speakers at this event, please follow the links below:
We welcome participation from all backgrounds, but this event will be of interest to numerical analysts. 
Register for free
Technology & Innovation Centre
University of Strathclyde
99 George Street
Glasgow
G1 1RD
"
https://www.turing.ac.uk/people/turing-ai-fellows/mirella-lapata,"
Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh. Her research focuses on getting computers to understand, reason with, and generate natural language. She is the first recipient (2009) of the British Computer Society and Information Retrieval Specialist Group (BCS/IRSG) Karen Sparck Jones award and a Fellow of the Royal Society of Edinburgh, the ACL, and Academia Europaea.
Mirella has also received best paper awards in leading NLP conferences and has served on the editorial boards of the Journal of Artificial Intelligence Research, the Transactions of the ACL, and Computational Linguistics. She was president of SIGDAT (the group that organizes EMNLP) in 2018. She has been awarded an ERC consolidator grant, a Royal Society Wolfson Research Merit Award, and a UKRI Turing AI World-Leading Researcher Fellowship.
"
https://www.turing.ac.uk/events/robotics-and-ai-health-and-social-care,"
182 KB
The UK government and parliament (2018 UK Parliament’s POST report on Robotics in Social Care) have identified the need to support research and innovation in the field of robotics and AI for health and social care. This is in response to the expected growth in needs for the ageing population, and the contribution AI and robotics technologies can offer.
This collaborative workshop will take place at the Alan Turing building at University of Manchester.
This workshop aims to achieve a shared awareness of the strengths and complementarity of UK and International projects and activities in this field, to identify the main research challenges, and foster interaction and collaboration. International experts will also be invited to share good practice and further support international collaboration.

Academic speakers include experts already involved in assistive robotics and AI projects, from the UK, EU and Japan. It will also have talks from experts in ethics and law for healthcare technologies and for responsible research and innovation. Participants will comprise stakeholders from charities, industry, and policy makers.
This workshop is sponsored by The Alan Turing Institute, the H2020 project MoveCare and CloudMinds.
 

Application now closed.
Room G2.07, 
Alan Turing Building, 
Oxford Road, 
Manchester,
M13 9PL
Maps and travel
Recommended hotels
"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/ensemble-mathematical-modelling,"
"
https://www.turing.ac.uk/news/publications/efficient-and-robust-model-benchmarks-item-response-theory-and-adaptive-testing,"
Progress in predictive machine learning is typically measured on the basis of performance comparisons on benchmark datasets. Traditionally these kinds of empirical evaluation are carried out on large numbers of datasets, but this is becoming increasingly hard due to computational requirements and the often large number of alternative methods to compare against. In this paper we investigate adaptive approaches to achieve better efficiency on model benchmarking. For a large collection of datasets, rather than training and testing a given approach on every individual dataset, we seek methods that allow us to pick only a few representative datasets to quantify the model’s goodness, from which to extrapolate to performance on other datasets. To this end, we adapt existing approaches from psychometrics: specifically, Item Response Theory and Adaptive Testing. Both are well-founded frameworks designed for educational tests. We propose certain modifications following the requirements of machine learning experiments, and present experimental results to validate the approach.
Song H., Flach P., Efficient and Robust Model Benchmarks with Item Response Theory and Adaptive Testing, International Journal of Interactive Multimedia and Artificial Intelligence, Vol. 6, No. 5 (2021)
1.5 MB
"
https://www.turing.ac.uk/people/enrichment-students/luisa-fassi,"
Luisa is entering her second PhD year at the University of Cambridge, where she works across the MRC Cognition and Brain Sciences Unit and the Department of Psychiatry. Her current research focuses on disentangling the links between social media use and adolescents' mental health. She is supervised by Dr Amy Orben and Prof Tamsin Ford. Luisa previously completed an Master of Science in Psychological Research at the University of Oxford, specialising in research methods and design. Previously, she completed a Bachelor of Science in Psychology at Erasmus University Rotterdam. During her academic path, Luisa has worked on research projects in the fields of molecular psychiatry, cognitive psychology, and neuroscience. Alongside her interest in psychological and medical sciences, Luisa is an active promoter of open research practices. She is a junior representative of RIOT, a Science Club promoting reproducible, interpretable, open, and transparent practices. Moreover, she is a Data Champion at the University of Cambridge and a certified instructor for The Carpentries, for which she teaches foundational coding and data science skills to researchers. 
Luisa’s doctoral research focuses on understanding the impact of social media use on adolescents’ mental health. Social media are a pervasive component of young people’s everyday life. However, empirical evidence is sparse despite the widespread public concern around their potential impact on youth’s well-being and mental health. Specifically, their association with and effect on different mental health conditions is still an open question. In her PhD, Luisa addresses this question by analysing data from large-scale national surveys to examine the associations and directional links between social media use and mental health outcomes for adolescents affected by different clinical conditions (e.g., mood disorders, conduct disorders, eating disorders) as compared to adolescents from the general population. At Turing, Luisa aims to broaden her training in quantitative analysis of large-scale and multidimensional data. 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/queens-university-belfast,"
"
https://www.turing.ac.uk/news/publications/machines-media-semantic-change-lexicon-mechanization-19th-century-british,"
The industrialisation process associated with the so-called Industrial Revolution in 19th - century Great Britain was a time of profound changes, including in the English lexicon. An important yet understudied phenomenon is the semantic shift in the lexicon of mechanisation. In this paper we present the first large-scale analysis of terms related to mechanisation over the course of the 19th century in English. We draw on a corpus of historical British newspapers comprising 4.6 billion tokens and train historical word embedding models. We test existing semantic change detection techniques and analyse the results in light of previous historical linguistic scholarship.
Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities, pages 85–95, Taipei, Taiwan. Association for Computational Linguistics.
"
https://www.turing.ac.uk/people/guest-speakers/yajing-zhu,"
Yajing Zhu is a Principal Data Scientist and Neuroscience network lead at Roche personalised healthcare centre of excellence. She is responsible for data and evidence generation strategy for digital health product developments, and co-PI for a few industry-academia collaborations.
"
https://www.turing.ac.uk/people/business-team/andre-piza,"
"
https://www.turing.ac.uk/research/research-programmes/defence-and-security/people,"

"
https://www.turing.ac.uk/news/how-begin-data-science-projects,"
I sometimes wonder whether being a professional means knowing what to do when you don't know what to do.
At one time in my life I was a teaching assistant for undergraduate physics courses. Quite a lot of learning undergraduate physics involves solving undergraduate physics problems and physics problems can be very daunting to undergraduates. You get stuck; you have no idea what to do; it's all very stressful. My advice to my students was this: if you've read the question and you don't know how to proceed, you should draw a diagram.
Me: What do you think you should do here?
Student: I don't know. I'm stuck.
Me: What do I always tell you you should do?  
Student: Draw a diagram.
Me: Great! Let's do that, then.
We had this conversation a lot. I don't know about you, but as a data scientist I frequently don't know what to do and I find that pretty stressful. What should we do when we don't know what to do?
I'd like to tell you what we do at the start of projects, which is frequently the time when it's not clear what to do.
At the start of a project, the first thing we do is write a document, called a “Backbrief,” in which we outline:
We try to write this document in collaboration with whoever it is that wants the results of the project but, in any case, we make sure to share it before starting.
Now, I'll grant you, this sounds pretty obvious. But then again, so did the advice to my students to draw a diagram (which they all remembered even if they didn't act on it). It turns out that people don't always do the obvious thing and I think that the reason we don't always do the obvious thing is the same reason my students didn't draw a diagram: panic.
We panic because we think we ought to see immediately how to proceed; we ought to know right away which method to apply. I mean, we're the professionals, right? Look at all those other people, with their deep learning, GPU-accelerated, non-parametric, feature extracting, cognitive architectures: they clearly know what to do. Why can't I see it straight away?
As the book says, don't panic. The reason you don't know what to do is not that you don't know the right technique, it's simply that the question isn't clear yet. So the first thing you should do is to try to figure out what the question actually is.
""The reason you don't know what to do is not that you don't know the right technique, it's simply that the question isn't clear yet.""
It turns out that there's a vicious circle here that you need to avoid. The people who have the problem – your clients, if you like – they've been working in this field for years. They know what the problem is. In fact, these days they find it hard to imagine what it’s like not to know the problem. They don't want you to waste time thinking about the nature of their reality; they want you to tell them that your fancy deep learning, GPU accelerated, etc, etc, method is going to solve their problem. So it’s easy to feel a lot of pressure to move quickly, to get back in your own domain of expertise where you are on familiar ground, to not “waste time” simply being taught the basics of the problem domain.
This is a trap! You absolutely need to spend time on the basics of the domain. (Also, frankly, it’s part of the joy of being a data scientist that we get to mess around in new domains.) Not only will you need to understand it in order to work out the question but there's every chance that your client doesn't really understand it themselves; or at least, not that part of it that resulted in them talking to you.
We've found writing a backbrief to be an extraordinarily valuable discipline. Let me try to say a bit more about how we approach it and why it’s so useful.
(Note, of course, that neither the name nor the basic idea is original to us. If you search for the term you'll find a lot of discussion of the idea, in the armed forces for example. It’s not quite the same thing but it is often helpful to give something a name, and this name seems appropriate.)
First, our understanding of the domain. We try to make this section technical yet informal. Technical, because we want to try to clarify the domain for ourselves, to learn the jargon, and to avoid ambiguity that could lead to confusion later. But informal, precisely because we are not experts in everything and we want to make clear the boundaries of our knowledge. We are not trying to pass an exam; we are trying to get to the bottom of how things work in this game. It should be clear what is true, what is our presumption and what we simply don't understand yet. Indeed, it is perfectly acceptable to present a simplified model.
Of course, at the beginning, you are likely to understand very little of the problem domain. That's okay! Part of the point of writing the backbrief is to make time for gathering this understanding.
Second, our understanding of the question. This is perhaps where you can be most valuable as an outsider. The path between the questions of a domain expert and the questions that can directly be addressed by data science is overgrown and winding. Our role is to make this path clear.
We try to be humble yet opinionated. Humble because getting the question right is hard; but opinionated because in the end you will answer some question so it had better be the question you want.
""Being explicit about how you will measure success (ideally quantitatively!) is tremendously helpful""
Two ideas can help. The first is to realise that there may be a difference between a question that we can answer and a question that we want to answer. Sometimes it’s better to have an approximate answer to your real question; sometimes you have to settle for a closely related question. The important thing is to be clear about which it is that you are doing. Second, in order to know that you can answer a question, it is at least necessary that you be able to say how you would evaluate an answer, were you to be given one. Just being explicit about how you will measure success (ideally quantitatively!) is tremendously helpful. Plan to build a system that will automatically quantify the success of any given answer.
And third, what we propose to do. You don't have to solve the problem straight away! However, it is useful to write down how you think you might solve it: what broad approach you might use, why it’s the right approach, what its limitations are. Our aim is in part to allow our client to “read ahead” and in part to convince ourselves that we have somewhere to start.
Perhaps the most important effect of having a thing called a backbrief is that it gives us permission to do things we ought to be doing when we don't know what to do. It gives us permission to talk about the basics of the domain and ask for an explanation of all the jargon. It gives us permission to ask fundamental questions, like “how will we know if we have succeeded?”. In other words, it gives us permission to think before we jump in.
"
https://www.turing.ac.uk/people/researchers/bilal-mateen,"
Bilal is a clinical-academic; he splits his time between his clinical commitments at King's College Hospital (KCH), and the Wellcome Trust where he’s the Clinical Technology Lead. Bilal’s work at Wellcome is focused on funding digital public goods and the software infrastructure that makes research possible, as part of the larger Data for Science and Health Priority Area. He also holds an honorary lecturer appointment at University College London’s Institute of Health Informatics, which is where he carries out most of his clinical research.
Bilal’s research is largely focused on applications of data science and machine learning in a variety of clinical settings, from the neuro-rehabilitation research he is involved in at the National Hospital for Neurology & Neurosurgery (UCL/Queen Square, and formerly Warwick Medical School), to his more recent work on diabetes and inflammatory bowel disease. Bilal regularly speaks to both technical and non-technical audiences about reproducible ML/AI, and how we should support innovation whilst implementing sensible regulation. He has had the privilege of being invited to speak at the Open Data Institute, the General Medical Council, the National Institute for Health and Care Excellence, as well as several clinical conferences.
Alongside his research at the Turing, Bilal was formerly the Clinical Data Science Liaison to the Data Study Groups Programme and the Turing-Warwick Data Science for Social Good (DSSG) summer programme. In this voluntarily role, Bilal worked to help academic-led groups take full advantage of these opportunities to explore how data science and artificial intelligence could be applied to cutting edge problems in health and social care.
Bilal’s research at the Turing is focused on creating a robust framework for reporting and assessing machine learning (ML) and artificial intelligence (AI)-based predictive modelling in medicine. This framework will allow researchers and regulatory bodies alike, to ensure that all potential ML/AI-based tools are transparent, reproducible, ethical, and effective before they are allowed to materially alter the care that patients receive. 
"
https://www.turing.ac.uk/events/image-ni,"
C﻿omputer vision, image analysis, and the use of artificial intelligence in creative imaging have seen a wide range of researchers engage in cross-cutting investigations. Whether you are involved in image generation, image processing/analysis, data analysis deriving from images, or creating real-world solutions, this workshop aims to connect people with interesting problems with those people with potential solutions and ideas.     
Travel bursaries are available (applicantions available through the event webpage) to support attendance from those located in GB and RoI.
T﻿he term ""image"" can be interpreted in many ways - whether that is a 2D picture, 3D video, high-dimensional multispectral image, or AR/VR, there is something for everyone at this workshop. The workshop will open with a keynote address from Turing AI Fellow, Prof Anna Scaife. A series of parallel breakout talks will then provide insights from academia, industry and healthcare on different themes of image analysis. Lunch will be provide alongside a poster session allowing teams or individuals to showcase their activities and interests with free poster printing available through the organisers. A﻿ full list of the talks and the speakers is available on the event website.
"
https://www.turing.ac.uk/research/research-programmes/data-centric-engineering/impact,"

As the data-centric engineering programme continues to develop, world-leading academic and major industrial partners are spearheading ground-breaking data science research that will be translated into major engineering solutions, leading to real world impact.
Using machine learning to improve the reliability of wireless communication systems
A collaboration involving the Turing has developed a new method for improving wireless technologies such as Wi-Fi.
 
 
 
Optimising the world’s first underground farm
In the heart of London there is a farm like no other. It’s subterranean, sustainable and energy smart. It also has a digital twin looking out for its every need.
 
 
 
­­­­World first as 3D printed steel smart bridge opens to pedestrians in Amsterdam
The world’s first 3D printed steel bridge opens to the public today: a pedestrian structure located in the city centre of Amsterdam. The bridge is a result of a multi-disciplinary team of experts collaborating on the future design of public spaces.
 
Bridging the gap between physical and digital
The Turing’s data-centric engineering programme and its collaborators are unlocking insights into the world-first 3D printed steel bridge, using innovative data science techniques and ‘digital twin’ technology’.
 
Why we created a ‘digital’ twin’ of the world’s first 3D printed steel bridge
As the bridge opens to the public in Amsterdam, the Turing’s Eric Daub explains the thinking behind our digital version.
 
 
Helping London to navigate lockdown safely
Project Odysseus monitors activity on the streets of London, allowing authorities to make interventions to keep people socially distanced.
 
 
 
 
 
 
 
Digital twin technology a ‘powerful tool’ but requires significant investment, say experts
Healthcare and aerospace experts at King's College London, The Alan Turing Institute, the University of Cambridge, and the Oden Institute for Computational Engineering and Sciences at UT Austin in Texas have said advances in digital twin technology make it a powerful tool for facilitating predictive and precision medicine and enhancing decision-making for aerospace systems. Their opinion piece has been published in Nature Computational Science.
 
 
Our infrastructure systems are undergoing a sea change. We need AI to point the way
Machine learning techniques can provide crucial information about how to optimise our transport and energy systems.
 
 
 
 
Streamlining jet engine design and manufacture
Algorithms developed at the Turing will allow aerospace companies to build engines that waste less fuel and have a lower carbon footprint.
 
 
 
 
New agreement to advance research at intersection of AI and manufacturing
The Alan Turing Institute and the University of Sheffield Advanced Manufacturing Research Centre (AMRC) have signed an agreement to work together to identify opportunities for artificial intelligence (AI) adoption in manufacturing, accelerate research collaboration and boost skills development.

 
The Turing delivers data-driven computational predictive methods for emerging complex engineering challenges

A well-known mathematical method, routinely used as a predictive tool in engineering and the physical sciences for more than 70 years, has been radically redesigned in landmark collaborative research led by The Alan Turing Institute.
 
 
 
 
 
The Turing to work with The University of Texas at Austin’s Oden Institute to advance data-centric engineering research
A Memorandum of Understanding (MoU) has been signed between The Alan Turing Institute and the Oden Institute for Computational Engineering and Sciences at The University of Texas (UT) at Austin. This formally creates an ambitious agreement led by the Turing’s data-centric engineering programme.

 
The Turing’s data-centric engineering programme helping to inform London’s response to COVID-19 lockdown
London is a particularly complex and varied environment in which to understand how the pandemic has affected people’s lives and how they are responding to it. Researchers from The Alan Turing Institute, backed by and in partnership with Lloyd’s Register Foundation (LRF), have been mobilised to provide crucial insights to help London authorities during lockdown and support planning for the future after lockdown.
 
 
 
The Turing teams up with Cervest to help predict climatic events in data scarce regions
The Alan Turing Institute today announces a collaboration with its Turing-Lloyds Register Foundation Data Centric Engineering Programme and UK startup, Cervest, the Earth Science AI company decoding climate uncertainty—as they work together to accelerate research and development to help communities and organisations quickly understand and measure the effects of climatic events.
 
 
 
Putting the AI in air traffic control
The UK’s leading air traffic control provider, NATS, has teamed up with the Turing to explore how the industry can evolve with machine learning.

Turing collaboration with new Training Centre in Data Analytics for Resources and Environments (DARE) in Australia
The Alan Turing Institute is pleased to announce a collaboration with the new Australian Research Council Training Centre in Data Analytics for Resources and Environments (DARE). The centre will be funded through the Australian Research Council Industrial Transformation Training Centres scheme over a period of five years and aims to boost data science skills in the natural resources sector.

The next step in precision agriculture
Growing enough food for the world’s burgeoning population may require smart farms that can sense their crop conditions and feed themselves on demand and Turing researchers are developing the technology they’ll need.
 
 
 
 
 
A machine learning revolution in disaster response
When a natural disaster strikes, emergency responders and aid agencies need all the intelligence they can get. Turing researchers have combined crowd-sourcing, machine learning and neural networks to rapidly revel the many dimensions of disasters, and deployed the technology in the aftermath of Hurricane Dorian.
 
 
 
 
 
 
 
Our built environment: How data-centric engineering can help tackle the climate crisis
Climate change is a critical issue requiring significant changes across sectors and disciplines, particularly around energy. Today (20 September 2019) a range of institutions are striking to highlight their call for more drastic climate action. This group includes many academic, construction, design and digital organisations. 
 
 
 
 
 
 
First UK based Facebook Fellowship awarded to Turing visiting researcher Jeremias Knoblauch
Turing visiting researcher Jeremias Knoblauch, a third year PhD student within the Oxford-Warwick Statistics programme, is the first graduate student based in the UK and one of only 21 graduate students worldwide to receive the Facebook Fellowship award.
 
 
Understanding urban air quality
Turing researchers are working with the Greater London Authority to use machine learning and statistical methodology to better analyse air pollution sensor data, design better policy interventions, and improve urban quality of life.

 
The Turing to work with the University of Sydney on data science research
The Alan Turing Institute and the University of Sydney's Centre for Translational Data Science have signed a memorandum of understanding to collaborate on joint research projects of strategic importance to the Australian economy, including  criminology, air quality, and geosciences. 

The programme on data-centric engineering and the Canadian Statistical Sciences Institute (CANSSI) have joined forces to develop a programme of joint workshops, research exchanges and fellowships. Led by Prof. Chris Oates from Newcastle University and Prof. Nancy Reid, the director of CANSSI, the partnership aims to advance research in the statistical sciences by increasing the points of contact among researchers in Canada and the UK.
 
 
The programme on data-centric engineering are delighted to partner with Science Foundation Ireland in the award of a joint Ireland-UK PhD studentship to work under Prof. Nial Friel (University College Dublin), Dr. Chris Oates and Prof. Mark Girolami on the project “Statistical Techniques for Engineering with Advanced Materials (STEAM)”. The funding was awarded to investigate “Approximate Bayesian computation with application to 3D printing”.
 
There is considerable demand for advanced statistical methodology to make scientific sense of the deluge of data emerging from the data revolution of the 21st century. The deluge of data has also created huge challenges in modelling, computation, and statistical algorithms. Following a joint bid with the University of Warwick, a team from The Alan Turing Institute’s data-centric engineering and defence and security programmes have been awarded £2.9 million from EPSRC for a five-year research project to address this challenge. Their goal is to create a step change in the use of principled statistical methodology, focusing on two specific application areas: engineering and defence.
See the EPSRC for full project details
 
Professor Mark Girolami FRSE has been appointed Lloyd’s Register Foundation / Royal Academy of Engineering Research Chair in Data Centric Engineering and will lead a five-year project to explore how big data can be incorporated into engineering practice, including the development of new data-centric techniques to monitor the safety of physical structures that will be trialled on the world’s first 3D printed stainless steel pedestrian bridge. Read more at this link.
 
In both 2017 and 2018 the programme has engaged with the Royal Statistical Society and led invited sessions at the annual conferences in Glasgow and Cardiff. Hetan Shah, CEO of the Royal Statistical Society welcomed the participation of the data centric-engineering programme in 2017, saying: “The RSS is delighted to have this session on data-centric engineering at our annual conference in Glasgow. It highlights the growing importance of statistics and data analysis in industry and in defining and designing a safer society.”
Watch the full session from 2017:

Watch the full session from 2018:

 
New technologies like artificial intelligence and machine learning could help cut delays and disruptions across the UK’s infrastructure network. The National Infrastructure Commission’s report examined the opportunities that these new innovations present, and made recommendations to increase open data sharing to make the most of them. A first Call for Evidence was made on 15 February 2017, with a second following on 27 July 2017
Members of the data-centric engineering programme contributed to the report, recommending that the government should encourage the uptake of new data-driven solutions to the asset management of critical infrastructures.
Read the report here
 
The programme is delighted to be hosting the 38th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, also known as Max Ent 2018. For over 37 years, the Max Ent workshops have explored the use of Bayesian and Maximum Entropy methods in scientific and engineering applications. This, the 38th edition of the Workshop, will be held at the Alan Turing Institute and feature tutorials and seminars from some of the programme’s Group Leaders.
For further details please visit the workshop website: https://max-ent.github.io/
 
The programme is delighted to be working closely with the statistical and applied mathematical sciences institute (SAMSI), with two joint workshops held in 2018:
The partnership brings together North Carolina State University, the University of North Carolina and Duke University, to jointly develop outstanding research collaborations with researchers at the Alan Turing Institute.
 
In December 2016 the programme funded Turing’s first Data Study Group which brought together 65 researchers to work on 6 data science problems provided by industry partners over a week long period. Industry participants included Siemens, Shell, National Grid, Syngenta, Airbus, Tata Steel and Shell’s Statistics & Chemometrics group.
Read our blog piece for more information on the Data Study Group. A series of scientific reports generated by the group will be published soon.
 
Members of the programme are partnering with MX3D, a 3D printing company, to measure, monitor and analyse the performance of the world’s largest 3D printed metal structure: a 12 metre-long stainless steel bridge due to be installed across a canal in Amsterdam in 2018. The Times have produced a short film profiling this groundbreaking work.
Read articles in The Times, The Engineer and MyScience.

Video courtesy of Neil Bowdler.
 
The programme announced a collaboration with the Mayor of London to tackle air pollution in London using data sensors.
Read articles in City AM, Business Daily and Air Quality News.
 
The following report was published following a six-month research project undertaken as part of the programme.
You can view the report below or download a copy at the LR Foundation website
 
"
https://www.turing.ac.uk/,"Everything you need to work together, all in one place.
Explore Zoom One’s Collaboration Tools
Connect virtually from anywhere with Zoom Meetings
Collaborate together with Zoom Team Chat
Call the world with Zoom Phone
Create and brainstorm with Zoom Whiteboard
Rich conversation analytics to improve sales
Send and receive messages and calendar invitations
Bring fluid interactions to hybrid teams with Zoom Huddles
Innovative video solutions for every meeting space.
Bring meeting spaces online with Zoom Rooms
Conference Room Connector links existing rooms to Zoom
Innovative solutions for every space
Solutions to host impactful virtual and hybrid experiences.
Find a Solution for Every Event
Host hybrid and virtual events with Zoom Events
Elevate your brand with single session events powered by Zoom Sessions
Broadcast at scale with Zoom Webinars
Host and attend classes, group events, and more OnZoom
An omnichannel cloud solution optimized for video.
Engage customers with Zoom Contact Center
Deliver intelligent support with conversational AI
Zoom solutions elevate collaboration across vertical use cases.
Discover Zoom Industry Solutions
Enabling exciting new ways to teach, learn, and connect globally
Transforming client engagement and employee experiences
Improving collaboration between agencies, ministries, and constituents
Connecting care, collaboration, and medical innovation
Real-time communication, anywhere in the world
Bridging the in-store and online experiences
Expert support and services for all your design, strategy, implementation, event, and hardware needs.
Expert consulting, technical delivery, dedicated support, and online event management
Flexible subscription plans for hardware
Protect certain data at rest with your own encryption keys
An open platform that allows developers to build Zoom apps and integrations.
Explore over 1,500 apps in Zoom App Marketplace
Documentation for building on Zoom’s platform using APIs, Webhooks, and SDKs
Resources that help developers evaluate & build with our solutions
Post your questions and get help from our developer community
Zoom Partners bring Zoom’s communications platform to market through alliance, sales, and service partnerships.
Explore Zoom’s technology ecosystem
Find a trusted Partner
Learn about Zoom’s Partner Programs
Access marketing & sales resources
Log in to the Partner Portal and click 'Learn’
Discover new ways to use Zoom solutions to power your modern workforce.
Access expert-led tutorials on Zoom products and features.
Network with other Zoom users, and share your own product and industry insights.
Get documentation on deploying, managing, and using the Zoom platform.
Keep your Zoom client up to date to access the latest features.
Download hi-res images and animations to elevate your next Zoom meeting.
Everything you need to work together, all in one place.
Explore Zoom One’s Collaboration Tools
Connect virtually from anywhere with Zoom Meetings
Collaborate together with Zoom Team Chat
Call the world with Zoom Phone
Create and brainstorm with Zoom Whiteboard
Rich conversation analytics to improve sales
Send and receive messages and calendar invitations
Bring fluid interactions to hybrid teams with Zoom Huddles
Innovative video solutions for every meeting space.
Bring meeting spaces online with Zoom Rooms
Conference Room Connector links existing rooms to Zoom
Innovative solutions for every space
Solutions to host impactful virtual and hybrid experiences.
Find a Solution for Every Event
Host hybrid and virtual events with Zoom Events
Elevate your brand with single session events powered by Zoom Sessions
Broadcast at scale with Zoom Webinars
Host and attend classes, group events, and more OnZoom
An omnichannel cloud solution optimized for video.
Engage customers with Zoom Contact Center
Deliver intelligent support with conversational AI
Zoom solutions elevate collaboration across vertical use cases.
Discover Zoom Industry Solutions
Enabling exciting new ways to teach, learn, and connect globally
Transforming client engagement and employee experiences
Improving collaboration between agencies, ministries, and constituents
Connecting care, collaboration, and medical innovation
Real-time communication, anywhere in the world
Bridging the in-store and online experiences
Expert support and services for all your design, strategy, implementation, event, and hardware needs.
Expert consulting, technical delivery, dedicated support, and online event management
Flexible subscription plans for hardware
Protect certain data at rest with your own encryption keys
An open platform that allows developers to build Zoom apps and integrations.
Explore over 1,500 apps in Zoom App Marketplace
Documentation for building on Zoom’s platform using APIs, Webhooks, and SDKs
Resources that help developers evaluate & build with our solutions
Post your questions and get help from our developer community
Zoom Partners bring Zoom’s communications platform to market through alliance, sales, and service partnerships.
Explore Zoom’s technology ecosystem
Find a trusted Partner
Learn about Zoom’s Partner Programs
Access marketing & sales resources
Log in to the Partner Portal and click 'Learn’
Discover new ways to use Zoom solutions to power your modern workforce.
Access expert-led tutorials on Zoom products and features.
Network with other Zoom users, and share your own product and industry insights.
Get documentation on deploying, managing, and using the Zoom platform.
Everything you need to work together, all in one place.
Explore Zoom One’s Collaboration Tools
Connect virtually from anywhere with Zoom Meetings
Collaborate together with Zoom Team Chat
Call the world with Zoom Phone
Create and brainstorm with Zoom Whiteboard
Rich conversation analytics to improve sales
Send and receive messages and calendar invitations
Bring fluid interactions to hybrid teams with Zoom Huddles
Innovative video solutions for every meeting space.
Bring meeting spaces online with Zoom Rooms
Conference Room Connector links existing rooms to Zoom
Innovative solutions for every space
Solutions to host impactful virtual and hybrid experiences.
Find a Solution for Every Event
Host hybrid and virtual events with Zoom Events
Elevate your brand with single session events powered by Zoom Sessions
Broadcast at scale with Zoom Webinars
Host and attend classes, group events, and more OnZoom
An omnichannel cloud solution optimized for video.
Engage customers with Zoom Contact Center
Deliver intelligent support with conversational AI
Zoom solutions elevate collaboration across vertical use cases.
Discover Zoom Industry Solutions
Enabling exciting new ways to teach, learn, and connect globally
Transforming client engagement and employee experiences
Improving collaboration between agencies, ministries, and constituents
Connecting care, collaboration, and medical innovation
Real-time communication, anywhere in the world
Bridging the in-store and online experiences
Expert support and services for all your design, strategy, implementation, event, and hardware needs.
Expert consulting, technical delivery, dedicated support, and online event management
Flexible subscription plans for hardware
Protect certain data at rest with your own encryption keys
An open platform that allows developers to build Zoom apps and integrations.
Explore over 1,500 apps in Zoom App Marketplace
Documentation for building on Zoom’s platform using APIs, Webhooks, and SDKs
Resources that help developers evaluate & build with our solutions
Post your questions and get help from our developer community
Zoom Partners bring Zoom’s communications platform to market through alliance, sales, and service partnerships.
Explore Zoom’s technology ecosystem
Find a trusted Partner
Learn about Zoom’s Partner Programs
Access marketing & sales resources
Log in to the Partner Portal and click 'Learn’
Discover new ways to use Zoom solutions to power your modern workforce.
Access expert-led tutorials on Zoom products and features.
Network with other Zoom users, and share your own product and industry insights.
Get documentation on deploying, managing, and using the Zoom platform.
"
https://www.turing.ac.uk/events/probabilistic-approach-situation-awareness-and-forecasting-covid-19-pandemics-norway,"
Your monthly invitation to hear from world leading experts in health surveillance research. The series will focus on statistical modelling and machine learning approaches, policy responses and international best practice in responding to national health emergencies.
Presented by The Turing/RSS Lab: Supporting the UKHSA
During the COVID-19 pandemic, governments have implemented interventions to control the spread of the SARS-CoV-2 virus. Often, decisions have been taken based on the assumptions of homogeneity of the pandemics, across local regions and time, also because of the lack of mathematical and computational instruments that can capture and predict local, temporal dynamics.
In this talk, Dr Arnoldo Frigessi and Dr Birgitte Freiesleben de Blasio will present a probabilistic model which estimates regional and local transmission in their complex interactions in space and time. Their model is a regional metapopulation model, informed by daily updated real-time mobile phone mobility data and multiple epidemical time series, including hospital admission and positive and negative test data.
Both propose a new inferential method, the split sequential Monte Carlo Approximate Bayesian Computation, tailored to handle the increasing parameter dimension in time. Their algorithm allows obtaining results timely enough to be usable in practical situation awareness. Their approach is in active use, informing the Norwegian crisis team about the regional epidemical situation in Norway.
Dr. De Blasio will comment on the practical use of modelling in the management of the pandemics by the Norwegian Institute of Public Health, presenting its impact and challenges.
Chaired by Professor Sylvia Richardson
Registration for this event is now closed.
"
https://www.turing.ac.uk/people/guest-speakers/laure-zanna,"
Laure Zanna is a Professor in Mathematics & Atmosphere/Ocean Science at the Courant Institute, New York University. Her research focuses on the dynamics of the climate system and the main emphasis of her work is to study the influence of the ocean on local and global scales. She is interested in the role of the ocean dynamics in shaping patterns of ocean heat and carbon storage and sea level under climate change and in the development of physics-aware machine learning models to deepen our understanding of climate processes and their representation in climate models.
Laure was the recipient of the 2020 Nicholas P. Fofonoff Award from the American Meteorological Society “For exceptional creativity in the development and application of new concepts in ocean and climate dynamics”. She is the lead principal investigator of the NSF-NOAA Climate Process Team on Ocean Transport and Eddy Energy, and M2LInES - an international effort to improve climate models with scientific machine learning. She currently serves as an editor for the Journal of Climate, a member on the International CLIVAR Ocean Model Development Panel and on the CESM Advisory Board.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/siemens,"
Siemens website
"
https://www.turing.ac.uk/research/research-areas/machine-learning/natural-language-processing,"
"
https://www.turing.ac.uk/people/external-researchers/emmanuel-candes,"
Emmanuel Candès is the Barnum-Simons Chair in Mathematics and Statistics, a professor of electrical engineering (by courtesy) and a member of the Institute of Computational and Mathematical Engineering at Stanford University. Earlier, Candès was the Ronald and Maxine Linde Professor of Applied and Computational Mathematics at the California Institute of Technology.
His research interests are in computational harmonic analysis, statistics, information theory, signal processing and mathematical optimization with applications to the imaging sciences, scientific computing and inverse problems. He received his Ph.D. in statistics from Stanford University in 1998.
"
https://www.turing.ac.uk/people/researchers/lasana-harris,"
"
https://www.turing.ac.uk/people/spotlights/premdeep-gill,"

I’m a PhD candidate at the University of Cambridge leading the ‘Seals from Space’ project with the Scott Polar Research Institute, British Antarctic Survey and WWF, using high-resolution satellite imagery to study polar seals and their sea ice habitats. I’m also a National Geographic Explorer, and with the BBC I’m helping to produce the landmark wildlife series Frozen Planet II.
I’ve been involved with the Turing in a few ways. Firstly, I delivered the first conservation-based Data Study Group (DSG) in 2019, where we investigated automating counting seals through satellite data and the mapping of their sea ice habitats. I also led a workshop and public event on ‘AI and immersive technology for conservation’ with the Turing and Digital Catapult. This successful event gave an opportunity for digital tech and data science experts to meet future environmental leaders working on the frontline of the climate emergency. I’m now returning to the Turing as an Enrichment student, where I hope to continue the work established in the DSG and workshops.
My work using state-of-the-art satellite imagery to monitor polar wildlife in remote regions means that I’m often working at the forefront of multiple fields. There is often either a lack of knowledge or tools to answer my research questions. Through my work, I hope to determine the extent to which we can use remote sensing as a cost-effective tool to gain new insights into the Antarctic seals and their habitats within some of the most rapidly warming regions of the world, and in doing so help develop the techniques required to monitor wildlife from space with greater confidence.
The most challenging aspect is dealing with the big data issue of very high-resolution satellite imagery. It’s awe-inspiring to be able to see blood from a seal giving birth all the way from space but equally it can be discouraging to spend countless hours manually going through a single image of sea ice and not find any seals.
The most rewarding part, however, has been running a week-long ‘Seals from Space Citizen Science’ workshop that gave students from under-represented backgrounds experience with state-of-the-art remote sensing technology and contact with leading polar scientists. On the final day, the students worked alongside my supervisor, who demonstrated the power of the techniques they had learnt by discovering 11 new emperor penguin colonies from space!
Any achievement that inspires you is the result of another human being just like you, so don’t be afraid to follow your curiosity or take on a challenge, even if it means you’re the first to try or you have to do things a little differently from others to achieve it.
From record temperatures of 38ºC in Siberia to the dramatic declines in ice across polar regions, it’s evident we’re entering a world unlike anything we’ve seen before. If we are to tackle the climate crisis and limit warming to 1.5ºC, COP26 must result in nations committing to plans for action that are far-reaching, unprecedented and inclusive, involving diverse groups in not only discussion and research but also as key decision makers.  
Relaxing somewhere green, running fun projects with Polar Impact, and listening to grime or creating it from the sounds of Antarctic seals and space with producer MarzmanJ.
"
https://www.turing.ac.uk/events/policies-and-guidelines,"

The purpose of this policy is to outline the rules governing the conduct of delegates attending and participating in the events and activities of The Alan Turing Institute and Group (hereafter the Institute) and provides guidance on the expected standards of conduct, both at events within the Institute and with external providers or partners.
This policy applies to all participants of events organised or hosted by The Alan Turing Institute; both in person and virtual.
The Alan Turing Institute is committed to making its events productive and enjoyable for everyone, regardless of gender, sexual orientation, disability, physical appearance, race, nationality or belief. We will not tolerate harassment in any form. All delegates are expected to adhere to the following key principles during an in person and virtual event:
Delegates are required to follow the key principles in point 3, as well as the additional principles for virtual events at the Turing. This section applies to the Institute and all entities owned and controlled by the Institute. This includes, but is not limited to, comments, questions and posts made during and within virtual events and their material, on platforms including Hopin, Zoom, Webex, Teams, Slido and YouTube.
Whilst we encourage delegates to use the chat function for discussion, debates and freedom of speech, we have a duty of care for our delegates and speakers.
"
https://www.turing.ac.uk/people/guest-speakers/ben-lyons,"
Ben Lyons is Head of External Affairs and Innovation at the Centre for Data Ethics and Innovation (CDEI), which sits within the Department for Science, Innovation and Technology, and drives the Government's agenda on trustworthy innovation in data and AI. Ben leads work on public engagement to data and AI, and has led a number of projects on AI governance and policy. Before joining the CDEI, Mr Lyons worked in public policy roles in the private and not-for-profit sectors.
"
https://www.turing.ac.uk/research/research-programmes/data-science-scale,"
This programme ran between 2016–2021.
Advances in high-performance computer architectures, and the way algorithms can take advantage of them, have been transformative for a variety of data science tasks.
This scientific programme at the Turing, in partnership with Intel, built upon these successes through co-design of algorithms and computer architectures, a range of applications, and by training a new generation of data scientists with the computational skills required to solve the major data analysis tasks of the future.
Researchers from both the Turing and Intel worked collaboratively under the shared goal to shape the future of computation for data science. This involved the following challenges and aims.
As data science continues to grow as an industry and research sector, data-driven algorithms such as those required by deep learning – multi-level networks that gradually identify things at higher levels of detail – take up an increasing amount of valuable time and energy in data centres. This provokes a need to rethink how the technical challenges caused by this emerging new science are managed.
Hardware needs to be designed to suit the needs of data science algorithms, and algorithms need to similarly be designed to suit the capabilities of the hardware.
 
The significant industry, government, and academic demand for data science skills creates a supply problem, with the UK facing a major skills gap which could inhibit the anticipated potential of data science and AI for our economy and society.
As well as conducting research, this programme’s partnership is training a new generation of data scientists through the Turing’s doctoral programme, ensuring students are equipped with the latest data science techniques, tools, and methodologies.
 
Intel has dedicated a hardware architecture team at the Institute’s facilities in the University of Edinburgh.
The programme aims to dramatically increase the speed and efficiency of data-driven computing tasks and provide Intel with the tools to build the next generation of computer processors and high-performance systems.

In a high-performance computing (HPC) environment, such as a data centre with hundreds or thousands of interconnected computers, well-designed algorithms and architectures allow huge data analysis tasks to be performed. For example, classifying millions of images of tissue samples to identify whether they contain anomalous features that should be examined by a doctor.
While these high-performance systems operate well for some computing needs, they often run at less than half their full capacity for many data science and machine learning tasks. Researchers at The Alan Turing Institute have been working in collaboration with Intel to co-design better architectures for their HPC systems. The collaboration has looked at how to improve communication between multiple machines that are sharing the workload of massive analyses, as well as how to rethink the formatting of the data used in HPC, to improve performance on data science and machine learning problems.
The output of the work is not only helping Intel improve their products and services, but also enabling data scientists to manage and analyse massive datasets with greater efficiency, in a range of machine learning applications.
Read the full impact story here.
 

An announcement was made about a collaboration between the data science at scale programme and the University of Warwick, Intel, and University Hospitals Coventry & Warwickshire NHS Trust (UHCW). The collaboration involves using ground-breaking artificial intelligence techniques to detect and classify cancer cells more efficiently and accurately.
Scientists at the University of Warwick’s Tissue Image Analytics (TIA) Laboratory – led by Professor Nasir Rajpoot from the Department of Computer Science – are creating a large, digital repository of a variety of tumour and immune cells found in thousands of human tissue samples, and are developing algorithms to recognize these cells automatically.
Professor David Snead, clinical lead for cellular pathology and director of the UHCW Centre of Excellence, comments that ‘the successful adoption of these tools will stimulate better organisation of services, gains in efficiency, and above all, better care for patients, especially those with cancer.”
Read the ZDNet article about the announcement – “NHS taps artificial intelligence to crack cancer detection”
"
https://www.turing.ac.uk/the-venue/,"



AI UK 2023 will be hosted at the Queen Elizabeth II Centre in Westminster, London. For the first time, our live audience will be able to discover the latest real-world applications of AI and data science in our exhibition, network with experts across the ecosystem, and access exclusive content.
Tickets to join us in-person are available for £150, and you can secure your place here. As an in-person guest, you will have access to:
The QE II centre is ideally located in the centre of London, within 5 minutes walking distance of both St James Park and Westminster tube stations. National rail stations Charing Cross and Victoria are also under a 15 minute walk from the venue, for those coming from further afield. There are several car parks within walking distance of the Centre but parking spaces are limited. Van and Car parking is available at the Q-Park Westminster, which is a 7-minute walk from the Centre. Disabled parking is available free of charge in the forecourt of QE II for those who hold a blue badge. It’s also possible to reach the venue via bicycle, with several docking stations (including for Santander cycles) available in close proximity to the venue.
The full of address for QE II is:
Queen Elizabeth II conference centre,
Broad Sanctuary,
London SW1P 3EE
With the AI UK 2023 schedule packed with unmissable sessions and opportunities, we’ve put together a range of accommodation recommendations suitable for a range of budgets, all located within walking distance of our venue. You can view the accommodation here.


"
https://www.turing.ac.uk/people/former-researchers/oscar-giles,"
Dr Oscar Giles is a Research Data Scientist at The Alan Turing Institute. He has a PhD in Cognitive Psychology from the University of Leeds, which involved developing computational models of human sensorimotor control. Before joining the Alan Turing Institute, Oscar worked as a postdoctoral researcher in the School of Mechanical Engineering at the University of Leeds, developing portable motion capture systems.
Most recently he worked on models of human road user behaviour as part of a EU Horizon 2020 project to improve interaction between autonomous vehicles and human road users.
Oscar has a strong interest in Bayesian statistical methods and reproducible data analysis. He will be applying his experience in software development, data analysis and statistical modelling to a broad range of research projects at The Alan Turing Institute.
"
https://www.turing.ac.uk/people/doctoral-students/edwin-fong,"
Edwin is a third year DPhil student at the Department of Statistics, University of Oxford under the supervision of Professor Chris Holmes. He previously completed an MEng at the University of Cambridge, specialising in information engineering and machine learning. Through this, he developed an interest for the mathematics behind machine learning methods, prompting his switch of focus to statistics.  Beyond his interest in methodology, he is motivated by the application of statistics in healthcare and medical research.
Within many scientific settings, there is the need to make quantitative statements of uncertainty to inform decision-making, and Bayesian inference offers a strong framework for achieving this. However, as datasets grow in size and complexity, the challenges of scalability and model misspecification become serious for the Bayesian. Edwin is particularly interested in the foundations of Bayesian inference, and is investigating Bayesian nonparametric methodologies that are flexible, scalable and robust to model error.
"
https://www.turing.ac.uk/people/governance/frank-kelly,"
Frank Kelly is Professor of the Mathematics of Systems at the University of Cambridge and was awarded a CBE for services to mathematical sciences in 2013. His main research is in random processes, networks and optimisation, with a specialist focus on applications to the design and control of networks and to the understanding of self-regulation in large-scale systems. He has received several prizes for his work. These include the Royal Statistical Society’s Guy Medal in Silver, the John von Neumann Theory Prize of INFORMS, the David Crighton Medal of the London Mathematical Society and Institute of Mathematics and Applications, and the Alexander Graham Bell Medal of the IEEE. He is a Fellow of the Royal Society and a Foreign Member of the National Academy of Engineering. He served as Chief Scientific Adviser to the Department for Transport (2003-2006), and as Master of Christ’s College, Cambridge (2006-2016).
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/medical-research-council,"
mrc.ukri.org
"
https://www.turing.ac.uk/sessions/what-can-data-science-do-for-policy-makers/,"


"
https://www.turing.ac.uk/people/researchers/sethu-vijayakumar,"
Professor Sethu Vijayakumar is Programme Director for Human-AI Interfaces and Robotics, and Turing Fellow at The Alan Turing Institute, with responsibility for defining and driving the institute's Robotics and Autonomous Systems agenda.
He is the Professor of Robotics at the University of Edinburgh, UK and the Director of the Edinburgh Centre for Robotics. He holds the prestigious Senior Research Fellowship of the Royal Academy of Engineering, co-funded by Microsoft Research and is also an Adjunct Faculty of the University of Southern California (USC), Los Angeles. He has pioneered the use of large scale machine learning techniques in the real-time control of several iconic robotic platforms such as the SARCOS and the HONDA ASIMO humanoids, KUKA-LWR robot arm and iLIMB prosthetic hand. His latest project (2016) involves a collaboration with NASA Johnson Space Centre on the Valkyrie humanoid robot being prepared for unmanned robotic pre-deployment missions to Mars. He is a Fellow of the Royal Society of Edinburgh, a judge on BBC Robot Wars and winner of the 2015 Tam Dalyell Prize for excellence in engaging the public with science.
"
https://www.turing.ac.uk/people/external-researchers/ben-godfrey,"
Ben Godfrey is the Network Strategy Manager for Western Power Distribution and is responsible for developing strategic network investment options for all four of WPD's licenced areas. He is an experienced electrical engineer within the UK electrical utilities industry, spanning a number of functions including operations, planning, innovation and strategy.
Latterly, Ben has been developing WPD's capability as it transitions from Distribution Network Operator to Distribution System Operator.
"
https://www.turing.ac.uk/people/researchers/gareth-roberts,"
Gareth's research interests include:
"
https://www.turing.ac.uk/people/guest-speakers/professor-jo-guldi,"
"
https://www.turing.ac.uk/people/former-enrichment-students/abeer-elbahrawy,"
Abeer is a third year PhD student in Mathematics at City, University of London under the supervision of Andrea Baronchelli. She is interested in the interplay between technological innovation and society and quantifying how technology is altering our world while we are shaping it with our beliefs. 
Currently she is focusing on decentralised systems, specifically cryptocurrencies and blockchain technologies  and working on analysing the dynamics and evolution of Bitcoin and cryptocurrencies market. During her masters she studied cooperation and altruism in social networks. She looked at how social networks can aid the diffusion of cooperation.
Abeer's research interests include complex networks analysis, computational social science and blockchain technology. 
While Bitcoin and cryptocurrencies were initially created as an alternative digital currency the lack of regulations and anonymity motivated undesired activities to grow in the system.  660 million Bitcoins were sent to darknet markets in 2017 and the price fluctuations of Bitcoin caused a loss of more than $500 billion in value. Abeer is interested in providing a better understanding of these systems to ease their regulation and development.
During her PhD, Abeer worked on the global view of the market and modelled the competition between cryptocurrencies using evolutionary models approach. She also used machine learning to predict cryptocurrencies prices. 
In the last year of Abeer's PhD and during her time at the Turing, she will be using data from Bitcoin's blockchain to form a network analysis perspective to review Bitcoin transactions. Abeer is aiming to uncover the evolutionary stages of the transactions network in response to key events in the system and the market.
"
https://www.turing.ac.uk/people/external-researchers/tony-hey,"
Tony Hey began his career as a theoretical physicist with a doctorate in particle physics from the University of Oxford in the UK. After a career in physics that included research positions at Caltech and CERN, and a professorship at the University of Southampton in England, he became interested in parallel computing and moved into computer science. In the 1980’s he was one of the pioneers of distributed memory message-passing computing and co-wrote the first draft of the successful MPI message-passing standard.
After being both Head of Department and Dean of Engineering at Southampton, Tony Hey was appointed to lead the U.K.’s ground-breaking ‘eScience’ initiative in 2001. He recognized the importance of Big Data for science and wrote one of the first papers on the ‘Data Deluge’ in 2003. He joined Microsoft in 2005 as a Vice President and was responsible for Microsoft’s global university research engagements. He worked with Jim Gray and his multidisciplinary eScience research group and edited a tribute to Jim called ‘The Fourth Paradigm: Data-Intensive Scientific Discovery.’ Hey left Microsoft in 2014 and spent a year as a Senior Data Science Fellow at the eScience Institute at the University of Washington. He returned to the UK in November 2015 and is now Chief Data Scientist at the Science and Technology Facilities Council.
In 1987 Tony Hey was asked by Caltech Nobel physicist Richard Feynman to write up his ‘Lectures on Computation’. This covered such unconventional topics as the thermodynamics of computing as well as an outline for a quantum computer. Feynman’s introduction to the workings of a computer in terms of the actions of a ‘dumb file clerk’ was the inspiration for Tony Hey’s attempt to write ‘The Computing Universe’, a popular book about computer science. Tony Hey is a fellow of the AAAS and of the UK's Royal Academy of Engineering. In 2005, he was awarded a CBE by Prince Charles for his ‘services to science.’
"
https://www.turing.ac.uk/people/researchers/james-robinson,"
James is a Senior Research Software Engineer at The Alan Turing Institute. He has a PhD in particle physics from UCL where he worked on the ATLAS experiment at the Large Hadron Collider at CERN. He continued working on ATLAS through postdocs at the University of Manchester and the Deutsches Elektronen-Synchroton (DESY) in Hamburg.
In each of these positions, he was heavily involved in the analysis of the large and complex datasets generated by the collider, as well as pioneering new methods for calibrating the detector itself.
James is a co-investigator of the Data safe havens in the cloud project where he has developed an open-source, reproducible cloud-solution for securely analysing sensitive data.
"
https://www.turing.ac.uk/events/connect-collaborate-create-data-and-ai-collaborating-communities-using-data-and-ai-create,"
A family-friendly free event opens to all at the Bristol Beacon on 8th June, inviting the public to join the conversation around Chat GPT, protecting vulnerable communities, and creating a sustainable future.  
Find out more and book your free space via The University of Bristol website.
This event is part of the #Bristol Data Week 2023 schedule of free activities.
Data is all around us, from our smartphones to our supermarkets, but how can we use it for good?
How do we use ChatGPT and AI to create a fairer future for all?
How can data from the past shape our future? 
Join the Jean Golding Institute and the University of the West of England to Connect, Collaborate, Create with Data and AI. Join the conversation with engaging scientists and local representatives to discover how communities could use data and AI like ChatGPT to tackle key issues including inclusivity, protecting the vulnerable, and climate change. Hear about the latest advances in Data-intensive research, and Artificial Intelligence, with inspiring speakers and interactive exhibitions. Meet speakers and mingle with guests during a complimentary lunch or sign up to one of the inclusive data workshops. There will be many opportunities to network, socialise and interact with the Bristol data and AI community.
Register now
"
https://www.turing.ac.uk/research/research-projects/automating-data-visualisation,"
There is a data science gap: the quantity of data collected increases every day while our human perceptual abilities stay unchanged and yet we are asked to come to judgements and make decisions using increasingly huge data sets. This project is researching how the power of cloud computing can be used to automatically layout data for digital twins to make it easier to see the message in the data. It's also looking at how to best represent important attributes of data, such as uncertainty, that enhance the truthfulness of data visualisations.
The algorithms created are being tested in an urban scale 3D atlas, but they will be equally applicable in manufacturing, defence, healthcare, risk planning, and anywhere where 3D/4D data needs to be presented in its physical context.
There are three interlinked science themes in this project. These three themes all involve novel research, which will be demonstrated and evaluated as part of a pilot software tool or tools.
In order to process, layout and visualise huge amounts of data new ways to use supercomputers for visualisation are needed. The project has researched, designed and tested a cloud based visualisation architecture that allows for scaling beyond 14petaFLOPS of compute performance on demand. This is something that would otherwise require dedicated time on one of the worlds fastest supercomputers to achieve - a level of resource not available currently in the UK.
While data is often represented as having a single value, or as not varying significantly, in reality measurements can only be made to a certain tolerance level and sensor readings vary randomly over time. For example, in air quality monitoring less expensive sensors are attractive, but may be less accurate. This track of the project is investigating how to visually represent a range of uncertainty alongside a measured value so that it is clear which sensors are more accurate (have less variance).
In any digital twin large amounts of data can obscure both the digital twin and other data items. While traditionally a graphic designer might layout the data to minimise these problems in large scale data visualisation this becomes infeasible, especially when the data is being used interactively or presented over many frames in a film. This theme in the project is looking to use models of human vision and cloud supercomputing to optimise the visual layouts of data. Multiple versions of a visualisation will be produced, then scored and ranked and only the best, in terms of layout, presented to the end user.
This project is researching novel methods for data layout and representation in the visualisation of 3D/4D digital twins that exploit the power of cloud computing. It will be investigating novel psychophysically informed algorithms to automate the visualisation process. This will address a growing gap in data science, the link from data and analytics results to human cognition and decision making.
It is evident, despite advances in AI, that without better human comprehension of data derived knowledge and its related uncertainty we stand little hope of improving the collective sum of human wisdom. Currently, the scale of big data, such as illustrated above, is challenging the capacity of visualisation designers to bridge this gap.
The second aim is to exploit the power of cloud computing to automate the process of laying-out and annotating a 3D/4D visualisation to best fit a specification. The project will take an agile approach that first investigates visualisation optimisation using fixed models of human vision, then later seeks to develop self-learning models.
For example, it will start by investigating the use of known contrast sensitivity metrics, crowding metrics and gradient limits as objective functions. In systems terms this requires creating a potentially stochastic optimisation loop that uses supercomputer-scale cloud computing power to generate a set of tentative visualisation from which the highest ranked are selected and presented to the designer or end user.
In this project the primary output will be published novel systems architectures and algorithms that are rigorously tested in the context of 3D/4D urban atlas representing data from an urban areas within its 3D context.
The project is being researched in the context of digital twins of urban areas, however it should be applicable to any digital twin that uses 3D/4D (spatio-temporal) data. Examples of areas of application include, urban monitoring and planning eg air quality, civil defence planning, digital factory twins for manufacturing, healthcare design, planning and monitoring.
 
The project has successfully demonstrated that it's possible to design, deploy and use over 14 petaFLOPS of compute performance in the cloud to produce the world's first terapixel visualisation of urban data in a digital twin. An academic article on the Terascope pilot was published on 13th February 2019 at arxiv.org/abs/1902.0482
[email protected]
"
https://www.turing.ac.uk/research/research-areas/social-data-science/research-methods,"
"
https://www.turing.ac.uk/people/external-researchers/anders-drachen-0,"
Anders is a Professor at the Department of Computer Science, co-director of the Digital Creativity Labs and Lead Analyst with Weavr at the University of York (UK). He also serves as Co-I and Lead Industry Liaison of the Intelligent Games and Game Intelligence Centre for Doctoral Research, and on a number of Special Interest Groups and Committees. He is a Co-Director of ARC.
"
https://www.turing.ac.uk/people/research-fellows/aretha-teckentrup,"
Dr Teckentrup is a lecturer in data science in the School of Mathematics. Before coming to Edinburgh, she held postdoctoral research positions at Florida State University and the University of Warwick. Dr Teckentrup received her PhD in applied mathematics from the University of Bath in 2013.
Aretha's research interests are at the interface of numerical analysis, statistics and data science. She is particularly interested in uncertainty quantification in simulation with complex computer models, with recent research focussing on multilevel sampling methods, Bayesian inverse problems, Gaussian process regression, approximation theory and deep Gaussian processes.
"
https://www.turing.ac.uk/research/research-areas/privacy-trust/identity-management,"
"
https://www.turing.ac.uk/events/computational-archival-science-cas-symposium,"
172.9 KB
King’s College London’s Department of Digital Humanities, together with The National Archives UK, the Digital Curation Innovation Center at the University of Maryland iSchool, and the Maryland State Archives in the US, were awarded an Arts and Humanities Research Council one-year International Research Networking grant for UK-US Collaborations in Digital Scholarship in Cultural Institutions, running from 1 February 2019 to 31 January 2020. 
The AHRC-funded CAS Network addresses the application of computational methods to the contextualisation of records within archival collections, at a time when the archive is becoming an increasingly digital space. Though a series of events held in both the US and the UK, the CAS Network explored how collections can be made available, digitally, for large-scale computational research. 
In collaboration with the Humanities and Data Science Turing Interest Group, the final event of the CAS Network will take place at The Alan Turing Institute on 20 January 2020.
By bringing together experts from various disciplines and backgrounds, this closing event of the Network aims to explore and discuss why, and how to best, integrate computational thinking into archival theory and practice.
The event will include a keynote speech, short talks and panel discussions where experts will be encouraged to share knowledge, exchange ideas and explore common interests and challenges. The investigators of the Network will present the first outputs of the collaboration and discuss next steps.
For more information about the CAS Network and its activities, you can visit the network’s page at computationalarchives.net.
For more information about the Humanities and Data Science interest group at The Alan Turing Institute, visit turing.ac.uk/research/interest-groups/humanities-and-data-science.
Tweet us at: @UkNatArcRes @EGoudarouli @kingsdh #DataAtTheArchives
 

"
https://www.turing.ac.uk/people/spotlights/erin-young,"

I co-lead the Women in Data Science and AI project within the Public Policy programme at the Turing, and most recently we’ve been working on a policy paper which is being released to coincide with International Women’s Day. The report presents the results of our social data science research into mapping gendered career trajectories - in other words, the gender gap - in the fields of data science and AI.
Recently, I’ve also been giving a number of talks about how data and algorithmic biases, alongside imbalances in the AI sector, result in a feedback loop which shapes and amplifies biases in AI and machine learning systems. It is crucial that we get ahead of this now, before these flawed technologies become irreversibly integrated into the fabric of society.
Firstly, it’s the striking scarcity of quality, disaggregated, intersectional demographic data available about the global AI and data science labour force. Responsibly collecting this is a crucial first step in interrogating and tackling these issues. We were also surprised to find the extent and impact of men’s dominance in AI and data science, especially in technical and leadership roles.
Our findings reveal persistent structural inequality in these fields, associated with disparities between women and men in skills, status, seniority, jobs and attrition rates. Especially alarming is that only 8% of UK researchers who contribute to the pre-eminent machine learning conferences are women. In other words: the world is being increasingly shaped by decision-making machines, but the people whose work underpins that vision are far from representative of the society these systems are meant to serve.
I hope for better recognition of the inequities in the global data science and AI workforce. The lack of this is currently a major stumbling block for the opportunities of women and marginalised groups in the broader technology industry. It’s not only an ethical issue of social and economic justice, but also crucially holds back innovation. Our policy paper highlights the need for effective policy responses if society as a whole is to reap the benefits of technological advances in AI.
I also hope that we can re-write the narrative; by this I mean heightening awareness of the gendered history of computing in order to avoid its replication in AI and data science. Finally, our Women in Data Science and AI Hub, including our list of resources, aims to help women build successful data and AI careers, and raise awareness of creating fair and equitable AI.
There have been a few but honestly getting to work with my amazing team in the Public Policy programme at the Turing has been fantastic. Also, being accepted to Cambridge for my undergraduate degree - albeit at the very start of my career - truly changed my life, so I’d have to add that too.
It’s a very, very long list, but to name a few:
I love working out (yoga, boxing, running, HIIT), dancing (I used to be a ballerina), and - in normal times - dinner parties with friends, and exploring the world. I’ve also picked up some new plant-based cooking skills in lockdown!
"
https://www.turing.ac.uk/about-us/impact/new-age-arctic-science-discovery,"
The Arctic is at the front line of climate change: complex feedback mechanisms between the atmosphere, ocean, ice and land mean that the Arctic is warming twice as fast as the rest of the planet. In September 2019, Arctic sea ice reached its second lowest minimum since satellite records began. Even if we curbed our greenhouse gas emissions sufficiently to limit the average global temperature rise to 2°C higher than pre-industrial levels (the upper limit of the Paris climate agreement) the Arctic could still warm by 4–5°C, with potentially serious regional and global consequences.
A new era of polar exploration has begun, but this time through collaboration between climate scientists and data scientists. Funded by the Institute’s data science for science programme, researchers at the Turing and British Antarctic Survey teamed up in September 2019.
They took AI algorithms formerly developed for use in the commercial sector and were among the first to apply them to climate science, aiming to uncover hidden relationships within sea ice data that are likely to be missed by traditional data analysis or simulation methods. With over 10 million data points in their satellite-derived dataset, the team is training its AI algorithms to forecast future sea ice at a resolution of 25 kilometres, with the ability to learn physical relationships between climate variables over both space and time. Sea ice is fundamentally important for Arctic wildlife and the indigenous communities that depend on them for food. Improving predictions of sea ice could aid local conservation efforts, as well as understanding how these fluctuations will affect weather patterns.
Furthermore, using bespoke AI-explainability methods developed through collaboration with Turing researchers is allowing the collaborators to ‘open up the black box’ of the trained model and start to draw conclusions on what can be learned from the data, potentially providing novel scientific insights.
These powerful new methods are being developed specifically for this project, but the researchers are already mapping out other avenues of climate research that could benefit from them, including understanding the drivers of urban heatwaves and predicting future regional water security. “Engaging with Turing Fellows has opened up our environmental datasets to new ways of exploration that we never thought possible,” said Scott Hosking, Head of the AI Lab at the British Antarctic Survey, and now Turing Senior Research Fellow. “The time has clearly come for the AI and environmental research communities to come together and tackle some of our greatest global challenges, including the climate emergency and the loss of biodiversity.”
"
https://www.turing.ac.uk/research/research-projects/field-data-global-indicators,"

A key task for data science is to develop systems through which diverse types of data can be aligned to provide common ground for discovery. These systems determine how data is incorporated into machine learning algorithms and whose perspective is incorporated or excluded from data-driven knowledge systems. Through sustained engagement with stakeholders involved in the development and use of plant data infrastructures, the project investigates current models and future prospects for managing environmental and biological data collected from experiments and field trials around the world.
The intersection of historically and sociologically informed philosophy of science and data science, including existing work on bio-ontologies and data visualisation tools, is uniquely equipped to provide understanding of both the technical and social conditions under which data can be mined and reused. This approach will help to develop semantics for the dissemination and linkage of phenotypic data that support a global, context-sensitive and sustainable knowledge base for research on food security and related environmental challenges. The overarching goal of the project will be achieved in the following three ways.
Facilitating innovative thinking and the pursuit of both technical and regulatory solutions to effective data linkage. The network would bring together data scientists at the Turing with experts in numerous other fields, including:
Providing a preliminary analysis of the conceptual, historical and sociological reasons for those differences, so as to develop an understanding of the current state of play that can inform analysis and technical solutions.
A framework for data linkage in this area that can be applied to produce innovative technical solutions for improving current data linkage strategies and tools.
The project aims to provide the building blocks towards investigating the conditions under which plant data can be efficiently and reliably linked across data platforms and infrastructures around the world, in ways that could serve the development of global indicators for the United Nations’ Sustainable Development Goals (particularly SDG 2: Zero Hunger, SDG 3: Good Health and Wellbeing; SDG 10: Reduce Inequalities and SDG 15: Life on Land).  
Having usable, meaningful and reliable ways to link plant data from different sources, species and approaches is essential to data analysis and interpretation, and thus to the development of evidence-based agriculture and governance strategies for food security, as well as to an improved translation between basic and applied research in the plant sciences.  
The project will thus benefit agricultural and farming policies, farming practices, biotechnology R&D, the development of policy over the international transfer and conservation of biological materials and related data, and research addressing food security challenges within the plant sciences. 
Williamson, H. and Leonelli, S. (eds.) (2022) Towards Responsible Plant Data Linkage: Data Challenges for Agricultural Research and Development. Cham: Springer Open Access. 
Williamson, H. and Leonelli, S. (2022) Cultivating Responsible Plant Breeding Strategies: Conceptual and Normative Commitments in Data-Intensive Agriculture. In: Williamson, H. and Leonelli, S. (ed.) Towards Responsible Plant Data Linkage. Springer.
Leonelli, S. and Williamson, H. (2022) Introduction: Towards Responsible Plant Data Linkage. In: Williamson, H. and Leonelli, S. (eds.) Towards Responsible Plant Data Linkage. Springer.
Leonelli, S and Williamson, H. (2023, in press) Artificial Intelligence in Plant and Agricultural Research. In: Choudhary A, Fox G and Hey T (ed.) AI for Science. World Scientific Publishers.
Leonelli, S. (2022) Process-Sensitive Naming: Trait Descriptors and the Shifting Semantics of Plant (Data) Science. Philosophy, Theory and Practice in Biology 
Williamson, H. and Leonelli, S. (2022) Accelerating Agriculture: Data-Intensive Plant Breeding and the Use of Genetic Gain as An Indicator for Agricultural Research and Development. Studies in the History and Philosophy of Science 95: 167-176.
Dupré, J and Leonelli, S (2022) Process Epistemology in the COVID Era: Rethinking the Research Process to Avoid Dangerous Forms of Reification. European Journal for the Philosophy of Science 12:20
Canali, S and Leonelli, S. (2022) Reframing the Environment in Data-Intensive Health Sciences. Studies in the History and Philosophy of Science 93: 203-214. 
Leonelli, S. (2022) “Scaling Up: The Radical Challenge of Democratic Data Governance.” Frontiers Policy Labs.
Leonelli, S (2022) Open Science and Epistemic Pluralism: Friends or Foes? Philosophy of Science.
Council of Canadian Academies (2022). Leaps and Boundaries, Ottawa (ON). The Expert Panel on Artificial Intelligence for Science and Engineering, Council of Canadian Academies (released March 2022). [SL was a member of this expert panel]
Welch E, Louafi S, Carroll SR, Hudson M, IJsselmuiden C, Kane N, Leonelli S, Marin A, Özdemir V, Reichman JH, Tuberosa R, Ubalijoro E, Wesseler J (2021) Post COVID-19 Implications on Genetic Diversity and Genomics Research & Innovation: A Call for Governance and Research Capacity. White Paper (released 6 July 2021).
Leonelli, Sabina, Bezuidenhout, Louise, Schuster, Doug, & Stall, Shelley. (2021, October 1). Democratization of Data. Zenodo.
Leonelli, S. (2021) Protecting High-Quality Scholarship through Fair Open Access: Reflections on the Diamond OA Study. Coalition S Blog.
Leonelli, S. (2021, published online: 8 September 2021) Apriti, Scienza! In: Wired Italia, Spring edition, pp. 48-53.
Williamson H, Brettschnedier J, Caccamo M, Davey R, Goble C, Kersey PJ, May S, Morris RJ, Ostler R, Pridmore T, Rawlings C, Studholme D, Tsaftaris S and Leonelli S.* Data management challenges for artificial intelligence in plant and agricultural research. F1000Research 2021, 10:324 (1) 
Leonelli, S. (2022, in press) How Data Cross Borders: Globalising Plant Knowledge through Transnational Data Management and Its Epistemic Economy. In: Krige, J (ed) Writing the Transnational History of Knowledge Flows in a Global Age. Chicago, IL: University of Chicago Press. 
Beaulieu, A. and Leonelli, S. (in press, 2021) Data and Society: A Critical Introduction. London, UK: SAGE.    
The Alan Turing Institute (2021) Report “Data science and AI in the age of COVID-19”   
Leonelli S, Lovell B, Fleming L, Wheeler B and Williams H. (2021) From FAIR data to fair data use: Methodological data fairness in health-related social media research. Big Data and Society 8 (1) DOI: 10.1177/20539517211010310      
Krige, J and Leonelli, S (2021) Mobilizing the Translational History of Knowledge Flows: COVID-19 and the Politics of Knowledge at the Borders. History and Technology.
Leonelli, S. (2021) Data Science in Times of Pan(dem)ic. Harvard Data Science Review.
Arnaud E, Laporte MA, Kim S, Aubert C, Leonelli S, Cooper L, Jaiswal P, Kruseman G, Shrestha R, Buttigieg PL, Mungall C, Pietragalla J, Agbona A, Muliro J, Detras J, Hualla V, Rathore A, Das R, Dieng I, King B (2020) The Ontologies Community of Practice: An Initiative by the CGIAR Platform for Big Data in Agriculture. Patterns 1: 100105
Geraint P, Yoselin BA, Gibbs D, Grant M, Harper A, Harrison J, Kaiserli E, Leonelli S, May S, McKim S, Spoel S, Turnbull C, van der Hoorn R, Murray J (2020) How to Build an Effective Research Network: Lessons from Twenty Years of the GARNet Plant Science Community. Journal of Experimental Botany, eraa307,
DATA TOGETHER (2020) Open Science for a Global Transformation. Data Together Response to UNESCO Consultation on Open Science. 29 pages.
Leonelli, S (2020) Scientific Research and Big Data. The Stanford Encyclopaedia of Philosophy (Summer 2020 Edition), Edward N. Zalta (ed.).
Leonelli, S. (2019) Data – From Objects to Assets. Nature 574, 317-321. DOI: 10.1038/d41586-019-03062-w
Leonelli, S. (2019) Data Governance is Key to Interpretation: Reconceptualising Data in Data Science. Harvard Data Science Review, inaugural issue.
Leonelli, Sabina (2020) big data; plurality; neutrality; AI; Coronavirus; pandemic; relativism; open science. In: The Index of Evidence, 61
Leonelli, Sabina. (2020). Opening the Research Process: From Publications to Data, and Back Again. Zenodo.
Cousins T, Leonelli S, Pentacost M, Rajan KS. (2020) Situating the Biology of COVID-19: A Conversation on Disease and Democracy. The India Forum  
Leonelli S and Williamson H. (2020) Intelligent plant data linkage: A view from history, philosophy and social studies of science.F1000Research 2020, 9:260
Leonelli, Invited Roundtable Speaker SMARTFOREST meeting, Cambridge, 27 October 2022: “The Challenges of Datafying Plants”.
Leonelli, Keynote, International Conference “Beyond Quantity”, Paris, October 21-22 2022: “The Qualitative Side of Automation: Lessons from the Reproducibility Debate”.
Leonell, Invited talk, Les jeudis de l’histoire et de la philosophie de la science, Paris, October 20 2022: “Globalising local knowledge beyond extractive epistemologies: Lessons from crop data science”
Leonelli, Invited talk, International Workshop “Scientific Knowledge Across Jurisdictions: Traceability, Access and Benefit Sharing (ABS), and Scientific Policy in a Multicultural World”, University of Edinburgh, October 11 2022: “Whither transnational science? Moving research data across communities and borders”
Leonelli, Keynote, International Conference “Engaging Ethics and Epistemology of Science” (EES 2022), University of Hannover, September 28-29 2022: ‘The Ethics of Empiricism’
Leonelli, Invited talk, Workshop “Science and its Others: Histories of ethno-science from the 18th century to the present-day”, University of Cambridge, September 19-20 2022: “Globalising local plant knowledge: Cassava research in 21th century crop science”.
Leonelli, Invited Enabling Openness in Australian Stem Cell Research (EOAR) seminar, University of Adelaide, September 14 2022: “Big and Open Data: Mind the Gaps”.
Leonelli, Invited talk, European Advanced School for Philosophy of the Life Sciences, University of Bordeaux, September 5-9 2022: “Handling biological, social and scientific complexity: Reflections on crop science”
Leonelli, Keynote & Anchor Teacher, WTMC Summer School 2022 “Opening Up Diversity”, August 22-26 2022: Five lectures delivered under the school theme.
Leonelli, Invited talk, Joint PHIL_OS/ISEED Roundtable “Good Science Across Diversity of Research Environments”, Università Cà Foscari, Venice, July 7 2022: “Citizen Science, Openness and the Diversity of Research Environments”
Leonelli, Contributed talk, Society for the Philosophy of Science in Practice 2022, University of Ghent, July 2-4: “Openness as Disruption: Epistemic Injustice in Crop Data Linkage”
Leonelli, Invited talk, University of Hannover, 17-18 June 2022: “Does Open Science Need Philosophy?”
Leonelli, Invited talk, Technische Universität Berlin, 13 June 2022: “Data Absence and the Failures of Big Data Empiricism”
Leonelli, Invited talk, International Conference “The Philosophy of Data Science: Data Science Governance”, Centre for Human and Machine Intelligence, University of Frankfurt, 10-11 June 2022: “Governing Data Circulation: An Underrated Concern for Scientific Inquiry”
Leonelli, Keynote, Reproducibility Conference, Belgian Royal Academy of Science, Brussels, June 1 2022: “The Many Faces of Reproducibility”.
Leonelli, Invited talk, Department of Philosophy, Freie Universität Berlin, 18 May 2022: “Datafication and Scientific Discovery: From Presence to Absence”
Leonelli, Keynote, STS Conference Graz 2022 “Critical Issues in Science, Technology and Society Studies”, May 2-4 2022: “Open Science Beyond ‘Sharing”, video
Leonelli, Invited seminar, Digital Humanities Centre Series, Freie Universität Berlin, April 27 2022: “Datafication and the Practice of Discovery”  (cancelled due to surgery)
Leonelli, Invited lecture, Seminar Series “Taming the Machines”, Research Group “Ethics in IT”, University of Hamburg, April 6 2022: “Trouble in the (big) data house” 
Leonelli, Invited seminar, Michigan Institute for Data Science (MIDAS), University of Michigan, April 4 2022: “Beyond Access: Framing the Use of Big and Open Data for Research”
Invited seminar, Deutsche Zentrum für Hochschul- und Wissenschaftforschung (DZHW), 30 March 2022: “What is the Trouble with Open Science?”
Leonelli, Keynote, International Conference “Development Counts: Data Skills for International Development Careers”, 16 March 2022: “Big and open data for development: Mind the gaps”
Leonelli, Invited seminar, Colloquium Series of the Wissenschaftskolleg zu Berlin, March 15 2022: “Wither Open Science?”
Leonelli, Invited talk, Westminster Higher Education Forum Policy Conference ‘Next steps for the Research Excellence Framework’, March 11 2022: “Open Science from Theory to Practice”. 
Williamson and Leonelli, Invited speakers, DivSeek Annual General Meeting, January 19 2022: “Globalising Plant Knowledge through Transnational Data Management” (with Hugh Williamson, presenting).
Leonelli, Panellist on The Vision for Data Spaces, High Level Panel on Future Data Space, FAIR Digital Objects Forum, January 19 2022. (400 audience)
Leonelli, Invited speaker, KLI seminar “Seeing Through COVID-19”, Austria (remote), January 13 2022: “Responses to the pandemic within history and philosophy of biology”.
Leonelli, Invited talk, Joint Centre for Environmental Intelligence Conference, University of Exeter, December 16-17 2021: “Steps Towards Intelligent Openness”
Leonelli, Invited talk, GEOS Seminar, University of Wageningen, 9 December 2021: “The Dilemmas of Openness: Sharing Knowledge Across Research Environments”
Leonelli, Keynote, Conference on Statistics, Data and Scientific Integrity, Flemish Interuniversity Council, VUB Brussels, 29 November 2021: “Research Integrity, Data Quality and Fair FAIR Data” 
Leonelli, Invited panelist, High Level Workshop “Research Culture in the ERA: Ensuring the Attractiveness of the Research Sector for Current and Future Generations”, Science Europe, Luxemburg, 24 November 2021:
Leonelli, Keynote, 1st International TRIPLE Conference “Empowering Discovery in Open Social Science and Humanities”, Polish Academy of Sciences, 22 November 2021: “Openness in the social sciences and humanities: lessons to and from the natural sciences”
Leonelli, Invited panelist, History of Science Society Conference (HSS 2021), Panel “The Future of Open Access in the History of Science, Technology and Society”, November 19 2021.
Leonelli, Contributed talk, Symposium “Opening Up Open Science”, Philosophy of Science Association 2020 meeting in Baltimore, November 15 2021: “Open Science and Epistemic Diversity: Friends or Foes?” (postponed from 2020)
Leonelli, Invited seminar, College de France, November 12 2021: “COVID research in a transnational perspective”
Leonelli, Invited talk at VolkswagenStiftung Symposium for Breakthroughs in the Social Science and Humanities (as a Winner in that category), Falling Walls Conference 2021, November 8: “Breaking Walls for Understanding Data”
Leonelli, Invited talk at Informal Advisory Group on Digital Sequence Information informing negotiations on the Global Biodiversity Framework (Convention for Biological Diversity COP 15), October 2021: “Intelligent Data Sharing”
Leonelli, Invited Panellist for Ada Lovelace Institute Webinar Series “Data for the Public Good”, October 14 2021: “Lessons Learnt from COVID-19: How should data usage during the pandemic shape the future?” Evidence Base of Artificial Intelligence: Supporting Data Fairness to Enhance Research Quality”
Invited speaker, “Data Democratization”, Data Sharing Seminar Series for Societies, 1 October, video
Invited seminar, History and Philosophy of Science Department at University of Indiana Bloomington, September 30: “Can – and should – philosophy help to “open” science?”
Keynote, Annual Conference (Jahresversammlung) of the Leopoldina “Biodiversität und die Zukunft der Vielfalt”, German National Academy of Sciences, Halle, September 23-24: “Reframing Biodiversity from the Data Up: Challenges from Plant Data Semantics” (postponed from 2020)
Invited talk, Symposium “Science Policies, Research Funding, and the Future of the University,” Biennial Conference of the European Philosophy of Science Association (EPSA 2021), Torino, September 18: “Research Excellence”.
Invited commentary, Symposium ‘Scientific Evidence and Expertise in the COVID-19 Pandemic and Beyond”, Biennial Conference of the European Philosophy of Science Association (EPSA 2021), Torino, September 17.
Keynote, UK Association for Studies of Innovation, Science and Technology (AsSIST-UK) 2021 Conference “Infrastructures, Institutions and Cultures: New Relations and New Challenges”, September 9-10: “The Open Data Challenge: Cultural, Institutional and Infrastructural Dilemmas of Data Sharing”
Keynote, Conference “Giornate di Epidemiologia Carlo Corchia”, University of Florence, September 9: “Open Science”
Leonelli, Keynote, INGSA2021 International Conference ‘Building Back Wiser: Knowledge, Policy and Publics in Dialogue’, August 30 – September 2. Q&A Plenary with Daniel Sarewitz: “Evidence and Democracy”.
Leonelli, Contributed talk, ISHPSSB 2021, July 15: “From philosophy of plant data to international science policy and back”.
Leonelli, Keynote, Plenary Session “Open science, data sharing and solidarity: who benefits?”, ISHPSSB 2021, July 14.
Leonelli, Keynote, Annual meeting of the British Society for the Philosophy of Science, July 7 2021: “Data and the Quest for Facts”.
Leonelli, Keynote, Joint CEPE/IACAP 2021 Conference: The Philosophy and Ethics of Artificial Intelligence organised by International Association of Computing and Philosophy and the International Society of Ethics and Information Technology, University of Hamburg, July 5 2021: “Pandemic Data Science: What Have We Learnt?”
Williamson, Invited seminar, ‘From Field to Byte: Digging into the Plant Science Data Landscape, Digital Humanities Lab seminar series, University of Exeter, 17 November 2021
Williamson, ‘Post-Genomic Plant Breeding: Shifting Expertise and Agricultural Knowledge in a Data-Driven Regime.’ Presentation at Society for Social Studies of Science (4S) conference, ‘Expert/Knowledge Politics in Food and Agriculture’ panel, 9 October 2021 (virtual).
Williamson, ‘Consolidating Plant Breeding Databases: Molecular Breeding and the Integration of International Agricultural Research.’ Invited presentation, Trajectories of Big Data Platforms workshop. University of Edinburgh, 14 September 2021.
Williamson, ‘Mapping Plant Data Communities: Challenges for Engaging Scientists and Policymakers’. Presentation at International Society for History, Philosophy and Social Studies of Biology (ISHPSSB) conference, ‘Plant data science between policy and technology’ panel. July 14 2021.
Leonelli, Contributed talk, Conference Historical Perspectives on the Consultative Group on International Agricultural Research (CGIAR), University of Cambridge, June 28: ”Describing Crops in the CGIAR Era” (with Helen Curry).
Leonelli, Keynote, Research Ethics Conference, Exeter, June 25 2021: ""Where Is Ethics in Research Practice?""
Leonelli, Invited panellist, Panel on Open Access, Launch Event of “Basic Research Programme”, National Science Centre, Poland, June 17 2021.
Leonelli, Invited speaker, Data Science & AI Day: Green Futures Conference, G7 Meeting in Cornwall, June 11 2021: “What do the data say about.. open and responsible science to cope with a changing planet?”
Leonelli, Invited seminar, POLICY AID Final Conference, University of Copenhagen, June 4 2021: “Data Linkage in Times of Pan(dem)ic: Towards Socially Robust Interventions”
Leonelli, Invited speaker, Edinburgh Futures Institute Seminar Series, University of Edinburgh, May 27 2021: “Big Data and the Nature of Scientific Inquiry”
Invited speaker, Conference on Agricultural Rights and Open Science, University of Trento, May 22: “Aprire I Campi? La Gestione Trasnazionale Dei Dati per l’Agricoltura di Precisione” (Opening the fields? The Transnational Governance of Data for Precision Agriculture).
Leonell, IRSA Distinguished Lecture, Conference “Big Data and Reproducibility”, Institute for Research on Statistics and its Applications and Minnesota Centre for the Philosophy of Science, May 6-8 2021: “Reproducing Reproducibility: The Role of Research Environments”.
Leonelli, Contributed talk, Conference “Philosophy of Plant Science”, Egenis, Exeter, May 5-7 2021: “Process-Sensitive Naming: Crop Descriptors and the Shifting Semantics of Plant (Data) Science”.
Williamson, Contributed talk, ‘Putting Markers in Their Place: Population Construction and Improvement in Contemporary Plant Breeding’. Paper presented at Philosophy of Plant Biology workshop. University of Exeter, 5 May 2021.
Leonelli and Williamson, Invited speaker, GODAN Webinar, April 28 2021: “Making Crop Data Sharing Responsible and Reliable:  How Social Intelligence Fuels Ethical Data Management Strategies for Precision Agriculture”
Leonell, Keynote, Boston University Graduate Student Philosophy Conference, April 23 2021: “Where to Go with the Philosophy of Data?”
Leonelli, Invited panellist, Festival dei Diritti Umani 2021 (Italian Festival of Human Rights) on “Algoritmocrazia” (Algorithmocracy), April 21.
Leonelli, Invited speaker, Symposium “A Critical Analysis of the Scientific Reform Movement” organised by the Centre for Open Science, April 20: “The Scientific and Social Implications of Implementing Open Science Policies and Procedures”.
Williamson and Leonelli, Contributed talk, Conference on Responsible Data Linkage, The Alan Turing Institute & University of Exeter, March 26: “Responsibility Beyond Ethics and Infrastructures”
Leonell, Invited talk, Session “Climate Change Mitigation and Adaptation”, AI UK 2021, March 24: “Making Data Work for the Planet”.
Leonelli, Keynote, International Conference “Digital Studies of Digital Science”, University of Louvain, March 15-17 2021: “Rethinking HPS through Digital Studies of Science"", Video
Leonelli, Invited panellist, Symposium ”Data, Society, and Open Science III: Challenges for data management and data-based research”, European Open Science Cloud seminar series, March 1 2021.
Leonelli, Invited talk, University of British Columbia, STS Seminar series, February 24 2021 (online lecture): ""Data Science in Times of Pan(dem)ic"" 
Leonelli, Invited panellist, Conversation on Covid and Global Health, Centre for the Study of Developing Societies, Delhi, 12 February 2021: ""Beyond Global Health? Diasporic Perspectives on Health and Social Justice"" (delivered remotely)
Leonelli, Invited talk, Seminar series of the Institute for Science in Society of Radboud University, January 26 2021: ”Reproducibility Rewired: The Dangers of Technical Fixes in Times of Pan(dem)ic”
Leonelli, Invited panellist, Symposium on Open Access in Philosophy of Science, January 13 2021 (online seminar): ”Why Open Access Matters”
Leonelli, Invited talk, GCRF Blue Communities Seminar Series, January 11 2021: ”How data governance supports good science” (online seminar, 120 participants)
Sabina Leonelli, Contributed talk at IGAD Seminar, Research Data Alliance, Helsinki, October 22-25 2019: “Intelligent Plant Data Linkage: A View from History, Philosophy and Social Studies of Science”
Sabina Leonelli, Invited panellist at Springer/Wellcome Trust Conference “Better Science through Better Data 2019” (#scidata19), panel “Who is afraid of data misuse?”, London, November 6 2019.
Sabina Leonelli, Invited speaker at Sawyer Seminar ""Information Ecosystems: Creating Data (and Absence) From the Quantitative to the Digital Age”, Humanities Centre, University of Pittsburgh, November 14 2019: “Data In and Out of Information Ecosystems: Lessons from the Study of Data Journeys”  
Sabina Leonelli, Keynote at Cidacs anniversary celebration, Salvador, Bahia, Brazil, December 6 2019: “Placing AI at the service of public health: The role of data management and linkage” 
Hugh Williamson and Sabina Leonelli, Invited talk at International Plant and Animal Genome conference (PAG) 2020, Workshop on “Challenges and Opportunities in Plant Science Data Management”, January 11-15 2020, San Diego: “Tracking data linkage for intelligent and responsible reuse”
Sabina Leonelli, Invited speaker at Francis Bacon Conference “Transnational Transactions: Negotiating the Movement of Knowledge Across Borders”, Caltech, Pasadena, February 19-22: “How Data Cross Borders: Globalising Plant Knowledge through Transnational Data Management”
Sabina Leonelli and Hugh Williamson, Invited Webinar, Webinar Series on Agriculture, Research Data Alliance (RDA) & Food and Agriculture Organisation (FAO), April 9: “Intelligent Plant Data Linkage: A View from History, Philosophy and Social Studies of Science”. URL:
Hugh Williamson and Sabina Leonelli, Invited talk, The CogX Global Leadership Summit and Festival of AI & Breakthrough Technology 2020, Session Data Science for Science: “Plant and Agricultural Science”, June 11 (with Hugh Williamson)
Sabina Leonelli, Keynote, Advances in Data Science Conference, University of Manchester, June 22-23 2020: “Intelligent Data Linkage and Distributed Semantics for (Big) Data Interpretation”
Sabina Leonelli, Invited speaker, Synthace industry webinar “The Metadata Responsibility”, 25 August 2020: “Understanding Data Science through Data Journeys”
Sabina Leonelli, Keynote, Opening Plenary Symposium “Science at the Speed of a Pandemic”, 2020 (Virtual) FORCE11, Scholarly Communications Institute, August 3 2020.
Hugh Williamson and Sabina Leonelli, Contributed talk, Political Ecology Network Conference POLLEN 2020, September 23 2020: “Speed, Statistics and Speed: Implications of Focusing Plant Breeding on Genetic Gain”.
Invited speaker, Falling Walls Circle Table “The Understanding of the Scientific Method in the 21st Century”, organised by the European Research Council for the World Science Forum, November 5 2020
Sabina Leonelli and Gavin Shaddick, Keynote, Conference “UK-China Tech for Global Good Roundtable: Climate, Clean and Green Tech”, British Embassy in Beijing, October 28 2020: “AI for the Green Agenda: What Data Landscape Do We Need?” 
Sabina Leonelli Keynote, Society for Computation in Psychology, virtual annual conference, November 19 2020: ”Prospects for the Automation of Research: Reproducibility and Human Agency”.
Sabina Leonelli, Invited lecture, Data Ethics Webinar Series, University of Oregon, December 9: “Data Science in Times of Pan(dem)ic: From FAIR Data to Fair Data Use” (delivered remotely)
Hugh Williamson and Sabina Leonelli, Contributed talk, International FAIR Convergence Symposium, Session “Plant Data Semantics and Food Security: Incorporating Local Imperatives into FAIR Data Linkage Tools”, Paris, November 30 2020: “FAIR Data and Climate-Adaptive Plant Breeding”
16 December: AI between Plant and Agricultural Science: Green Paths towards Environmental Intelligence
This one-day workshop will bring together experts in the plant and agricultural sciences who are working with complex datasets spanning genomic, physiological and environmental data and computational methods of analysis with data scientists interested in the application of cutting-edge technologies to this field. The workshop aims to map future directions for:
Particular emphasis will be given to mapping the current needs of the plant and agricultural science community, in order to establish a guiding framework for the efficient and responsive deployment of data science and artificial intelligence resources in those fields. Short research presentations will be followed by an extended discussion format, which will provide a forum for identifying possible collaborations and developing proposals for project applications.
Download the detailed agenda here
Plant Data Semantics and Food Security: Incorporating Local Imperatives into FAIR Data Linkage Tools at the International FAIR Convergence Symposium,  November 30 2020: “FAIR Data and Climate-Adaptive Plant Breeding” 
Towards Responsible Plant Data Linkage: This workshop series brought together leading researchers from the plant and agricultural sciences with scholars from the history, philosophy and social studies of science to discuss the challenges of plant data linkage. More information and video of workshop presentations can be accessed at our website. Proceedings will be collected into an edited volume for publication in 2022.
[email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-bristol,"
"
https://www.turing.ac.uk/people/external-researchers/farshid-amirabdollahian,"
"
https://www.turing.ac.uk/events/alan-turing-institute-presents-spotlight-data-science-careers,"
Photo by Conny Schneider on Unsplash
We invite you to join the first talk of our new Spotlight on Data Science Careers series. These talks are part of our ongoing work to increase awareness and knowledge of careers in Data Science and AI.
Through our Spotlight on Data Science Careers talks, we hope to shine a light on the career journeys of data scientists working in government, defence and security and academia and share advice, challenges and successes for those interested in working in these areas.
Dr Johanna Hutchinson, Director of Data and Data Science, Department of Health and Social Care, joins us in an interactive virtual talk to discuss her career journey thus far. 
Dr Johanna Hutchinson is the UK Health Security Agency’s (UKHSA) Director of Analytics & Data Science and Data IQ’s third most influential person in Data (Data IQ 100 The Most Influential People in DATA 2021).
Johanna moved to DHSC in April 2020 and joined the Joint Biosecurity Centre, now part of UKHSA, to build the growing data capability and promote transparency. She is a keen advocate of increased use of data and analysis in public services and has always been a key player in promoting diversity in the workforce.
This event is online only. We welcome participation from all backgrounds, but this event will be of interest to those interested in pursuing Data Science Careers.
Registration required
"
https://www.turing.ac.uk/people/research-engineering/callum-mole,"
Dr Callum Mole is a Research Data Scientist at the Alan Turing Institute. He has a background in Cognitive Science, using simulation, eye tracking, and computational modelling to study the perceptual & motor processes governing human behaviour.
Most recently he has applied these methods to the domain of driving automation, understanding human capabilities when taking over control of an autonomous vehicle.
At the Turing, Callum's research interests focus on supporting open and reproducible science, bayesian statistics, and data visualisation.
"
https://www.turing.ac.uk/events/urban-analytics-and-digital-twins-regional-engagement-event,"
The University of Birmingham’s Exchange Building, in the heart of Birmingham, is a place that brings together communities to shape the ideas and solutions that will make our cities and regions better places to live, work and learn - an ideal location for the Urban Analytics and Digital Twins Turing Regional Engagement event, taking place on 31st May 2023. 
 
This event will convene regional partners, including universities, city councils, businesses and other organisations interested in the opportunities presented by urban analytics. An example to be explored within the day includes digital twins, bringing the potential to provide cities with a bridge between the real and digital world, where smart buildings and infrastructure share information with a virtual environment providing potential decision-making tools for areas such as energy and transport. The day will:
Audience: This event is open to those in universities, city councils, businesses and other organisations, with an interest in urban analytics and their impact; no in-depth technical knowledge is required to attend. 
Register now
"
https://www.turing.ac.uk/research/research-projects/fair-framework-responsible-adoption-artificial-intelligence-financial,"
 
The FAIR programme brings together academia and industry to advance research and develop practical and scalable solutions needed to fully realise the transformational benefits of responsible adoption of AI across the financial services industry. The programme will also drive innovation that maintains public trust and meets regulatory expectations whilst acting as a beacon for responsible AI adoption globally.
 
Sign up to the Finance and economics mailing list to stay up-to-date with FAIR and related activities.
AI technologies have the potential to unlock significant growth for the financial services sector through novel personalised products and services, improved cost-efficiency, increased consumer confidence and more effective management of financial, systemic, and security risks. However, there are currently significant barriers to adoption of these technologies which stem from a capability deficit in translating high-level principles concerning trustworthy design, development, and deployment of AI technologies including safety, fairness, privacy-awareness, security, transparency, accountability, robustness, and resilience to concrete engineering, governance, and commercial practice.
In developing an actionable framework for trustworthy AI, the major research challenge that needs to be overcome lies in resolving the tensions and trade-offs which inevitably arise between all these aspects when considering specific application settings. FAIR seeks to realise high-performing solutions by developing cutting-edge AI methodologies drawing on cross-disciplinary expertise from statistics, computer science and mathematics, alongside the social science domains.
The project has five main research themes:
1. Robustness and Resilience will provide fundamental developments to the theory and practice of sequential decision making and will advance the state of the art of offline-to-online based learning theories.
2. Privacy and Security will lead to a greater understanding of the challenges of developing privacy-enhancing technologies (PETs) in the context of the Financial Services industry.
3. Fairness and Transparency focuses on interpretability and trust of AI-based decision making. This will move academic approaches to trustworthy AI from the lab into real-world practical use and will enable the  development of methodologies for improving reliable human-machine performance that goes beyond myopically focusing only on the algorithm.
4. Verification and Accountability focuses on the theoretical foundations and software tools for continual validation and verification of AI components. This will involve providing certifiable guarantees of robustness against distributional, adversarial and strategic interventions, with respect to objectives such as safety and fairness.
5. Integration Environment will develop synthetic data generation methodologies, allowing statistically accurate but fictional data to be generated in a variety of settings. Synthetic Data Generators (SGDs) will enable researchers to work with data in safe environments and to share and link data in settings when, currently, this is not possible due to regulatory or privacy constraints.
 
We are driven by our vision to enable the finance sector to leverage transformational benefits through the responsible adoption of AI by:
- Developing an actionable framework for safe and trustworthy deployment of AI in financial services, underpinned by foundational research methodologies formulated by cross-disciplinary and cross-sectoral teams.
- Developing digital sandbox environments to enable validation and testing and co evaluation of emerging technologies in a transparent manner.
- Identifying industry-wide standards and processes to address trade-offs between regulatory and ethical dimensions facing industry and regulators across a range of contexts and use-cases.
FAIR is a partnership between HSBC and The Alan Turing Institute, supported through an investment from EPSRC. It is one of eight business-led Prosperity Partnerships announced in support of the government’s ambitious new Innovation Strategy. For a full list of funders and collaborators, please click here.
 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/cgiar-big-data-platform,"
"
https://www.turing.ac.uk/research/research-projects/co-designing-algorithms-and-computer-architecture,"
As data science continues to grow as an industry and research sector, data-driven algorithms such as those required by deep learning - multi-level networks that gradually identify things at higher levels of detail - take up an increasing amount of valuable time and energy in data centres. This provokes a need for computing companies to rethink how they manage the technical challenges caused by this emerging new science.
In a high-performance computing environment, such as a data centre with hundreds or thousands of interconnected computers, well-designed algorithms - often complex sequences of repeatable steps - allow huge data analysis tasks to be performed. For example, classifying millions of images of tissue samples to identify whether they contain anomalous features that should be examined by a doctor, and give an effective yes/no output.
While these high-performance systems operate well for some computing needs, they often run at less than half their full capacity for many data science tasks. This is because when new computer systems are designed they are typically developed to perform well for a number of model algorithms, which don't include some of the most important data science algorithms used today. At the same time, data science algorithms are designed so as to perform well on existing, rather than potential future, hardware. The absence of feedback between these two design tasks is impeding progress in high-performance, large-scale data analysis.
In this five-year project funded by Intel, scientists at the Turing are working to address the technical challenges of data science, through co-designing computer hardware and software. That is, hardware will be designed to suit the needs of data science algorithms, which will similarly be designed to suit the capabilities of the hardware. The research, once complete, promises to dramatically increase the speed and efficiency of data-driven computing tasks and will provide Intel with the tools to build the next generation of computer processors and high-performance systems.
[email protected]
"
https://www.turing.ac.uk/people/researchers/damien-coyle,"
Professor Damien Coyle is a Director of The Bath Institute for the Augmented Human (University of Bath) and a UKRI Turing AI Acceleration Fellow 2021-25. His research focuses on developing AI to address challenges associated with translating electrophysiological signals into control signals for brain-computer interface (BCI) based neurotechnology and trialling these developments on a large scale.
He has published over 150 research papers in areas such as computational intelligence/AI, bio-signal processing, computational neuroscience, neuroimaging, neurotechnology and brain-computer interface (BCI) applications and has won a number of prestigious international awards including the 2008 IEEE Computational Intelligence Society (CIS) Outstanding Doctoral Dissertation Award, the 2011 International Neural Network Society (INNS) Young Investigator of the Year Award and the IET and E&T Innovation of the Year Award 2018.
He was an Ulster University Distinguished Research Fellow in 2011, a Royal Academy of Engineering/The Leverhulme Trust Senior Research Fellow in 2013, a Royal Academy of Engineering Enterprise Fellow in 2016-2017 and is currently an Ulster Senior Distinguished Research Fellow. 
He is a founding member of the International Brain-Computer Interface Society, an IEEE Brain Steering Committee member, an advisory board member for the UK Neurotechnology Innovation Network, and chairs the IEEE Computational Intelligence Society (CIS) UK-Ireland chapter. He is Founder and CEO of NeuroCONCISE Ltd, an award-winning, AI-enabled, wearable neurotechnology company.
"
https://www.turing.ac.uk/people/external-researchers/michael-fisher,"
Professor Fisher is the co-organiser of the Neuro-symbolic AI interest group.
"
https://www.turing.ac.uk/people/researchers/yi-yu,"
Yi Yu is an Associate Professor in the Department of Statistics, University of Warwick and a Turing Fellow at the Alan Turing Institute, previously a Lecturer in the University of Bristol, a postdoc of Professor Richard Samworth and a graduate student of Professor Zhiliang Ying. She obtained my academic degrees from Fudan University (B.S. in Mathematics, July 2009 and Ph.D. in Mathematical Statistics, July 2013). 
Yi's research interests are high-dimensional statistics and network studies. She has been working on theoretical, methodological and computational aspects of variable selection, post-selection inference and tuning parameter selection in regression problems, survival analysis, network analysis, and time series analysis.
"
https://www.turing.ac.uk/taxonomy/term/1177,"

Find out more about the expert commentary the Turing can provide
Sign up to our mailing lists to get the latest Turing updates
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/uk-space-agency,"
UK Space Agency website
"
https://www.turing.ac.uk/events/interpretability-safety-and-security-ai,"
This conference - organised under the auspices of the Isaac Newton Institute “Mathematics of Deep Learning” Programme — brings together leading researchers along with other stakeholders in industry and society to discuss issues surrounding trustworthy artificial intelligence.
This conference will overview the state-of-the-art within the wide area of trustworthy artificial intelligence including machine learning accountability, fairness, privacy, and safety; it will  overview emerging directions in trustworthy artificial intelligence, and engage with academia, industry, policy makers, and the wider public.
The conference consists of two parts:
Panel discussion hosted by Hannah Fry

The public event brings together leading academics, leading industrialists, policy makers, or journalists in order to foster conversations with the wider public about the merits and perils of artificial intelligence technology.

"
https://www.turing.ac.uk/research/research-projects/palaeoanalytics,"
Research in human evolution has been transformed by the impact of genomics and the development of ancient DNA methodologies, with insights into past demography, dispersal and admixture patterns, social behaviour, selection, disease history, and more. The challenge for classical palaeo-sciences is to develop methodologies and data capture that matches the richness of genomic data. This project is designed to develop machine learning and data science methodologies for non-genomic data to provide a more comprehensive and integrated understanding of human evolution.
Human evolution is a central research area in biology and anthropology, and has a history of research going back more than 150 years. For most of that time, evidence has come from digging up fossils and archaeological remains. This still remains central, but molecular genetics has revolutionised the field. Evolutionary genomic data is vast in scale, and complex in various ways. It is, however, tractable to very sophisticated analytical techniques, especially those developing under the banner of machine learning and data science. Archaeological and fossil data is less amenable, being complex, variable in nature and collected or published format, and often lost in older publications.
The methods to be developed in this project will aim to extract much of this data, and the work will develop ways in which this can be systematised into highly quantifiable and comparative forms. Where currently there is at best a visual or verbal comparison between genomic and non-genomic data in evolutionary science, the aim of this project will be to develop methods that will allow for analytical integration across data domains. This is particularly important as it has become increasingly clear that recent human evolution is a complex process of origination, isolation, dispersals, hybridisation and replacement. 
Genomics has transformed human evolutionary studies as much as it has other parts of biology. One of the reasons for this impact is the sheer scale of data now available, and power of the analytical techniques used. Machine learning and data science have in effect, swamped traditional approaches to human evolution. However, the palaeosciences – palaeontology, archaeology, earth sciences – have a major role to play, supplying hypotheses, providing, contextual information and above all, providing evidence for the evolution of the phenotype and extended phenotype.
The major challenge is to develop data structures and analytical methods for these aspects that can be integrated with genomics. The aim of this project is to take up this challenge, and develop methods drawn from machine learning and data science that would greatly enhance the quantity and quantification of the complex data of the palaeosciences – morphometrics of fossils, attributes of the millions of stones tools that reflect hominin behaviour, environmental context and more. The data are in the form of books, papers, reports, and are in text, tabular and image form. These will require advanced algorithm-based input methods. Turning these into usable data will be based on classification of the features that will form the basis of the output data. Methods used will include string-searching algorithms, deep learning and computer vision. The primary output is to produce a widely applicable protocol/workflow from raw archived data to analysable database that can be applied widely to modern human evolution relevant data. 
The project will form the platform for integrating genomics and palaeo-phenotype data, and so greatly increase the range of analyses possible on the patterns and processes of human evolution. Human evolution is a central problem in biology, both for its intrinsic interest and for the implications for both the medical and cognitive sciences, and the relationship between humans and biodiversity overall. 
The humanities and historical sciences such as palaeontology and archaeology share a problem that data are complex, not easily quantified, and scattered in non-uniform sources. Data science and machine learning is being used to improve this situation, partly to advance the fields, and partly to strengthen open access. This project will have applications in this broader endeavour, making human evolutionary data available for analysis across many pure and applied fields of science. 
"
https://www.turing.ac.uk/how-do-people-feel-about-ai,"

In late 2022 The Alan Turing Institute and the Ada Lovelace Institute conducted a nationally representative survey of over 4,000 members of the British public, to understand their awareness, experience and attitudes towards different uses of artificial intelligence (AI), including views and expectations on how these technologies should be regulated and governed.
Artificial intelligence (AI) technology and its widespread use in many aspects of daily life is developing at a rapid pace. It is therefore crucial to understand how people currently experience the many applications of AI, including people’s general awareness of these technologies, how they perceive the benefits and concerns, and whether their attitudes differ across key demographic factors. To effectively inform the design of policy responses, it is also important to understand people’s views on how these technologies should be governed and regulated.
The Alan Turing Institute and the Ada Lovelace Institute partnered to conduct a new, nationally representative survey of the British public’s attitudes towards, and experiences of, AI.
Understanding people’s perspectives on data and AI is a key component in ensuring these technologies are developed to align with societal values, and are able to address societal needs. We hope this research will help AI companies and policymakers to understand and address the public’s diverse opinions and attitudes towards AI, so that technology developed in the future works better, for everyone.
The full report can be viewed and downloaded here.
 
 

This project was made possible by a grant from The Alan Turing Institute and the Arts and Humanities Research Council (AHRC), and with substantial input from LSE's Methodology Department.
 
1.9 MB
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/smart-data-foundry,"
Smart Data Foundry is a data innovation organisation serving the public, private and third sectors.  In September 2020, Smart Data Foundry received initial funding from the UK Government's Department for Business, Innovation & Industrial Strategy through the UK Research & Innovation Strength in Places Fund.   
Its mission is to improve people's lives by safely unlocking the potential of financial data to enable research, innovation, and skills development in Financial Services and FinTech.  Smart Data Foundry is a collaboration between the University of Edinburgh, the Financial Data and Technology Association (FDATA) and FinTech Scotland.  
Smart Data Foundry was one of the first major data-themed partnerships following the launch of the University's Data-Driven Innovation (DDI) initiative, part of the City Region Deal. Under DDI, the University is creating a network of hubs to help public, private and third sector organisations improve products and services through research coupled with high-powered data analytics. 
"
https://www.turing.ac.uk/people/researchers/daniele-guariso,"
Dani is a Senior Research Associate in the Policy Modelling Theme, within the Institute's public policy programme.
After an intensive period spent in political activism, he developed a keen interest in a more quantitative approach to political dynamics and decision making. This led him to explore methodologies from different fields and disciplines, ranging from survey design to text and network analysis. Eventually, he ended up at The Alan Turing Institute, where he fell in love with the data science community.
Dani holds a BA in political science and international relations from Università Degli Studi di Pavia (Italy), a Certificat d’Études Politiques from Sciences Po Toulouse (France), an MSc in development economics and a PhD in economics from the University of Sussex (UK). He has also acted as a consultant for IGC, IDS, GAIN, and UNDP.
When he is not too busy doing nerdy things, Dani still engages in politics through different grassroots associations that carry out social-impact projects at the community level. He is a passionate runner, swimmer, and (more recently) boxer.
Dani’s research interests span a broad range of topics across political science, development economics and political economy. He has worked on social media, sustainable development, food policy, public financial management, trade and innovation, political violence, and decision-making dynamics. From the methodological side, in his research he has employed tools from text analysis, network science, agent-based modelling, machine learning and more traditional econometrics.
At the Turing, he works on the ESRC-funded project “Agent Computing and AI to Achieve the 2030 Agenda”, where he applies computational methods to inform policymakers on how to allocate public resources to achieve the UN Sustainable Development Goals.
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-explainable-ai-health,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of Explainable AI in Health, exploring how we ensure AI-assisted decisions, methods and tools are transparent and clear to all involved, which is becoming particularly crucial in the health sphere.
 
We will be hearing from Ali Boushehri, a PhD Student in Computational Biology at Roche and Antonella Maia Perini, a Research Associate in the Public Policy Programme at the Turing Institute.
Ali will be will be discussing 'scifAI' which is an explainable AI python framework for the analysis of multi-channel imaging flow cytometry data.
Antonella will discuss how explainability is critical to respond to equity challenges in AI-enabled medical devices. She will draw on recent work by the Turing’s Ethics Team on responsible research and innovation to illustrate how the core ethical principle of explainability can improve AI performance and minimise harms in the health and healthcare sector.
The event will conclude with a Q&A, we also hope to continue discussions pre and post the event via our Turing-Roche Slack Workspace which you can join.
You can watch a recording of this event here.

"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-ai-clinical-practice,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of AI to Clinical Practice. Despite advancements in the AI space, we are still seeing limited uptake and use of algorithms and tools in clinical practice. This event will explore current opportunities and challenges around bringing digital solutions to clinical healthcare.
We will be hearing from Konstanty Korski, Research and Development Lead, Digital Pathology at Roche and Nicholas Fuggle, Clinician, Associate Professor in Rheumatology at the University of Southampton and co-organiser of the Turing Clinical AI Interest Group.
Konstanty's talk will be on computer vision based algorithms. In the last couple of years, there has been an exponential increase of publications covering the development of different algorithms in this area, especially in the medical field. However, despite such impressive progress there are still disproportionately few solutions that have reached the market and become wildly adopted products. There are several reasons for that discrepancy and they will be discussed in the talk. 
Nick’s talk will introduce the Clinical AI Interest Group and highlight some of his research in musculoskeletal computer vision. The Clinical AI Interest Group is celebrating its first anniversary and has three main aims:
Nick’s research focuses on the diseases of musculoskeletal ageing including osteoporosis, osteoarthritis and sarcopenia, and in this session he will focus on the deployment of computer vision in the analysis of bone microarchitecture.
You can watch a recording of this event here.

"
https://www.turing.ac.uk/people/guest-speakers/joshua-hoang,"
"
https://www.turing.ac.uk/news/quaisr-raises-31-million-develop-digital-twins-industry,"
Quaisr, a startup created by researchers from The Alan Turing Institute and Imperial College London, has raised $3.1 million to further develop its work on digital twins. The money will be used to expand the team, with a focus on software engineering and business development.
“Our top priority is to determine product-market fit, and to do so we need people to be focused on product development while we obtain valuable feedback from our customers,” Dr Assen Batchvarov, Quaisr’s Product Manager told Imperial. 
Quaisr was set up in 2020 by Imperial and Turing researchers Professor Richard Craster, Imperial’s Dean of Natural Sciences, and Professor Omar Matar, Dr Indranil Pan and Dr Lachlan Mason who are part of the Data-Centric Engineering Group at the Turing. The team is expected to grow to a dozen full time members over the next couple of quarters.
The pre-seed funding round was led by Crane Venture Partners, with the participation of Acequia Capital, Hybris Founder Carsten Thoma, Encord Founder Eric Landau and additional strategic angel investors. 
A digital twin is a virtual representation of an object or system that can be updated from real-time data. This can then be used to monitor and predict the performance of the object or the evolution of the system under different conditions, to help decision-making or inform further research and development.
Apart from applicability in operations, using a digital twin can significantly shorten the discovery cycles for new materials, for example, or help optimise the performance of physical systems, such as manufacturing lines or chemical production facilities, in real-time.
However, connecting, scaling and democratising the building blocks that represent digital replicas of assets and processes is time-consuming and resource-intensive. Because of the limitations of the tools available, the Quaisr founders set out to develop an easier and more cost-effective approach that enables multinationals to harness the power of digital twins.
“The Quaisr platform will empower heavy industries to build reliable digital twins of their assets and processes, making their operations efficient, sustainable and more reliable,” says Dr Batchvarov.
The Alan Turing Institute has recently launched a Turing Research and Innovation Cluster (TRIC) focusing on digital twins which aims to democratise access to emerging digital twin technology by providing open and reproducible computational and social tools for digital twin development and deployment as a national service. You can read more about the Turing's work on digital twins in its Annual Report. 
Read the full story by Imperial College London here. 
 
"
https://www.turing.ac.uk/research/research-areas/theoretical-mathematics/logic,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/food-and-agriculture-organisation,"
"
https://www.turing.ac.uk/people/akira-endo,"
Akira Endo is a PhD student at the Department of Infectious Disease Epidemiology, London School of Hygiene & Tropical Medicine. While he studied medicine at The University of Tokyo, he was also involved in research projects analysing transmission dynamics of infectious diseases using mathematical models. After graduating and being qualified as a physician, he went on to the PhD program to further pursue his passion in the theoretical aspects of infectious disease outbreaks.
Compared with other types of diseases, infectious diseases are unique in that protecting an individual from a disease also means protecting other people by reducing their risk of exposure to the pathogen. This “interdependency” makes the control of infectious disease outbreaks an important social agenda. Akira’s current research interests include disease spread within and across households/schools/workplaces, improving vaccine evaluation methodologies and optimal control strategies. During his Enrichment placement at the Turing, he also tries to incorporate rapidly advancing machine learning techniques into infectious disease modelling.
"
https://www.turing.ac.uk/people/researchers/kaspar-beelen,"
Kaspar Beelen is a digital historian, who explores the application of machine learning to humanities research. After obtaining his PhD in History (2014) at the University of Antwerp he worked as postdoctoral fellow at the University of Toronto. As researcher on the Digging into Linked Parliamentary Data (Dilipad) project, he published several papers situated at the interface of data science, political science and history, which explored a wide range of topics, including: the representation of women in Westminster, the evolution of public health discourse, and the use of affect in parliamentary language.
In 2016, Kaspar moved to the University of Amsterdam where he first worked as a postdoc for the ""Information and Language Processing Systems"" group, and later became assistant professor in Digital Humanities (Media Studies). Since February 2019, he works at the Turing Institute as research associate for the Living with Machines project.
Kaspar specialises in creating and mining longitudinal historical corpora. He has worked extensively on digitising and enhancing the parliamentary proceedings, the ""verbatim"" records of speeches made in plenary sessions. In previous projects he worked on releasing the Belgian, Canadian and British proceedings, and investigated how semantically enhanced search can enhance access to such vast historical collections.
Methodologically, Kaspar's research applies machine learning to historical data. Topics of interest include: computational models of semantic change (over time, but also between communities); measuring emotion in discourse; detecting ideology in language. His historical research has focused on the substantive representation of women in Westminster; the changing discourse on public health during the interwar period innovation in the book market during the Dutch Golden Age.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-sheffield,"
"
https://www.turing.ac.uk/people/external-researchers/maren-fiege,"
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/new-report-explores-police-perspectives-algorithmic-transparency-standard,"

Algorithmic transparency is both achievable for policing and could bring significant rewards, a new research report finds.
Led by Dr Marion Oswald, MBE, the research explores the implications for police forces of participation in the Government’s new Algorithmic Transparency Standard. Based on interviews with police personnel and representatives of commercial providers, the research was conducted in parallel to the piloting of the Standard by the Cabinet Office and the Centre for Data Ethics and Innovation. The aim of the Standard is to ‘promote trustworthy innovation by providing better visibility of the use of algorithms across the public sector, and enabling unintended consequences to be mitigated early on.’
Interviewees generally thought that the rewards for the police of a carefully tailored Standard implemented at the right stage of algorithmic development outweighed the risks.  Participation in the Standard provides an opportunity for the police to demonstrate the legitimacy of technology use and build earned trust. As one research participant commented:
‘If people are worried, there's an opportunity there as opposed to a risk, there's an opportunity to be open. There's an opportunity to be transparent. There's an opportunity to explain, to reduce concerns.’
The Standard could also be used to develop increased sharing among police forces of best practices (and things to avoid). Research participants were keen for compliance with the Standard to become part of a system to drive reflective practice across policing around the development and deployment of algorithmic technology. This could enable police to learn from each other, facilitate good policy choices and decrease wasted costs.  For example, one interviewee suggested:
‘It might also be possible to create a peer reviewing mechanism of sorts. I feel that without this oversight, we will be missing the main benefit of transparency; the ability to ensure that algorithms used in the public sector are up to the task, and are being built properly, with proper attention to the data, and to the drift that occurs with algorithms.’
In order to contribute to improving the quality of policing technology, the report concludes, the Standard should be linked to methods of oversight and promotion of best practice on a national basis. Otherwise, the Standard may come to be regarded as an administrative burden rather than a benefit for policing.
The research recognises the confidentiality concerns around policing contexts and tradecraft, where revealing technical details could provide an advantage to criminals. Ways that these concerns could be mitigated include a non-public version of the Standard for sensitive applications and tools, which would be available for review to bodies with an independent oversight function.
To support police compliance with the Standard, supplier responsibilities – including appropriate disclosure of algorithmic functionality, data inputs and performance - should be covered in procurement contracts and addressed up front as a mandatory requirement of doing business with the police.
The report also suggests a number of areas for amendment and improvement, which could improve the Standard for the benefit of all participants. These include clarification of the scope of the Standard and the stage of project development at which the Standard should apply. A lighter touch version of the Standard – ‘Standard-Lite’ – is proposed for early or trial stage projects.
Dr Oswald said: ‘Transparency has not traditionally been associated with policing technology, for understandable reasons, but this is changing. Our interviewees – both from policing and the commercial sector – recognised how transparency can bring rewards. It can enable the public to understand more about how the police are using technology and the reasons for this. It can also enable the police themselves to learn from each other and improve what they are doing.’
The full report ‘The UK Algorithmic Transparency Standard: A Qualitative Analysis of Police Perspectives’ by Marion Oswald (The Alan Turing Institute, Northumbria University), Luke Chambers (Northumbria University), Ellen P. Goodman (Rutgers University), Pamela Ugwudike (Southampton University) and Miri Zilka (Cambridge University) is available to read here.
"
https://www.turing.ac.uk/news/publications/nowcasting-signature-methods,"
https://arxiv.org/abs/2305.10256
Alongside the paper on Arxiv, the ONS have produced a blog and technical report available on the ONS website.
Blog: https://datasciencecampus.ons.gov.uk/helping-decision-makers-understand-the-economy-quickly-through-new-methods/
Technical report: https://datasciencecampus.ons.gov.uk/projects/technical-report-nowcasting-uk-household-income-using-the-new-signature-method/
GitHub code repo: https://github.com/datasciencecampus/SigNow_ONS_Turing
 
1 MB
"
https://www.turing.ac.uk/news/publications/enabling-dependability-driven-resource-use-and-message-log-analysis-cluster,"
In the 24th IEEE International Conference on High Performance Computing, Data, and Analytics (HiPC 2017) http://hipc.org/hipc-2017-accepted-papers-keynote-speakers/
Recent work have used both failure logs and resource use data separately (and together) to detect system failure-inducing errors and to diagnose system failures. System failure occurs as a result of error propagation and the (unsuccessful) execution of error recovery mechanisms. Knowledge of error propagation patterns and unsuccessful error recovery is important for more accurate and detailed failure diagnosis, and knowledge of recovery protocols deployment is important for improving system reliability.
This paper presents the CORRMEXT framework which carries failure diagnosis another significant step forward by analyzing and reporting error propagation patterns and degrees of success and failure of error recovery protocols. CORRMEXT uses both error messages and resource use data in its analyses. Application of CORRMEXT to data from the Ranger supercomputer have produced new insights.
CORRMEXT has: (i) identified correlations between resource use counters that capture recovery attempts after an error, (ii) identified correlations between error events to capture error propagation patterns within the system, (iii) identified error propagation and recovery paths during system execution to explain system behaviour, (iv) showed that the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. CORRMEXT will be installed on the HPC clusters at the Texas Advanced Computing Center in Autumn 2017.
E. Chuah et al., ""Enabling Dependability-Driven Resource Use and Message Log-Analysis for Cluster System Diagnosis,"" 2017 IEEE 24th International Conference on High Performance Computing (HiPC), Jaipur, 2017, pp. 317-327. doi: 10.1109/HiPC.2017.00044
Edward Chuah, Arshad Jhumka, Samantha Alt, Theo Damoulas, Nentawe Gurumdimma, Marie-Christine Sawley, Bill Barth, Tommy Minyard, James C. Browne
"
https://www.turing.ac.uk/people/doctoral-students/andrew-manderson,"
Andrew is a doctoral student at the MRC Biostatistics Unit, University of Cambridge and The Alan Turing Institute. His doctoral project includes methods for modular Bayesian model construction, with applications to electronic health record data and intensive care unit delirium. Prior to this he was a Research Assistant on a joint project between The Alan Turing Institute and The University of Western Australia's Oceans Institute. Andrew has previously completed a bachelor's degree, with honours, and a master's in statistics at The University Of Western Australia. Outside of the academic realm, Andrew is a keen cyclist. 
Andrew's interests include efficient, distributed computational techniques for Bayesian statistical inference. Specifically, his project is focused on developing and applying Markov melding, a new technique for simplifying the construction of large Bayesian models, and other Monte Carlo methods to better understand intensive care unit delirium. Andrew is also interested in statistical model development in modern probabilistic programming languages such as Stan and BUGS. He is supervised by Robert Goudie.
"
https://www.turing.ac.uk/news/publications/deep-learning-framework-pricing-financial-instruments,"
We propose an integrated deep learning architecture for the stock movement prediction. Our architecture simultaneously leverages all available alpha sources. The sources include technical signals, financial news signals, and cross-sectional signals. Our architecture possesses three main properties. First, our architecture eludes overfitting issues. Although we consume a large number of technical signals but has better generalization properties than linear models. Second, our model effectively captures the interactions between signals from different categories. Third, our architecture has low computation cost. We design a graph-based component that extracts cross-sectional interactions which circumvents usage of SVD that's needed in standard models. Experimental results on the real-world stock market show that our approach outperforms the existing baselines. Meanwhile, the results from different trading simulators demonstrate that we can effectively monetize the signals.
Wu, Qiong; Zhang, Zheng; Pizzoferrato, Andrea; Cucuringu, Mihai; Liu, Zhenming.  ""A Deep Learning Framework for Pricing Financial Instruments"", arXiv:1909.04497 (2019)
1.3 MB
"
https://www.turing.ac.uk/people/researchers/indranil-pan,"
Indranil is an independent research fellow and holds the Imperial College Research Fellowship working on smart energy systems. He is also a Group Leader in the Data Centric Engineering Program at the Turing.
Indranil's publications span across multiple discipline boundaries in engineering on the themes of modelling, process control, computational intelligence and applied statistics.
"
https://www.turing.ac.uk/events/audience-type/general,"
"
https://www.turing.ac.uk/blog/what-are-digital-twins-and-why-do-we-need-them,"
From agriculture to engineering, digital twins are being used by different sectors to give powerful insights into the behaviour of physical infrastructure and systems. This could help make them safer, more efficient and improve their design. But what do we mean when we talk about a ‘digital twin’? And how and where can this technology be useful?
A digital twin can most simply be described as a computational model designed to replicate a physical object or process. This can range from a virtual copy of a single structure, such as a bridge, to more complicated infrastructure, such as an entire city.
A digital twin is designed to behave as closely as possible to its real-world counterpart. It is updated using data collected from its physical twin and can simulate and predict its behaviour in real-time. This can help offer insights into how an object is working, allowing the opportunity to drastically improve the functionality of that object. It even offers the potential to make improvements before something is built.  
New pieces of physical infrastructure have to be analysed to make sure they are safe, efficient and functioning correctly. Digital twins offer up data and insights that can change the way their real-world versions operate - for the better.
Industries including manufacturing, urban planning, aviation and maritime and agriculture are already using this technology to make their systems more efficient and reliable. These advanced models are allowing engineers to monitor the stress and strain of people walking over a bridge or helping farmers to optimise growing conditions for their crops.
In Clapham, London, for instance, growers at an underground farm are using a digital twin, built at the Turing, to help them manage conditions as they produce salad greens for local restaurants and shops. The farm’s digital twin (nicknamed CROP) works as a remote dashboard, enabling farm managers to assess and optimise conditions on the farm.
At The Alan Turing Institute, researchers are working with digital twins of bridges, farms, wind turbines, ships and even the UK’s airspace.
For example, the digital twin that Turing researchers are creating for the world’s first 3D printed steel bridge in Amsterdam could help researchers assess the wear and tear of the bridge. Everyone that crosses the bridge on foot or by bike generates data which helps to monitor the bridge’s structure and how it’s being used in the real world. This helps engineers understand when the bridge needs maintenance and how 3D printed steel might be used for larger scale and more complex building projects.
And the Turing’s research with NATS, the UK's leading provider of air traffic control services, aims to deliver the world’s first artificial intelligence (AI) system to control a section of airspace in live trials. This system will use digital twinning and machine learning technologies to predict future flight trajectories and their likelihoods – essential information for decision-making.
The launch of the Turing’s  Turing Research and Innovation Cluster in Digital Twins (TRIC) is dedicated to democratising digital twin technologies. The next phase of the Institute’s work in digital twins will see increased emphasis on translating its research into industrial applications and commercialising the tools we need to use these technologies, whilst also establishing the standards to do so safely.
The TRIC-DT’s focus on the environment, infrastructure and health will help to unlock benefits across multiple sectors from transport to energy. Digital twins have an essential role to play in tackling some of the biggest challenges facing society and are a great example of the physical and virtual worlds working together in harmony.
"
https://www.turing.ac.uk/events/phd-enrichment-scheme-supervisor-webinar,"
The Enrichment scheme is the Turing’s flagship PhD programme which has evolved year on year since its inception in 2016. The scheme offers 6 or 9 month placements at the Turing HQ and is designed to give students the opportunity to enhance, refresh, and broaden their research with the Turing’s community.  
The application process is highly competitive. Eligible applicants must be supported by their supervisor, who will continue this role throughout the student's time on the Enrichment scheme. It should be noted that the scheme does not require students to stop the clock on their PhD. For this reason we want to offer a platform for supervisors to hear more about the scheme, understand the aims and benefits and ask any questions they might have. We hope that supervisors will feel better equipped to encourage students to apply and support them with writing a successful application. 
The 50-minute webinar will comprise of an introduction to the Enrichment scheme, a brief session on eligibility, a presentation by an academic who has previously supervised Enrichment scheme students, and an open forum Q&A session. 
Following on from our successful student open days held in November and December 2022, we recognise that there has not been an opportunity previously for supervisors to engage with us regarding the scheme. This webinar is open to university staff from all institutions, regardless of affiliation with the Turing, which we hope will support our objective of building wider connections and diversifying our applicant pool via future cohorts. 
Supervisors must support their student’s application and be available to provide a reference if the candidate is successful. This event will provide an open forum to query anything that could affect their decision to do so and will enable those who have not heard of the scheme to encourage their students to apply. 
The webinar aims to provide supervisors with an opportunity to understand the benefits of the Enrichment scheme and better support their students to gain a place. 
 
We welcome participation from all backgrounds, but this event will be of interest to those who supervise PhD students in a wide range of disciplines.
Register for free
 
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/multi-agent-reasoning,"
"
https://www.turing.ac.uk/exhibitions/,"



Covering societal challenges in areas such as environmental innovation, the digital economy and civil defence, AI UK’s 40+ showstopping scientific demonstrations are not to be missed. Explore the exhibition to discover and consider how automated data science is being used in the battle against earth’s disappearing biomass or how already-proven software has been using AI to identify and prioritise vulnerable children.
The exhibition areas will be open all day, every day; so, those attending AI UK 2023 in person can drop in as and when they’d like as the event progresses. Live stream attendees will have access to live-filmed content from these brilliant demonstrations. Find out more about our interactive demonstrations by selecting one of the key themes below.
How does the synthetic population catalyst (SPC) simulate the populace’s daily activities, and how can we use this to solve issues of sustainability and life sciences at a population level? Using a spatial microsimulation model, The Alan Turing Institute’s Hadrien Salat and team create synthetic individuals mimicking the socio-economic, health and time-use characteristics of the British people. Drop in to meet the team and try out the software yourself, modelling the role of BMI in the pandemic, and climate change.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
While recent times have seen an unprecedented explosion in both the quality and availability of satellite data imaging the Earth, the information is highly unstructured. Daniel Arribas-Bel and colleagues (University of Liverpool) are using AI to bring it all into focus and put to use what the raw data gives us – the most accurate ever representation of Earth’s natural and built environments – to model, understand and plan the built environment. Drop in to meet the team behind the tech and gain a comprehensive, interactive introduction to this fascinating work.
Through the use of interactive 360 videos and VR headsets, UCL SpaceTimeLab’s Tao Cheng and Dr James Haworth (in collaboration with GLA and TfL) give three interactive case studies of urban footfall, travel and transport habits, and cycling safety. Meet the SpaceTimeAI team and get the latest across three of their most relevant and deep-dive projects, looking at the recovery of 600 London high streets post-pandemic, how low-traffic neighbourhoods are changing residents’ and traffic behaviours, and using the latest tech to enhance cyclists’ safety.
Polly Hudson (The Alan Turing Institute) showcases the Colouring Cities Research Programme (CCRP): a global collaboration programme utilising AI and machine learning to support international progress towards the UN’s Sustainable Development Goals – specifically, around our planet’s built environment. Currently working in eight far-reaching countries, CCRP facilitates knowledge and data sharing across and within governments, industry, academia and the third sector. Stop by to meet the team, learn about the project’s details and discover what’s next for collaborative learning and how we tackle issues of fragmentation, quality and accessibility – among others – in data.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
With high-stakes factors such as environmental health, people’s health and quality of life, and economic vitality influenced by land use, decision-making in this area is extremely complex. These far-reaching impacts are explored at Anna Zanchetta’s (The Alan Turing Institute) demo, in partnership with the Geospatial Commission and Turing colleagues. Their innovative programme constitutes a modelling system that leverages data science and AI to cut through inherent complications in land use determination: visit the interactive demo to find out more and meet the minds behind the tech.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
While there is an almost endless range of situations in which artificial intelligence can have huge and meaningful impact, does it always function perfectly without a human touch? Headed by Mohammad Soorati, this University of Southampton team demonstrates their unique simulator, which utilises varying ratios of autonomy to human input for maximum efficiency. Visit their AI UK 2023 demo for a hands-on experience, meeting the team – and the swarm – and aiding decision-making in a search and rescue context.
Headed by Zena Wood (University of Exeter), this collaborative exhibition from the universities of Exeter, Liverpool and Surrey showcases the work of the Dstl-funded Data Defence Research Centre (DDRC). The transdisciplinary centre brings together experts from data science, computer science, and social science to help the Defence Science and Technology Laboratory (Dstl) use data more effectively. Visit the demonstration to deep dive into some of the DDRC’s fascinating ongoing data science and AI projects, advancing technical and cultural capabilities.
The answer to those sub-par holiday snaps may be just a click away at AI UK: discover David Berman’s (Queen Mary University of London, Cambridge Consultants) image distortion clean-up. Stop by the SharpWave demonstration to upload a photo and watch in real-time as the machine learning tech cleans up your pic. SharpWave utilises a generative adversarial network (GAN) machine learning framework, and the demo features camera, physical distorting semi-transparent screen and display screen, so you can witness machine learning in action.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
Taking visitor attractions to the next level, Tag With Me offers location-based automated guides, AI recognition and interactive treasure hunt apps for parks, cultural destinations and visitor attractions. Eiman Kanjo’s (Nottingham Trent University) IoT and proximity-based digital game can also be used in a variety of beneficial settings. Stop by to find out how the AI has been put to use at Highbury Mental Health Hospital, Nottingham County Council and Rufford Country Park to improve patients’ health and well-being in a fun and interactive way.
Escape the dreary every day in this completely immersive, generative virtual reality (VR) environment. Plug in and enjoy the most advanced tech ever produced by this team in an organic generative AI virtual space, where your body movements cause the VR environment to respond and change shape. Not only is this impressive tech beautiful to see and experience, but its range of applications is remarkable, too. Stop by to meet the team – led by William Latham (Goldsmiths) – and find out how it’s been applied to protein folding and virus visualisation, including COVID-19.
Need to find a new pair of shoes online, but finding it hard to come up with a sufficiently precise text description? Yi-Zhe Song (University of Surrey) has you covered. This demo shows you a ground-breaking new paradigm where you can take out your phone, quickly scribble a rough sketch and watch as the system returns all shoes matching your sketch. The result of over a decade of sketch research at the University of Surrey’s SketchX lab, the system works across different drawing styles – and doesn’t baulk if your artistic skills are somewhat underdeveloped.
Drop by this futuristic demo to find out how you can build your very own metaverse – simply by finger scribbling in the air. This bleeding-edge research from the SketchX research lab (University of Surrey) uses the latest AI technologies in the form of deep neural networks, which convert your rough scribbles into realistic 3D objects. Visit Yi-Zhe Song’s (Centre for Vision, Speech and Signal Processing) demo for the chance to wear a VR headset and experience this immersive tech reality first-hand.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
One of the most advanced polar research vessels in the world, RRS Sir David Attenborough represents the UK Government‘s largest investment in polar science since the 1980s. But, with shipping the largest carbon emissions contributor of the British Antarctic Survey (BAS), the BAS’ goal to be net zero by 2040 could be in jeopardy. See how George Coombs’ (BAS) AI virtual modelling system creates navigational routes that optimise fuel efficiency. Discover the RRS Sir David Attenborough Digital Twin, get to know the cutting-edge route-planning tech and plan your own Antarctic explorer routes at AI UK.
Following its appearance at AI UK 2022, IceNet is now here to demonstrate its impressive real-world progress. The machine learning’s operational (MLOps) capabilities and specific applications have grown in both predictive and deliverables capability on the key environmental goal of sea ice forecasting since 2022. Gain insight into MLOps architecture and utilisation of digital twin tech, experience tech demonstrations and explore the future possibilities of IceNet, from James Byrne and Ellen Bowler’s (British Antarctic Survey).
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
Did you know that the world’s first underground farm is 33 metres below Clapham High Street? And, through sustainable and hydroponic farming techniques and a Cambridge-based digital twin, it’s shaping the future of urban farming. Its plants yield a harvest roughly every 10 days all year round, grow without soil on carpet cut-offs that were destined for landfill and are watered by tanks housed even further underground. Everything is measured and represented in a virtual 3D representation of what’s happening on the ground: the subterranean farm’s digital twin. Discover more about this incredible working project at AI UK 2023.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
The burgeoning landscape of digital twin technology has the potential to make almost unimaginable impact across in a vast array of industries. These virtual representations of physical systems can encompass connectivity, accessibility, data and models. Drop by this University of Sheffield and Turing Research and Innovation Cluster in Digital Twins (TRIC-DT) demo to discover the BAE T1A Hawk aircraft digital twin, which uses the DTOP framework to provide a simple browser-based user interface, connectivity to recorded datasets, access to remote computational resources, and connectivity to expert-made models using commercial software.
Researchers at King's College London are looking at ways we can learn more about and better treat heart conditions like arrhythmias and heart failure. By creating a digital replica of a person's heart, the “digital twin” of the heart can be used to predict responses that cannot be obtained experimentally; for example, to see what a patient's response to a particular treatment method might be. These models can also be 3D printed to understand better their shape. Drop by this demo – part of the TRIC-DT group – to learn more, find out what these digital twins look like and even see how pacemakers have evolved over time.
Harnessing the power of digital twin tech to predict and mitigate the negative impacts of climate change, reduce natural hazard impact and support sustainable development, are key focuses of the Turing Research & Innovation Cluster in Digital Twins (TRIC: DT). Working with the Natural Environment Research Council (NERC/UKRI), this demo showcases a pilot digital twin of the Haig Fras Marine Protected Areas (UK), used to monitor biodiversity.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
The mission: accelerate the world’s transition to renewable energy. The challenge: the planning process. The development of crucial infrastructure assets such as transmission lines, off- and onshore power cables and pipelines can be fraught with expensive and time-consuming setbacks. The infrastructure needed to meet net zero targets simply cannot be designed and approved in time with current methodologies. Continuum Industries are using AI to help major energy networks bridge the gap between existing procedures and the pace at which project development needs to happen.
Tackling the climate crisis one scoot at a time, University of Bristol’s James Thomas and Sam Gunner have taken pollution monitoring on the road by developing e-scooter mounted pollution monitoring hardware capable of measuring pollution in multiple locations and with flexibility hitherto unachievable. Focusing on public health impacts such as particulate matter, volatile organic compounds, carbon monoxide and nitrogen dioxide, the devices are also able to measure ambient noise. The project is ready to be trialled in Bristol this year: have a sneak peek at the hardware in action at AI UK 2023.
Do you live near a local street not designed for high vehicle traffic, but see many drivers shortcut through? Low-traffic neighbourhoods (LTNs) help keep drivers on main roads, but they can be hard to design properly. Come and meet Dustin Carlino (The Alan Turing Institute) to try out an interactive tool to decide how to place modal filters and adjust one-way streets. You can design an LTN anywhere in the UK with just a web browser, share the result with neighbours and even propose to your local council.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
Using data science and AI to quantify and visualise the risks climate change poses to populations, infrastructure and the economy, CLIMAR creates accessible decision-ready information for industry, policy makers and the public alike. The University of Exeter’s Matthew Thomas and the Met Office’s Theo McCaie present this game-changer on the path to Net Zero, crucial across multiple sectors and key in enhancing our resilience to climate change for everyone and everything, from industry to society’s most vulnerable individuals.
Displaying the enormous breadth and power of digital twins in transforming our world, this forward-thinking demo shows how new technologies can be leveraged to solve complex sustainability issues. Heavy goods vehicles (HGVs) generated over 4% of greenhouse gas emissions in 2019, and up to 30% of their journeys were freight-free: at AI UK 2023, Liming Xu and team (University of Cambridge) showcase an autonomous supply chain digital twin, including self-driving lorries. Can it influence the UK’s carbon emissions? Visit the interactive demo to find out.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
With multiple recent events throwing the speedy spread of misinformation into the spotlight, Yulan He (King's College London) and team (University of Warwick, Queen-Mary University of London, Cranfield University) have worked together to shed light on the darker corners of the internet. Stop by their exhibition for a live demo of their cutting-edge web-based misinformation detection system, PANACEA. Focusing on COVID-19-related claims online, the AI-enabled evidence-driven framework for claim veracity assessment fights misinformation in two key ways: fact-checking and rumour detection.
With stroke a major cause of death and long-term disability, Michael Allen and team (University of Exeter) have implemented machine learning to explore inter-hospital variation in clinical practice in the emergency stroke pathway. Supported by Stroke Association and the National Clinical Director of Stroke, SAMuel elucidates those patient characteristics that most influence the varying treatment decisions across the country. Visit the demo to see this machine learning and data science in action, and find out about the team’s detailed stroke outcome models, predicting likely disability.
This pioneering live demonstration featuring the companion robot and digital twin from Alexandre Colle (Edinburgh Centre for Robotics) combines innovative design, technology for social good, and data ethics. The customisable and sustainable robot, Maah, is a complex sensory device with a pillow-like form that uses a wide range of skins developed with 3D knitting technologies. Able to capture many data from users, the robot and its digital twin can reveal to both users and care stakeholders a wide range of relevant information.
Has AI really been as reliable as some thought in identifying COVID-19 infection? Harry Coppock (Imperial College London) and team’s interactive demo of AI audio models shows data in action through voice and cough audio sampling and interactive heat maps. Aimed at educating policy makers, AI practitioners and members of the public as to the dangers of falsely inflated AI performance, this project has input from across the UK and abroad with the demo featuring findings from the Bioacoustics Markers team in the Turing-RSS Health Data Lab.
Can AI decision systems reduce treatment delays? Medical imaging is routinely used for cancer diagnosis and treatment monitoring, but newer analysis techniques can lead to bottlenecks in treatment journeys. Lorena Escudero Sanchez (University of Cambridge), alongside colleagues from UK and Swedish research institutes, gives a live demo of how they have integrated deep learning models for automated segmentation of ovarian cancer, and how they are using machine learning models to predict chemotherapy response.
Discover how Charles Harris (University of Cambridge, Cambridge Centre for AI in Medicine) and team are designing new drugs using AI models. This forward-thinking demo displays a highly visual generative process, showing how state-of-the-art technologies can generate new drugs within a protein pocket structure. Find out how diffusion models could supercharge drug discovery at the demo: choose your own molecules and properties, optimise them and watch the model do its work.
Can AI be used to detect future dementia in patients? Working collaboratively to build AI models – as well as the necessary supportive research infrastructure, public engagement and policy strategy – this collaboration between Alzheimer’s Research UK, the Alan Turing Institute, the National Physical Laboratory and Oxford, Cambridge, Exeter and Newcastle Universities has a resounding answer: absolutely. Visit the EDoN demo to learn about this initiative utilising an inexpensive, low-burden digital toolkit to develop machine learning models capable of detecting dementia 15-20 years in advance of current techniques.
Discover the future of x-ray assessment with the University of Manchester’s Claudia Linder and team: BoneFinder is the software tool that automatically locates bones and joints in x-rays. It’s already being used across the world, enabling researchers and clinicians to examine differences between patient groups, how bone shapes are changed by disease, and whether certain shapes make someone more likely to suffer a particular disease. Stop by the team’s demo to see the software in action on new images, including x-rays from children with Cerebral Palsy.
Young people’s education, work and social life is increasingly influenced by AI systems. This project from Coventry University provides innovative opportunities for young people to deepen their understanding of AI through an interactive audiovisual performance – God Mode – and an online game – God Mode Epochs. Created by artist duo dmstfctn, both the performance and game (which run in real-time 3D software) aim to demystify AI by inviting the audience to interact with the process of training such systems. With sell-out performances under their belt, the team invites you to visit their demo at AI UK 2023 and find out more.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
AI trailblazers have made leaps and bounds when it comes to tech progress: but how good are we at working together internationally? Dedicated to advancing trustworthy and responsible machine learning, AI Standards Hub's Florian Ostmann (the Alan Turing Institute) leads on developing an interactive online platform offering a range of pathbreaking tools and resources to support you. Drop by to watch a demonstration of the platform’s features, explore the platform on your own device and engage with members of the Hub team. Demo partners: DCMS, Office for AI, British Standards Institution, and the National Physical Laboratory.
Get hands-on with Policy Priority Inference (PPI), a brand-new web app created at the Turing, and explore the nuances of allocating government spending. Upload development indicators to use this AI tool to simulate policy experiments and investigate budgetary scenarios using an evidence-based decision model. At AI UK 2023, you can watch demonstrations on how to use this app and even try it out for yourself.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
10-20% of UK adults have experienced online abuse and, with large-scale public abuse part of daily life for many, it’s clear we need new and improved safety measures. Project-led by Jonathan Bright (The Alan Turing Institute), the Turing Online Safety team are using state-of-the-art language models to automatically detect abuse in social media data and produce responses. Explore the science behind these models and discover how data is created, with interactive language models and real-time results, at the team’s AI UK 2023 demo.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
Mobilising for Data Justice is the 20-minute documentary produced by the ethics team at The Alan Turing Institute and Fable Studios that fills the gap in data justice research and practice, and provides resources that help policymakers, practitioners and impacted communities gain a broader understanding of data governance. This demonstration provides an accessible introduction to the broader project using audio-visual elements as well as the interactive premiere of the documentary-style film. Be among the first to see the premiere of the second episode in a three-part documentary series, at AI UK 2023.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
There’s no doubt that AI is revolutionary to many key industries. But, with greater data science comes greater data responsibility; especially when it comes to healthcare, education, finance and more. We’ve seen great progress on this with the advent of Trusted Research Environments (TREs) – computing environments with restrictions in place to keep sensitive data secure. Discover and learn more about TREs with Hari Sood (The Alan Turing Institute) through a comprehensive overview as well as live demos, games and discussion.
How can Turing.jl boost scientific research, foster collaboration, and achieve social good? The University of Cambridge’s Hong Ge, Zoubin Ghahramani and Yongchao Huang present the Turing.jl: the Julia-based, high-performance probabilistic programming language.
Drop by to meet the team; understand what Turing.jl is, how it works and how we can benefit from its use. Experience the fast, flexible, modular and easy-to-use language in three real-world probabilistic modelling uses: demonstrating its applications in fraud-detection (digital economy), patients’ drug-tracking (health and life sciences), and exhibiting its novel Bayesian approach for Li-Ion battery fade modelling (sustainable environment).
The situation: you are in charge of climate policy. The challenge: can you make the right decisions to avoid disaster? Test your theories with the University of Exeter’s Uncertainty Quantification modelling apps. Peter Challenor and team present two interactive apps replicating the real methods behind crucial and intimidatingly large-scale policy making decisions, one modelling future global temperature increases and one aiming to reduce prohibitive costs in state-of-the-art green engineering. Meet the team behind the tech, test your climate-saving mettle and deep-dive into uncertainty quantification with the University of Exeter team.
Looking for deeper insights into your data? Raphtory is an analytics platform combining graph structures and time-series analysis, allowing you to look for time-respecting paths through your data, model community evolution, extract temporal patterns and much more. With potential for meaningful application across a huge range of research, visit the demo to meet the team (Pometry / The Alan Turing Institute / QMUL) for an exciting overview of the tech and a comprehensive understanding of how the platform works.
Does the increasing sophistication of AI present more of a risk or more of an opportunity when it comes to safeguarding children? Stephen Anning (Trilateral Research) and Lincolnshire Police have co-designed CESIUM, a ground-breaking ethical AI system that identifies and prioritises vulnerable children. The system – which pays particular attention to explainability – has already demonstrated success, having identified 16 vulnerable children months before they were referred through normal channels. AI has the potential to unlock otherwise-hidden insights than more general-purpose safeguarding tools: at AI UK, meet the team behind this tech, ask the big questions and discover proof of concept with discussion and case study walk-throughs.
What are the major pitfalls in data ethics? And how do we – as the UK’s leaders in data science, machine learning and AI – ensure we’re treading the path to ethical progress together? The oft-fraught area of data ethics is absolutely key to both progress and success. Discover Nina Di Cara and Natalie Zelenka’s (University of Bristol) Data Hazards system at AI UK 2023 and have your say in this crucial conversation.
The expert team behind the Turing Way are here at AI UK 2023 to meet you, at their pop-up collaboration café. Drop by for a chin wag over pertinent topics like open source, AI ethics, reproducibility or public and stakeholder engagement or pop in for one of several introductory sessions to different areas covered by The Turing Way and the newly funded Turing Practitioners Hub.
Led by experts from across the board and with an important breadth of know-how, the sessions will present concepts and practices from each guide, as well as practical sessions on how to use The Turing Way book, contribute to the project and reuse its open-source infrastructure.
Interested in learning more? Some of the research featured was brought to you in collaboration with The Alan Turing Institute’s AI for science and government fund and Engineering and Physical Sciences Research Council (EPSRC).
Launching a fascinating AI/human experiment, this cross-institutional team led by Matija Franklin (UCL) presents The Blame Game. Get seriously involved playing judge to fictional crimes with suspects and witnesses – both human and AI. Decide how you’ll apportion blame in this visual novel, interview witnesses, investigate sources, and witness your decisions impact the narrative. Inspired by the Moral Machine Experiment, this project aims to collect data on how people think about the actions of AI agents: be part of the discussion on the world of humans and machines at AI UK 2023.
The need for comprehensive understanding of vast quantities of data has never been more crucial. From disease spread and epidemiology education to cyber security, Daniel Archambault, Cagatay Turkay and Nick Holliman (of Swansea University, University of Warwick and King's College London, respectively) pose the question: is comprehensive data visualisation the answer to bridging the gap between human and computer to achieve actionable insights? Stop by to experience live demos of the team’s innovative AI-driven data visualisation technique.
Biomass is disappearing at a truly alarming rate. With Germany reporting a massive 75% decline in biomass, leading to substantial concerns of an “insect Armageddon”, The Centre for Ecology and Hydrology’s (UKCEH) Tom August invites you to set mind against machine for the ultimate solution. UKCEH present a new automated, non-lethal AI moth-monitoring tool; but can it outperform current data science methods? Try your hand at the game and catch up with the humans behind the machine at AI UK 2023. 
The Turing’s Data Study Groups (DSGs) bring together some of the country’s top talent from data science, AI and wider fields to analyse real-world data science challenges in intensive “collaborative hackathons”, which connect organisations from industry, government and the third sector with talented multi-disciplinary researchers from across a huge academic range. Join The Turing’s Jules Manser and team for an informal walk-through of how they scope DSG challenges – and how a DSG could transform your business problem – and take part in a DSG taster brainstorming session. Pop by the stand to register for a taster session, or email the team ahead of time at: [email protected]
The Alan Turing Institute is launching its very first online learning platform. Sarah Nietopski, Chloe Poon and Mishka Nemes (Turing Skills Team) invite you to be one of the first to experience this exciting new venture; visit the AI UK demo at any time across the two days to learn about the new suite of courses focused on responsible AI. With skills team members on hand to introduce the platform and guide you through one of two sample courses, discover the fresh beginnings of a stimulating new access point to artificial intelligence.
Discover how NATS (formerly National Air Traffic Services) is using the latest in AI technology to advance their decades of airspace simulation experience at this demo, headed by Kayleigh Ockelford (NATS). From the “Time to conflict” model – which proactively reduces reliance on humans and enhances safety – and the machine learning “Michelangelo” – which forecasts air traffic flows for detailed recommendations – to bespoke cutting-edge products such as the Demand Capacity Balancer and the Strategic-Airport Capacity Management, there is a wealth of know-how and fascinating tech use to explore.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.


"
https://www.turing.ac.uk/people/researchers/aidan-osullivan,"
Aidan O’Sullivan is Associate Professor in Energy and Ai at UCL Energy Institute and a Turing Fellow at the Alan Turing Institute. He is also the Co-Founder and CTO of Carbon Re a spin out from UCL and Cambridge University focused on reducing carbon emissions by Gigatonnes by using AI to improve the efficiency of energy intensive materials manufacture, cement, steel and glass etc. He is an electrical engineer by background and was awarded a PhD from the department of Mathematics in Imperial College London. Prior to joining UCL he worked as a postdoc at MIT.
Aidan's research at the Turing will be focused on how artificial intelligence methods can be deployed to improve the efficiency of the energy system, from how electricity is generated to how the National grid is operated.
"
https://www.turing.ac.uk/people/enrichment-students/alberto-caron,"
Alberto is a PhD candidate in Statistics & Machine Learning at the Department of Statistical Science, University College London. His research interests lie at the intersection of Causality and Machine Learning, with his current focus being on Bayesian Nonparametric modeling for inferring and predicting Individualized Treatment Effects and for Policy Learning. He holds a MSc in Statistics and Econometrics, and has worked on a number of consultancy projects involving policy analysis and evaluation.
"
https://www.turing.ac.uk/people/sebastian-ahnert,"
Sebastian Ahnert is a University Lecturer at the Department of Chemical Engineering & Biotechnology, University of Cambridge. He gained his PhD in theoretical physics from the University of Cambridge and then worked as a postdoc in the Bioinformatics group at the Institut Curie in Paris, before returning to Cambridge for a Leverhulme Fellowship, followed by a Royal Society University Research Fellowship and a Gatsby Career Development Fellowship. 
Sebastian is a Senior Research Fellow at the Turing, developing a research programme that applies Machine Learning and AI to Molecular Biology. 
Sebastian’s research interests lie in the intersection of theoretical physics, biology, mathematics and computer science.
He is particularly interested in using algorithmic descriptions of structures and functional systems in order to quantify and classify their complexity. Examples of the application of such approaches include pattern detection in gene expression data, the classification of protein quaternary structure, the structure of genotype-phenotype maps, and the identification of large-scale features in complex networks. 
Connected to this he is also interested in interdiscliplinary applications of network analysis, particularly in the context of digital methods and large-scale data analysis in the humanities. He is a Co-I on the AHRC-funded 'Networking the Archives: Assembling and Analysing Early Modern Correspondence’.
"
https://www.turing.ac.uk/research/research-projects/crop-crop-research-observation-platform,"
The Growing Underground farm is located 33m below the pavements of London's Clapham High Street in former World War II air raid shelters. The space-efficient farm produces micro greens under artificial lighting all year round. Optimisation of resources is key to ensuring minimal environmental impact. The CROP digital twin is being developed for monitoring environmental and crop conditions and simulating potential operational changes. The platform will maximise the interpretability of operational data and facilitate improvements to crop growth efficiency.
By providing immediate access to visualisations of current and historic conditions in the farm and crop growth data, insights can be gained which improve farm efficiency and enhance yield. Real-time data are accessible via a 3D visualisation with direct links to sensors in the farm. Enhanced visualisations are also provided that show how many hours the environmental conditions at different locations within the farm were within different bands over the previous day and week. This gives immediate insight into whether acceptable conditions were maintained over the crop cycle.
Temperature forecasting tools are also provided, together with a scenario evaluation tool that will allow a user to 'virtually' change conditions in the farm and explore the potential impact.
The CROP cloud-based web application is the main interface for the digital twin. This includes a Unity 3D model of the farm with links to live data streams from sensors within the farm. A PostgreSQL database lies at the heart of the digital twin; this stores incoming data from the sensors and the associated models and is accessed by the web application.
 
The overall aim of the CROP project is to prototype a digital twin of the Growing Underground farm which:

At the same time an infrastructure is being developed and trialled which could potentially form the basis of a digital twin in any built environment project.
This project has a direct real-world application and potential for impact on development of digital twin infrastructure, particularly in the built environment.
CROP2, completed in December 2021, saw the implementation of a temperature forecasting model and a scenario evaluation tool.
CROP3, currently underway, will see the incorporation of more detailed crop monitoring and yield data, and increased functionality of the dashboard.
Rebecca Ward[email protected]
"
https://www.turing.ac.uk/people/researchers/alessandro-vatri,"
Alessandro is working on a seed funded project which focuses on the computational models of meaning change in natural language texts. The project focuses on Bayesian models for change-point detection in multiple languages, stimulating collaborations with British Library and National Grid. Alessandro joins us from Oxford where he has been Postdoctoral Research Assistant in Comparative Philology and Non-Stipendiary Junior Research Fellow of Wolfson College since 2014.
"
https://www.turing.ac.uk/people/external-researchers/ben-aylott,"
"
https://www.turing.ac.uk/people/external-researchers/elif-ozkirimli,"
Elif is the Head of Data Science at Pharma International Informatics at Roche. She leads a global team of data scientists that focus on understanding the patient and health care professional journeys as well as on building commercial decision support tools to achieve better patient outcomes at lower cost to society. The group builds state-of-the-art ML and advanced mathematical modeling tools to provide interpretable and trustworthy insights.
Elif has a PhD in Structural Biology from Purdue University, where she used molecular dynamics simulations to study the Src kinase activation mechanism. She later joined the Chemical Engineering Department at Bogazici University as a faculty member. Her 10 years of research spanned both experimental and computational studies on elucidating protein - compound interaction mechanisms and structure based drug design for proteins related to antimicrobial resistance and sphingolipid binding. Elif holds a BS and MS in Chemical Engineering from Bogazici University.
"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/stochastic-mathematical-modelling,"
"
https://www.turing.ac.uk/taxonomy/term/1217,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/hdr-uk,"
www.hdruk.ac.uk
"
https://www.turing.ac.uk/research/research-projects/enhancing-critical-ecosystems,"
This work aims to explore how combinations of sensing instrumentation, actuation, and a spectrum of data analytics can improve and protect the assets within ecosystems such as a cities and farms. The work also aims for ‘joined-up thinking' about how such assets interact and impact upon each other in an ecosystem. The work has the potential to improve the long-term safety of such systems and the safety of people and property.
An example of how critical ecosystems are considered is how water companies want to optimise customer quality of service and pipe lifetimes, but are not really concerned with transport infrastructures, and vice versa. Imagine a future where when one network detects a leak and the road traffic is automatically re-routed to avoid this. Better still the water network predicts a leak and schedules maintenance activities. This would enable more efficient ecosystem operation which in turn can make resources more sustainable.
Such a 'joined-up' system has the potential to improve the long-term safety of such systems (i.e. water delivery and food production) and the safety of people and property.
This multi-disciplinary project brings together researchers and user partners with expertise across sensor networks, food production (farming), and water networks. It addresses key priorities, end user needs, and challenges, with a diversity that means the solutions produced have the potential to be truly transformative and impactful.
The work is scheduled to run over five years and aims to answer the following overarching research question: Through data analysis, how can we enhance critical ecosystems and the cyber-physical systems that support them continuously?
The main research objectives are:
Taking water distribution networks and precision agriculture as two examples of critical infrastructure, we will initially instrument a sensing computing infrastructure to obtain data from each type of critical infrastructure. Using data-driven modelling, the operation of these computing infrastructures will be mapped to a control model and the control devices will then be deployed to close the loop.
Water distribution networks are large scale and topologically complex with many constraints that impact their operation (customer demand, weather, pipe lifetime etc). The main barriers for such systems are cost and resilience. In these kinds of systems, the data required to operate the control loop, guarantee computer system and water systems lifetimes, and keep both types of system secure, is not necessarily all the same, therefore understanding these trade-offs is very important.
Agriculture also brings large scale and a diversity of data (soil, foliage, weather etc), but also the need to convince farmers to trust in precision agriculture systems. However, farmers do acknowledge the benefits that can be obtained from being part of a network of farms (e.g. early warning of infestations etc.).
[email protected]
"
https://www.turing.ac.uk/blog/five-ways-turing-helping-tackle-climate-change,"
The climate crisis is arguably the single biggest threat to humanity. But science provides a powerful weapon, and research communities around the world are racing to find ways to predict, and safeguard, the future of our planet. This Earth Day, we’re highlighting five projects at The Alan Turing Institute that are contributing to this effort, involving researchers across a range of disciplines and collaborations. Environmental challenges are global, and the sheer volume of data that needs to be incorporated into decision-making, and into computational models of our climate, means that data science and AI will be a crucial part of the response.
Algorithms are essential for extracting useful intelligence and insights from large datasets. In this area, the Turing is working with the British Antarctic Survey AI Lab to use machine learning techniques tomake seasonal forecasts of Arctic sea ice cover.
Researchers hope that being able to provide a reliable forecast of Arctic sea ice cover will help improve regional weather model predictions, and could provide an early warning system to mitigate the risk to wildlife and indigenous communities who depend on the ice. To meet this aim, our researchers have teamed up with the World Wide Fund for Nature (WWF) to develop forecasts of the sea ice freeze-up and break-up days in Hudson Bay in north-east Canada to support polar bear conservation efforts there.
In order for environmental scientists to store, analyse and process data, they need a flexible, reliable and scalable digital infrastructure. Since 2020, the Turing has been working alongside the Met Office and Microsoft to support Pangeo – a project that is helping the Earth science community to analyse data in the cloud. The platform allows for interactive and scalable computing and can be used by ocean, atmosphere, land and climate scientists.
The Turing's Research Engineering (REG) team is working closely with researchers in the Met Office’s Informatics Lab and Microsoft’s AI for Earth programme to improve Pangeo’s ability to handle petabyte-scale datasets on the Microsoft Azure platform. So far, the researchers have developed and deployed a platform that allows easy access to the Pangeo suite of tools on Microsoft Azure’s cloud infrastructure.
There are no comprehensive records of the location of the UK’s solar panels, which means that precisely how much solar energy is being pumped into the UK’s electricity grid at any time is not well known, even by the National Grid. In the absence of accurate measurements and predictions of solar energy input, fossil fuels are burnt unnecessarily to keep generators running so they can take the strain when the network is underpowered. Our project, in collaboration with Open Climate Fix and OpenStreetMap, aims to resolve that by using a combination of AI (machine vision), open data and short-term forecasting.
Through a crowdsourcing exercise, volunteers tagged the locations of solar panels on OpenStreetMap, mapping one-quarter of all the solar panels in the UK. Using this data, the researchers created an open dataset of solar panel locations that will help provide a short-term forecast of how much solar power is being fed into the National Grid, in turn helping to cut carbon emissions. The team is also working on machine learning methods to detect solar panels from satellite images, which would automate the process and fill in some of the map's gaps, further improving solar power forecasting.
Recent developments in data science, statistical modelling and machine learning can help researchers to better model, predict and forecast the impacts of climate change on populations, livelihoods and natural resources, and so gain a better understanding of how conflict, political stability and migration might be affected. A project being undertaken by the Turing’s defence and security programme is exploring the implications and risks of climate change to human and national security.
Researchers have so far analysed the interactions between the societal, economic and environmental factors that can affect instability and conflict resulting from climate change. This gives a better understanding of the modelling and data requirements needed to effectively build a data model(s) that can be used in various scenarios to show future potential climate security tipping points. A recent report details this initial research.
Beneath the busy streets of Clapham, London, lies an underground farm that is growing subterranean salad leaves. Based in a former World War II air raid shelter, the aim of Growing Underground is to grow crops all year round in optimum conditions, while saving on valuable land resources.
To support the farm, researchers at the Turing and the University of Cambridge have developed a ‘digital twin’ – a computer model of the farm that uses sensors to help the growers optimise crop growth and quality while minimising energy usage. For instance, the growers can use the model to predict when they might need to alter the light or heat levels, and then find out how effective those adjustments were.
For more information on the Turing’s response to climate change, head to our dedicated climate action page. We also have a new environment and sustainability interest group, which aims to build a cross-disciplinary community to explore the role of data science and AI in addressing key environmental challenges.

"
https://www.turing.ac.uk/people/researchers/antonella-maia-perini,"
Antonella Maia Perini is a Research Associate in Data Justice and Global Ethical Futures within the public policy programme.
Prior to joining The Alan Turing Institute, Antonella worked as Project Manager at the Latin-American think tank Asuntos del Sur, where she coordinated training programmes on democratic innovation and digital rights, and later led a research consortium of seven universities and organisations studying the governance models in the responses to COVID-19. She also conducted digital security workshops and research on open government and digitalisation. Furthermore, Antonella assisted research on disinformation and hate speech at the Oxford Internet Institute and contributed to research led by the Centre of Technology and Society (CETyS) and Centro Latam Digital that aimed to understand the levels of awareness and implementation of AI ethical principles in start-ups developing AI systems in Latin America. She co-founded Youth IGF Argentina and between 2018 and 2019 was the Latin America and Caribbean Representative at the Executive Committee of the Non-Commercial Users Constituency of the Internet Corporation for Assigned Names and Numbers.
 
Antonella holds a BA in International Relations from Universidad de San Andrés (Argentina), an MA in International Relations by the University of Bologna, and an MSc in Social Science of the Internet by the Oxford Internet Institute from the University of Oxford. Her MSc dissertation sought to understand the way Latin American civil society organisations engage with discussions about ethics and governance of AI in Internet Governance arrangements, and how they interact with each other and the broader ecosystem.
At the Turing, Antonella has worked on the Advancing Data Justice Research and Practice project. She is working currently on a project on Equity in Medical Devices, as well as other projects which focus on the ethics of AI.
"
https://www.turing.ac.uk/research/research-areas/optimisation/stochastic-optimisation,"
"
https://www.turing.ac.uk/events/workshop-integration-biomedical-data-ai-and-data-science,"
Our ability to understand and control biological systems is being augmented by developments on two fronts: our ability to collect more data describing them, and our capability to comprehend these data using machine learning and AI models. 
 
 
The Universities of Southampton and Exeter are jointly organizing this event with the support of The Alan Turing Institute. The workshop will bring together experts in omic data generation and analysis to discuss the opportunities and difficulties involved in effective biological data integration.  Topics discussed at the event will include the importance of generating suitable data, the requirement for close collaboration between experimental and computational researchers, and the challenges we face to do this fairly and effectively. 
The co-Chairs are Dr Owen Rackham, University of Southampton and Dr Krasi Tsaneva-Atanasova, University of Exeter.
 
In order to encourage a coordinated and curated interdisciplinary engagement between experimental and computational biologists and meet the goals of this workshop, registration for this event will be invitation-only in the first instance. The list of invited applicants includes a number of Turing Fellows and Turing-affiliated researchers. The remaining spaces will be filled through an open invitation. If oversubscribed, the organisers will assess/accept applicants to ensure the goals of the workshop are met.
Interested participants should contact the organiser Susan Davies at [email protected]
The keynote talk by Dr Francesca Nadalin, European Molecular Biology Laboratory (EMBL), will be live-streamed and open to anyone to attend online. Please register via Eventbrite – the closing date for registration is Tuesday 6 June. 
 
"
https://www.turing.ac.uk/people/researchers/adrian-weller,"
Adrian Weller is a Programme Director and Turing Fellow leading work on Safe and Ethical AI at The Alan Turing Institute, the UK national institute for data science and AI. Adrian is a Director of Research in Machine Learning at the University of Cambridge, and at the Leverhulme Centre for the Future of Intelligence where he is Programme Director for Trust and Society. His interests span AI, its commercial applications and helping to ensure beneficial outcomes for society. He serves on several boards including the advisory board for the Centre for Data Ethics and Innovation. He received an MBE for services to digital innovation in 2022. Previously, Adrian held senior roles in finance.
"
https://www.turing.ac.uk/research/research-projects/understanding-arctic-sea-ice-loss,"
While scientists have observed and studied the rapid decline in Arctic sea ice over the past few decades, a high level of uncertainty remains about how the rate of ice loss will change over the coming years. Machine learning techniques have the potential to help us untangle the underlying mechanisms in order to reduce these uncertainties. Providing reliable forecasts of likely Arctic sea ice cover would help improve regional and global climate model predictions.
The decrease of Arctic sea ice cover is concerning as its white surface reflects up to 80 percent of incoming sunlight, deflecting additional energy away from the Earth's lower atmosphere and back towards space. With a declining spatial extent of sea ice, the dark ocean surface which replaces it absorbs considerably more sunlight energy. This leads to a self-perpetuating cycle with further warming of the atmosphere and surface waters and more melting of ice. This process is known as ‘ice-albedo feedback’ and is most prevalent during the summer months. Conversely, in the darker months other climate drivers, such as variability in oceanic currents and atmospheric weather patterns, are more likely to dominate the variation in sea ice.
This research is being undertaken by the British Antarctic Survey (BAS) AI Lab.
The aim of this research is to further our understanding of the complex inter-connectedness of large-scale climate phenomena and sea ice variability in order to reduce uncertainties in future climate predictions. 
Applying machine learning techniques in new ways to this area will enable us to work more efficiently and effectively than using current climate model simulators alone, not least as these simulators are highly computationally expensive. The machine learning predictions generated within this project will be benchmarked against those from state-of-the art climate simulators to track ongoing improvements.
The vast areas of white ice and snow in the Arctic region is responsible for reflecting sunlight back to space, keeping our planet cool and regulating regional and global weather patterns. It is also an extreme living environment which supports a wide range of indigenous people and wildlife.
Being able to better predict the likely fluctuations in sea ice extent across the Arctic region, and the impact this can have on the global climate, is fundamentally important as these changes will effect weather patterns which in turn will have impacts on food, water, and economic security across the globe. Furthermore, improving our understanding of the potential changes facing this irreplaceable environment can also be used to support wildlife conservation efforts and indigenous peoples around the polar regions, whose way of life is under threat through environmental change. 
 
"
https://www.turing.ac.uk/research/research-projects/streaming-data-modelling-real-time-monitoring-and-forecasting,"
Organisations increasingly want to gain actionable insight from data as it is generated or recorded, in near real time. Statistical models describing the data-generation process, used in combination with recently developed fitting algorithms and computational infrastructure designed for streaming data, together form a very promising approach to usefully solving this problem. The methods developed in this project will be applied to challenging real-world problems in urban analytics and healthcare.
Integrated modelling of data sources from streaming data networks will typically require the inversion of explanatory stochastic state-space models. Although significant computational and methodological advances have been made in this area in recent years, true simultaneous inference for both static parameters and dynamic states of non-trivial models remains challenging. Recent developments in sequential Monte Carlo (SMC) methodology provide a number of promising approaches for further study.
Recent methods from the literature for online analysis and forecasting of state-space models will be tested, developed and extended through application to two challenging real-world applications. The methods will be published in both the statistical and applied literature, and an open-source software library will be produced to enable routine application of the newly developed methods by any interested parties.
In addition to producing improved statistical algorithms, this work will also directly advance the state-of-the-art in two user domains.
The first application will be to the live streaming data from Newcastle’s Urban Observatory – one of the largest public sources of smart city data in the world. Data from environmental sensors which are both multivariate and spatially distributed will provide a challenging use case for online statistical modelling.
The second application will be to healthcare analytics data, such as from wearable sensors. Joint modelling of multiple heterogeneous sensors in real time will be undertaken for forecasting and alerting.
"
https://www.turing.ac.uk/people/external-researchers/joseph-alderman,"
Joseph Alderman is an anaesthesia & intensive care registrar at University Hospitals Birmingham NHS Foundation Trust, and a doctoral researcher in AI and digital health at the University of Birmingham, UK. After completing his intercalated BMedSc and medical degree, Joseph was appointed as an NIHR Academic Clinical Fellow in Intensive Care Medicine at the University of Birmingham. In 2022 he was awarded fellowship of the royal college of anaesthetists (FRCA).
 
Joseph’s doctoral research focusses on the performance and fairness of prognostic models and clinical risk scores used in healthcare, with a focus on the implication of the use of these tools on health equity. He is a key member of the STANDING Together initiative, which aims to improve the documentation and use of datasets used in healthcare AI development.
 
Joseph is also one of the organisers of the Turing's Clinical AI Interest Group.
"
https://www.turing.ac.uk/people/kasra-hosseini,"
Kasra is interested in (artificially) intelligent systems, machine learning, and data analysis and visualisation.
He completed his PhD in computational seismology at the Ludwig-Maximilian-University of Munich (LMU). During his PhD, he worked on the design and implementation of an algorithm (""No Data Left Behind"") for seismic tomography inversions using large seismological datasets. After that, he worked as a postdoctoral research assistant at the University of Oxford from 2016-2019, where he focused on inverse theory, uncertainty estimation and interactive data visualisation.
Kasra was awarded the OxTALENT award for his work on interactive data visualisation, and he was awarded 1st prize by Earthscope in their “Visualising EarthScope Science: Motion” competition.
"
https://www.turing.ac.uk/events/the-turing-lectures,"


The Turing Lectures series features influential figures from the dynamic world of data science and artificial intelligence. These prestigious, landmark lectures lead and shape the national conversation, sparking debate, informing and, of course, entertaining audiences.
Our thought-leaders are renowned experts – often world authorities – each with their unique perspectives. From scientific advisors to high-profile scientists, entrepreneurs, industry insiders, and leading academics, The Turing Lectures prides itself on diversity, and authoritative and an engaging style.
Speaking to packed audiences, the lectures have already included the lead scientist of Google Deepmind’s Machine Learning team, Nando de Freitas; Buzzfeed News Editor, Craig Silverman; computer scientist, academic and social entrepreneur Sue Black OBE; and mathematician, author and broadcaster, Hannah Fry. 
Topics explored in the series are as diverse as the speakers and reflect a rapidly changing world. With subjects as intriguing as the ethical implications of 'deepfakes', AI’s effect on economics and the commercial sector, data trusts, the opportunity of the digital humanities, and the promise of wearable tech.
The reach of The Turing Lectures series goes well beyond its highly engaged live audience and you can discover or rediscover many of our lectures via our YouTube channel. The series continues to grow in scope, ambition and reputation and we look forward to welcoming you.
Sign up to the Turing’s event newsletter for more details.
 

"
https://www.turing.ac.uk/people/guest-speakers/zachary-kenton,"
Zachary Kenton is a Research Scientist in AI safety at DeepMind, mainly interested in specification/alignment. He was previously a Postdoc at the University of Oxford, in the Oxford Applied and Theoretical Machine Learning (OATML) group, working under Yarin Gal. Prior to that he was a Research Assistant under Owain Evans at the Future of Humanity Institute, University of Oxford and a Visiting Researcher at the Montreal Institute for Learning Algorithms (MILA), under Yoshua Bengio. He also worked as a Data Scientist at Faculty.
Zachary completed his PhD in 2017 at Queen Mary University of London in Theoretical Physics, where he worked on string theory and cosmology. Prior to his PhD, he studied Mathematics at the University of Cambridge.
"
https://www.turing.ac.uk/research/research-projects/measurement-theory-data-science-and-ai,"
Just like a human, any AI algorithm has inherent capabilities and limitations which are important to understand. To assess either human or AI skills one has to go beyond what can be directly observed (e.g. exam scores or match results). This project will adapt techniques from psychological skill assessment to model and explain fundamental algorithmic abilities in data science and AI.
Measurement theory studies the concept of measurement and scale. If you have a way to measure, say, the length of individual rods or planks, this should also allow you to then calculate the combined length of concatenated rods or planks. What relevant concatenation operations are there in data science and AI, and what does that mean for the underlying measurement scale?
Psychometrics explores the idea that many variables of interest - such as the difficulty of a test or the ability of a student - are latent variables that manifest themselves only indirectly through test results. Luckily, latent variable models are widely used in machine learning and so this will be an area of direct relevance to the project.
Ultimately, the kinds of conclusions this project wants to draw from its experiments with data are causal: 'this algorithm out-performs that algorithm on this data set because the classes are highly imbalanced'. The underlying reasoning is counterfactual: 'had the classes been balanced, the outcome would have been different'. Causal models are a topic of considerable current interest in machine learning and AI, and so here is a third set of ideas to be tapped into.
An important objective of the project is to develop awareness in the data science and AI community of the importance of measurement scales, and how specific scales have associated allowable operations and statistics.
A second objective is to develop a machine learning equivalent of psychometrics, which might be called discometrics (from Latin: discere -- to learn). Just as psychometrics has developed tools to model the skills of a human learner and develop standardised (SAT) tests, so we need similar tools to model the skills of learning machines and have standardised benchmarks.
Thirdly, an important set of AI capabilities and skills is associated with privacy, fairness and prevention of discrimination: we want to make sure that AI algorithms take decisions for the right reasons and operate within the confines of the law. Developing measurement procedures and calibrated test suites for these latent skills is hence of particular significance.
The project will most directly affect data science, machine learning and AI methodology, in particular the empirical work needed to demonstrate that the AI algorithms do what they are supposed to do.
The outlook is that this can eventually lead to standardised skill rating scales similar to the well-known Elo rating in chess. Eventually this will enable performance certificates for AI algorithms, not dissimilar to energy efficiency assessments of buildings and appliances, or food hygiene certificates, and hence be important for users of AI technology, i.e. all of us.
A video explaining the main ideas and results of the project can be accessed here.
Other related talks are:
- AI UK
- Discovery Science 2020 keynote
- Classifier Calibration tutorial
A code repository with explanatory notebooks can be accessed here.
[email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/nesta,"
www.nesta.org.uk
"
https://www.turing.ac.uk/events/turing-lecture-how-smartphones-and-wearable-technology-can-help-care-chronic-illnesses,"
455.8 KB


In this talk, Stephen Friend will propose a way of using smartphones and wearable technology to help in the care of those with chronic conditions, such as Parkinson’s, diabetes, arthritis, and dementia. Stephen suggests giving those suffering from these conditions the chance to pause, reflect and share, in order to build a system of guidance and support based on direct objective data with insights by and for each other. When augmented by an AI approach, this data could enable the forecasting of symptom changes for individuals and offer health guidance that rests on empowerment. This approach is well suited to the care of those with chronic conditions in the UK, and in very low resource areas worldwide.
The talk will be followed by a Q&A session and a complimentary drinks reception. 
Wellcome Collection
183 Euston Rd, London NW1 2BE
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/mozilla,"
"
https://www.turing.ac.uk/news/publications/data-justice-stories-repository-case-studies,"
The idea of “data justice” is of recent academic vintage. It has arisen over the past decade in Anglo-European research institutions as an attempt to bring together a critique of the power dynamics that underlie accelerating trends of datafication with a normative commitment to the principles of social justice—a commitment to the achievement of a society that is equitable, fair, and capable of confronting the root causes of injustice.However, despite the seeming novelty of such a data justice pedigree, this joining up of the critique of the power imbalances that have shaped the digital and “big data” revolutions with a commitment to social equity and constructive societal transformation has a deeper historical, and more geographically diverse, provenance. As the stories of the data justice initiatives, activism, and advocacy contained in this volume well evidence, practices of data justice across the globe have, in fact, largely preceded the elaboration and crystallisation of the idea of data justice in contemporary academic discourse. 
Leslie, David, Briggs, Morgan, Perini, Antonella, Jayadeva, Smera, Rincón, Cami, Raval, Noopur, Birhane, Abeba, Powell, Rosamund, Katell, Michael, & Aitken, Mhairi. (2022). Data Justice Stories: A Repository of Case Studies. The Alan Turing Institute in collaboration with The Global Partnership on AI. https://doi.org/10.5281/zenodo.6408326
This report was commissioned by the International Centre of Expertise in Montréal in collaboration with GPAI's Data Governance Working Group, and produced by the Alan Turing Institute. The research was supported, in part, by a grant from ESRC (ES/T007354/1), Towards Turing 2.0 under the EPSRC Grant EP/W037211/1, and from the public funds that make the Turing's Public Policy Programme possible.
"
https://www.turing.ac.uk/people/business-team,"

To contact a specific team please use our contact form.
Senior Leadership
Office of the COO
People & Operations
IT & Information Security
General Counsel
Programme Management
Applied Research Centre in Defence and Security
Programme Leadership
Academic Engagement, National Skills and Strategy
Communications & Engagement
Partnerships
Research Engineering
Public Policy
"
https://www.turing.ac.uk/people/guest-speakers/susannah-odell,"
Susannah Odell is the Deputy Director of Digital at the Confederation of British Industry, the UK's largest business group that represents 190,000 businesses of all sizes and sectors across the UK. Susannah leads the CBI's digital and technology team, which works to create a thriving UK digital economy, supporting businesses in investing in, developing and adopting technologies, and leading industries of the future. This includes cyber security, digital regulation, data, AI, connectivity, and skills. 
Susannah is also an advocate of diversity in technology and set up the CBI's first Women in Technology Group in 2019 to bring more diverse voices into the development and adoption of technologies that are shaping the economy and society. Susannah previously worked for the UK's national academy of science, the Royal Society, on AI policy and has degrees from the University of Cambridge and London School of Economics. 
 
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/monte-carlo-methods,"
"
https://www.turing.ac.uk/blog/data-study-group-researchers-perspective,"
In May, The Alan Turing Institute hosted a week-long Data Study Group event organised by Turing Fellow Sebastian Vollmer: an opportunity for academics to work on data science challenges posed by industry.
As a Turing Doctoral Student, this was my second time taking part in a data study group, and on this occasion, I was a facilitator for the Defence Science and Technology Laboratory (Dstl) – Urban Analytics group.
Initially, I was a little nervous about guiding a team of mainly strangers (the data study groups involves academics from all around the world) to deliver some useful methods for Dstl to take forward in less than a week! Luckily, unlike a hackathon, we were not obliged to fully complete a solution to tackle the problem. In fact, the data study group’s intention is to provide companies with several methods they can take forward either in-house or as a future collaboration with the Institute.
I think this is great as although hackathons are fantastic at generating innovative solutions quickly, due to the time constraints often ideas haven’t been thoroughly tested and then fail at the implementation stage. Instead, providing industry clients with cutting-edge academic knowledge on how to apply different data science approaches to tackle their problem is far more valuable, as there is real knowledge these companies can take back and apply to other challenges the companies might face in the future.
The project that Dstl proposed was a very interesting challenge to tackle. We analysed large amounts of spatial data, such as highways networks, points of interest and locations of crime activities, in order to develop models which can predict likely locations of facilities (e.g. nuclear power plants) or events (e.g. civil unrest) at a global level.
The challenge was particularly interesting from a research point of view as there could be many different approaches to solving this problem. Therefore, the project drew a fantastic selection of candidates to our team from different fields – varying from spatial analysis and epidemiology to mathematics and physics.

Urban Analytics group working on the Defence Science and Technology Laboratory (Dstl) challenge
In the first few hours, we produced a plethora of ideas on how to tackle the challenge Dstl had posed to us. We decided to split into subgroups and explore our different approaches, with daily catch-ups to update the group on our progress. This didn’t mean that the subgroups purely stayed working on their own approach: as soon as the other groups saw how the other approaches could enhance their own approach, they started to include these insights in their own work. I saw some great interdisciplinary collaboration.
Dstl were extremely impressed with how quickly we managed to come together with so many approaches to tackle their challenge, as well as with the dedication of the team members to find useful solutions.
Matt, a Dstl representative, commenting on the work developed, said:
“The Data Study Group week was one of the best experiences I’ve had working with external suppliers of any kind in my 8 years in the Ministry of Defence. A large team of very capable academics worked extremely hard on my problem and demonstrated multiple approaches to solve it. To have so many interesting approaches thoroughly explored in such a short time period is very impressive and incredibly useful to our organisation. The team was very well led by the Turing facilitator, Chanuki Seresinhe, the few IT issues that arose were dealt with immediately and on occasion world leading experts in particular fields were called in from elsewhere in the Institute. I have already recommended to colleagues that more of our problems should be pitched to the Turing Data Study Group events.”
Overall, this was a fantastic experience. If you are an academic who wants to get some hands-on experience working on industry challenges in a dynamic and inspiring atmosphere, then I would highly recommend joining a future Data Study Group. I also think the Data Study Group is equally valuable to industry partners, as it is rare to be able to work with a group of top academics around the country all at once for some valuable insights into cutting-edge approaches and also just some fresh inspiration.

Defence Science and Technology Laboratory (Dstl) – Urban Analytics research group
This blog was written by Chanuki Seresinhe, Turing Enrichment Year Doctoral Student and facilitator for the urban analytics group which worked on challenges posed by the Defence Science and Technology Laboratory (Dstl).
The Alan Turing Institute organises several data study groups per year. If your organisation would like to get involved please email [email protected].
"
https://www.turing.ac.uk/people/researchers/erin-young,"
Dr Erin Lorelie Young is a Research Fellow in the Public Policy programme. She has a Ph.D. (D.Phil) from the University of Oxford, where she studied the socio-technical practices of interdisciplinary research and development projects building new technologies. Prior to joining the Turing, Erin was an H-STAR Visiting Researcher at Stanford University, and a Researcher at the Oxford Internet Institute, working on various projects including investigating the potential of AI for lifelong learning.
Erin has also held positions as a Consultant at the United Nations (IIEP-UNESCO) in Paris, Kantar in London, and Thomson Reuters in New York City. She has a PGC in International Business Administration and Practice and Organisational Behaviour, and earned scholarships to study at the British Schools of Athens and Rome. She holds an MSc (Distinction) from the University of Oxford in Education (Learning and Technology), and an MA in Classics from the University of Cambridge.
Erin’s selected invited lectures, talks, and presentations include:
'Shaping technological innovation', Invited Keynote, 1st International Seminar of Women in Intellectual Property (IP), Associação Brasileira da Propriedade Intelectual (ABPI), Rio de Janeiro, Brazil, March 2023.
‘Diversity in AI’, Invited lecture, Northern Lights Deep Learning Conference (NLDL 2023), UiT The Arctic University of Norway, Tromsø, Norway, January 2023.
‘Ethics and culture in the AI field’, Invited panellist, The AI Summit, New York, USA, December 2022.
‘The Future of Women in Technology’, Invited talk, Opening of the Norwegian-British Women in Tech Business network, The British Embassy, Oslo, Norway, May 2022.
‘Where are the women? Mapping the gender gap in data science’, Invited Lecture, HMRC Data Science Conference, December 2021.
‘Exploring gender gaps in AI’, Invited talk, Committee on Women's Rights and Gender Equality (FEMM), ‘Building an inclusive economy for women in the green and digital transformation’, Conference on the Future of Europe, October 2021.
‘Mapping gender gaps and inequalities in data and AI’, Invited talk, Covid-19 Task Force, Cabinet Office, September 2021.
‘Data Innovation at the Bank of England’, Invited panellist, Bank of England, August 2021.
‘Women in Data Science and AI: Mapping the gender job gap,’ Invited Lecture, Turing Talks for GCHQ Staff, GCHQ, June 2021.
‘If it’s not diverse, it’s not ethical’, Invited panel member, Ethics & Society stage, CogX, June 2021.
‘Where are the Women? Mapping the Gender Job Gap in AI’, Invited talk, ‘Let's Multiply: Data Science Community of Interest’ (International Day of Women and Girls in ICT), Data Science Campus, Office for National Statistics (ONS), April 2021.
‘Gender Bias in AI: Women in Data Science and AI at The Alan Turing Institute,’ Invited Lecture, Civil Service Fast Stream, Digital, Data, Technology and Cyber Winter Conference (DDaTCon), February 2021.
‘Online learning for all: Can digital literacy be used to leapfrog illiteracy?,’ Invited Lecture, Second Chance Education and Vocational Learning Programme (SCE), United Nations (UN Women), December 2020.
‘Building inclusive technology and diverse workplaces through data,’ Invited Lecture, ‘Diversity, inclusion and fairness in safety tech’, Safety Tech Innovation Network, KTN, Department for Digital, Culture, Media and Sport (DCMS), November 2020.
Erin is interested in the social, economic and governance-related implications of technologies, particularly Artificial Intelligence (AI), machine learning and other data-driven innovations. Her work sits at the intersection of technology and society, drawing from Science and Technology Studies (STS), in particular Actor-Network Theory (ANT), organisational sociology and mixed methods research, exploring responsible and ethical AI design, social data science for policy-making, information science, and data governance and technological innovation across sectors.
"
https://www.turing.ac.uk/news/alan-turing-institute-and-intel-form-strategic-partnership,"
​
The Alan Turing Institute and Intel have agreed to form a long term strategic partnership to deliver a research programme focused on high-performance computing and data analytics.
Researchers from both organisations will work together on the programme alongside co-funded research fellows and software engineers.
In addition, Intel will dedicate a hardware architecture team at the Institute’s facilities so that new algorithms developed by The Alan Turing Institute feed into the design of Intel’s future generations of microprocessors.
As well as conducting research, the partnership will train a new generation of data scientists through The Alan Turing Institute’s doctoral programme, ensuring students are equipped with the latest data science techniques, tools, and methodologies.
Both organisations will work together to raise awareness of the importance and potential of data science and algorithms.

Minister for Universities and Science, Jo Johnson, said:
“Government is committed to ensuring the UK is the best place in Europe to innovate, patent new ideas and start a business. Big data offers huge potential for innovation which is why Government invested £42m in the Alan Turing Institute to secure the UK’s future in this important field.”
“I welcome this new strategic partnership with Intel which is testament to the strength of the UK’s research base and an exciting opportunity for growth in our digital economy.”
Professor Andrew Blake, director of The Alan Turing Institute, said:
“Intel is a global leader in computing innovation and I am delighted that it has become a strategic partner. This is a great development for the Institute and for data science globally.
“Alan Turing was one of the first people to build an electronic computer. The partnership with Intel means that, true to his legacy, the Institute named after him will be contributing to the design of future generations of computers.
“The goal of the Alan Turing Institute is to drive scientific and technological discoveries in the use of big data and algorithms, which will create new business opportunities, and accelerate solutions to global challenges. We have, today, taken a significant step towards that goal.”
Christian Morales, Corporate Vice President, General Manager Intel EMEA, said:
“The Data Scientist is a very captivating and crucial job of the 21st century. With the right combination of people and technology, Big Data has the potential to solve big problems in public health, medicine, science, agriculture and engineering. We are committed to helping the Alan Turing Institute develop a fertile breeding ground for data scientists, with the greater purpose of driving critical data analytics across all industries.”
 For more information contact Christopher Buratta at The Alan Turing Institute on [email protected] or 07747 862039
 
Notes for Editors:
The Alan Turing Institute is a joint venture between the universities of Cambridge, Edinburgh, Oxford, Warwick, UCL and the Engineering and Physical Sciences Research Council (EPSRC). The Institute will promote the development and use of advanced mathematics, computer science, algorithms and big data for human benefit.
The Institute will bring together leaders in advanced mathematics and computing science from the five lead universities and other partners and will conduct first class research and development in an environment that brings together theory and practical application. It will build on the UK’s existing academic strengths and help position the country as a world leader in the analysis and application of big data and algorithm research.
The Institute is being funded over five years with £42 million from the UK government. The university partners are contributing £5 million each, totalling £25 million. The Institute is based at the British Library at the heart of London’s Knowledge Quarter. Its work is expected to encompass a broad range of scientific disciplines and be relevant across multiple business sectors.
​
"
https://www.turing.ac.uk/people/doctoral-students/julien-vaes,"
Julien Vaes is a DPhil student at the University of Oxford. He has previously completed a master's degree in applied mathematics engineering at the Université catholique de Louvain (Belgium), as well as a master's degree in engineering at the Ecole Centrale de Paris (France).
Julien’s supervisor, Professor Raphael Hauser, investigated, with his former student Miha Troha, the problem of estimating the term structure of future prices in power markets on the basis of risk-averse optimal trading by producers and wholesalers, subject to the physical constraints of each power plant connected to the grid. In his DPhil project, Julien's objective is to further develop the structure problem established by Hauser-Troha by adding three important components that are currently missing: firstly, the market clearing auction as a last decision stage, secondly, the network capacity and regulatory constraints, and, thirdly, the renewable power sources into the energy mix considered.
"
https://www.turing.ac.uk/news/turing-programme-director-joins-editorial-board-new-ai-journal,"
Professor Chris Holmes has joined the editorial board of a new AI journal, launched this month by the publisher of the New England Journal of Medicine, NEJM Group. 
 
The journal, NEJM AI, aims to identify and evaluate state-of-the-art applications of artificial intelligence (AI) to clinical medicine. 
 
The journal will publish original research and provide reviews, policy perspectives, and accessible educational material targeted at practicing doctors and clinician leaders interested in applying AI, computer scientists seeking to translate algorithmic advances to clinical practice, and policy makers and regulators. 
 
It will pair pre-clinical and clinical articles to offer important context to both clinicians and non-clinician researchers and will connect the fast-moving developments in AI, informatics and technology in medicine with the application of these advancements to clinical practice.  
 
Professor Holmes joins the editorial board of the new journal bringing with him a wealth of experience in theory, methods and applications of robust statistical machine learning and causal inference to addressing real-world problems in health.  
 
Professor Holmes, Programme Director for Health and Medical Sciences at The Alan Turing Institute, said: “I’m delighted to be a part of this new and important journal. Artificial intelligence has the power to transform medicine and medical research, but careful evaluation of the robustness of empirical results and awareness of issues of fairness are vital. I’m really excited to see how NEJM AI will become a showcase for best practice across different disciplines in AI research for medical science.” 
 
The Health and Medical Sciences Programme aims to accelerate the scientific understanding of human disease and improve human health through data-driven innovation in AI and statistical science. They have provided COVID-19 expertise to the UK government and developed the first international guidelines for clinical trials involving AI. 
Top image credit: National Cancer Institute
 
"
https://www.turing.ac.uk/people/guest-speakers/aidan-peppin,"
Aidan is a Senior Researcher at the Ada Lovelace Institute. He leads public attitudes and engagement research projects, like the Citizens’ Biometrics Council and a survey on digital health inequalities. Aidan previously worked at the Wellcome Trust on the social and ethical issues of health data and technology. He has a background in sociological approaches to understanding relationships between people, society and emerging technologies.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/ucl,"
"
https://www.turing.ac.uk/people/former-enrichment-students/gabriele-beltramo,"
Gabriele is a PhD student in mathematics at Queen Mary University of London. He has a BSc and MSc in theoretical mathematics from the University of Turin, Italy.
During his last year of study he got interested in topological data analysis. At Turing he is interested in growing his knowledge in the field of machine learning and AI, and embed that into his research.
Topological data analysis is a new set of tools for extracting geometric and topological features from data. This information is obtained in the form of persistence diagrams. These objects, combined with statistical techniques, have been proved useful in a number of real-world applications. Fields interested include neuroscience, object recognition, and protein compressibility.
Much work is still required to understand the interplay of topological data analysis and machine learning. Gabriele's project goal is to clarify this relation, and to improve on previous results. In the period he will spend at Turing, he will integrate machine learning methods with persistence diagrams in novel ways.
"
https://www.turing.ac.uk/people/guest-speakers/tim-mcgarr,"
Tim McGarr is the Sector Lead for the Digital area within Knowledge Solutions in BSI. Tim has specific responsibility for the direction and development of key digital standards areas, primarily Artificial Intelligence and Cyber Security. Tim’s role includes the development of thought leadership, market development and new areas of standardization. Tim has previously developed new areas of standardisation in IoT, Blockchain, Virtual/Augmented Reality, Privacy, Risk Management and Organizational Resilience. Prior to BSI, Tim has worked in corporate strategy and management consultancy. Tim has an MBA from HEC in Paris, France. 
"
https://www.turing.ac.uk/people/enrichment-students/gladys-kenyon,"
Gladys is a second year Urban Analytics PhD researcher based in the Geographic Data Science Lab at the University of Liverpool. Gladys' research utilises machine learning and spatial analysis to explore house price variation in Southern Europe. During her time as an undergraduate studying Human Geography, she developed an interest in the use of data to understand spatial inequalities. In the future, she is keen to use data science for public good.
Machine learning computer vision methods are enhancing our ability to research urban morphology. Gladys' Turing project uses image data to explore residential urban form and its relationship with house prices. A deep neural network will be used to score street view and outdoor images of houses in terms of neighbourhood desirability. The classification can then be used in a regression model to understand how the built environment shapes housing wealth.  Potentially, the research could be used to identify deprivation and measure inequality. 
"
https://www.turing.ac.uk/people/external-researchers/john-moriarty,"
John Moriarty is Professor of Mathematics in the School of Mathematical Sciences, where he is also the School impact lead. He specialises in applied probability and applications in energy. Prior to joining Queen Mary, John was a Senior Lecturer in Probability and Statistics at the University of Manchester.
He holds a BA in Mathematical Sciences, M.Sc. in Applied Statistics and D.Phil. in Mathematics from the University of Oxford. After the completion of his studies, he held a post-doctoral research fellowship at University College Cork. From 2013 to 2020 he held an EPSRC Early Career Research Fellowship, and from 2020-22 he holds a Research Fellowship at the Alan Turing Institute. He served as Principal Organiser for the 2019 Isaac Newton Institute programme on the Mathematics of Energy Systems.
"
https://www.turing.ac.uk/people/researchers/andrew-duncan,"
Andrew Duncan is RAEng Lecturer (Assistant Professor) in the Department of Mathematics at Imperial College London and group leader for the Data-Centric Engineering Programme at The Alan Turing Institute.
Andrew's research interests span applied stochastic modelling, modelling and inference in a variety of applications including aerospace engineering, energy systems and predictive health monitoring for complex industrial processes.
His theoretical research pertains to methods for approximate inference, anomaly and change-point detection for complex statistical models based on Stein’s method and related methodology. His recent work includes developing methods for assessing convergence of MCMC samplers and for learning intractable generative models.
Andrew's interests lie generally within the intersection between computation, analysis and probability, with a particular focus on applications in biology and chemistry, particularly systems involving stochasticity and/or multiple scales. These include the analysis and construction of MCMC-based methods for sampling from probability distributions, coarse graining of stochastic models involving multiple scales, classical and stochastic homogenisation of PDEs and SDEs, and the Bayesian formulation of inverse problems.
"
https://www.turing.ac.uk/people/researchers/alisha-davies,"
Alisha Davies is the Health Theme Lead for the AI for science and government (ASG) programme at the Turing. She has a PhD in epidemiology from the London School of Hygiene and Tropical Medicine, and holds an Hon Professorship in the Faculty of Health and Life Sciences at Swansea University. Alisha is also an NHS Consultant in Public Health and Head of Research and Evaluation at Public Health Wales where she leads a multi-disciplinary division working alongside academic partners to address knowledge gaps in public health policy and practice. Examples include using linked routine health data to explore patterns of mental health in children and young people, national surveys to address digital exclusion in health, and social science methods to sustain community resilience.
Alisha is also a member of the NIHR Public Health Research Prioritisation Committee, Deputy Director of the Centre for Population Health Research in Wales and leads the Health Foundation funded Wales Networked Data Lab.
Alisha is primarily interested in health inequalities and the wider determinants of health. Her research career spans mental and physical health – from health care through to the wider determinants such as employment and digital exclusion.
She is interested in the application of AI and data science to the complexity of health, how such advances in science can have an impact in practice and policy decisions, to help us to address the drivers of health and inequalities.
"
https://www.turing.ac.uk/events/turing-lecture-statistics-decision-making-and-privacy,"
​
In the latest event in our series on data science in the public realm, the UK’s National Statistician, John Pullinger, will explain how data is helping Government and others make better decisions, from the National Census and many other sources, firmly based on evidence. Reflecting the importance of data ethics and privacy to making data as freely available as possible, in the second lecture Microsoft Research’s Cynthia Dwork will discuss the technical challenges of privacy and how new research may help increase access to valuable data in a way that safeguards the privacy of data owners.  

‘Good’ data about the state of our economy and society, helps governments, businesses and citizens, to make informed decisions for the future. Demands to generate this data faster, in more detailed and tailored forms are growing, whilst the data revolution is providing rich opportunities for the data science community to come together to identify novel solutions to these demands. It also raises important questions about data ethics, data security and data quality, amongst other concerns. In this talk, John Pullinger will explore the nature of these questions, current responses to them, and the potential for building relationships between The Alan Turing Institute, the Office for National Statistics, the Government Statistical Service, and others who share an interest in developing insights from data to benefit the public.
Biography
John Pullinger is the UK National Statistician and most recent Chair of the United Nations Statistical Commission. He has previously been President of the Royal Statistical Society and Chair of the Getstats campaign for statistical literacy. He has worked in statistical, research, policy and leadership roles in the UK government at the Department of Trade and Industry, Department of Environment, and Office of Manpower Economics, as well as the UK Statistics Authority. He also spent ten years leading information services at the UK House of Commons as Librarian of the House, and developing new services to enable the public to engage with Parliament.
Privacy-preserving data analysis has a long history, spanning at least five decades and numerous disciplines. Despite this extensive history, it is only in the last decade that an understanding has formed of the risk that the accumulation of small, seemingly insignificant statistical hints about confidential data, can pose to privacy. In this talk, Cynthia Dwork will discuss the implications of the loss of these ‘breadcrumbs’ of data, and show how differential privacy, a notion of privacy tailored to statistical analysis of large datasets, can control this risk, while at the same time permitting complex data analysis.
Biography
Cynthia Dwork, Distinguished Scientist at Microsoft Research, is renowned for placing privacy-preserving data analysis on a mathematically rigorous foundation. A cornerstone of this work is differential privacy, a strong privacy guarantee, frequently permitting highly accurate data analysis. Dr. Dwork has also made seminal contributions in cryptography and distributed computing, and is a recipient of the Edsger W. Dijkstra Prize, recognizing some of her earliest work establishing the pillars on which every fault-tolerant system has been built for decades. She is a member of the US National Academy of Sciences, the US National Academy of Engineering, and the American Philosophical Society, and a Fellow of the American Academy of Arts and Sciences. Dwork will join Harvard in January, 2017, as the Gordon McKay Professor of Computer Science at the Paulson School of Engineering and Applied Sciences, Radcliffe Alumnae Professor at the Radcliffe Institute for Advanced Study, and an Affiliated Faculty Member at Harvard Law School.

"
https://www.turing.ac.uk/research/research-projects/data-safe-havens-cloud,"
The data science research community frequently encounters a need for:
These requirements have, to their knowledge, not yet been realised simultaneously.  
Some solutions render research unproductive by making it hard to author new code while engaging with the data, or to experiment with the many software libraries realising new analytical techniques. Others lack access to significant GPU compute power.  
This perversely reduces security: researchers route around carefully constructed secure environments to avoid perceived productivity loss, reverting to 'folk security practices', (such as over-reliance on imperfect anonymisation), increasing the risk of a breach.  
An organisation then lacks a clear inventory of all the datasets it is handling, and the risk profile in terms of consequences and threat actors for each. Email- and document-based processes create a confused environment with costly reporting and audit.
This project is building a system with:

A cloud-based approach to secure infrastructure means every aspect of the system is defined by fully scripted configuration manifests, and these can be interrogated by any data provider to audit the system. This also provides scalable high performance computing.
The web-based management solution will provide clear reporting of data inventory across all tiers.
The framework developed in this project combines prevailing data security threat and risk profiles into five sensitivity tiers, and, at each tier, specifies recommended policies for data reclassification, data ingress, software ingress, data egress, user access, user device control, and analysis environments. The framework presents design patterns for security choices for each tier, and uses software defined infrastructure so that a different, independent, secure analysis environment can be instantiated for each project appropriate to its classification. The aim is to maximise researcher productivity and minimise risk, allowing research organisations to operate with confidence.

As well as this design research, the project is developing, and will publish, a reference implementation of the design based on Microsoft Azure.
One-page overview: Poster with overview of our data classification approach, security measures, data management and technical architecture. This is the best one-page high-level overview of our systems and process. Presented at the poster session at the second annual workshop of the Research Software London and South East community on 06 February 2020.
Two-minute video overview: Lightning talk about the Data Safe Haven. Presented at AI UK 2022.
In-depth overview: Slides and recording of an in-depth overview of the project (50 minutes). Presented at the Warwick Research Software Engineering in Data and AI Workshop on 15 February 2023.
Demonstration: An overview presentation that also demonstrates our data classification web application and using the Data Safe Haven as a researcher. Presented at the Software Sustainability Institute's Collaborations Workshop on 01 April 2020.
Design choices: Our preprint ""Design choices for productive, secure, data-intensive research at scale in the cloud"", outlining our policies, processes and design decisions for the Safe Haven. Pre-print initially submitted on arXiv on 23 August 2019, last revised on 15 September 2019.
Reference implementation: This is currently available under an open source BSD licence. If you would like to evaluate our implementation or collaborate in its development, take a look at the public GitHub repository containing the code and the documentation to deploy our reference implementation to Microsoft's Azure cloud (or contact James Robinson or Martin O'Reilly).
Community engagement: Along with The University of Dundee, we’ve co-established the RSE TRE community - bringing together TRE teams & people around the UK to collaborate on building & discussing TRE infrastructure. Get involved by joining the mailing list.
Current focus: We're currently working with the University of Dundee, UCL, Ulster University, Health Data Research UK and Research Data Scotland as part of the SATRE project to capture common principles and requirements for Trusted Research Environments (TREs) like our Data Safe Haven, as well as community accepted approaches for meeting these standards. As part of this work we will be validating our Data Safe Haven and two other open source TRE implementation against these standards, bringing each of them into closer alignment. This work is being funded by the DARE UK programme as one of its five Driver Projects. Check out our blog and keep and eye on our GitHub repository, where we will be publishing our outputs. If you'd like to get involved, please get in contact at [email protected]
"
https://www.turing.ac.uk/people/researchers/david-llewellyn-jones,"
David (he/him) is a Research Data Scientist at The Alan Turing Institute. In previous lives he's been a computer games programmer, a researcher in computer security, and most recently a software engineer developing a commercial Linux-based smartphone operating system.
David has a BA in Maths and Philosophy from the University of Oxford and a PhD in Mathematical Model Theory from the University of Birmingham. He's worked as a Reader at Liverpool John Moores University and a researcher at the University of Cambridge.
David is passionate about open source and open research. He's involved in open source software development and the open source community, especially the community around Sailfish OS, where he's been a long-time contributor to the Sailfish Community Newsletter.
Besides programming David enjoys playing computer games, but after 81 hours of playtime is still stuck on the last level of Celeste.
 
David is hoping to take the best practices learnt from working on commercial and open source software projects and apply them in a research context.
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/codecheck-challenge-showing-how-food-choices-affect-climate-change,"

Whether it’s microbeads, palm oil, hormonal active substances or too much sugar – with its free barcode scanner app, CodeCheck shows users detailed information about food and cosmetic products. So far it hasn’t been possible to give information on how emission-intensive a product might be and Turing Data Study Group researchers were challenged to design this new function. This would be added to the barcode scanner app as a new ratings feature, enabling consumers to make food choices based on carbon-footprint.
Vanessa Dilg, CodeCheck’s Head of Science and Scientific Product Innovation explains: “People today consume much more consciously and individually. They want to know if a product is ecological and healthy and if it might cause climate-damaging emissions. We want to be able to show them this information.”
Over the course of the Data Study Group, the team of researchers with expertise across mathematics, computer science and statistics, worked with CodeCheck’s dataset and developed an initial model that could be used to show specific carbon footprints for food products like pizza, cereals or juices.
Vanessa explains: “Together with the team, we found some approaches to estimate carbon footprints. A lot of information about the production chain is held by the manufacturer, so we struggled a bit with some missing data. But it was a good beginning because together we built a strong foundation for the CO2 function of CodeCheck’s app.”
Vanessa is optimistic about what data science can achieve: “We just need to find more and better data. Then we can expand the model and make more accurate comparisons.”
“I had an amazing experience here, really positive and productive. It was great to see the researchers in action""
Vanessa Dilg, Codecheck’s Head of Science and Scientific Product Innovation
Reflecting on the experience and the week’s main highlights, she says: “I had an amazing experience here, really positive and productive. It was great to see the researchers in action... I think these are important connections which enable new insights and opportunities for future collaboration between research and industry. I think there is a mutual benefit for us and for the researchers.”
Summing up she concludes: “In addition to the great facilitator and team-working, the coolest thing about this experience was watching the final presentation coming together incrementally. We had discussions about it before lunch and dinner each day, so I saw it all building up. It’s been an amazing experience and I was happy to be here.”
In terms of advice to anyone considering a collaboration with the Institute via Data Study Groups, Vanessa says: “In the beginning it takes a bit of work to organise the data and prepare, but it’s definitely worth the effort. It was an interesting week which gave us great ideas and new contacts. It’s also best to stay for the whole week.”
"
https://www.turing.ac.uk/research/research-areas/programming-languages/probabilistic-programming,"
"
https://www.turing.ac.uk/terms-acceptable-use,"

These terms set out the content standards that apply when you upload content to any of our sites, make contact with other users on any of our sites, link to any of our sites, or interact with any of our sites in any other way. 
Please read these terms carefully before using any of our sites.
Click here to view the terms.
"
https://www.turing.ac.uk/research/impact-stories/co-designing-computing,"
In a high-performance computing (HPC) environment, such as a data centre with hundreds or thousands of interconnected computers, well-designed algorithms and architectures allow huge data analysis tasks to be performed. For example, classifying millions of images of tissue samples to identify whether they contain anomalous features that should be examined by a doctor.
While these high-performance systems operate well for some computing needs, they often run at less than half their full capacity for many data science and machine learning tasks. Researchers at The Alan Turing Institute have been working in collaboration with Intel to co-design better architectures for their HPC systems. The collaboration has looked at how to improve communication between multiple machines that are sharing the workload of massive analyses, as well as how to rethink the formatting of the data used in HPC, to improve performance on data science and machine learning problems.
The output of the work is not only helping Intel improve their products and services, but also enabling data scientists to manage and analyse massive datasets with greater efficiency, in a range of machine learning applications.
The Institute established a relationship with Intel that built on existing links between Turing researchers and the technology giant. Peter Boyle, a Turing Fellow from the University of Edinburgh, explains: “I had been working with Intel on HPC workloads in Edinburgh for a while. The Turing’s engagement helped grow that relationship further, expanding the scope to include a broad spectrum of AI and machine learning workloads. It allowed Intel to place two of their engineers in Edinburgh to help work on these active co-design projects.”
Katrina Payne, Business Development Manager at the Turing, says, “The organisational relationship between Turing and Intel is about putting a framework in place that facilitates direct, personal working relationships between individuals in each organisation.”
“Co-design… is mutually beneficial. We get better science and Intel get a better product”
Peter Boyle, Turing Fellow
“The co-design process involves identifying the elements of computer architecture that are limiting performance,” Boyle continues, “then trying to ‘change the rules of the game’; seeing whether it makes engineering and economic sense to change the computer architecture. If it does, it’s mutually beneficial – we get better science out of the product and Intel get a better product to sell”
A key aspect of the work looked at how to make communication between machines in HPC environments more efficient. AI and machine learning problems often involve the use of multi-layered ‘neural networks’, which are trained to learn the mapping between inputs and outputs. Each individual ‘neuron’ or node in the network is given a set of parameters (or weightings) which are iteratively adjusted. These adjustments help to form smart ‘neural pathways’ that optimise the network’s ability to fulfil certain tasks, e.g. accurately translating a sentence from one language to another.
Despite recent advances in training methods, as well as in hardware and network architectures, training these neural networks with data can take an impractically long time on a single machine. Distributed training across multiple machines allows for significantly more efficient development of neural networks. “If you have a thousand devices training a neural net rather than just one, you can potentially turn a three-year job into a one-day job,” Boyle explains.
The most common form of distributed training is data parallelism, in which each machine gets a different portion of the input data, but a complete copy of the network, and then each machine’s results are subsequently combined. “Sometimes millions of weights parameterise a neural network, so combining results requires efficient network communication,” Boyle says.
Boyle and his collaborators at Intel started by taking an existing benchmark algorithm – published by the Chinese technology company Baidu – that aims to reduce the amount of time spent communicating between different cores in a computing network. They applied the algorithm to Intel’s HPC Omni-Path Architecture (OPA) and identified where the code wasn’t running efficiently. “With help from Intel engineers we managed to enable more cores to drive the network at the same time, improving bandwidth and resulting in a 10 times improvement in speed,” says Boyle. The code that this work produced is now shipped as standard in Intel products.
“As part of our work we considered: suppose we have the freedom to change the hardware to be whatever the heck we wanted,” says Boyle. “This led us to try different floating-point formats.” Floating-point formats are used to represent, with a fixed number of bits (binary digits), numbers of different orders of magnitude, made up of ‘mantissa’ and ‘exponent’ bits. For example, in the number 1.011 x 2101, the mantissa is 011 and the exponent is 101 (the first 1 is ignored as all numbers in standard floating-point format start 1.something, and the 2 shows we’re in base 2, otherwise known as binary).
In order to ensure accuracy when working with neural networks, a 32-bit format is often used, but Boyle and his colleagues explored whether they could use a 16-bit format instead. Karl Solchenbach, Director of Exascale Labs Europe at Intel, explains: “If you can do the same calculation with the same accuracy with 16 bits rather than the standard 32 bits, that’s great! It saves you half the memory, it makes the calculations much faster, and you can save silicon space in hardware.”
“[This work] saves you half the memory, makes calculations much faster…and saves silicon space”
Karl Solchenbach, Director Exascale Labs Europe at Intel
“We discovered that the standard 16-bit IEEE [Institute of Electrical and Electronics Engineers] floating-point format has a problem in that it only has 5 exponent bits and the range of data that can be represented with this is insufficient for a lot of machine learning problems,” says Boyle. “Using standard software libraries across multiple neural network benchmarks, we varied the number of mantissa and exponent bits to see the effects on performance.” They found that by changing the 16-bit format to 8 exponent bits and 7 mantissa bits, they were able to train neural networks that had previously failed to train with existing formats. A simple seeming solution with significant benefits.
One of the main takeaways from this work has been seeing the power of co-design and strong working relationships. Boyle says: “The relationship between scientists and engineers, like those at Intel, needs to be evidence-based, bottom-up and well-founded on an individual level.”
As well as the work described here, Intel has also been working with Turing Fellow Kenneth Heafield at the University of Edinburgh on training neural networks, and developing the related hardware, to be better at translating millions of words of online text.
“We’ve seen a positive impact on our architecture as a result of our work with the Turing”
Anil Rao, Vice-President, Data Center Group and General Manager of Data Center Security and System Architecture at Intel
On the future, Anil Rao, Vice-President, Data Center Group and General Manager of Data Center Security and System Architecture at Intel, says: “We’ve seen a positive impact on our architecture as a result of our work with the Turing and continue to work together to develop other similar successful projects.”

"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/satellite-applications-catapult,"
sa.catapult.org.uk
"
https://www.turing.ac.uk/people/turing-research-fellows/ruoyun-hui,"
Ruoyun is interested in using data and algorithmic tools to support civil society and the public sector. At the Turing Institute, she has been working with forensic scientists to develop statistical tools for interpreting the weight of evidence. She is also working on decision support tools to help detect organised exploitation and safeguard vulnerable individuals with other members of the Accountability, Inclusion and Human Rights team.
Before joining the Turing, Ruoyun worked as a population geneticist for the project “After the Plague: Health and History in Medieval Cambridge”. She is fascinated by the interplay between tangible formal systems, including data, algorithms and concepts, and our participation in the world.
"
https://www.turing.ac.uk/people/researchers/alejandro-beltran,"
Alejandro Beltran is a postdoctoral research associate within the public policy programme. He received his PhD from the University of Arizona where in his dissertation he identified the institutional and political determinants of corruption investigations in subnational audit agencies of Mexico. As a computational social scientist, he uses machine learning and natural language processing to generate quantitative measures of often unmeasurable phenomena from text in Spanish. Alejandro completed his undergraduate studies in Public Policy at the Universidad Autónoma de Sinaloa. Prior to joining the Alan Turing Institute, he was a postdoctoral researcher at the School of Government and Public Policy at the University of Arizona where he developed tools for tracking migrant caravans and gangs across Central America using local newspapers. He has also collaborated with the Inter-American Development Bank in building readily available data on subnational public finances for countries in Latin America.
Alejandro is interested in developing tools that can reduce corruption and improve performance of local governments.
"
https://www.turing.ac.uk/people/guest-speakers/gabor-csanyi,"
Gábor Csányi is an expert in atomistic simulation, particularly in multi-scale modelling that couples quantum mechanics to larger length scales. He is currently engaged in applying machine learning and other data intensive techniques to physics, chemistry and material science and in particular to the problem of deriving force fields (interatomic potentials) from ab initio electronic structure data.
Gábor is also interested in statistical problems in molecular dynamics, e.g. in enhanced sampling algorithms that can be used to explore the global configuration space of materials and molecules. He contributes to the running of the Lennard-Jones Center in Cambridge which brings together modellers who work on the atomic scale.
"
https://www.turing.ac.uk/people/external-researchers/marius-garmhausen,"
Dr Marius Garmhausen is a principal data scientist in Personalised Healthcare (PHC) Analytics at Roche, Switzerland, where he focuses on integrating multi-modal data and applying machine learning methods to create a more complete understanding of individual patients' health status and their disease. Prior to this role, he worked for Roche Diagnostics where he applied machine learning methods to a broad variety of challenges, including liquid biopsy gene panel design, prediction of clinical trial success, drug repurposing or patient response prediction in oncology. Previously, Marius was trained as a software developer in the telecommunications industry but decided to change direction and follow his passion for biology. He holds a diploma in molecular biology and obtained his PhD in computational biology from the University of Cologne in 2017. 
"
https://www.turing.ac.uk/people/researchers/angelo-cangelosi,"
Angelo Cangelosi is Professor of Machine Learning and Robotics at the University of Manchester. Previously he was Professor of Artificial Intelligence and Cognition, and founding director, at the Centre for Robotics and Neural Systems at Plymouth University. Cangelosi studied Psychology and Cognitive Science at the Universities of Rome La Sapienza and at the University of Genoa, and was visiting scholar at the University of California San Diego and the University of Southampton. Currently, he is the coordinator of the EU H2020 Marie Sklodowska-Curie European Industrial Doctorate “APRIL: Applications of Personal Robotics through Interaction and Learning” (2016-2019).
He is also Principal Investigator for the ongoing projects “THRIVE++” (US Air Force Office of Science and Research, 2014-2022), the H2020 project MoveCare, and the Marie Curie projects SECURE and DCOMM. He has secured over £30m of research grants as coordinator/PI. Cangelosi has produced more than 250 scientific publications. In 2012-13 he was Chair of the IEEE Technical Committee on Autonomous Mental Development. He has been Visiting Professor at Waseda University in Japan and at Sassari and Messina Universities in Italy. Cangelosi is Editor of the following journals “Interaction Studies” and “IET Cognitive Computation and Systems”, and in 2015 was Editor-in-Chief of IEEE Transactions on Autonomous Development. His latest book “Developmental Robotics: From Babies to Robots” (MIT Press; co-authored with Matt Schlesinger) was published in January 2015, and recently translated in Chinese and Japanese.
Cangelosi's main research interests are in artificial intelligence and cognitive robotics. He is one of the pioneers in the field of developmental robotics, with his main scientific work on neuro-robotic modelling of the grounding of language and of embodies cognition. Application areas include social robot companion for health and social care, and trust and acceptability in human-robot interaction.
"
https://www.turing.ac.uk/events/event-type/lecture,"
"
https://www.turing.ac.uk/people/researchers/huiling-tan,"
Huiling Tan is a programme leader at the MRC Brain Network Dynamics Unit at the University of Oxford. The goal of her research is to define how activity in large populations of neurons is coordinated in healthy brain functioning and how such coordination may go awry in diseases, translating this information to improved treatment for Parkinson’s Disease, Essential Tremor, and other disorders. 

To achieve this, her group takes a multidisciplinary approach, combining experimental manipulations in healthy subjects and patients with sophisticated signals analysis and modelling. The experimental manipulations include non-invasive brain stimulation, and often involve patients who have had deep brain stimulation electrodes implanted as treatment for problems with movement. Her group have made major contributions in understanding how abnormal interactions between brain cells cause slowness of movement, tremor and stiffness in people with Parkinson’s disease. At the same time, they have leveraged these insights to pioneer closed-loop approaches to therapeutic brain stimulation, and are exploiting local dynamics in deep brain nuclei as a basis for Brain-Computer Interfaces that control the environment for paralysed patients.
 
Dr Huiling Tan is interested in working with the Turing Institute and its network of Fellows to advance her research, particularly that using machine learning to improve the detection and control of brain activity dynamics for therapeutic benefit.
"
https://www.turing.ac.uk/people/researchers/chanuki-seresinhe,"
Chanuki Illushka Seresinhe is the Director of Data Science at Culture Trip, a global startup operating in experiences, travel, media and entertainment. Chanuki began working as a commercial data scientist at Channel 4 and then went on to lead the data science team at Popsa. Her research work at the University of Warwick and the Alan Turing Institute, which involves understanding how the aesthetics of the environment influences human wellbeing, has been featured in the press worldwide including the Economist, Wired, The Times, BBC, Spiegel Online, Guardian, Telegraph, Scientific American, Newsweek and MIT Technology Review. Chanuki has a PhD in Data Science from the University of Warwick.  Prior to pursuing a career in data science, Chanuki had a successful design career that included running her own digital design consultancy for over eight years.
Chanuki's research entails using big online datasets and deep learning to understand how the aesthetics of the environment influences human wellbeing. For example, how might we design our future cities to be conducive to our wellbeing?

"
https://www.turing.ac.uk/events/first-workshop-multimodal-ai,"
Multimodal AI combines multiple types of data (image, text, audio, etc.) via machine learning models and algorithms to achieve better performance. Multimodal AI is key for AI research and applications including healthcare, net zero, finance, robotics, and manufacturing. Multimodal AI in these areas is challenging due to the inherent complexity of data integration and the limited availability of labelled data. Unimodal AI for a single type of data input is maturing at an accelerating pace, thus creating vast opportunities for tackling multimodal AI challenges.
MultimodalAI’23 brings together researchers and practitioners from AI, data science, and various scientific and application domains to discuss challenges, share experiences, explore future directions, and build a vibrant community on multimodal AI.
 
We have three keynote speakers covering academic research, industrial research, and industrial applications: Professor Mirella Lapata (University of Edinburgh, UKRI Turing AI World-Leading Researcher Fellow), Dr Yutian Chen (Google DeepMind, AlphaGo Developer), and Dr Chew-Yean Yam (Microsoft, Principal Data and Applied Scientist).
 
Participants can give 3-min pitches and present posters, with four prizes (£150 each) to be won and funds to support travel costs.
https://multimodalai.github.io/
"
https://www.turing.ac.uk/research/research-areas/programming-languages/software-framework-development,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/british-heart-foundation,"
www.bhf.org.uk
"
https://www.turing.ac.uk/news/publications/bayesian-probabilistic-numerical-methods-industrial-process-monitoring,"
The use of high-power industrial equipment, such as large-scale mixing equipment or a hydrocyclone for separation of particles in liquid suspension, demands careful monitoring to ensure correct operation. The task of monitoring the liquid suspension can be posed as a time-evolving inverse problem and solved with Bayesian statistical methods.
In this paper, we extend Bayesian methods to incorporate statistical models for the error that is incurred in the numerical solution of the physical governing equations. This enables full uncertainty quantification within a principled computation-precision trade-off, in contrast to the over-confident inferences that are obtained when numerical error is ignored. The method is cast with a sequential Monte Carlo framework and an optimised implementation is provided in Python.
Oates CJ, Cockayne J, Aykroyd RG. Bayesian Probabilistic Numerical Methods for Industrial Process Monitoring.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/innovate-uk,"
www.gov.uk/government/organisations/innovate-uk
"
https://www.turing.ac.uk/people/business-team/lelle-demertzi,"
"
https://www.turing.ac.uk/people/arron-lacey,"
Arron holds a BSc in Physics, MSc and MRes in Computer Science and a PhD in Health Data Science at Swansea University. Arron has 10 years experience conducting Neurology and Cardiovascular research at the SAIL databank and more recently lead the Natural Language Processing group at Swansea University and became a Lecturer in 2019. Since 2014 Arron has been module lead on Machine Learning in Healthcare on the MSc Health Data Science course at Swansea University, and since 2019 two undergraduate modules on the BSc Population Health.
Arron is now an Honorary Lecturer at Swansea University after taking up the role of Senior Community Manager on the Early Detection of Neurodegenerative Diseases (EDoN) initiative lead by Alzheimer's Research UK. He hopes to use his experience in Machine Learning to help the EDoN initiative achieving it's goal of detecting cognitive decline 10-15 years before symptoms show in the brain, making current treatments more effective. EDoN aims to do this by building a digital toolkit consisting of mobile apps, Fitbit and cutting edge passive EEG monitoring to help detect signals of early cognitive decline.
Arron works in the Tools, Practices and Systems programme at the Turing and contributes to the Turing Way Project from within the Whitaker Lab.
"
https://www.turing.ac.uk/research/research-projects/diffpriv-enhancing-privacy-secure-computation,"
Usually when personal data is used by a company it is collected in a manner that allows the company to see it, unobscured. By adding noise to the data before the company sees it the amount of information leaked about a person can be provably bounded, however getting good bounds is hard. In this project, alternative security assumptions and cryptography are being used to reduce the information leakage for a given noise level.
In this project the primary notion of privacy is that of 'differential privacy'. To understand this notion imagine you are a data provider with a choice to tell the truth to a data collector or to fabricate a (presumably more flattering) lie. A mechanism for data collection is said to be differentially private if your choice of whether or not to lie doesn't significantly change the probability of the collector seeing any particular outcome. The collector thus can't infer what the data you as an individual provided.
Whilst this guarantee intrinsically requires uncertainty to be added, with a sufficient number of providers it is possible to obtain accurate statistics for the population.
It seems intuitive that if you provide your data anonymously that will be helpful to keeping your data private. Whilst anonymity alone can lead to substantial loss of personal information, it turns out that it can significantly enhance the privacy guarantees provided so long as it is combined with at least a little randomization.
 
This project hopes to produce new algorithms for computing statistics of interest. This project will be a success if the algorithms produced compute useful statistics accurately whilst providing a stronger privacy guarantee and/or requiring substantially less input data than the previous state of the art for that statistic. It will also be strongly preferable that the algorithms are practical enough to be deployable in the real world.
Such algorithms will allow privacy-preserving practices based on cryptography to be deployed not just for products with millions of users but also for products with thousands of users. Thus they would be a step towards allowing privacy-preserving data collection/release techniques to become a standard in industry.
 
Privacy-preserving efficient data collection could be deployed in an array of different scenarios. Multiple hospitals could run statistical analyses on their combined datasets, without endangering patient privacy. Better guarantees could be provided for the safety of releasing census data.
The type of application most appropriate to the work of this project is that of a company with an application that they wish to update on the basis of how users interact with it. The ultimate goal of this line of research would be to allow most of these applications to be updated without anyone having their data being thoroughly exposed (i.e. whilst being provided differential privacy).
 
"
https://www.turing.ac.uk/people/external-researchers/victor-ok-li,"
"
https://www.turing.ac.uk/research/research-projects/project-explain,"
AI and machine learning technologies are helping people do remarkable things. From assisting doctors in the early detection of diseases and supporting scientists who are wrestling with climate change to bringing together diverse groups from around the globe through real-time speech-to-speech translation, AI systems are enabling humans to successfully confront an ever-widening range of societal challenges.
This progress has, however, brought with it a new set of difficulties. Many machine learning applications, such as those in natural language processing and computer vision, complete their assigned tasks by identifying subtle patterns in large datasets. These systems accomplish this by linking together many hundreds, thousands—or sometimes even millions—of data points at a time. Humans don’t think this way and because of this have difficulty understanding and explaining how these sorts of AI systems reach their results.
This gap in AI explainability becomes crucial when the outcomes of AI-assisted decisions have a significant impact on affected individuals and their communities. If an AI system is opaque then there is no way to ensure that its data processing is robust, reliable, and safe. Similarly, in cases where social or demographic data are being used as inputs in AI decision-support systems—for instance, in domains such as criminal justice, social care, or job recruitment—the employment of ‘black box’ models leaves designers and deployers no way to properly safeguard against possibilities of lurking biases that may produce inequitable or discriminatory results.
To respond to these challenges and gaps in AI explainability, the best practice document, Explaining Decisions Made with AI. Since its publication, the guidance has served as an essential reference point for public and private sector organisations around the world who are trying to navigate the complicated terrain of AI explainability.
Increasingly, organisations are using AI to help them make decisions. Where they are processing personal data to do this, they have to comply with certain parts of the General Data Protection Regulation. Moreover, where their AI-assisted decisions raise possibilities of discrimination against protected characteristics such as age, disability or race, organisations must comply with the 2010 Equality Act.
But beyond this, an organisation’s capacity to explain its AI-assisted decisions to those affected by them builds trust among the public. It also improves the transparency and accountability of internal governance processes by having an informed workforce that can then maintain oversight of what these systems do and why. Society benefits too, as the priority of designing explainable AI models can improve their reliability, safety, and robustness. It can also help surface the existence of potential issues of bias within these AI systems and in the data they use, which can then be addressed and possibly mitigated.
Project ExplAIn is a collaboration between the Information Commissioner's Office and The Alan Turing Institute to provide guidance to organisations on the key principles, concepts, and tools that can help provide explanations in practice. In the second phase of the project Manchester Metropolitan University was a key collaborator and helped to produce workbooks for organisations to help communicate this guidance.
The project underpinning this work, Project ExplAIn, came about as a result of Professor Dame Wendy Hall and Jérôme Pesenti’s 2017 independent review on growing the AI industry in the UK. This was followed in 2018 by the Government’s AI Sector Deal, which tasked the ICO and the Turing to “…work together to develop guidance to assist in explaining AI decisions.”
In February 2019, two five-day-long citizens’ juries on AI explanation were staged in Coventry and Manchester. These were designed to elicit public preferences about what people expect from explanations of AI-assisted decisions. The juries used a deliberative format with the assistance of expert witnesses, who provided jurors with background information about the technical, legal and ethical dimensions of AI explainability. The juries were followed by three roundtables, where the feedback from the citizens were presented to and then discussed by a range of academic and industry stakeholders, from data scientists and researchers to data protection officers, C-suite executives and lawyers. The results of these public engagement activities as well as extensive desk research have provided the basis for the guidance.
Wherever organisations use personal data to make AI-assisted decisions, they should be able to explain those decisions to the people affected by them. The guidance we have produced provides an accessible overview of the key principles, concepts and tools that can help organisations provide explanations in practice.
At the heart of the guidance is a series of related questions: What makes for a good explanation of decisions supported by AI systems? How can such explanations be reliably extracted and made understandable to a non-technical audience? How should organisations go about providing meaningful explanations of the AI-supported decisions they make? What do the people affected by these decisions deserve, desire and need to know?
The main focus of the guidance is the need to tailor explanations to the context in which AI systems are used for decision-making. This vital contextual aspect includes the domain or sector in which an organisation operates, and the individual circumstances of the person receiving the decision.
The guidance also stresses a principles-based approach to the governance of AI explanations. We present four principles of explainability that provide ethical underpinnings for the guidance and that steer the practical recommendations contained in it:
Building off these principles, we identify a number of different explanation types, which cover various facets of an explanation, and will often be used in concert with each other:
For organisations, the emphasis is on how to set up and govern the use of AI systems to be suitably transparent and accountable, and that they prioritise, where appropriate, using inherently explainable AI models before choosing less interpretable models, such as ‘black box’ systems. We outline the art of the possible in these considerations, to help the governance and technical teams in organisations think about how to extract explanations from their AI systems.
When delivering an explanation to the individual affected, there are a number of contextual factors that will inform what they should be told first, and what information to make available separately. We call this ‘layering’ explanations, which is designed to avoid information overload. These contextual factors are:
 
The Alan Turing Institute and the Information Commissioner's Office collaborated to produce guidance for organisations; and The Alan Turing Institute subsequently collaborated with Manchester Metropolitan University to produce workbooks to communicate the guidance to organisations through case studies.
At the beginning of 2021, the project team assembled two workbooks to help support the uptake of the guidance. The goal of the workbooks was to summarise the main themes from Explaining Decisions Made with AI in a non-technical way. Additionally, each workbook served as the basis of a workshop exercise built around one of two use cases, created to help organisations and individuals gain a flavour of how to put the guidance into practice.
The workbooks were written to support the second phase of Project ExplAIn, centred on stakeholder outreach and practice-based evaluation. This included a series of engagement activities held in January 2021 to assess the usability, accessibility and clarity of the guidance, as well as the readiness levels of organisations to put explainable AI principles into practice. In partnership with Manchester Metropolitan University (MMU) and the ICO, two workshops – one with SMEs from advertising, AI development, finance, recruitment, health, education, fraud protection, media and insurance sectors, and a second with public sector organisations – were held virtually. The workshops engaged participants from a variety of different backgrounds, levels of seniority, and roles across the public and private sectors. We are extremely grateful to them for their energy, enthusiasm, and tremendous insight.
We hope that our workbooks will allow for more widespread use and dissemination of the guidance. The workbooks begin with a truncated form of the Explaining Decisions Made with AI guidance, presenting the four principles of AI explainability, the basics of an explanation-aware approach to AI innovation, and the practical tasks needed for the explanation-aware design, development and use of AI systems. They then provide some reflection questions, which are intended to be a launching pad for group discussion. The appendices of the workbooks are primarily focused on both the workshop setting and the case studies. Appendix A provides a structure for how to use the workbook in a workshop setting, including details on necessary resources, personnel, and recommended timelines. These recommendations are based on the workshops co-hosted with ICO and MMU in January 2021. Appendix B contains the case study, followed by appendix C which consists of a checklist for one or more of the explanation types to be used in tandem with the case study.
Case studies found in the workbooks:
These workbooks would simply not exist without the commitment and keenness of all our collaborators and workshop participants, and we would like to thank them again for their involvement.
 

This infographic video produced by Fable Studios consists of an introduction to guidance on Explaining Decisions Made with AI. The video provides basic information about the importance of explainable AI. It includes an introduction to the four principles of AI explainability and a description of the six explanation types which are meant to assist organisations with delivering understandable explanations to relevant stakeholders. The purpose of the video is to provide an accessible entry point to the guidance and to direct towards the complete version of the guidance, to learn more about how to implement it in practice. 
Read guidance
 
The Explaining Decisions Made with AI guidance was released in May 2020. Since its publication, the guidance has served as an essential reference point for public and private sector organisations around the world who are trying to navigate the complicated terrain of AI explainability.
In mid-2022, to assess the guidance’s usability and uptake, our team hosted a series of workshops and a mini public - a diverse group of people brought together to deliberate on a topic and inform decision-making — on the content of the guidance — hosted in Bristol in collaboration with Traverse, a former public engagement company.
​The aim of the mini public was to gather views and opinions on the guidance from a diverse and inclusive sample of people living in the UK. 40 participants were recruited to partake in our Bristol mini public, with 31 participants attending all four days. We prioritised diversity from the outset of the engagement design to ensure that an optimally inclusive range of voices were present in the room. In the final sample of 31 participants, 19 self-identified as female, 13 self-identified as members of an ethnic minority group, 7 were providers of unpaid care, and 17 had long-term health problems or disabilities. We believe the diversity of this set of participants contributed greatly to a rich dialogue surrounding possible improvements to the guidance itself.
While there was a vast array of interesting findings, key themes from the mini public include the following:
More about these themes can be read on our associated blog post.
Find out more
 
This work was supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/W006022/1, particularly the “Criminal Justice System” theme within that grant & The Alan Turing Institute.
"
https://www.turing.ac.uk/news/uk-universities-receive-first-ever-turing-network-development-awards,"
Universities across the length and breadth of the UK have been named among the first-ever successful applicants to The Alan Turing Institute’s Network Development Awards.
In all, the innovative awards have been made to 24* UK universities including institutions from Scotland, Wales, Northern Ireland and England.
Each of the outstanding universities demonstrated its own particular proven research excellence and a track record of translation in data science, AI, or a related field. The work of the successful award recipients will now be significantly enhanced through active involvement with the Institute’s thriving network.
Adrian Smith, Institute Director, said
“The awards reflect the demand across a range of sectors to work with the Institute. Data science and AI doesn’t stand still, and so we look forward to working together with this network of universities; exploring new ways to grow the UK’s dynamic research and innovation landscape.”

Professor Gladden, who is currently Executive Chair of the Engineering and Physical Sciences Research Council (EPSRC), said
“UKRI’s ambition is to bring together expertise in both academia and business to retain and extend the UK’s reputation and leadership in AI. The Turing Network Development Awards are an important first step in networking UKRI investments and developing wider partnerships across academia and business with The Alan Turing Institute.”
As the national institute for data science and AI, the Turing already has a well-established network of university partners. However, these awards enable the Institute to extend its reach. The group of successful universities will be awarded up to £25,000, to: 
""The University of Glasgow is delighted to receive this award, led by Professor Ana Basiri, providing opportunities for closer interaction with The Alan Turing Institute and its wider network. This reflects the importance we place on ethical and impactful Data Science and AI as critical enablers for both curiosity-driven and challenge-oriented research and innovation."" – Chris Pearce, Vice Principal for Research, University of Glasgow 
‘AI is central to the University’s Strategy 2030 ambitions in secure connected intelligence. This award helps to underpin our ambitions to work collaboratively with humanities and medicine to provide a cohesive, inclusive community and a ‘one health’ vision by working more closely with The Alan Turing Institute.’ – Roger Woods, Dean of Research, School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast
""We are delighted to be networking with The Alan Turing Institute through this award. It is very timely indeed for Wales, enabling the Wales Data Nation Accelerator consortium, including partnership with the universities of Aberystwyth, Bangor, and Swansea, to further collaborate on the innovation and research agenda, alongside support from Welsh Government and other organisations."" - Roger Whitaker, Professor of Collective Intelligence, School of Computer Science and Informatics, Cardiff University 
“Receiving this award is a fantastic achievement and a credit to all colleagues involved in the high-quality research at Northumbria University. We look forward to collaborating with world-class researchers at The Alan Turing Institute and its partners to advance AI and data science research and apply it to real-world problems.” – Edmond Ho, Associate Professor, Computer and Information Sciences, Northumbria University
The full list of successful applicants is below:
The Alan Turing Institute is the UK’s national institute for data science and artificial intelligence.
The Institute is named in honour of Alan Turing, whose pioneering work in theoretical and applied mathematics, engineering and computing is considered to have laid the foundations for modern-day data science and artificial intelligence. The Institute’s goals are to undertake world-class research in data science and artificial intelligence, apply its research to real-world problems, driving economic impact and societal good, lead the training of a new generation of scientists, and shape the public conversation around data and algorithms.
* Since the initial announcement, LSE has also been named as a TNDA recipient 
"
https://www.turing.ac.uk/people/business-team/katy-henderson,"
"
https://www.turing.ac.uk/people/guest-speakers/jon-saltmarsh,"
Jon is Deputy Director, Engineering and Research, within the Science and Innovation for Climate and Energy directorate, in the UK government’s department for Business, Energy and Industrial Strategy.  He is responsible for engineering studies, technical analysis and research that the department undertakes to provide robust technical evidence and advice to underpin policies on reducing energy demand and decarbonising energy supply.  This work covers low carbon generation (e.g. wind, carbon capture, utilisation and storage, solar); energy efficiency (in buildings and industry); fuel switching (e.g. electrification, hydrogen); and integration of solutions (e.g. smart technologies). He leads the disruptive technology theme within the £1 billion net zero innovation portfolio. Jon is also responsible for the UK’s input into international innovation initiatives including Mission Innovation, where he co-leads the innovation community on affordable heating and cooling of buildings.
Prior to joining BEIS, he worked in the private sector on a range of cutting-edge defence technology programmes including solar powered unmanned aircraft, automated landing systems and simulation-based training. 
Jon holds a degree in engineering from St John’s College, University of Cambridge and an MBA from Cranfield University.
"
https://www.turing.ac.uk/events/sandpit-data-science-education,"
The Sandpit will offer participants the opportunity to network, share best practices, and explore ongoing challenges related to data science education.
Following the sandpit, individuals and groups will have the opportunity to bid for pilot research support to be undertaken by Newcastle’s Research Software Engineering (RSE) team.
Through the sandpit we will explore themes including widening participation, EDI, professionalisation of data science, and data science across disciplinary boundaries.
The primary aim of this sandpit event is to elicit ideas for short, implementation-focused projects in the area of data science education and will also provide a focal point for the growing data science education community with the Turing Network and the region.
 
Speakers confirmed:
We anticipate funding up to five projects. Typical project awards will be £5,000-10,000 and funding can be used for Newcastle RSE time or cloud computing costs.  
There is also a bursary for travel and hotel costs - please email [email protected] for more information.
We welcome attendees from a diverse range of backgrounds, with an interest in training and teaching of data science at all levels.
Registration link: Data Science Education Sandpit
"
https://www.turing.ac.uk/people/researchers/trevor-martin,"
Trevor Martin is a Professor of Artificial Intelligence at the University of Bristol, and a BT Senior Research Fellow, working with the Applied Research Cyber Defence Team. His research covers soft computing in artificial intelligence applied to areas such as security analytics, extraction and integration of semi-structured information, and soft concept hierarchies. He is a Chartered Engineer and member of the BCS and IEEE, and is named inventor on 15 patents.
 
"
https://www.turing.ac.uk/research/research-projects/environment-and-sustainability,"
The effects of climate and environmental change constitute some of the most important challenges facing our society and quality, accessible, and reliable information across multiple scales, from global to molecular, is needed to understand the scale of these challenges and to develop strategies for mitigation and adaption.
Developing methods to provide the meaningful insight to inform decision-making, improve risk management and enhance our resilience to climate change will require working across disciplines, bringing together methodology and expertise from different fields to develop tools and computational frameworks that can integrate data from multiple sources, available at different spatial and temporal resolutions and with different biases and uncertainties.
The resulting frameworks will be applicable across a wide variety of applications, including integrating data from remote sensing satellites and traditional methods for ground observation with that from new sensor technologies; and integrating; integrating crop models, measurement data and climate projections to develop strategies for future agriculture; and understanding the threedimensional structure and dynamics of molecules.
This project is supported entirely by public funds, through Wave 1 of the UK Research and Innovation Strategic Priorities Fund, under EPSRC Grant EP/T001569/1.
 
The programme has a number of aims including:
The programme combines inter-disciplinary expertise, covering a variety of areas including data science, AI, agriculture, environmental science, policy, meteorology, and ecology, from across the Turing community and external partners. Partners in this work include British Antarctic Survey, Earlham Institute, Norwich Bioscience Park (John Innes Centre), Rothamsted Research, The Met Office & Joint Centre for Excellence in Environmental Intelligence, University of Cambridge, MRC Laboratory of Molecular Biology and The Science and Technology Facilities Council.
The environment and sustainability programme currently consists of three projects:
Work in the environment and sustainability area addresses one of the defining crises of our time: the climate emergency. Developing robust and complex models integrating multiple datasets, real-time monitoring and data sources will enable enhanced decision making across a suit of policy areas. Sophisticated molecular modelling techniques will unlock more advanced and sustainable materials. Researchers will also be developing a toolbox of generalisable tools that will open the advances made to other areas of research, compounding the long-term potential impact of the work.
"
https://www.turing.ac.uk/research/publications/understanding-artificial-intelligence-ethics-and-safety,"
A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms.
Innovations in AI are already leaving a mark on government, by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. 
The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur. 
In order to manage these impacts responsibly and to direct the development of AI systems toward optimal public benefit, The Alan Turing Institute's public policy programme partnered with the Office for Artificial Intelligence and the Government Digital Service to produce guidance on the responsible design and implementation of AI systems in the public sector. 
The guide, Understanding Artificial Intelligence Ethics and Safety, is the most comprehensive guidance on the topic of AI ethics and safety in the public sector to date. It identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. The guide stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems.
The guidance is relevant to everyone involved in the design, production, and deployment of a public sector AI project: from data scientists and data engineers to domain experts, delivery managers and departmental leads. Our aim -- and hope -- in writing the guide is to encourage civil servants interested in conducting AI projects to make considerations of AI ethics and safety a first priority. 
Leslie, D. (2019). Understanding artificial intelligence ethics and safety: A guide for the responsible design and implementation of AI systems in the public sector. The Alan Turing Institute. https://doi.org/10.5281/zenodo.3240529
5.4 MB
"
https://www.turing.ac.uk/news/publications/set-based-state-estimation-probabilistic-consistency-guarantee-under-epistemic,"
Consistent state estimation is challenging, especially under the epistemic uncertainties arising from learned (nonlinear) dynamic and observation models. In this work, we propose a set-based estimation algorithm, named Gaussian Process Zonotopic Kalman Filter (GP-ZKF), that produces zonotopic state estimates while respecting both the epistemic uncertainties in the learned models and aleatoric uncertainties. Our method guarantees probabilistic consistency, in the sense that the true states are bounded by sets (zonotopes) across all time steps, with high probability. We formally relate GP-ZKF with the corresponding stochastic approach, GP-EKF, in the case of learned (nonlinear) models. In particular, when linearization errors and aleatoric uncertainties are omitted and epistemic uncertainties are simplified, GP-ZKF reduces to GP-EKF. We empirically demonstrate our method’s efficacy in both a simulated pendulum domain and a real-world robot-assisted dressing domain, where GP-ZKF produced more consistent and less conservative set based estimates than all baseline stochastic methods.
5.3 MB
"
https://www.turing.ac.uk/research/research-areas/social-data-science/cognitive-science,"
"
https://www.turing.ac.uk/people/former-enrichment-students/marina-riabiz,"
Marina Riabiz received her undergraduate and master’s degree in Mathematical Engineering from Politecnico di Milano, Italy, specialising in Applied Statistics.  She completed her PhD in the Signal Processing Group, Information Engineering, at the University of Cambridge, UK, with a thesis  ""On Latent Variable Models for Bayesian Inference with Stable Distributions and Processes"", supervised by Prof. Simon Godsill. 
In October 2018, she joined the Cardiac Electro-Mechanics Research Group (CEMRG) at King's College London, where she works with Prof. Steven Niederer on uncertainty quantification for myocyte models. 
Marina is developing Monte Carlo methods for Bayesian high-dimensional parameter inference in dynamical systems describing calcium transients and their contribution to the action potential. The project is in collaboration with Prof. Chris Oates and other researchers from the Alan Turing Institute. 
Marina’s broader research interests are at the intersection of probabilistic machine learning, computational statistics and uncertainty quantification, applied to engineering and medical sciences. 
"
https://www.turing.ac.uk/news/filed-under/opinion,"
"
https://www.turing.ac.uk/people/researchers/maria-wolters,"
Maria's research focuses on supporting people with long-term conditions live rich and meaningful lives. She has a background in computational linguistics and speech science (PhD, 2000, University of Bonn), human-computer interaction, assistive technology, and eHealth, and a long-term interest in statistics.
Maria is interested in why people collect health data. In particular, missing data is often missing for a reason. This could be because the smartphone is out of battery, or because one has no energy left to complete a diary entry. She wanta to find patterns of missing data and describe them qualitatively and quantitatively in a way that leads to new insight - in other words, treating missingness as information.
"
https://www.turing.ac.uk/people/enrichment-students/adam-o-conghaile,"
Adam is a PhD student in algorithms and descriptive complexity theory at the University of Cambridge. His research focuses on the use of logic, algebra and topology in efficient algorithms for constraint satisfaction and graph isomorphism problems. His current work at the Turing seeks to establish upper bounds on the expressive power obtained by computing topological features of data. 
"
https://www.turing.ac.uk/people/enrichment-students/yumeng-zhang,"
Yumeng is a PhD student in the Communication and Media department at Loughborough University, under the supervision of Dr Kathryn Brown and Professor Craig Richardson. She holds an MA in Arts Management and an undergraduate degree in Computer Information Systems. Her interdisciplinary background in art and computing has guided her toward her current PhD research in Artificial Intelligence and Museums. Yumeng was awarded a scholarship from the China Scholarship Council for her PhD research. Recently, she is working as a research assistant on a British Academy funded art history project led by Dr Kathryn Brown, “Sensing the Surface: A Computer Vision Analysis of Claude Monet’s Water Lily Paintings”. She has a keen interest in machine learning, natural language processing, museums and digital art.
Yumeng is currently working on her PhD research on 'Artificial Intelligence and the Museum: Opportunities, Trends, and Tensions', where she is studying the impact and changes that the combination of art museums and artificial intelligence will bring to museums. She is researching how the combination of art museums and artificial intelligence will impact and change museums. Her research interests focus on the application and development of AI in curation, collection conservation and visitor services, as well as AI art and technology utopias.
During her time at The Alan Turing Institute, she is looking forward to gaining inspiration in the fields of art and artificial intelligence, as well as having exciting collaborations.
"
https://www.turing.ac.uk/people/external-researchers/amelia-defalco,"
"
https://www.turing.ac.uk/people/researchers/neil-lawrence,"
Neil Lawrence is the DeepMind Professor of Machine Learning at the University of Cambridge in the Department of Computer Science and Technology. He is co-host of ""Talking Machines"" podcast and a Visiting Professor at the University of Sheffield.
He received his bachelor's degree in Mechanical Engineering from the University of Southampton in 1994. Following a period as a field engineer on oil rigs in the North Sea he returned to academia to complete his PhD in 2000 at the Computer Lab in Cambridge University. He spent a year at Microsoft Research in Cambridge before leaving to take up a Lectureship at the University of Sheffield, where he was subsequently appointed Senior Lecturer in 2005. In January 2007 he took up a post as a Senior Research Fellow at the School of Computer Science in the University of Manchester where he worked in the Machine Learning and Optimisation research group. In August 2010 he returned to Sheffield to take up a collaborative Chair in Neuroscience and Computer Science. From 2016 to 2019 he was Director of Machine Learning at Amazon where he worked on deploying machine learning solutions for Prime Air, Alexa and in the Amazon supply chain.
Machine learning technologies are being deployed as independent components in a wider system. This means that the immediate effect of changing one component is not immediately understood. To compound the problem, there is currently no standardised way to declare that a machine learning model has been used. This means that in a large system the downstream consequences of a change may not be understood. 
Most of these systems today are deployed using ""Service Oriented Architecture"" for maintaining the large complex system. Professor Lawrence's interest is in going beyond this to view the production of data in the system as the service itself, so-called ""data as a service"". The result is a new approach to complex software systems design known as ""data oriented architecture"".
Data oriented architectures allow for a more holistic approach to machine learning system design, including notions of ""progression testing"" and ""hypervision"". This allows for the monitoring of the complex system after deployment, ensuring that any unforeseen challenges can be quickly identified and rectified.
Neil was Associate Editor in Chief for IEEE Transactions on Pattern Analysis and Machine Intelligence (from 2011-2013) and is an Action Editor for the Journal of Machine Learning Research. He was the founding editor of the Proceedings of Machine Learning Research (2006) and is currently series editor. He was an area chair for the NIPS conference in 2005, 2006, 2012 and 2013, Workshops Chair in 2010 and Tutorials Chair in 2013. He was General Chair of AISTATS in 2010 and AISTATS Programme Chair in 2012. He was Program Chair of NIPS in 2014 and was General Chair for 2015. He is one of the founders of the Gaussian Process Summer Schoo, the DALI Meeting and Data Science Africa and is a member of the UK's AI Council.
"
https://www.turing.ac.uk/people/researchers/sandra-wachter,"
Dr. Sandra Wachter is a Researcher in Data Ethics and Algorithms at the Oxford Internet Institute, she is a member of the Ethics and Philosophy of Information research cluster and the Digital Ethics Lab. Sandra is also a Turing Research Fellow at the Alan Turing Institute in London. Her research focuses on the legal and ethical implications of Big Data, AI, and robotics as well as governmental surveillance, predictive policing, and human rights online.
Prior to joining the OII, Sandra worked the Royal Academy of Engineering on topics such as connectivity, AI, and autonomous systems. Sandra holds a Master’s and PhD in law specialising on European, International, and human rights law as well as technology and data protection law. In her PhD, she explored the concept of democracy according to the European Court of Human Rights and tested whether democracy is compatible with mass surveillance methods such as the European Data Retention Directive. Sandra also holds a Master’s of Science from the Oxford Internet Institute. Her thesis looked at tensions between freedom of speech and the right to privacy on social networks.
Sandra's immediate research focuses on ethical design of algorithms, including the development of standards and methods to ensure fairness, accountability, transparency, interpretability, and group privacy in complex algorithmic systems. Sandra’s research also addresses legal and ethical aspects of robotics (e.g. surgical, domestic and social robots) and autonomous systems (e.g. autonomous and connected cars), including liability, accountability, and privacy issues as well as international policies and regulatory responses to the social and ethical consequences of automation.
Research interests: Data Ethics; Big Data; AI; machine learning; algorithms; robotics; privacy; data protection and technology law, European, -International-, and human rights law, governmental algorithmic surveillance, and emotion detection; predictive policing .
"
https://www.turing.ac.uk/people/programme-committee/adrian-smith,"
Adrian Smith joined The Alan Turing Institute as Institute Director and Chief Executive in September 2018. In November 2020, he became President of the Royal Society, in addition to his leadership of the Turing. He is also a member of the government's AI Council, which helps boost AI growth in the UK and promote its adoption and ethical use in businesses and organisations across the country. 
Professor Smith’s previous role was Vice-Chancellor of the University of London where he was in post from 2012. He was Director General, Knowledge and Innovation in BIS (now BEIS) between 2008-2012. He has worked with the UK Higher Education Funding and Research Councils and was Deputy Chair of the UK Statistics Authority from 2012—2019. In 2014, he was appointed Chair of the Board of the Diamond Synchrotron at Harwell and in 2018, a board member of the UK Atomic Energy Authority.
Professor Smith is a past President of the Royal Statistical Society and was elected a Fellow of the Royal Society in 2001 in recognition of his contribution to statistics.   
In 2003-04 Professor Smith undertook an inquiry into Post-14 Mathematics Education for the UK Secretary of State for Education and Skills and in 2017, on behalf of Her Majesty’s Treasury and the Department for Education, published a 16-18 Maths Review.
In 2006 he completed a report for the UK Home Secretary on the issue of public trust in Crime Statistics. He received a knighthood in the 2011 New Year Honours list.
"
https://www.turing.ac.uk/research/research-areas/theoretical-mathematics,"
"
https://www.turing.ac.uk/people/researchers/peter-boyle,"
"
https://www.turing.ac.uk/events/turing-lectures-how-speak-whale,"
In 2015, wildlife filmmaker Tom Mustill was almost crushed by a humpback whale while kayaking in California. Fascinated by these otherworldly creatures, he began to investigate how researchers are using AI, undersea robots, and animal spying devices to do something incredible: decipher whale song.
Tom will break down what it will take to speak to a whale, in a whistlestop tour covering the biology and lives of whales, new discoveries about their communications, what AI is and how it can help, and updates from the latest research expeditions.
The event will feature videos from the backs of whales, recordings from the deep sea, pieces of real whales and dolphins, and the chance to feel the vibrations of the biggest voices on the planet through your entire body.
It’ll be an afternoon whale-spent for the whole family!
This event is organised in collaboration with the Royal Institution of Great Britain.
Photo credit: Tom Mustill
 
In our first Turing Lecture for everyone aged 11+, wildlife filmmaker Tom Mustill reveals how a close encounter with a humpback whale inspired him to find out whether we could ever talk to these ocean giants.
We invite you to submerge yourself in Tom’s underwater world, hear about the amazing advances scientists are making in decoding animal communication, and be rumbled by the voices of our deep-sea orca-stra! He'll be joined by Michael Bronstein, DeepMind Professor of AI at the University of Oxford, to explain how the development of new machine learning tools are making waves in the field.
Featuring lively pyrotechnic demonstrations exploring how whale voices travel, an opportunity for audience members to get involved and try to speak like a whale, and awe-inspiring footage from the bodies of whales and dolphins, join us as we take a deep dive into their world under the sea.

This event is organised in collaboration with the Royal Institution of Great Britain.
The Royal Institution
The Royal Institution is an organisation dedicated to scientific education and research. Located in Piccadilly, it was founded in 1799 to ""spread knowledge and facilitate general access to mechanical and useful inventions; to teach, through courses composed of philosophical conferences and experiments, the application of science to ordinary life.""
Please note that registration for this event is managed by the Royal Institution and tickets to attend this event in-person start from £10. An access fund is available for anyone who would otherwise be unable to attend. Please contact [email protected] to enquire.
You can attend virtually free of charge, please click the 'register now' button to attend.
This series of Turing Lectures is organised in collaboration with The Royal Institution of Great Britain.

The Royal Institution of Great Britain
21 Albemarle Street
London
W1S 4BS
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/ploras-challenge-predicting-language-outcome-and-recovery-after-stroke,"

Globally, stroke is the second most common cause of death, and the third most common cause of disability. More people in the UK are surviving after experiencing a stroke, but many leave hospital with complex difficulties with communicating and understanding language, both written and spoken.
The Predicting Language Outcome and Recovery After Stroke (PLORAS) project funded by Wellcome and based at the UCL Institute of Neurology is a database of post-stroke structural MRI scans, and demographic and behavioural data. It aims to empower stroke survivors and clinicians with more and better information about a patient’s likely course of recovery. Using the demographic data and scans that the PLORAS team provided, Turing Data Study Group researchers were challenged to develop a clinical tool that could accurately predict whether and how quickly individual patients might recover language skills impaired after stroke. A breakthrough in this domain would allow clinicians to create better personalised treatment plans for each survivor and gain deeper scientific understanding of how language works in the brain and how the brain adapts to damage.
Dr Tom Hope explains: “Someone suffers a stroke roughly every three minutes in the UK alone, and a lot of stroke survivors suffer language difficulties, which lead to social isolation and depression and make it difficult to return to work. These patients want to know whether and when they might hope to recover, but current medicine cannot answer those questions. This is a problem both for the patients and for the health system itself, because we currently have no way to predict what ongoing care or support people will need. And because we don’t know which patients will or can recover spontaneously (without much therapy), we also cannot test therapeutic interventions properly. To start to answer these questions, you’ve got to collect the right data, and that’s what the PLORAS project is doing.”
A diverse team of 15 researchers with backgrounds in quantitative science, machine learning, glaciology, medical imaging, chemistry, biology, data science, computer science and neuroscience worked intensively on the challenge over the five days. To Tom’s great satisfaction they were able to deliver promising results with the potential to lead to some important clinical and scientific breakthroughs. Reflecting back on the week Tom says:
“I’ve been researching this area since 2011 and was sceptical that in a week they would be able to get to something I haven’t already tried. I also have a background in AI, so I know quite a bit about machine learning. This was helpful as it meant I could direct them away from things I had already tried, which shifted their focus.”
This approach worked and led the researchers to bypass the brain data that Tom had been using for many years consisting of three dimensional legion images which showed where the patient’s brain had been damaged. He explains: “They decided to use the scans of the brains instead (with no explicit measurement of what regions were damaged) and this led to a result that seems better than anything I’ve seen before. They used a deep neural network to encode the most important features from the scans, then used those features to make what looked like highly accurate predictions of patients’ language outcomes after a stroke.
“At this point further work needs to be done on this to ensure that the method and results are solid, but it is definitely worth pursuing. If the predictions are really as good as they seem, we will be closer than ever to a tool that can actually be used in practice.”
“A researcher opted to pursue an approach I thought was a mad idea, not likely to work. But I was proved wrong.”
Dr Tom Hope, UCL
Another positive result emerging from the challenge may contribute to providing answers to the scientific aspects relating to how language is organised in the brain. Toms says: “A lot of researchers have claimed that language is a lot less complicated than it seems: that all of the apparently separable skills like reading and writing still broadly get impaired and recover together, in concert. One of the group’s researchers produced a very simple but powerful demonstration that this is wrong: that more complex language skills (like understanding a sentence) recover in a different way to more simple skills (like repeating a single word). I’m not really surprised that this true, but I am gratified that we can now show that it’s true.
“When this researcher opted to pursue an approach which explains rather than predicts I thought it was a mad idea, just not likely to work or produce results we could really use. But I was proved wrong and we will explore this further in a paper.”
These were two key outcomes from the Data Study Group that Tom will follow up, but he says there were actually five leads that could potentially be explored further.
Summing up and thinking back to the highlight of the week he says: “The week was very worthwhile and exceeded my expectations. It was also a personal education for me where I learned how to direct and guide people and how to explain and prepare the data in a way that non-experts could understand.
“The key highlight was on day two when they stopped talking. That was when I knew that they understood the data well enough and had started to properly engage with the problem. At that point I felt confident that something constructive was happening and that they might produce something.”
In terms of advice to any organisation or individual considering bringing a challenged to Turing Data Study Group, Tom says: “Do it, but definitely assign someone to be there all day every day. Also find a version of the data that minimises any technical restrictions (like confidentiality), this will make it easier.”
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/privitar,"
"
https://www.turing.ac.uk/events/potential-ai-help-better-prescribing-and-medicines-optimisation,"
Everyone is welcome but the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn about the challenges of defining multiple long term conditions and strategies that can be applied to their respective areas of work.
Here’s a description of the session:
After first providing a reminder of the current context of prescribing and medicines optimisation in primary care in England, Professor Avery will expand on some of the areas where AI may have an important future role. This includes decision support when considering whether to prescribe a medicine; collation of information when undertaking a medication review; decision support when reviewing a patient’s medicine; identifying patients who may need additional support with their medicines, and identifying overprescribing.
Register now
"
https://www.turing.ac.uk/news/publications/covi-agentsim-agent-based-model-evaluating-methods-digital-contact-tracing,"
The rapid global spread of COVID-19 has led to an unprecedented demand for effective methods to mitigate the spread of the disease, and various digital contact tracing (DCT) methods have emerged as a component of the solution. In order to make informed public health choices, there is a need for tools which allow evaluation and comparison of DCT methods. We introduce an agent-based compartmental simulator we call COVI-AgentSim, integrating detailed consideration of virology, disease progression, social contact networks, and mobility patterns, based on parameters derived from empirical research. We verify by comparing to real data that COVI-AgentSim is able to reproduce realistic COVID-19 spread dynamics, and perform a sensitivity analysis to verify that the relative performance of contact tracing methods are consistent across a range of settings. We use COVI-AgentSim to perform cost-benefit analyses comparing no DCT to: 1) standard binary contact tracing (BCT) that assigns binary recommendations based on binary test results; and 2) a rule-based method for feature-based contact tracing (FCT) that assigns a graded level of recommendation based on diverse individual features. We find all DCT methods consistently reduce the spread of the disease, and that the advantage of FCT over BCT is maintained over a wide range of adoption rates. Feature-based methods of contact tracing avert more disability-adjusted life years (DALYs) per socioeconomic cost (measured by productive hours lost). Our results suggest any DCT method can help save lives, support re-opening of economies, and prevent second-wave outbreaks, and that FCT methods are a promising direction for enriching BCT using self-reported symptoms, yielding earlier warning signals and a significantly reduced spread of the virus per socioeconomic cost.
Gupta, Prateek, Tegan Maharaj, Martin Weiss, Nasim Rahaman, Hannah Alsdurf, Abhinav Sharma, Nanor Minoyan et al. ""COVI-AgentSim: an Agent-based Model for Evaluating Methods of Digital Contact Tracing."" arXiv preprint arXiv:2010.16004 (2020).
 
7.2 MB
"
https://www.turing.ac.uk/people/sebastian-vollmer,"
"
https://www.turing.ac.uk/people/enrichment-students/andrew-mitchell,"
Andrew Mitchell is a doctoral researcher in urban soundscapes at University College London (UCL). His research interests include soundscape analysis, machine learning, and human perception of complex sounds. His current work focusses on creating computational models for predicting soundscape assessment in urban public spaces, making use of cutting edge acoustics analysis and machine learning methods and applying them to modern smart city design.
"
https://www.turing.ac.uk/people/atoosa-kasirzadeh,"
Atoosa is a tenure-track assistant professor (Chancellor’s Fellow) in the philosophy department and the Director of Research at the Centre for Technomoral Futures in the Futures Institute at the University of Edinburgh. Prior to this, she was a visiting research scientist at Google DeepMind in London and a postdoctoral research fellow at the Australian National University. She holds a Ph.D. in philosophy (2021, specialized in philosophy of science and technology) from the University of Toronto and a Ph.D. in applied mathematics (2015, specialized in large-scale algorithmic decision-making) from the Ecole Polytechnique of Montreal. 
Atoosa's research is focused on AI safety and ethics, including topics such as algorithmic transparency and accountability, algorithmic fairness and justice justice, ethical and social implications of generative AI models and recommender systems. 
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/dynamical-systems-differential-equations,"
"
https://www.turing.ac.uk/people/former-doctoral-students/karel-devriendt,"
Karel Devriendt is a doctoral student in applied mathematics at the University of Oxford and The Alan Turing Institute. The focus of his PhD project is to further develop the theoretical foundations of network science. Karel obtained a BSc in engineering from the KU Leuven in Belgium and spent one year at KTH Royal Institute of Technology in Sweden studying multimedia signal processing. His current interest in networks started during his MSc in electrical engineering at Delft University of Technology in the Netherlands, from which he graduated cum laude. The multidisciplinary nature of network science has led Karel to research projects in a number of different fields, with positions held at the VU Medical Centre in Amsterdam and the Quantum & Computer Engineering Department of the Delft University of Technology.
Many of today’s biggest challenges in science and engineering involve complex systems. In these systems, such as the human brain, the internet or social networks, complexity means that the whole cannot simply be described as the sum of its parts. In a social network, for instance, knowing the behaviour of each individual separately is not enough to understand the diverse social patterns that can emerge from interactions between people. Network science provides a framework to model such systems by explicitly including the interactions between constituents of the system. Whether these networks are made up of interconnected neurons, websites or individuals, they can all be studied using the same general methodology and mathematical tools developed by network scientists.
Karel’s research focuses on further developing these mathematical tools and applying and testing them on various real networks. In previous research at the Network Architectures and Services group in Delft, he studied the spreading of diseases, super-spreaders in networks, robustness of power grids and geometric tools for network science. At the Turing, Karel will continue his research on network science under supervision of Professor Lambiotte.
"
https://www.turing.ac.uk/research/interest-groups/causal-inference,"
The CIIG hosts monthly seminars which discuss recent advances in the field of causal inference, from both empirical and formal perspectives. Everyone with an interest in discussing causal inference is very welcome to come along.
Presenters are drawn from both academia and industry. If you would like to present your research (or related causal inference material) at the CIIG, please contact the organiser.
View schedule
[email protected]
 
"
https://www.turing.ac.uk/research/interest-groups/data-ethics-group/,"
Understanding the ethical and societal implications of data is one of The Alan Turing Institute’s key research priorities. We have created the Data Ethics Group to lead our research in this area.
Made up of a range of researchers specialising in ethics, social science, law, policy-making, and big data and algorithms, the Data Ethics Group drives the Institute’s research agenda in data ethics and works across the organisation on ethical best practice in data science.
The Group works in collaboration with the broader data science community, supports public dialogue on relevant topics, and sets open calls for participation in workshops, as well as public events.
Click here to join and request sign-up
DEG Theme for 2022-23: AI, Power, and the Public Interest: Who’s in control?
Despite the current swell of appeals to data science and “AI for the social good’ in the academic literature, the sociotechnical reality of contemporary data scientific practice seems to be telling a very different story. Present-day AI research and development ecosystems are contested fields composed of complex and contending interests, asymmetrical power relations, and prohibitive entry costs.
Notwithstanding their projections of charitable rhetoric, large tech companies are increasingly engaged in a “commercially-driven production of the social good” that, many have argued, produce outcomes which run counter to the public interest. The corresponding rise of phenomena like “data colonialism” and “philanthro-capitalism” are complicating “AI for the social good” narratives. They are heralding the coalescence of high-entry-cost digital innovation ecosystems, multinational corporate business models, and marketing strategies with humanitarian data work, digital development schemes, and market forces.
The result is the proliferation of privately controlled forms of extractionist data work, that are, in fact, often at cross-purposes with inclusive, equitable, and societally beneficial innovation. Trends such as these appear to be signalling “a profound rebalancing of power and governance in the domain of social life, privileging corporations with large-scale data power and making states (and other commercial and civil society actors) dependent on those corporations.” Similarly, incipient forms of data governmentality and commodification ever more infiltrate academic venues and research environments.
In these, large tech companies’ proprietary data sets, seemingly unlimited financial resources, and massive computing power are, in effect, ‘de-democratising’ AI and data science. Several processes and dynamics hasten this. They include the control that corporations possess over access to data and compute resources, their command over labour power through the university-corporate hybridisation of ‘dual-affiliation’ career trajectories, and their manipulation of the terms of open research to protect their own rentiership claims to monopolistic control over intellectual property and infrastructural assets.
The constellations of asymmetrical power relations, private corporate interests, and high entry costs for data innovation seem to establish the terms of engagement for accessing critical digital infrastructure that should otherwise be tightly bound up with the pursuit of the public good. The theme for our DEG dialogues this year, “AI, Power, and the Public Interest: Who’s in control?,” is framed around the difficult questions raised by these trends.
Questions of concern include: Is the current universe of data scientific innovation—that is, the global assemblage of data infrastructures, compute infrastructures, algorithmic infrastructures, funding schemes, and research and delivery capabilities and resources—equipped to actualise responsible and sustainable data work that serves the public good? Or do these infrastructural, resourcing, and human factors operate, in fact, to curtail and elide possibilities for that actualisation?
Is the human and biospheric interest in the realisation of the global public good being fostered by the current national and global configuration of power relations, sociotechnical affordances, platform ecosystems, and infrastructural networks that characterise contemporary data scientific research and innovation environments? How should terms such as “public good,” “social good”, and “public interest” be construed and reinterpreted in the light of the factors that purport to promote their realisation, or are seen as inhibiting them?
What role should states and governments play in the provision of, and the regulation and governance of, critical digital infrastructure? How can, and should, effective technology governance (including laws, regulatory regimes, and other policy instruments) work in the complicated, multivalent, and transnational innovation ecosystems and platform political economy that is dominated by big tech?
Open Dialogue: 'Foundation Models and the Public Interest’ on 11 May 2023, at 17:00 BST on Zoom.
There is heightened anxiety about potential disruptions to markets, governments and civil society resulting from rapid developments in transformer architecture and roll-out of models trained on a burgeoning volume of data. While foundation models are trained on large corpora of data to be task-agnostic, their proliferation and interaction include are likely to exacerbate existing bias, discrimination and mis/disinformation which can augment structural inequities and power imbalances at both a domestic and global level. Further, the limitations of existing regulations, threats of model hallucination and negative environmental externalities, among others, call for a close inquiry into these contemporary developments and the future landscape of AI. 
The Data Ethics Group will convene for an ‘Open Dialogue: Foundation Models and the Public Interest’ on 11 May 2023, at 17:00 BST on Zoom. In contrast to our previous dialogues, the upcoming event will not feature a presentation. Instead, members can utilise this as a platform to discuss the rapidly changing domain in relation to the 2023 theme: AI, Power, and the Public Interest: Who’s in control? The dialogue may present an opportunity for members of the DEG to develop a position paper on the proceedings.If you would like to attend, please sign up to the group by clicking the link under 'How to get involved' section.
Our Public Dialogues have previously brought together a range of experts to engage in an open format discussion on how their work addresses the DEG theme. Previous events include: 
[email protected]
"
https://www.turing.ac.uk/news,"

Find out more about the expert commentary the Turing can provide
Sign up to our mailing lists to get the latest Turing updates
"
https://www.turing.ac.uk/people/guest-speakers/eve-riley,"
Based at Macmillan Cancer Support, Eve leads The Richmond Group of Charities' work on improving care and support for people living with multiple long-term health conditions. She is particularly interested in health equity, the wider determinants of health and how to ensure lived experience guides and informs decision-making. Between 2018-2021 she managed The Taskforce on Multiple Conditions, a partnership programme with the Royal College of General Practitioners and Guys and St Thomas’ Charity to improve the lives of people living with, or caring for, people with multiple long-term conditions.
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/exploring-supported-decision-making-early-stage-bowel-cancer-diagnosis,"

In April 2021, Odin Vision collaborated with The Alan Turing Institute in a Data Study Group (DSG) to explore the interpretability of artificial intelligence (AI) models applied to early-stage characterisation of bowel cancer. Odin Vision – a UK company that has developed AI technology for detecting and characterising bowel cancer – hopes that this collaboration will help to boost user trust and demystify AI tech in the healthcare sector.
In the UK, there are over 42,000 new cases of bowel cancer and 16,000 related deaths per year, making it the second leading cause of cancer deaths. With the number of related deaths predicted to increase by around 50% over the next 15 years, tackling this disease is an urgent health priority.
In its early stages, bowel cancer is treatable and curable – the challenge is catching it early enough. Bowel cancer can be prevented through the detection and removal of precancerous polyps during a colonoscopy. However, even highly experienced clinicians can have difficulty spotting these often small and flat lesions, and then characterising them as precancerous or benign.
“I’m an experienced colonoscopist but some of these lesions are just extremely hard to find.”
Professor Laurence Lovat, Honorary Consultant Gastroenterologist, University College Hospital, and Clinical Advisor, Odin Vision
By using AI to overcome clinical challenges, a new era of healthcare is emerging. Combining clinician experience and AI can provide solutions which assist doctors in their duties, improve patient outcomes and increase value for healthcare providers.
This philosophy was employed by Odin Vision when it designed CADDIE, an AI system that combines the expertise of eminent clinicians and AI researchers. CADDIE is a cloud-based software that integrates with existing endoscopy equipment to augment its functionality. It detects the presence of polyps in real time on the endoscopy video feed and displays information about their visual characteristics, helping endoscopists to characterise the tissue as benign or precancerous.
Although it is clear that AI has an important role in the future of healthcare, many questions remain regarding the interpretability of AI models. Some AI models are considered to be ‘black boxes’. Such models are validated through rigorous testing, but this provides little insight into what is going on inside the model. Users have visibility of the inputs (image or video data) and the outputs (predictions) but the internal workings of the model are difficult to interpret; in other words, it’s difficult to understand exactly why the model makes a particular decision.
When clinicians make an optical characterisation, they use a set of features to classify polyps as either benign or precancerous – but what features do AI models rely on? In ambiguous cases, clinicians are able to seek more information through histopathological analysis before making a final diagnosis, but many AI models are unable to indicate the certainty with which they serve their prediction.
Odin Vision collaborated with the Turing in a Data Study Group (DSG) to help answer these questions, leading to an intensive research project which brought together some of the UK’s top rising talents in data science, AI and wider fields.
Odin Vision invited DSG participants to explore:
For the early-stage researchers, this was an opportunity to apply their expertise to a real-world challenge, in many cases outside their usual domain. Led by Dr Paul Duckworth of the Oxford Robotics Institute, the team came from a broad range of disciplines – including astrophysics and computer animation – and institutions spanning the UK.
“The multi-disciplinary team assembled by The Alan Turing Institute was fantastic! They immediately got to grips with the challenge, rapidly prototyping a range of different machine learning solutions. It’s been a pleasure to work with everyone on such an interesting and important project.”
Dr Paul Duckworth, DSG PI and postdoctoral researcher at the Oxford Robotics Institute
The DSG researchers split into smaller teams to focus on different parts of the challenge. The teams set objectives for each day and worked towards creating measurable outcomes for each objective. At the end of each day, the researchers presented ideas to Odin Vision’s research team and advisory clinicians, and fine-tuned their ideas based on the feedback. Over the course of the two weeks, the DSG researchers worked in close collaboration with Odin Vision to brainstorm innovative solutions to the problems.
Some of the researchers explored the use of Bayesian neural networks to create an uncertainty measure for the model’s predictions. These methods enable the algorithms to state how sure they are about the answers they give. In much the same way that a human might say “hold on, I’m a little unsure about this”, AI models should also be afforded the same luxury. In a clinical setting, “I don’t know” is more acceptable than giving an incorrect answer – in this case, the clinician can seek further clarification through histopathological analysis.
The team also prototyped a user interface to convey this valuable information to clinicians, and developed a condensed ‘uncertainty score’ of 1-5, which is understandable for general users.
The researchers also explored attribution methods to detect which optical polyp features the model uses to determine whether the tissue is precancerous or benign.
The team focused mainly on gradient-based attribution methods that identify which pixels on the input image predominantly contributed to the model’s predictions. This approach provides quick results that can be displayed in real time, which is highly important in a clinical setting.
One such method, ‘Guided Grad-CAM’, highlighted the vessels and surface patterns on the polyp. Interestingly, the features highlighted were similar to those the endoscopist would use when making an optical characterisation.
“Working with the Turing DSG has been an incredibly positive experience. The DSG perfectly illustrates how a diverse group of researchers from different technical backgrounds are able to bring new perspectives and create unique ideas to solve difficult problems.”
Peter Mountney, CEO of Odin Vision
The team has published a paper summarising its collaboration, available to read here.
Following the DSG, there are many exciting research avenues to pursue. The tools and insights discovered during the project will help to make ‘black box’ models more interpretable, and mark a major step in the team’s efforts to demystify AI in the healthcare sector. Odin Vision is now working to further develop, fine-tune and productionise the ideas developed during the DSG, and incorporate these into its product range.
“Making machine learning models more interpretable is an important and promising direction. There are a lot of exciting possibilities.”
Shuyu Lin, DSG participant and computer science PhD student at University of Oxford
Creating the next generation of AI-enabled endoscopy tools requires expertise from a multitude of backgrounds and disciplines. That’s why Odin Vision is so delighted to foster an ongoing, collaborative relationship with the Turing. Watch this space as innovation unfolds.
This piece was written for the Turing website by Odin Vision.
Top image: Siwakorn TH / Shutterstock
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/bridgeai,"
BridgeAI is an Innovate UK-funded programme with a mission to drive the adoption of Artificial Intelligence (AI) and machine learning (ML) in sectors of the UK economy with high potential for growth for AI. The priority industries for the programme are transport, construction, agriculture, and the creative industries. By doing so, BridgeAI aims to promote AI-driven productivity and efficiency gains, which will support the UK's transition to an AI-enabled economy.
The programme focuses on these four target sectors to encourage collaboration and build meaningful relationships between UK-based AI developers and industry adopters. These relationships will enable knowledge sharing, support the development of new AI solutions to address sector-specific challenges, and address the technical and business skills gaps currently limiting the adoption of AI solutions. Sustainable and responsible AI innovation is also a core focus of the programme.
 
The Alan Turing Institute is a key partner in BridgeAI. The Turing will provide independent scientific advice to companies encountering challenges associated with AI adoption. The Turing harnesses the UK data science and AI community to accelerate the development of AI capabilities in the four target sectors through the delivery of online training tools, advice and bespoke training targeted to the needs of Bridge AI participating organisations as well as access to the Turing Way. The Turing will also provide standards expertise through access to the UK AI Standards hub, bespoke delivery of workshops on AI and develop standards road maps.
 
BridgeAI is a £100m investment in the UK’s future that will stimulate both AI demand and supply side engagement, develop end-user driven AI technologies and address systemic barriers limiting adoption.

 
 
"
https://www.turing.ac.uk/events/turing-lecture-information-manipulation,"
114.4 KB

Craig Silverman is an award-winning author and journalist and one of the world's leading experts on online misinformation, fake news, and media manipulation. He is the media editor of BuzzFeed News, where he leads a team covering how our information environment is being shaped and exploited. 

Every digital product or service can be gamed or exploited, and this is especially true of today’s media and information environment. Whether it’s Russian trolls trying to create division in a democracy, marketers scheming to get the best reviews for their product, or criminals stealing billions thanks to ad fraud, our media environment is more open to manipulation than ever before. This talk exposes how it’s being done, and outlines ways to reclaim the media space from frauds and fakers.
The talk will be followed by an audience Q&A and drinks reception. Email your questions ahead of the event to Turing Events.
 
King's Place
90 York Way, London N1 9AG
"
https://www.turing.ac.uk/people/external-researchers/devjoy-dev,"
Devjoy was intrigued by the depth of Biomedical Engineering during his bachelors degree and later pursued research work at the Surgical Navigation and Robotics Laboratory at Harvard Medical School. After spending just under a year there, he now currently stands as a Postgraduate Researcher at Imperial College London working on redesigning an autonomous medical device for astronauts with cardiac arrhythmias. 
Devjoy gave a TEDx talk in 2015 - ’20 Countries at the Age of 20: What I learnt about Community’.
 
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/information-retrieval,"
"
https://www.turing.ac.uk/people/enrichment-students/benjamin-wallis,"
Ben is a PhD student in the Institute for Climate and Atmospheric Science at the University of Leeds, supervised by Anna Hogg and David Hogg. His work is focused on applications of statistics, machine learning and computer vision techniques to Earth observation glaciology. Using these methods with synthetic aperture radar and other satellite datasets he aims to improve measurements of ice velocity and mass balance, informing predictions of sea-level rises and the climate impact of a changing cryosphere.

"
https://www.turing.ac.uk/news/alan-turing-institute-launches-new-research-programme-data-science-and-ai-public-policy-led,"
Government has fostered and embraced important advances in technology, from critical investment in the iPhone and the internet, to early adoption of large-scale computer systems in the 1960s. Today, government is a major holder of data, which data science and artificial intelligence (AI) can harness to improve the design and provision of public services as well as to inform policy-making across all levels of government.
In a new research programme launched today, the Turing aims to foster government innovation: working with policy-makers to explore how data-driven public service provision and policy innovation might solve long running ‘wicked’ policy problems, to develop the ethical foundations for the use of data in policy-making, and to restore government’s role as a leader in tech innovation.
 The new public policy programme at The Alan Turing Institute will be led by Professor Helen Margetts, a Turing Fellow and Professor of Society and Internet at the University of Oxford.
The programme will be led by Professor Helen Margetts, a Turing Fellow and Professor of Society and Internet at the University of Oxford, where she has also been Director of the Oxford Internet Institute from 2011 to 2018.
Helen is a political scientist specialising in digital governance and politics, investigating political behaviour and institutions in the age of the internet, social media and data science. She has published over a hundred books, articles and policy reports in this area, including the recent book Political Turbulence: How Social Media Shape Collective Action (Princeton University Press), which won the W.J.M. Mackenzie prize for the best politics book of 2017, and the earlier book Digital Era Governance (Oxford University Press).
The new programme builds on the Turing’s ongoing work with government. Working with the Institute's Data Ethics Group -- which leads the Turing's research in understanding the ethical and societal implications of data science and AI -- the Institute is advising the Department for Digital, Culture, Media and Sport on the recently announced Centre for Data Ethics and Innovation and is a founding partner of the Ada Lovelace Institute, which will be launched by the Nuffield Foundation later this year. The Turing is also collaborating with the Information Commissioner’s Office on a framework for explaining the processes, services and decisions delivered by AI, and partnering with the Office for National Statistics Data Science Campus to co-fund 5 PhD places in the coming academic year for students working on areas of mutual research interest in data science for public good.
Commenting on her appointment as Programme Director for Public Policy, Helen said:
'This is a time of tremendous progress for data science and AI research - and we want to maximise the public good benefits of that progress. Policy-makers have a huge opportunity to use these technologies to improve the provision of public services, to design more effective policies, and to measure policy outcomes more accurately. However, data science and artificial intelligence can be technically daunting for government, requiring new methodologies and expertise, and introducing new moral dilemmas and bias. We want to help policy-makers to take up these challenges.
'I am delighted to have the opportunity to lead the Turing's public policy programme at such an exciting time. I look forward to working with researchers and policy-makers to ensure that data science and AI will improve the lives of as many people as possible.'
Alan Wilson, CEO of the Institute, commented:
'As the national institute for data science and AI, The Alan Turing Institute has a vital role to play in ensuring that these technologies are used for the good of society and everyone in it.  The launch of the public policy programme, under Helen's leadership, serves to underscore our commitment to promoting public policy, regulation and practices that are effective, ethical and equitable. The new programme will lend further strength to our mission to leverage these powerful technologies in order to change the world for the better.'
-Ends-
To find out more, visit the public policy programme's website
The public policy programme is now hiring an Ethics Fellow and an Ethics Research Assistant. The deadline for applications is 6 June 2018.
Media inquiries
Beth Wood
Press and Communications Manager
The Alan Turing Institute[email protected] (0)20 3862 3390
"
https://www.turing.ac.uk/news/publications/ai-human-rights-democracy-and-rule-law-primer-prepared-council-europe,"
In September 2019, the Council of Europe's Committee of Ministers adopted the terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI). The CAHAI is charged with examining the feasibility and potential elements of a legal framework for the design, development, and deployment of AI systems that accord with Council of Europe standards across the interrelated areas of human rights, democracy, and the rule of law. As a first and necessary step in carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its plenary in December 2020, has explored options for an international legal response that fills existing gaps in legislation and tailors the use of binding and non-binding legal instruments to the specific risks and opportunities presented by AI systems. The Study examines how the fundamental rights and freedoms that are already codified in international human rights law can be used as the basis for such a legal framework. It proposes nine principles and priorities that are fitted to the novel challenges posed by the design, development, and deployment of AI systems. When codified into law, these principles and priorities create a set of interlocking rights and obligations that will work towards ensuring that the design and use of AI technologies conform to the values of human rights, democracy, and the rule of law. 
The purpose of this primer, co-produced by The Alan Turing Institute and the Council of Europe, is to introduce the main concepts and principles presented in the CAHAI's Feasibility Study for a general, non-technical audience. It also aims to provide some background information on the areas of AI innovation, human rights law, technology policy, and compliance mechanisms covered therein. In keeping with the Council of Europe's commitment to broad multi-stakeholder consultations, outreach, and engagement, this primer has been designed to help facilitate the meaningful and informed participation of an inclusive group of stakeholders as the CAHAI seeks feedback and guidance regarding the essential issues raised by the Feasibility Study.
Leslie, D., Burr, C., Aitken, M., Cowls, J., Katell, M., and Briggs, M. (2021). Artificial intelligence, human rights, democracy, and the rule of law: a primer. The Council of Europe.
2.7 MB
"
https://www.turing.ac.uk/people/researchers/anna-knack,"
Anna Knack is a Senior Research Associate in the defence and security programme. Her research interests focus on defence futures and the impact of data science, artificial intelligence and other relevant emerging technologies on defence and security. Prior to joining the Turing, Anna was Deputy Co-ord Lead of the Technology, Disruption & Uncertainty research workstream in RAND where she coordinated a portfolio of work that leveraged futures methods to help policy-makers understand and prepare for the future strategic operating environment. This involved studies that informed strategic approaches towards military capability development, defence innovation ecosystems, defence industrial base issues, CBRN defence, cybersecurity, cybercrime and counter-terrorism.
Her recent work includes studies that informed the UK Ministry of Defence’s Climate Change and Sustainability Strategy and the Defence and Security Industrial Strategy, as well as the European Commission’s Blueprint for Sectoral Cooperation on Skills for the European defence industrial base. Her previous research also informed policy at UK Strategic Command; UK Army HQ; the Development, Concepts and Doctrine Centre; the UK Foreign, Commonwealth and Development Office; the French Ministry of Defence; the Australian Defence Science and Technology Group; the European Defence Agency; the European Commission; the European Union Agency for Cybersecurity and Europol.
She has presented her research at national and international conferences and has conducted policy briefings for senior decision-makers in national government, NATO and the EDA. Anna holds an M.A. in International Relations: Conflict Studies & Human Rights from the University of Utrecht and a B.A. in Politics & Social Policy from the University of York.
 
"
https://www.turing.ac.uk/about-us/our-strategy,"
Changing the world for the better with data science and AI.
The Institute was founded in 2015 with a clear and compelling purpose: ‘To make great leaps in the development and use of data science and artificial intelligence (AI) in order to change the world for the better.’  
In 2023 that purpose remains unchanged, but we are reassessing our founding assumptions and setting a course for the next five years – our overall ambitions and vision for this future, what we are calling Turing 2.0, and how we will get there – our strategy. 
Our aim is that ten years from now, the Turing will be internationally recognised as a centre of research and innovation that harnesses the power of data science and artificial intelligence to make a lasting impact on the world's most pressing societal issues.  
When people think of data science and AI, they will think about the positive difference we have made to the lives of millions of people around the world. 
Innovate and develop world-class research in data science and AI that encourages next generation theoretical developments and is applied to national and global challenges, supporting the creation of new businesses, services and jobs.
Contribute to training people across sectors and career stages with the necessary breadth and depth of technical and professional skills in data science and AI to match the UK’s growing industrial and societal needs.
Provide balance in the public conversation on data science and AI by speaking to its technical, social and ethical dimensions through public engagement and the provision of advice to policymakers, industry and civil society.
In support of our goals, the following set of principles will guide everything we do as part of Turing 2.0. They will help the Turing fulfil its purpose by driving our culture and delivering on the promise that data science and AI hold for the public good. They apply to how individuals and teams at the Turing work, as well as how the Turing operates as a responsible leader in the overall landscape. 
Lead responsibly 
Build confidence, ensure independence 
Enable impact, at scale 
Drive interdisciplinarity 
Move with agility 
Continually Innovate
Embed equality, diversity and inclusion 
Collaborate and convene 
Learn and help others learn 
Democratise access 
Data science and AI technologies will continue to evolve in unpredictable ways, and the speed of these developments will make it increasingly difficult to connect the people, ideas and data required to make progress against important societal issues. 
The Turing will, therefore, provide an end-to-end, interdisciplinary pathway in data science and AI that enables impact at scale and major progress against societal challenges.  
 
A grand challenge-led approach for Turing’s science and innovation agenda provides the common focus, clear direction and a mandate to prioritise activity, as well as the inspiration and motivation for diverse communities and capabilities to come together in support of these organising principles. In alignment with national priority areas, we have selected the following areas of focus for grand challenges: 
Data science and AI offer many opportunities to improve people's lives in the ‘digital society’. For example, these technologies might be used to provide more inclusive, fair and robust public services and tackle longstanding problems such as low productivity. Using our unique position and established expertise, we can tap into the enormous potential for data science and AI to drive a new generation of services which are efficient, smart and responsive to both complexity and uncertainty. Similarly, these technologies can help build resilience and tackle long-running policy problems, for example through real-time monitoring of the economy, financial systems, and supply chains.   
The Turing will draw and expand on its established research and partnerships  in public policy and data-driven economics to drive this work forward. We will focus on developing data science and AI to serve society and to improve measurement of the economy. We will build frameworks to tackle cross-cutting issues of standards, regulation, ethics and policy. We will develop methodologies to understand public attitudes to technology, incorporate that understanding into AI research and innovation, and foster ways of helping the public to understand and navigate this complex landscape.   
To drive advances in the Grand challenge areas, multi-faceted core capabilities are required to provide the underpinning infrastructure, this will involve: 
Core capabilities
Building our foundational capability in data science and AI in support of the above Grand Challenges.
Core capabilities
Expanding our work in the development and provision of open-source infrastructure that is accessible to all.
Core capabilities
Growing our core research software engineering capability to continue to contribute skills in research software engineering and data science in support of national priorities.
Core capabilities
Expanding our capability through Research Application Managers and Community Managers to support translation of our communities’ science and innovation activity.
To remain a science and AI superpower the UK has to invest in skills and people. The data, data science and AI skills gap is well documented and the Turing alone cannot solve all the problems in the skills pipeline. 
What we will do is fill some of the gaps, and in keeping with our approach of accelerating the transfer of skills across academia, industry, government and third sector, we will continue to grow our flagship programmes and build new capability where needed. 
Over the next five years we will extend our existing flagship programmes: Data study groups, Connections, Enrichment, Internships, to work on AI and data science problems identified under our Grand Challenge areas, and covering all career levels.
We will also launch our newly developed Turing Research Fellowship programme (for postdoctoral-level researchers) and develop an offer for apprenticeships to support entry into the field of data science and AI.  
We will scale existing initiatives such as our Data science and AI educators programme, which build skills and capacity through our ‘train-the-trainer’ approach for potential instructors and domain experts, our Research software engineering and research data science courses, as well as courses on AI ethics and governance (Turing Commons).  
We have recently launched an Online learning platform, which includes courses on responsible AI.  
Our first steps will include working with others in this space (eg, Catapults and Innovate UK, Data Skills Task Force) drawing on work already done, to create business leader personas, including pain points and challenge and support networks, in order to design suitable learning interventions.     
Public perceptions of data science and AI range from futuristic science-fiction to very real distrust and concern about the fairness and transparency of algorithms being applied in public life.

Addressing concerns around AI requires the development of frameworks, codes of practice, tools, laws and regulations to ensure that ethical principles and public voices are built into every stage of the design, development, and implementation of AI systems.  
Finally, it is important that the complex data science and AI landscape itself is easier to navigate and understand, both to those working in it and those looking to connect to it.
The Turing can provide balance, speaking to both the technical, and social and ethical dimensions of these technologies, presenting both the positive opportunities, as well as the risks.
We want to ensure high quality and reliable information about data science and AI reaches across all parts of society. This will in turn improve AI literacy in support of all our goals.
The Turing can drive a new generation of governance and regulation for data science and AI that ensures fairness, safety, and the protection of public interests across the digital economy and society. For information visit our AI Standards Hub.
As the national institute, the Turing is uniquely placed to support with linking up different groups. We will seek to provide a bespoke and appropriate offer to the range of audiences and stakeholders which have a vested interest or are directly impacted by the advancement of this technology.
In hearing the needs, concerns and expectations of the wider ecosystem, we can reflect this in the research we undertake and the activities that we pursue, including our AIUK conference. 
The Turing would like to thank the members of the Advisory Groups which were stood up during this strategy development to help shape and direct the future vision and approach of the Institute.
Prof. Tom Rodden, Chief Scientific Advisor, Department of Digital, Culture, Media and Sports (Chair), Prof. Ulrike Tillmann, Director, Isaac Newton Institute, Prof. Richard Samworth, Director Statistical Laboratory, University of Cambridge, Alex Van Someren, Chief Scientific Advisor, Government Office for Science, Dr Ruth Boumphrey, Chief Executive Officer, Lloyd’s Register Foundation, Dr Danijela Horak, Head of Machine Learning, R&D, BBC
Prof. Dame Wendy Hall, Regius Professor of Computer Science, University of Southampton (Chair), Frank Kelly, Trustee; Professor of the Mathematics of Systems, University of Cambridge, Iain Styles, Professor of Computational Life Sciences in the School of Computer Science at the University of Birmingham. Chair, Turing Skills Group, Vivienne Blackstone, Senior Manager, Artificial Intelligence and Robotics, EPSRC, Jon Rowe, Programme Director for the Data Science for Science and Humanities programme, Alan Turing Institute, Graham Cormode, Professor of Computer Science, University of Warwick, Darren Seymour-Russell, Growth and Strategy Lead, Responsible AI, Data Science, Associate Director Accenture, Jane Goff, Skills and Training Initiative Lead Defence Science and Technology Laboratory, Kate Hazeldene, Leads delivery of AI widening participation scholarship projects; Course programme manager, Office for Student
Tabitha Goldstaub MBE, Chair, AI Council (Chair), Dr Kanta Dihal, Lecturer in Science Communication, Imperial College London, Tania Duarte, CEO, We and AI, Stefan Janusz, Head of AI Narratives, Office for AI, Dr Shakir Mohamed, Research Scientist, DeepMind, Kerry Sheehan, Head of Service Development and Innovation, Civil Service, Dr Stephanie Hare, Researcher, broadcaster and author, Amanda White, Director of Communications and Policy, HDRUK, Katie Matthews, Interim Chief Executive, Poet in the City, Cassie Smith, Head of Legal, Trust and Ethics, HDR UK, Jonathan Lenson, CEO, Milltown Partners
The Turing will remain open to feedback and responses from the community on the approach outlined in the strategy, understanding that our long-term vision must remain adaptive to changes in the wider landscape. At this time any queries should be directed to [email protected].
"
https://www.turing.ac.uk/people/researchers/yannick-wurm,"
Yannick Wurm is Senior Lecturer in Bioinformatics at Queen Mary University of London, a fellow of The Alan Turing Institute and the Software Sustainability Institute.
His lab takes advantage of the 50,000-fold drop in DNA sequencing cost over the past 10 years to study the evolution of social behaviour in ants and the effects of environmental challenges on pollinators. Working on such species, for which modern molecular and genomic resources are scarce, requires careful balancing of analytical efforts for robustness and productivity.
Yannick's background is highly computational. His first degree was as an engineer in bioinformatics and modelling from the French National Institute of Applied Sciences (INSA de Lyon). He subsequently obtained PhD in sociogenetics of ants at University of Lausanne in collaboration with the Swiss Institute of Bioinformatics.
Yannick aims to combine his lab's expertise with that of the Turing by developing methodologies to:
"
https://www.turing.ac.uk/people/external-researchers/tom-mustill,"
Tom Mustill is a biologist turned writer and filmmaker, specialising in stories where people and nature meet. His film collaborations, many with Greta Thunberg and David Attenborough, have received numerous international awards, including two Webbys, a BAFTA, and an Emmy nomination. They have been played at the UN, in Times Square and on the Pyramid Stage at Glastonbury, and been shared by heads of state, the World Health Organisation, and Guns N’ Roses.
His first book, ""How To Speak Whale: A Voyage into the future of Animal Communication"" was released in September 2022 to critical acclaim.
He lives in London with his wife, Annie; daughter, Stella; and the inhabitants of his small but surprisingly deep pond.
Photo credit: Sam Mansfield
"
https://www.turing.ac.uk/research/research-projects/intuitive-human-robot-interaction-work-environments,"
Intelligently manipulating objects, especially in collaboration with humans, is still a widely researched topic in robotics. This project aims to create intelligent methods for natural and intuitive human-robot interaction (HRI), with the goal of equipping robots with the required intelligence to understand a given task in such a way to actively support a human completing it. The focus will be on natural, intuitive tool handover in a work environment, such as an operating theatre, a factory floor or the extreme environment of a nuclear power plant.
While humans (amongst each other) conduct handover tasks with great ease, there are still many shortcomings when attempting to carry out this HRI task using existing robotic systems. The project aims to create new AI-based approaches to instil robots with capabilities that allow them to interactively negotiate object 'affordances' between humans and robots and to dynamically adapt to shifting view points during the handover for reliable HRI.
The field of human-robot interaction (HRI) has seen a dramatic development over the last decade, with a wide range of researchers moving the field forward considerably. HRI has gained in importance with an observed need for robots that can operate in the vicinity of humans or even get in physical contact with them; application areas include robot-assisted minimally-invasive surgery, cooperative robotics in manufacturing (Industry 4.0), for the remote handling of objects in extreme environments including those in the nuclear industry, for rehabilitation and the care of the disabled and elderly.
Purposefully handling objects comes very naturally to humans – we instinctively understand how to pick up a tool and rearrange it in our hands so that we can use the tool to conduct a specific task. We are also adept at anticipating when and how an object will be received. Even the way in which a hand approaches an object can tell us a lot about how it will be used. The term affordance is used in this context; it describes the physical action possibilities of an object from the perspective of a (prospective) user, e.g. the handle of a cup can be used to easily pick up the cup and drink from it; a screwdriver is held by its handle to turn screws.
Current research in computer vision and robotics has focused on automatically detecting either robot-centric object affordances (i.e. the robot is the prospective user), to underpin intelligent object handling by robotic devices, or human-centric object affordances (i.e. the human is the prospective user), to infer the possible uses of everyday objects. However, the interplay between human-centric and robot-centric affordances of the same object has been largely overlooked. This research take a different perspective, as the robot and the human-centric models have to be considered when an object is handed over. 
This project plans to equip robots with the ability to integrate and interactively negotiate human-centric and robot-centric affordances of objects (and in particular, of tools) and to anticipate (i.e. before the actual handover) and dynamically adapt to shifting perspectives during the handover, so that a natural and intuitive collaboration between robot and human can be achieved. The robot needs to obtain an understanding of the object affordance from the human user’s perspective and to make use of the object’s complementary or collaboration affordances when picking up the object as to facilitate the handover to the human user who aims to complete a specific task using the object. For example, the robot should grasp a scalpel from the side of the blade so that the surgeon can grasp the handle during handover and perform the required incision.
The work introduces the term ‘coffordances’ to describe these collaboration affordances. Moreover, it will be investigated how different cues (i.e. touch, spoken dialogue, body movements) can be integrated both before and during the handover task to improve the robot's understanding of the situation, and to trigger corrective adjustments.
This project aims to answer a number of pertinent research questions relating to the challenge of natural and intuitive handover of tools in the context of human robot interaction:

The operating theatre is commonly a stressful working environment often requiring the clinicians to make quick and, at times, life-saving decisions on how to continue with a surgical procedure. Quick and appropriate reaction from all involved to make available required tools is essential. Ideally, the transferring of instruments is achieved without too much additional commands being given, but rather anticipated by attending support staff.
An intelligent robotic device could make a difference here, providing instruments when and where required, and at the same time understanding the affordances/coffordances of the instrument to be delivered so that the human surgeon can grasp the instrument and seamlessly conduct the intended procedure. This element of the proposed research programme will exploit the expertise of the applicants in robot-assisted minimally invasive surgery and robot-assisted ergonomics, HRI, etc, and will bring on board the expertise of involved surgeons and experts for tool-handover in surgical environments.
This work will make use of the surgery simulation facilities at QMUL (BARTS Health) providing the opportunity to obtain pilot data in a realistic setting without the need to apply for ethical approval as required for real surgery situations. The work will also involve collaboration with Professor Christian Heath (KCL) who is an expert in work and interaction. With input from interaction experts and clinicians, grant applications will be prepared in the area of robotics technologies to aid surgery.
 

Despite advancements in robot-assisted remote operation and tele-operation, a large number of work tasks still need to be done manually, e.g. in legacy plant contaminated by alpha-emitting plutonium dust. To clean up the UK’s nuclear legacy, it is estimated that one million entries of human workers will be needed into hazardous zones, wearing plastic suits with air-hoses or respirators, only capable of working for two hours work per day. Fine tool manipulation with up to seven layers of protective gloves is extremely hard. For the foreseeable future, humans will be required to enter highly ionised areas to carry out tasks that cannot be done by robotic devices.
This project will investigate new approaches where robots can join the human workers so that they can conduct the required tasks swiftly and, thus, reduce the time in the potentially harmful radioactive environment. The project will focus on developing assistant robots that will be particularly adept in handing over required tools in a way that the human user can focus on the task at hand.
The element of the work programme is strongly linked to the currently running research in the framework of the National Centre for Nuclear Robotics (NCNR) led by Birmingham and where QMUL is a partner of. The project will be strongly supported by Prof Rustam Stolkin (Birmingham and Coordinator of NCNR). It is the intention to apply for follow-on funds jointly with Birmingham and other suitable partners of NCNR – especially Innovate UK and SBRI funds for higher TRL industrialisation, increasingly calling for practical robotic devices to assist humans when working in difficult and hazardous conditions.
 

Extending from work conducted in the framework of EU project FourByThree, methods will be developed that will allow the robot to consider not only the coffordances of an object to be passed on, but also understand the need of the user to assume an ergonomically optimised posture at the handover point and during the subsequent tasks; specifically, one approach will explore how to create robots that can assist the worker by re-orientating the workpiece so that the worker’s posture is ergonomically optimal at all times.
The PI has many years of expertise in the area of robot-assisted ergonomics. With the work-related injuries due to bad posture on the rise, this element of the work will have a beneficial impact in virtually all manufacturing sectors. In addition to manufacturing assembly, a challenging task is disassembly of complex multi-material objects for recycling in the circular economy. This strand of work will be in collaboration with the Faraday Institute project on robotic disassembly and recycling of electric vehicle batteries (Stolkin, Birmingham), which currently can only be done by expert humans. 
 
[email protected]
"
https://www.turing.ac.uk/people/external-researchers/finn-janson,"
Finn Janson is a Data Science Degree Apprentice based in the UK. He works to develop Machine Learning algorithms and Statistical models for a variety of tasks, including Synthetic Data generation for clinical datasets. He also researches methods for auditing and mitigating bias in datasets and predictive models. He's always keen to learn about new technologies and how they can solve problems related to sustainability and equity. 
"
https://www.turing.ac.uk/people/former-enrichment-students/nina-otter,"
"
https://www.turing.ac.uk/events/turing-lecture-if-i-can-do-it-so-can-you,"
139 KB

“If I can do it, so can you” is the inspiring and motivating story of one woman’s determination to succeed. 25 years ago, Dr Sue Black was a single parent with three small children living on a council estate in Brixton. She is now one of the top 50 women in tech in Europe, received an OBE and just been appointed to the Government Advisory body shaping digital services. Sue talks about how she brought her family out of poverty and built a successful career through education, passion and a determination to succeed.
#TuringLecture #YouCanToo

The Alan Turing Institute is delighted to present this special event in Newcastle, with Sue Black OBE and #techmums, a social enterprise that teaches mothers technology skills and builds their confidence, encouraging them into education, entrepreneurship and employment.
Sue Black is a Technology Evangelist and Digital Skills Expert. She was awarded an OBE for ‘Service to technology’ in the 2016 Queen’s New Year’s Honours List and will share the motivational story about how she rose from humble beginnings through sheer determination. She is known not only for being an award-winning computer scientist and UK government adviser, but also as an activist and passionate social entrepeneuer. She led the campaign to save Bletchley Park 2008-2011 and today continues to help support women find their way in these tech-driven times - from learning how to access universal credit online to coding entire websites. 
The event will begin with lunch, before Sue shares her story, and will be followed by an optional beginners workshop in app design, with all levels welcome (no prior experience necessary). 
If you require any support in getting to us, from childcare to transport costs, please email [email protected] or call Jessie on 07961 479 488, to apply for funding.
International Centre for Life, Newcastle
Bio Science Centre Times Square, Scotswood Rd, Newcastle upon Tyne NE1 4EP
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/amnesty-international-challenge-monitoring-online-violence-against-high-profile-individuals,"

More than three quarters of women worldwide have experienced online violence and abuse according to a report commissioned by the UN Broadband Commission in 2015. Amnesty International’s own research in the UK has revealed that women in leadership positions or from ethnic minority, religious, disability or LGBQT communities are the most susceptible targets.
To tackle this problem Data Study Group researchers were challenged to develop a tool that would enable quantifiable and real time monitoring of online violence against high profile individuals, and particularly women. This would enable Amnesty International to gather concrete evidence which could be used to hold social media platform owners such as Twitter to account and call on them to take action to protect their users.
Reham Al Tamime, PhD student in web sciences at the University of Southampton facilitated the challenge and worked with a team of 9 researchers with skills and expertise in natural language processing (NLP), network analysis, data analytics, neuroscience, computer science, computational biology, cognitive science and machine learning. 
Explaining the researchers’ approach to the challenge she says: ""The team had a diverse range of backgrounds and skills and were able to contribute in multiple ways. Before embarking on the design of a training model we had to figure out how online abuse can be detected. The researchers were divided into three sub groups. The first group focused on finding good data annotations and removing bias to standardise what constitutes an abusive or non-abusive tweet. This was the first step to ensure data quality control. The second group worked on network analysis and visualisation. The team was able to set up a real time dashboard with the objective of making the behaviours of abuse more transparent. The third group worked on building and evaluating models to detect abusive tweets using NLP techniques. Overall these approaches complemented each other and none could have worked in isolation.”
Although limitations in quantity and quality of the data meant that the group were not able to develop a full working solution, Reham is positive that the work done during DSG is definitely a step in the right direction. She says: “We encountered some challenges because the quality and quantity of the data wasn’t sufficient. However, what we discovered as a result of the annotation experiment is that there was a lot of bias embedded in our own interpretations of what constitutes ‘abusive’ and we found that we couldn’t easily agree. We only tried to label 30 tweets so it was a very small data set, but our results were in line with other similar experiments that have been carried out by other researchers.”
She continues: “This is still useful for Amnesty and is something for them to take forward and build on. They are positive about the outcomes we came up with and are aware of the issues.”
""As a facilitator I could observe and learn about everyone’s work; I feel that I have learned so much""
Reham Al Tamime, Facilitator and PhD student at the University of Southampton
Reham was thrilled when her application to participate in DSG was accepted and even more pleased when she was selected to facilitate the challenge she had set her heart on. She says: ""I was so happy, as this is something I haven’t done before. It’s a totally new experience for me and I feel that I have learned so much about data science and been given the chance to work with some really smart and amazing people. As a facilitator I could observe and learn about everyone’s work. Everyone has put in so much time and effort and accomplished a lot.”
She adds: “I think the team did a really great job and I thought they worked really well together. I was impressed by the sub-groups and how they made the effort to make it feel inclusive. They were easy to talk to and everyone was able to connect and contribute to the best of their ability.”
In response to the question about what was the best thing about the Data Study Group Reham replies: “Being able to do something completely different to what you do in your PhD and being able to experience and be part of a new project. I feel it was useful academically and personally too. It was definitely worthwhile and I would definitely do it again.”
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/control-theory,"
"
https://www.turing.ac.uk/research/research-projects/neutron-scattering-and-machine-learning,"
Inelastic neutron scattering provides rich structural information about magnetic materials, but this data is incredibly complex and time consuming to analyse. This project is applying deep neural networks to analyse experimental outputs, without the need to many hours of expert input. The project will develop models capable of inferring magnetic structure from neuron data, and also capable of predicting the important regions of experimental hyperspace in which to find important information, helping to guide future experiments.
Neutrons have a magnetic moment, so when they interact with a material the changes in neutron energy and momentum tell us about the magnetic structure of that material. Rich information about magnetic structure can be obtained by carefully analysing the behaviour of neutrons in the presence of a magnetic sample. However, interpreting the data related to these changes results in a multi-dimensional data analysis problem where finding the right regions of this multi-dimensional space and then analysing them requires large amounts of expert time. As a result, powerful neutron experiments on the magnetic structure of materials are only feasible for a small number of systems. 
This work aims to develop methods which are significantly easier and quicker than existing analysis techniques for inelastic neutron scattering (INS). The project will implement networks for analysing existing datasets, to extract both model Hamiltonians (the sum of the kinetic energies plus the potential energies for all the particles in a system) and to parameterise these Hamiltonians from the data.
The project will also use attention methods to probe the results of these models in order to identify important regions of the experimental hyperspace, to guide experimentalists on where to concentrate their efforts. The resulting workflows will be integrated to the Proper Analysis of Coherent Excitations (PACE) suite of tools used for analysis of INS data and will interface with the Spin-W code, which is used for numerical simulation of INS data.
This work will remove the critical bottle neck to INS experiments i.e. data analysis and will allow for planning of experiments in advance by identifying critical regions of interest. This will greatly increase the bandwidth of INS analysis and allow this powerful technique to be more widely applied for example in the analysis of next-generation quantum materials.
The work will be applied at the ISIS Neutron and Muon Source facility. When incorporated into the PACE package it will be available to users of this national facility. The experiments that are conducted at ISIS which will be best served using this approach will be those interested in magnetic structure of materials. Understanding the magnetic structure is critical in the rational design of materials in a number of contexts, for example data-storage and spintronics. 
"
https://www.turing.ac.uk/people/researchers/albert-chen,"
Prof Albert Chen is an Associate Professor at the Centre for Water Systems (CWS), University of Exeter with over 20 years of experience in Water and Human Environments. His research vision is unleashing the power of hydroinformatics, enabling efficient and effective solutions to systematically strengthen the resilience of human and environment to the impacts of water-related challenges, for the present and the future climate scenarios. Albert is enthusiastic in building digital solutions such that he has developed a series of tools and techniques, including physical modelling, machine learning, data analytics, and high performance computing, to analyse interrelationships between different phenomena, services, and consequences related to water, human and environment. The research has enhanced our ability to predict the behaviours of water and its interactions with natural and human environments. His work has helped international stakeholders involving in different water management practices to determine adequate strategies and measures to better manage the valuable resources for sustainable development.
 
Albert is a  NERC Digital Environment Expert Network, a part of the NERC/UKRI Constructing a Digital Environment (CDE) Programme, to develop the thinking and practice around a ‘digitally enabled environment’, providing benefits for policy makers, businesses, communities and individuals. He also co-leads the Policy Action Group in the European Commission’s ICT4Water cluster to review existing challenges and to identify the advantages and opportunities of integrated ICT applications within water sector. The outcomes will contribute to shaping policy recommendations at super-national, national, and local levels to facilitate the uptake of digital solutions.
"
https://www.turing.ac.uk/research/project-status/ongoing,"
"
https://www.turing.ac.uk/contact-us,"

For general enquiries, further information about The Alan Turing Institute or potential partnerships please use our contact form. Our contact telephone number is +44 (0)20 3862 3352. 
Contact us
Contact [email protected] and view our press office page.
Follow us on Twitter at @turinginst, Facebook, LinkedIn, and Instagram, or join our newsletter mailing list to find out about our latest news, events, and opportunities at the Turing.
You can view our past events on our YouTube channel.
The Alan Turing Institute is headquartered at the British Library, 96 Euston Road, London NW1 2DB. Find out how to get to the Institute.

"
https://www.turing.ac.uk/news/publications/national-infrastructure-commission-data-public-good,"
New technologies like artificial intelligence and machine learning could help cut delays and disruptions across the UK’s infrastructure network. The National Infrastructure Commission’s report examined the opportunities that these new innovations present, and made recommendations to increase open data sharing to make the most of them. A first Call for Evidence was made on 15 February 2017, with a second following on 27 July 2017.
The recommendation is made that the government should encourage the uptake of new data-driven solutions to the asset management of critical infrastructures. Potential barriers to rollout include the differing levels of readiness and uncertainty towards integrating new methods into large-scale existing practices. The development of a national ‘digital twin’ of UK infrastructure can help to bridge geographic and sectorial divides, provide a framework for determining sensor locations, and serve as a technology demonstrator for new tools.
The submission also identifies smart traffic management as another key way to utilise data to improve our cities. By deploying open and social media data it is possible to facilitate smart urban management, repurposing existing data created by third parties and the government itself, thus creating ‘lightweight’ smart cities.
Lastly it is suggested that the effective use of big data requires greater standards to make the data accessible and usable. Currently, combining datasets from numerous sources and getting value from them is an arduous task. This would be made easier by having defined and widely accepted standards for data structures, labelling, cleanliness, security procedures, and sharing methods.
Weisi Guo, Turing Fellow, University of WarwickMark Girolami, Programme Director for Data-Centric EngineeringDarren Grey, Programme Manager for Data-Centric EngineeringDin-Houn Lau, Department of Mathematics, Imperial CollegeRicardo Silva, Turing Fellow, UCLVictoria Stephenson, EPICentre, UCLTim Sullivan, Applied Mathematics, Free University of BerlinSebastian Vollmer, Turing Fellow, University of WarwickDaniel Graham, Department of Civil and Environmental Engineering, Imperial CollegeScott Hale, Turing Fellow, University of OxfordMohammed Elshafie, Department of Engineering, University of CambridgeJulie McCann, Department of Computing, Imperial College
"
https://www.turing.ac.uk/people/researchers/adrian-bevan,"
Dr Adrian Bevan is head of the Particle Physics Research Centre at Queen Mary University of London. He uses a variety of data science techniques for his day-to day research in order to learn about the sub-atomic nature of particles. His research interests include understanding the matter-antimatter asymmetry in the Universe and developing a deeper understanding of the Higgs boson. He also works on the development of semiconductor sensor devices for operation in novel experiments and the Large Hadron Collider at CERN.
Dr Adrian Bevan participates in Big Data-based experiments at international laboratories and specialises in parameter estimation through complex modelling and simulation of data, and has almost two decades of experience in this area. Having worked on experiments at international laboratories in Switzerland (CERN and PSI), the US (SLAC National Accelerator Laboratory) and Italy (Frascati) Many of the research topics of interest have significant amounts of background ""noise"" obscuring the signal and as a result Adrian has had to use machine learning techniques in order to identify signal for subsequent analysis. 
Dr Adrian Bevan has written a book on Statistical Data Analysis and edited a book on The Physics of the B Factories. 
"
https://www.turing.ac.uk/research/research-projects/critical-infrastructures-control-system,"
Robustness and resilience are among the most critical issues in modern smart infrastructures, since they provide essential services to our daily life, such as food, transport, and water. This project aims to develop advanced control strategies and communication protocols based on better understanding of data and wireless communications for complex critical infrastructures. The developments can be applied to large geographically distributed infrastructures, whose monitor and control resources, such as wireless communication bandwidth and energy supply, are limited.
The main methodological basis for this project is event-triggered control. Normally, sensors and controllers are executed periodically, called periodic time-triggered control. Correspondingly, wireless communications are required periodically. However, this strategy does not consider the computer (sensor/actuator device) system requirements, and thus wastes system resources.
Event-triggered control, on the other hand, carries sensing/processing and communications only when necessary, such as satisfying some pre-designed conditions, which is usually based on the sensor system output. Therefore, resource consumption can be reduced. However, as a trade-off, adapting a non-periodic approach increases issues regarding the scheduling of sensors and wireless transmissions to maintain the stability of the system.
To overcome these problems, this project applies hybrid system methods and system verification to capture when the next event will come. Advanced communication protocols will be designed for this type of event-triggered control system. Providing guarantees out of low-cost, low-resourced hardware systems consisting of embedded microcontrollers and actuators operating over wireless networks is a massive challenge.
Automatic control of our infrastructure has the potential to make that infrastructure more resilient to change and failures (adaptive to environmental change and breakage), more sustainable (consuming less resources) and last longer (predictive maintenance). This project aims to guarantee the robustness and resilience of critical infrastructures that are controlled by embedded systems, and to minimise computational resource consumption.
The project aims to initially provide theoretical results for control strategies and communication protocols, then demonstrate practical results in construction machinery and equipment, smart cities and advanced farming. Success will involve being able to control critical infrastructures for long durations whilst giving guarantees that the control behaves in the way it is expected to.
The project has the following key objectives:
This project is part of the Data-centric engineering programme's Grand Challenge of 'Resilient and robust infrastructure'.
The developed control strategies and communication protocols should be totally or partially applicable to critical infrastructures that have: large physical scales, agile and flexible cyber components, and wireless sensing/actuating networks. The aim is to apply this work in water distribution systems.
Water distribution companies aim to demonstrate a high quality of service to their customers with reduced waste water, energy, and material. Next generation water systems will add even more sensors, actuators, controllers, and wireless networks to existing water infrastructure. The data will be collected from sensors, transmitted to the controllers to either compute control inputs to guarantee pre-designed quality of service, or to provide data for further analysis. In order to cover large physical scales, low power wide area (LPWA) technologies will used for efficient, cost-effective communication. 
However, since LPWA data delivery rates and bandwidths are relatively low, the design of control strategies and communication protocols becomes a greater challenge, yet to be addressed so far. Analytics should demonstrate how water infrastructure maintenance costs will be reduced and the service quality and robustness of supplies better guaranteed.
Other candidate applications that will benefit from this work include precision farming, smart grids, smart transports, and treatment plants.
Publication: Anqi Fu, Ivana Tomic, Julie A. McCann, Asynchronous sampling for decentralized periodic event-triggered control', submitted.
Grant: ABB has built and donated a 2-phase separator control system and corresponding tools to the project for experiments (worth 130k GBP).
Partnership: Discussing next steps with Water company partner regarding the data-centric engineering of the Active Sludge System.
Industry: Discussing with WRC Plc and Bristol Water Plc for data and parameters for regarding real-world water network control.
"
https://www.turing.ac.uk/people/external-researchers/adam-hosford,"
"
https://www.turing.ac.uk/people/guest-speakers/margaret-mitchell,"
Margaret Mitchell is a researcher working on Ethical AI, currently focused on the ins and outs of ethics-informed AI development in tech. She has published over 50 papers on natural language generation, assistive technology, computer vision, and AI ethics, and holds multiple patents in the areas of conversation generation and sentiment classification. She previously worked at Google AI as a Staff Research Scientist, where she founded and co-led Google's Ethical AI group, focused on foundational AI ethics research and operationalizing AI ethics Google-internally. Before joining Google, she was a researcher at Microsoft Research, focused on computer vision-to-language generation; and was a postdoc at Johns Hopkins, focused on Bayesian modeling and information extraction. She holds a PhD in Computer Science from the University of Aberdeen and a Master's in computational linguistics from the University of Washington. While earning her degrees, she also worked from 2005-2012 on machine learning, neurological disorders, and assistive technology at Oregon Health and Science University. She has spearheaded a number of workshops and initiatives at the intersections of diversity, inclusion, computer science, and ethics. Her work has received awards from Secretary of Defense Ash Carter and the American Foundation for the Blind, and has been implemented by multiple technology companies. She likes gardening, dogs, and cats.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/information-commissioners-office,"
"
https://www.turing.ac.uk/people/doctoral-students/sedar-olmez,"
Sedar is a PhD student at the University of Leeds, department of Geography. He graduated in Computer Science at King’s College London 2018. From a very early stage of his degree, he focused on artificial intelligence planning implementations on drones in a search and rescue domain, and this was his first formal attempt to study artificial intelligence. He participated in summer school at Boğaziçi University in Istanbul working on programming techniques to reduce execution time. During his final year, he concentrated on how argumentation theory with natural language processing can be used to optimise political influence. In the midst of completing his degree, he applied to Professor Alison Heppenstall's research proposal focusing on data analytics and society, a joint endeavour with the Alan Turing Institute and the Economic and Social Research Council. From 2018 - 2023 he will be working on his PhD at the Alan Turing Institute and Leeds Institute for Data Analytics. 
Sedar will be focusing on data analytics and smart cities, developing a programming library to try simulate how policies can impact a small world of autonomous intelligent agents to try deduce positive or negative impact in the long run. If the impact is positive and this is conveyed collectively taking into consideration the agent's health, happiness and other social characteristics then the policy can be considered. Furthermore, he will work on agent based modelling to solve and provide faster solutions to economic and social elements of society, establishing applied and theoretical answers. Some other interests are: machine learning, agent communication language, argumentation, artificial intelligence planning, natural language processing, evolutionary programming. 
Sedar hopes to collaborate with academics and professionals at the Alan Turing Institute to broaden the horizon of the field of intelligent agents.
 
"
https://www.turing.ac.uk/research/research-projects/capturing-complex-data-streams,"
Understanding multimodal data streams – complex sequences of data from different sources – is a key challenge of data science. By developing mathematical descriptions of these streams, using ‘rough path’ theory, it’s possible to gain insights into the data that can be used to generate meaningful decisions and actions. Applications of these models range from recognising Chinese handwriting on a smartphone, to classifying physical human actions and assisting mental health diagnosis.
When streamed data arrives, it rarely comes all at once, or from a single scalar source, but in multiple modes. Multimodal data streams are found in a huge range of situations and on all scales, and successfully summarising these streams is key to understanding them, and many facets of the world around us. The order of different signals in different modes in these streams provides key information. For example, the order in which glucose levels and insulin levels rise and fall in someone’s blood.
The key question that can be asked about a data stream is how to summarise it over short intervals, to create actionable information that can predict the stream’s effect and interaction on other systems. For example, summarising web click history well enough to be able to discuss and evaluate a range of strategies for effective advert placement, in systematic and automatic ways.
Many analysis techniques aren’t well equipped to deal with multimodal data, as they often treat each mode independently, nor do they deal well with randomness in the data. This is where Rough Path theory, a highly abstract and universal description of complex multimodal data streams, is incredibly useful. It allows us to directly capture the order in which events happen and better model the effects of these data streams, without needing to do high-dimensional recovery of the individual data points.
This is done by generating what’s called the ‘signature’ of the data stream; a set of step-by-step descriptions (or iterated integrals). The elements of the signature form, over short intervals of time, an ideal ‘feature set’ of inputs that can then be used to enhance conventional machine learning algorithms. The signature can dramatically reduce the size of certain learning problems and therefore the amount of data needed to train the related algorithms.
A successful application of using the signature with deep learning is the product gPen, a smartphone app developed by SCUT (South China University of Technology), and initiated by Ben Graham, formerly of the University of Warwick and now at Facebook, that converts user input of finger drawn Chinese handwriting into on-screen Chinese printed text. The 3-dimensional data stream here being the x-y position of the pen and the pen off/pen on action. The app has recognised billions of characters and is a part of a programme trained with data that can deliver a 96% accurate result on an iPhone in milliseconds. It does this with potentially thousands of outcomes, using less than 30MB of data and program, all without needing an internet connection.
 
The projects led by Turing Fellows Terry Lyons and Hao Ni are looking at further developing fundamental, signature based mathematical tools and introducing them to contexts where it is possible to achieve significant outcomes.
One part of the work is the development of useful open source software tools that could be utilised in various machine learning environments. Another bigger part is the interaction with complex, real world data, to be able to easily tackle questions where there is a variety of different data to consume. Themes include mental health, action detection, human/computer interfaces, and astronomy.
These projects aim to build bridges between high quality, fundamental mathematics and data science applications, and bring these new mathematical technologies into wider use. The Turing will act as ‘an intellectual nursery’, as well as a valuable conduit to transfer ideas between mathematics and critical business activity.
Mental health: Using speech and self-reported mood data from a prior clinical trial, a tool has been developed which looks in an automated way at self-reported data from individuals. When trained on the data provided across the participants in the trial, the tool captures the order in which individuals become more or less angry, anxious, elated, irritable, and sad.
The tool has already shown that, over a relatively short window of measurement, it’s possible to meaningfully position an individual on the bipolar spectrum. The sensitivity of this methodology offers the potential for its use in providing feedback for clinicians and clients.
Action Detection: Using signatures and computer vision to classify physical human actions from real time data is an important challenge, with a range of potentially interested parties. In particular, through the Turing, the research group have linked to the Health and Safety Executive. The annual cost of work related injuries in the UK is estimated at £4.6bn, and these injuries are often due to poor practice and training in manual handling. As the HSE currently does a lot of manual, frame by frame video analysis at a substantial cost, real time analysis would be of great value.
Human Computer Interfaces: This is related to the Chinese handwriting example detailed above. Further advances in this area are to be able to detect who has written the handwriting and also improved accelerometer analysis. As much of the computation of signatures for this must be done on simple, lightweight devices this of interest to the research group’s industrial partner ARM. Through this partnership, and with support from the Turing, the group aims to engage with other potentially parties that could be interested in this area such as Apple, Fitbit, Google etc.
Astronomy: Radio telescopes are used to receive and study huge complex astronomical radio wave data. Non-linearity is a crucial aspect of the way these telescopes collect and analyse their data. Rough path models have potential to help the development of measurement instruments and processing techniques used in new telescopes such as SKA in Cambridge. Our aim, in collaboration with SKA, is to improve detection sensitivity and make new observations of fast transients.
[email protected]
"
https://www.turing.ac.uk/people/researchers/mark-girolami,"
Professor Mark Girolami is the Chief Scientist of The Alan Turing Institute, the UK’s National Institute for data science and artificial intelligence and took up this role in October 2021. He was one of the original founding Executive Directors of the Institute and previous to his role as Chief Scientist he led the Turing’s Data-centric Engineering programme, which is principally funded by Lloyd’s Registry Foundation.   
In 2019, Mark was elected to the Sir Kirby Laing Professorship of Civil Engineering at the University of Cambridge where he also holds the Royal Academy of Engineering Research Chair in Data Centric Engineering. He is a fellow of Christ’s College Cambridge.  
Prior to joining the University of Cambridge, Mark held the Chair of Statistics in the Department of Mathematics at Imperial College London. He is an elected fellow of the Royal Academy of Engineering and the Royal Society of Edinburgh.  
"
https://www.turing.ac.uk/events/phd-student-presentation-and-networking-june-2023,"
Via the 'Turing Connections' initiative, The Alan Turing Institute creates and signposts opportunities which are relevant to PhD students connected with data science and artificial intelligence. The aims of the wider initiative are to:
Toward these aims, PhD students across the UK are invited to register to attend PhD student presentation and networking events.
These events are free to attend. If your plans change and you can no longer attend, please cancel your registration so that other students can benefit from the opportunity.
Registration includes a complementary sandwich lunch and use of venue facilities throughout the day.
Each event agenda will change depending on attendee presentations but each day includes a networking session, student presentations and lunch. 
Register now
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-university-birmingham,"
Achieving and maintaining normal sub-lingual blood flow in small ( 20m) vessels, termed as microcirculation, is essential for critically ill intensive care patients since this is where the delivery of blood and oxygen to tissues occurs. However, historically, most clinical trials & treatments have focused on blood flow in the larger blood vessels (macrocirculation) [20, 21, 14], largely due to the greater ease with which this can be practically measured.
Technological advances have enabled video recordings of the sub-lingual microcirculation (i.e. from under the tongue) to be obtained using darkfield microscopy (DFM). However, the difficulties in analysing these videos has hindered the uptake and utilisation of this imaging modality. Currently, these short video sequences are analysed mostly by hand, to quantify the vessel density and flow within vessels within the field of view. The manual analysis and vessel segmentation is an extremely labour intensive procedure, which can take up to one hour to score a single video [12].
This challenge aimed to establish whether a single validated measure of microcirculatory perfusion (microcirculatory flow index) can be predicted directly from a DFM video sequence, without intermediate manual analysis steps. Automatic analysis that can be carried out in (near) real-time would facilitate the incorporation of microcirulatory targets into clinical trials by enabling the impact of interventions to be quantified and enacted upon with the aim of optimising the microcirculation and improving patient outcomes.
Data Study Group team. (2022, July 5). Data Study Group Final Report: University of Birmingham. Zenodo. https://doi.org/10.5281/zenodo.6799096
PI: Kashif Rajpoot
4.9 MB
"
https://www.turing.ac.uk/about-us/equality-diversity-and-inclusion,"

The Alan Turing Institute (the Turing) recognises the importance of building a diverse community. To make the great leaps in research that we aspire to as the national institute for data science and artificial intelligence, promoting and embedding equality, diversity and inclusion (EDI)  is integral to achieving our mission.
Data science itself is a complex combination of diverse scientific disciplines. To ensure that we can change the world for the better, we need to:

EDI is everyone’s responsibility – watch our Equality, Diversity and Inclusion (EDI) video
The Alan Turing Institute has launched an Equality, Diversity and Inclusion (EDI) video for the Turing Community*.
The video highlights the importance of EDI at the Institute and how it is everyone’s responsibility in creating a welcoming, respectful and inclusive environment. EDI should be actively promoted and embedded in our daily interactions and behaviours at the Institute. This is a major part of who Turing is as an employer, a research institute, and a national body.
The video shows a number of ways how Turing People* can get involved with EDI at the Turing. This includes:
The EDI video is shown in the Turing’s Human Resources (HR) induction for new starters and is also available to watch on our YouTube channel.
*Turing Community and Turing People refers to all employees, fellows, researchers, students and anyone affiliated with the Turing.
In September 2021, we launched our first EDI strategy and accompanying action plan. This strategy was developed by the Turing community working with an external EDI consultant. We thank all contributors to the consultation exercise for their valuable input and for their help ensuring that this document is a comprehensive statement on the Turing’s commitment to EDI.
This strategy and action plan provide a statement of what we want to achieve and the roadmap as to how we will get there. The strategy and accompanying actions are built around the three roles in which the Turing will act on its EDI commitment.
We recognise that in order to lead at a national level we must work to embed diverse and inclusive practice at the Institute, and so our leadership efforts will start at the Turing with the implementation of our new EDI framework and annual EDI reports summarising progress against the strategy and our diversity objectives. The Turing Management Team is responsible for delivering on our EDI strategy and action plan with implementation led by our EDI team. If you would like to discuss our strategy and work in this area please contact the team at [email protected].
Read the strategy and action plan
Each individual at the Turing has a responsibility to help drive our EDI programme, guided by these principles:
We will support and enable our community to recognise and challenge behaviour at all levels which goes against these principles. Members of the Turing community can find out more via our internal EDI pages, including information on Report and Support. 
 
Our revised EDI framework is designed to ensure that the Turing’s commitment to EDI is translated into measurable positive progress against objectives set out in the strategy.
The Remuneration, EDI and People Committee (REPCo) is a sub-committee of the Board and will hold the Institute to account on its performance on EDI and ensure it remains a high priority for senior leadership.
The EDI Advisory Group is responsible for shaping and coordinating the EDI strategy and ensuring the tasks set out in the accompanying action plan are completed in good time.
The role of the staff network groups is to act as ‘critical friends’ to the Institute and to provide staff with the opportunity to contribute to the developing EDI agenda. The network groups will identify and suggest ways to address any barriers to the progress of disadvantaged groups.
The Turing’s current network groups are as follows:
Find out more about our EDI framework
 
The Turing conducts a range of research that intersects both directly and indirectly with themes related to equality, diversity and inclusion, ethics and fairness, and how to conduct inclusive, collaborative and inclusive research. A selection of our recent research projects in these areas are included below, and further information can be found on our research pages.
 
2022 will by the first year The Alan Turing Institute is reporting its gender pay gap. The gender pay gap is an equality measure that shows the difference in average earnings between women and men. It is a comparison across all roles within an organisation and is different to a measure of equal pay for comparable work. The Turing is committed to being an equal pay employer.  
Our median gender pay gap was 13.2% and our mean gender pay gap was 16.2%. The national median gender pay gap was 14.9% in 2022.  
While this report focuses on gender, we recognise multiple groups face barriers in achieving equality at work. The Turing has committed to publishing a yearly EDI Annual Report (see below) which provides further measures of diversity at the Institute and updates on our progress towards our EDI commitments.  
One of the key goals of the Institute is to train the leaders of the future, and we recognise to be successful this must include ensuring careers in data science, AI and research are accessible and inclusive to all.
The Turing has developed a number of initiatives to support greater access to training and diversity of our cohorts including:
You can find out more about the programmes currently running at the Turing on our skills page.
 
The Turing partnered with the Mixedracefaces platform to give members of our community who are of mixed heritage a chance to share their stories. View the profiles on the Mixedracefaces website. 
In our EDI work, the Turing is committed to learning from leaders in our sector and beyond. The Turing is proud to have membership of the following organisations.
 

The Athena SWAN charter was developed in 2005 and is open to research institutes and universities committed to the advancement and promotion of the careers of women in science, engineering, technology, maths and medicine in higher education and research.
 

The Business Disability Forum is a not-for-profit member organisation which provides a unique range of advice, support and expert tools to help organisations become disability-smart.
BDF has more than 20 years’ experience of working with public and private sector organisations.
 

We are a certified Disability Confident Employer – Disability Confident is a government scheme designed to encourage employers to recruit and retain disabled people and those with health conditions.
 

The Stonewall Diversity Champion programme is the leading employers' programme for ensuring all LGBTQ+ staff are accepted without exception in the workplace.
We create an environment where we have trust and can be trusted
We expect our Turing community to contribute to a culture that is inclusive and free of barriers
We all have different roles, priorities and challenges but our shared purpose is the same
Leadership is everyone’s business; Turing leaders set the right tone and lead by example
Everyone should understand the how and the why of our decisions and actions
We are all ambassadors for the Turing’s mission of changing the world for the better
 
Download (PDF) Download accessible version (Word)
Download (PDF) Download accessible version (Word)
Download (PDF) Download accessible version (Word)
"
https://www.turing.ac.uk/news/publications/artificial-intelligence-government-concepts-standards-and-unified-framework,"
Recent advances in artificial intelligence (AI) and machine learning (ML) hold the promise of improving government. Given the advanced capabilities of AI applications, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI systems may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full breadth of theoretical perspectives needed to understand the consequences of embedding AI into a public sector context is lacking. Here, we unify efforts across social and technical disciplines by using concept mapping to identify 107 different terms used in the multidisciplinary study of AI. We inductively sort these into three distinct semantic groups, which we label the (a) operational, (b) epistemic, and (c) normative domains. We then build on the results of this mapping exercise by proposing three new multifaceted concepts to study AI-based systems for government (AI-GOV) in an integrated, forward-looking way, which we call (1) operational fitness, (2) epistemic completeness, and (3) normative salience. Finally, we put these concepts to work by using them as dimensions in a conceptual typology of AI-GOV and connecting each with emerging AI technical measurement standards to encourage operationalization, foster cross-disciplinary dialogue, and stimulate debate among those aiming to reshape public administration with AI.
Straub, V. J., Morgan, D., Bright, J., and Margetts, H. (2022). Artificial intelligence in government: Concepts, standards, and a unified framework. arXiv preprint arXiv:2210.17218. DOI: https://doi.org/10.48550/arXiv.2210.17218.
131.7 KB
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/get-involved-researcher,"

On the first day you will hear the different challenge pitches. After lunch you choose which of these challenges you would like to work on for the rest of the week. With groups formed around the challenges, you will then begin to explore the data and brainstorm ideas which you will then develop over the next three days.
During the week, you will need to keep a record of your work, which you and your group will compile into a report for the DSG Challenge Owner. On the Friday, you will present your work to all participants and Challenge Owners.
Throughout the week you will be supported by your group facilitator, chosen from the participant applications, who will help move the project forward. There will be a representative from the DSG Challenge Owner’s organisation who will work alongside you and give you insights into the business problem, the datasets, and approaches that have been tried before. You will also be able to call upon the knowledge and help of the whole data study group.
But it won’t all be work; there will also be social activities planned throughout the week.
""The challenge was particularly interesting from a research point of view as there could be many different approaches to solving this problem. Therefore, the project drew a fantastic selection of candidates to our team from different fields.""
Chanuki Seresinhe, Turing Enrichment Year Doctoral Student
 
Researchers are typically PhD students and early career academics from statistics, computer science, engineering, mathematics, and computational social science, as well as wider disciplines where data science and AI skills are increasingly becoming relevant. Participating researchers include talent from across the UK and are recruited through regular open calls on the Institute’s website. 
 
You will need to be able to commit to an intensive five days at the Institute, based in the British Library in London.
Catering and travel to and from the event (up to £200 for those who are travelling from abroad) will be covered by the Turing as well as accommodation for those who don’t live in London.
 
If my application is unsuccessful, will individual feedback be available?
Unfortunately due to the high volume of applicants we are unable to provide feedback on an individual basis. 
I think I am a high calibre candidate, why has my application been unsuccessful?
Remember, you must explain and demonstrate your ability in the application form. Answering with a few short sentences is probably not enough to show reviewers your ability. It may be that your skillset is not a perfect match to the featured challenges. We do our best to ensure each DSG caters to different strengths so please try again next time! 
The equal opportunities questions on the application form are a little invasive – do I have to answer them?
No. Answering these questions is optional and will not affect your application.
 Do I need to enter my real address on the application form?
No. You may enter a superfluous address, however, please note that other Turing opportunities may require a genuine address.
Do I have to create a Flexi-Grant account to apply for a DSG?
You will need to register with Flexi-Grant in order to access the application form. Registration should take no more than one minute.
I will need to obtain a visa to attend, is it possible to be notified any earlier if my application is successful or not?  
If you are an especially strong candidate we may be able to notify you up to one week in advance. Please be sure that you indicate that you will require a visa on the application form (there is a question on this) and email us at [email protected].
 What happens if I need to obtain a visa to attend the DSG?
We try our best to ensure that participants are notified of acceptance in time to obtain a visa, additionally please email [email protected] flagging your visa deadlines and we will endeavour to ensure your application is dealt with promptly. Additionally, we are happy to provide you with a visa letter of invitation should it be required. The cost of the visa is included in the travel allowance of up to £200 which will be reimbursed.
Please note that overseas participants that are invited to attend the DSG should not purchase travel tickets until they have received their visas. 
Do you accept applicants from industry?
Yes, however applicants from industry are expected to take annual leave to attend in order to represent themselves as individuals rather than their organisations.
  Do I need to be able to code to attend the DSG?
Yes. The Data Study Group is considered a collaboration opportunity more than a training exercise. As a minimum, participants should be able to code using Python. 
Do I have the necessary experience/qualifications/skills to apply?
In general, we look for the following qualities in a reasonable application:
I have attended at DSG previously, am I able to apply again?
Yes, past applicants are welcome to apply.
 Are all applicants expected to have a PhD?
Generally, only a small hand full of MSc applicants are successful, the majority will be PhD and postdoc candidates. Students working towards a Masters level qualification may need to provide a reference letter from their supervisor.
Do you have to have a university affiliation to attend?
No. You do not have to have a university affiliation; this will have no impact on your application.
Where does the DSG take place?
The DSG takes place at The Alan Turing Institute, which is located within the British Library, central London.
 Will I be reimbursed for my travel expenses?
Yes. Travel and visa costs up to a maximum of £200 will be covered by The Alan Turing Institute. Please note: This is only applicable to in-person events, not virtual events.
 Is accommodation provided?
Yes, free accommodation is available throughout the week (Sunday - Friday morning) for an in-person Data Study Group. Please note: this is not applicable to virtual events.
What catering is included?
Lunch and dinner is provided during the week. We will try to cater for all dietary requirements provided you let us know when you have been accepted to participate in the event. Please note: This is only applicable to in-person events, not virtual events.
 How are participants assigned challenges?
On the first day of the event, after the morning presentations, all participants will be asked to list their first and second choice. If a group is too large or too small we will ask if participants would like to swap based on their second choice selections.
What kind of time commitment does the DSG involve?
The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected] if you feel that you may have to miss certain days/sessions of the Data Study Group.
If you know that you will likely have to miss a day/s or that you will not be able to engage fully due to revision, deadlines, teaching commitments, applications or other reasons, please do get in touch with us at [email protected] as we are happy to approach everyone's individual situation.
There will be frequent breaks during the Data Study Group, with participants able to turn their attention to their other commitments such as childcare etc. Please do let the Data Study Group Team know ahead of the event and we will be more than happy to accommodate your needs as best we can.
 Who owns any IP/code produced during the DSG week?
Although this is unlikely, any IP produced during the week is owned by the Turing, participants are not able to keep or publish code. If published at a later date credit will be given to the authors. 
 
Find out first when the next DSG challenges will be taking place and more – select 'Receive Applied Skills updates'.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/waze,"
www.waze.com
"
https://www.turing.ac.uk/people/researchers/mariona-coll-ardanuy,"
Mariona is a computational linguist at the Alan Turing Institute. She received her PhD from the Computer Science department of the University of Göttingen (Germany), where she worked on developing language-independent methods for exploring and mining collections of digitised historical documents.
Mariona has worked as a researcher at the Center of Digital Humanities in Trier and the Centre of Digital Humanitites in Göttingen, in close collaboration with historians and data scientists. After completing her PhD in 2017, she worked as a computational linguist in an artificial intelligence company in Barcelona, prior to joining The Alan Turing Institute as a Computational Linguistics Post-doctoral Research Associate on the Living with Machines project.
Mariona's research interests lay in the intersection between the humanities and language technology.
"
https://www.turing.ac.uk/people/honorary-fellows/cynthia-dwork,"
Cynthia Dwork is renowned for placing privacy-preserving data analysis on a mathematically rigorous foundation. A cornerstone of this work is differential privacy, a strong privacy guarantee, frequently permitting highly accurate data analysis. Dr. Dwork has also made seminal contributions in cryptography and distributed computing, and is the recipient of two ""test-of-time"" awards. She is a member of the US National Academy of Sciences, the US National Academy of Engineering, and the American Philosophical Society, and a Fellow of the American Academy of Arts and Sciences. Dwork, previously Distinguished Scientist at Microsoft Research, joined Harvard in January 2017 as the Gordon McKay Professor of Computer Science at the Paulson School of Engineering and Applied Sciences, Radcliffe Alumnae Professor at the Radcliffe Institute for Advanced Study, and an Affiliated Faculty Member at Harvard Law School.
"
https://www.turing.ac.uk/people/guest-speakers/pushmeet-kohli,"
Pushmeet Kohli is the Head of AI for Science at DeepMind, leading efforts including AlphaFold, a state-of-the-art AI system for predicting the 3D structure of proteins. Prior to this, he was the director of research at the Cognition group at Microsoft Research.
Pushmeet's research centres on using AI to solve impactful real world science-related problems. He is particularly interested in the use of machine learning techniques to enable and accelerate life sciences research, and the use of this knowledge for understanding and intervening in disease. Pushmeet also actively leads research on new techniques to ensure that AI systems are safe, reliable and trustworthy.
Pushmeet’s papers have won multiple awards and appeared in conferences in the fields of machine learning, computer vision, game theory and human computer interaction. His research has also been covered by popular media outlets such as Wired, Forbes, BBC, New Scientist and MIT Technology Review.
"
https://www.turing.ac.uk/people/external-researchers/alastair-gregory,"
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/distributed-parallel-cluster-computing,"
"
https://www.turing.ac.uk/people/researchers/carsten-maple,"
Professor Carsten Maple is Professor of Cyber Systems Engineering at the University of Warwick's Cyber Security Centre (CSC). He is the director of research in Cyber Security working with organisations in key sectors such as manufacturing, healthcare, financial services and the broader public sector to address the challenges presented by today's global cyber environment.
Professor Maple was previously Professor of Applicable Computing and Pro Vice Chancellor (Research and Enterprise) at the University of Bedfordshire. He has an international research reputation and extensive experience of institutional strategy development and interacting with external agencies. He has published over 200 peer reviewed papers and is co-author of the UK Security Breach Investigations Report 2010, supported by the Serious Organised Crime Agency and the Police Central e-crime Unit. Carsten is also co-author of Cyberstalking in the UK, a report supported by the Crown Prosecution Service and Network for Surviving Stalking.
He has given evidence to government committees on issues of anonymity, child safety online. He works with various departments such as the Association of Chief Police Officers, the College of Policing, Interpol, the Equality and Human Rights Commission, the Department of Business Innovation and Skills, to name a few. Additionally he has advised executive and non-executive directors of public sector organisations and multibillion pound private organisations.
Professor Maple is a Fellow of the British Computer Society and Vice chair of the Council of Professors and Heads of Computing, UK.
"
https://www.turing.ac.uk/people/programme-leadership/alan-wilson,"
Sir Alan Wilson is Director, Special Projects of The Alan Turing Institute, with responsibility for leading specific research and partnership projects. He was CEO of the Institute between 2016-2018. From 2018-2021 he has directed the Turing-UKRI Programme, AI for science and government, and now continues to work on other research programmes, including learning machines, the economics of cities and the Living with machines project. He is a member of the Board of Directors of the Banking Standards Board.
He is a Cambridge Mathematics graduate and began his research career in elementary particle physics at the Rutherford Laboratory. He turned to the social sciences, working on cities, with posts in Oxford and London before becoming Professor of Urban and Regional Geography in Leeds in 1970. He was a member of Oxford City Council from 1964-1967. In the late 1980s, he was the co-founder of GMAP Ltd, a University spin-out company. He was Vice-Chancellor of the University of Leeds from 1991 to 2004 when he became Director-General for Higher Education in the then DfES. After a brief spell in Cambridge, he joined UCL in 2007.  He was Professor of Urban and Regional Systems in the Centre for Advanced Spatial Analysis at University College London up to 2018.
From 2007-2013 he was Chair of the Arts and Humanities Research Council; from 2013-2015, Chair of the Lead Expert Group for the Government Office for Science Foresight Project on The Future of Cities; from 2013-2018, Chair of the Home Office Science Advisory Council; and from 2017-2019, Chair of the Ada Lovelace Institute.
He is a Member of Academia Europaea, an FBA, an FAcSS and an FRS. He was knighted in 2001 for services to higher education.
His research field covers many aspects of mathematical modelling of cities and the use of these models in planning. These techniques are now in common use internationally – including the use of the concept of entropy in building spatial interaction models – summarised in Entropy in urban and regional modelling (re-issued in 2011 by Routledge). These models have been widely used in areas such as transport planning, demography and economic modelling.
His recent research is on the applications of dynamical systems theory in relation to modelling the evolution of urban structure in both historical and contemporary settings. This led to the laying of the foundations of a comprehensive theory of urban dynamics described in Complex spatial systems (2000).
He has published over 200 papers and his recent books include The science of cities and regions (2012), his five volume Urban modelling (2012, edited), Explorations in urban and regional dynamics (2015, with Joel Dearden), Global dynamics (2016, edited) and Geo-mathematical modelling (2016, edited). He has a particular interest in interdisciplinarity and published Knowledge power in 2010; he writes the Quaestio blog.
"
https://www.turing.ac.uk/people/guest-speakers/noa-zilberman,"
Noa Zilberman is an Associate Professor in the Department of Engineering Science at the University of Oxford.
Her research focuses on the integration of micro-level architectures and macro level, large scale networked-systems. It considers the convergence of different disciplines, such as programmable hardware, networking, computer architecture, and data science, as a means to achieve new functionality, higher performance and better power efficiency.
Before joining the University of Oxford, she was a Fellow and an Affiliated Lecturer at the University of Cambridge. Prior to that, Noa spent close to 15 years in industry, last as an Engineering Manager and a Chip Architect at Broadcom.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/wordsworth-trust,"
"
https://www.turing.ac.uk/people/robin-mitra,"
Robin Mitra is an ONS Senior Lecturer in Statistics at Cardiff University and Research Theme Lead in Structured Missingness for the Turing-Roche Partnership. Prior to his appointment at Cardiff University he was a Lecturer at Lancaster University and prior to that a Lecturer at the University of Southampton.
He enjoys working collaboratively, both with colleagues in academia as well as with non-academic partners. Some of his previous collaborations have included working with the Office for National Statistics and the National Health Service Blood and Transplant as well as the Institute of Employment Research in Germany.
He is an active member of the Royal Statistical Society (RSS) and was the Chair of the Medical Section from January 2017- January 2020 and is currently a RSS Council member. 
 
His main research areas are dealing with problems arising due to missing data and data confidentiality. He also has interests in Bayesian methods more generally.
"
https://www.turing.ac.uk/research/interest-groups/learning-organisational-principles-living-systems,"
The purpose of this group is to bring together a diverse set of researchers with the aim of identifying new ways of using machine learning and AI to discover fundamental organisational principles of living systems. By bridging across domains ranging from mathematical modelling to image analysis and digital twins, we seek to form a comprehensive view of how modern computational techniques are shedding light on fundamental biological questions.
 
 
In 1952, Alan Turing published a seminal paper that sought to describe organisational principles of living systems from a mathematical view. Living systems exhibit multiple layers of complexity from molecular scales to the whole organisms and ecosystems. Now, both the volume and diversity of experimental data types in biology is growing rapidly, making integrated quantitative analysis across disparate data types a key challenge in modern biological research. 
By bringing together researchers with different expertise this interest group aims to explore the use of machine learning and AI to address fundamental and general biological research questions in a way that would be very difficult to achieve for a single lab or research group.
In particular we aim to:
How do we build interpretable machine learning models of living systems?
Challenges: Machine learning is a powerful tool for uncovering patterns within data. However, the internal representation of these models may be difficult to interpret.
Example output: Strategies for design and interpretation of machine learning models.
What are the key challenges in combining diverse and multi-scale datasets to build models of living systems?
Challenges: Experimental datasets are often not standardised or interoperable, have sufficient metadata or are not available in formats well suited for large scale machine learning.  
Example output: Case study bringing together different dataset types for integrated quantitative analysis.
 
Click here to join us and request sign-up
"
https://www.turing.ac.uk/people/enrichment-students/everlyn-kamau,"
Everlyn Kamau is a PhD student at Oxford University at the Nuffield Department of Medicine, studying clinical epidemiology of enterovirus infections. She is interested in infectious disease surveillance and mathematical modelling for public health. 
Everlyn is implementing mathematical models to reconstruct historical changes in virus transmission and disease incidence using serological survey data.
"
https://www.turing.ac.uk/people/researchers/robert-foley,"
Robert Foley is Leverhulme Professor of Human Evolution at the University of Cambridge, and a Fellow of King's College. He obtained his PhD from Cambridge, and was subsequently a lecturer in biological anthropology at the University of Durham. Since 1986 he has been at Cambridge, and in 2001 co-founded the Leverhulme Centre for Human Evolutionary Studies, an inter-disciplinary research centre focused on developing multi-disciplinary and integrated approaches to human evolution. He was elected a Fellow of the British Academy in 2007.
Robert Foley's research has focused on the evolution and ecology of humans, especially their behaviour and adaptations. Much of this work has concentrated on understanding humans in terms of general Darwinian patterns and processes, and relating human evolution to more general models of evolution. Among his contributions are the development of off-site archaeology, community ecology and co-evolutionary approaches to hominin evolution, ecological models for human evolution, phylogenetic methods for analysing technological, cultural, social and linguistic evolution, the multiple dispersal model of human origins, and multi-disciplinary approaches to the evolution of human diversity.
His research combines traditional palaeoanthropological investigations with emerging approaches such as ancient DNA, agent-based modelling and human behavioural and cognitive experiments. This research has included early African hominins, the evolution of modern humans, and more recent prehistory and anthropology. Current research is focused on developing comparative and quantitative approaches to human evolution, including genotype-phenotype relationships. He has carried out field projects in Africa and Melanesia. He is currently involved in major field projects in northern and central Kenya. His books include Off-Site Archaeology, Another Unique Species: Patterns in Human Evolutionary Ecology, Humans before Humanity, and Principles of Human Evolution.
"
https://www.turing.ac.uk/people/guest-speakers/jen-tarnate,"

"
https://www.turing.ac.uk/turing-fellows,"

Our Researcher list is optimised to use JavaScript. Without Javascript enabled our filter and search function, which helps you find Turing researchers by speciality, title and institution, can’t function. Please consider allowing Javascript to run on this page or use the site wide search function. Contact [email protected] if you have any issues.
Find out more about Research Engineering at the Turing
"
https://www.turing.ac.uk/research/research-projects/project-odysseus-understanding-london-busyness-and-exiting-lockdown,"
One of the key UK policies in tackling the coronavirus crisis has been to try and ‘flatten the curve’ through social distancing and evidence is beginning to show these may be influencing the number of total cases within London. Furthermore, it's now necessary to start designing strategies for exiting lockdown in a staged, principled, and data driven manner that will restore the economy and our way of life.
This project aims to bring together multiple large-scale and heterogeneous datasets capturing mobility, transportation and traffic activity over the city of London to better understand “busyness” and enable targeted interventions and effective policy-making. The team from the Turing's Data Centric Engineering programme, sponsored by the Lloyd's Register Foundation, will work together with researchers from the University of Warwick, UCL and University of Cambridge to develop models, infrastructure and machine learning algorithms for understanding how and when busyness is changing across the capital in the wider context of COVID-19.
The project is codenamed “Odysseus” because Corona is also known as the “Cyclops' eye”.
This project will combine multiple data sources with varying spatio-temporal resolutions to provide a cohesive view of the city’s level of activity. These vary from JamCam cameras, traffic intersection monitors, and aggregate GPS activity from the Turing's London air quality project, to point of sale counts and public transit activity metrics.
Integrating these datasets into a unified pipeline that collects live data is a challenging data science and software engineering problem. The infrastructure, systems and databases are the foundation upon which the analysis is conducted with fast and efficient access to information. Data often contains anomalies, noise or missing values which must be handled robustly.
From this vast array of datasets, state of the art machine learning algorithms [Hamelijnck et al. 2019; Knoblauch et al. 2018], statistical time-series analysis [Haycock et al. 2020, Aglietti et al. 2019] and image processing [Walsh et al. 2020] will be applied to historical observations to generate a seasonally dependent set of pre-lockdown profiles. Upon construction of a live data-ingestion pipeline it is possible to provide the city with live deviation from these profiles. These metrics offer outlier detection in areas requiring intervention, and monitoring post-intervention, as we hopefully enter the ‘recovery’ period [O'Hara et al. 2020].
 
As an example, selecting a time range at camera, we may group bounding box pixels counting toward intensity within the whole scene. With object tracking disabled here, intensity is higher where objects stay in frame. This is most evident with vehicle traffic waiting at closest set of traffic li­­­­ghts, and is adjusted via selecting distinct samples. 
Given a calibrated scene, we can quickly generate distance estimates triangulation of pedestrians, estimate edge lengths, and record those less than 3 meters in the scene over time.
Selecting a contour of 90th percentile activity in our calibrated scene and normalizing pedestrian densities relative to m^2 we may visualize the activity overlaid to satellite imagery. The pavement and active pedestrian crossing are selected as expected. 
 
This project strives to help the London authorities understand the extent to which people are staying at home, i.e. the effectiveness of the current government approach. Pre-social distancing profiles will be produced as estimates of “normal activity” within the city, which will then be compared with post-distancing readings in near real-time, enabling direct assessment of the impact of policy intervention.
The project will act to serve as an ‘early warning system’, that can trigger intervention review within London boroughs and increase targeted communications in the event that people prematurely do not observe social distancing or exit isolation.
 
It will operate at a more nuanced level as restrictions are partially relaxed – to assess how people respond to the changes and provide statistics on whether to mediate the response.
As the country moves into a ‘recovery’ period, busyness will actually be a good thing and understanding the extent to which London is returning to normal will also be important for policy makers and to guide interventions to stimulate the economy.
The end product of this project will be an application program interface (API) with which the Greater London Authority, Transport for London, London Data Commission, and the Office for National Statistics may query for access to both the data sources and the analysis outputs from the algorithms and statistical models developed.
The work will support the GLA with their implementations of visualisation of the analysis through dashboards and analysing the effect of policies and interventions.
The outputs will be integrated inside the London Datastore, will also benefit the 'London air quality' project as additional emission sources are captured, and will be made available to as wide audience as possible together with open access to the software and models for benefiting other cities and groups worldwide.
Finally, the output of the system can be used by routing algorithms that maximise social distancing when walking through London's streets. These algorithms will extend existing work [O'Hara et al, 2019] from the London air project to help Londoners avoid crowded roads and paths when exercising or commuting in the city.
"
https://www.turing.ac.uk/research/research-projects/sustainable-infrastructures,"
Data analytics provide very effective tools to solve complex problems; however they are typically constrained where the computing system infrastructure lacks the ability to provide reliable data and is prone to data retrieval delays. This project will seek to provide guarantees of data reliability with new tools for failure detection, diagnosis, and recovery, while minimising delays caused by data communication and processing. This will allow decision makers to better react to real-time changes that currently reduce sustainability in infrastructures, and processes, causing system inefficiencies.
Sensing, processing and actuation, are the three main steps adopted by recent cyber-physical systems (CPS), where data are usually collected with Internet of Things (IoT) devices. These devices are typically constrained to limited resources, such as small battery and low computation capacity. State-of-the-art machine learning (ML) and artificial intelligence (AI) can aid complex CPS problems, but are heavily dependent on the quality of data they receive.
To ensure the quality of data collected by ubiquitous sensing networks, where the devices therein are typically fault prone and the communications between devices are unreliable, an online system is needed which can help to identify and diagnose failures and recover from a faulty state. Different mathematical tools such as statistics, machine learning, and graph theory can be exploited to build such systems. However, these IoT networks are typically constrained by limited resources such as battery and computational capacity. Therefore, lightweight approaches are required to replace traditional methodologies.
Most data collected in CPSs are in the form of time series, which is a series of data points indexed (or listed or graphed) in time order. However this time series data usually includes noise and trivial information which may cause bias in post analysis even with state-of-the-art ML and AI solutions. To improve the quality of analytic results, meaningful information can be extracted from their raw format by using analytic tools such as motif discovery, feature extraction and visualisation. By extending these approaches, new solutions, which can handle data in an adaptive manner, are proposed to support mining tasks on time series streams.
This work seeks to provide reliable tools that allow decision makers to effectively understand the behaviour of infrastructures, such as bridges, gas pipes etc, and to enable them to react to changes or failures in the infrastructure in real time. The work will explore how sensing devices embedded in the infrastructure can provide this information.
The following research questions will be tackled:
The project aims to develop approaches that can be generalised across different scales and industrial settings (e.g. from small factory operations to supply chains), and fulfil the requirements of real-world applications which will engage users and stakeholders.
The contribution to sensor systems, failure management, time series analytics, and design engineering disciplines will be evidenced by outputs in international research conferences and journals.
This project is part of the Data-centric engineering programme's Grand Challenge of 'Resilient and robust infrastructure'.
This work is applied to and can benefit real-time sustainable infrastructure, such as water supply networks, smart grids, transportation and precision farming. By providing reliable data and analytic tools in real-time, decision makers can better understand the interdependencies between relevant processes, discover potential impacts caused by the changes in these processes and react accordingly.
EPSRC Water-Energy-Food Nexus Networking Event, London.
"
https://www.turing.ac.uk/people/guest-speakers/yacine-jernite,"
Yacine Jernite is a researcher at Hugging Face working on exploring the social and legal context of Machine Learning systems, particularly ML and NLP datasets. He has worked on several aspects of data curation, documentation, and governance, and has most recently been coordinating data and multidisciplinary research efforts within the BigScience workshop on Large Language Models. Before joining Hugging Face, Yacine obtained a Ph.D. from New York University working on language modeling and medical applications of machine learning from 2012 to 2018, followed by a postdoc at FAIR NY (Meta AI) working on automatic summarization and abstractive question answering from 2018  to 2020.
"
https://www.turing.ac.uk/research/research-projects/security-cloud,"
Cloud computing services offer hugely scalable resources, but don't always guarantee confidentiality and integrity of compute jobs. By taking advantage of new hardware technology for trusted computing, this project aims to create a new, secure big data platform that allows for secure computing in the cloud.
Much of today's data is stored and processed in the cloud. Cloud computing entails many, often vast data centres in heavily protected environments the size of warehouses. Data scientists would like to rent these scalable computing resources. However, the cloud may not always meet stringent technical, legal, and regulatory requirements, especially for data that is personal and subject to privacy laws, or that is commercial-in-confidence.
Data 'safe havens' provide strict isolation between different tenants or customers of a data centre but this may not be sufficient for some uses - patient records, criminal record data, financial service audit data - where there is still a residual risk from insider attacks or from software (or hardware) vulnerabilities. The next step is to take advantage of new hardware technology for trusted computing from Intel and from ARM, who have built hardware 'enclaves' to protect sensitive data.
Secure enclaves are trusted hardware that provide a secure container into which the cloud user can upload encrypted private data, securely decrypt it, and compute on it. Both the decryption and the computation are run in a processor which, in principle, can’t even be broken into by their owner.
This project investigates and prototypes the secure software components that are needed to take advantage of new hardware like secure enclaves. The aim is to create a new, secure big data platform that provides explicit guarantees about the confidentiality and integrity of compute jobs in untrusted cloud environments.
It will also be key that such a platform still remains compatible with existing programme models so that data scientists can continue to use the familiar set of tools (for example Hadoop, Spark, Flink) that are so effective.
The resulting research will answer an industry need to enable data sharing without compromising data privacy, with a potentially transformative effect for sectors such as health, financial services, and crime.
[email protected]
Members of the Research Engineering Group at the Turing are contributing their expertise to this project.
The group are working on an extension to the Apache Spark big-data platform to allow the secure enclave technology described above to be used for distributed computation, as well as a significant amount of the supporting operating-system-level infrastructure.
"
https://www.turing.ac.uk/people/researchers/zoe-kourtzi,"
Zoe Kourtzi is Professor of Computational Cognitive Neuroscience at the University of Cambridge. Her research aims to develop predictive models of neurodegenerative disease and mental health with translational impact in early diagnosis and personalised interventions. Kourtzi received her PhD from Rutgers University and was postdoctoral fellow at MIT and Harvard. She was a Senior Research Scientist at the Max Planck Institute for Biological Cybernetics and then a Chair in Brain Imaging at the University of Birmingham, before moving to the University of Cambridge in 2013. She is a Royal Society Industry Fellow, Fellow at the Alan Turing Institute, and the Scientific Director for Alzheimer's Research UK Initiative on Early Detection of Neurodegenerative Diseases (EDoN).
Predicting individual socio-cognitive health is of high priority for health economies, given the translational potential for early diagnosis and personalised treatment. Yet, predicting intervention outcomes in health and society is challenged by variability across individuals. Zoe's work addresses this challenge by developing predictive models based on machine learning approaches that synthesise large-scale multivariate and longitudinal data to characterise individualised profiles of health across the lifespan. This work has strong translational applications in education and healthcare for the design of a) practical cost-effective tools for early diagnosis of decline and disease progression in clinical practice, b) training programmes tailored to individual needs.
"
https://www.turing.ac.uk/people/external-researchers/emma-chapman,"
Dr Emma Chapman is a Royal Astronomical Society Research Fellow based at Imperial College London where her research centres on the first stars. Emma is a member of The 1752 Group, a research organisation lobbying to end staff-student sexual misconduct in higher education and regularly works with sector bodies, institutions, the press and legal services to ensure the safeguarding of students is taken seriously.
 
 
 
"
https://www.turing.ac.uk/research/research-projects/digital-twins-multiphase-flow-systems,"
Data-informed 'digital twins' of complex systems are seeing adoption in design, manufacturing and distribution. In the energy and manufacturing sectors, digital twins are living models that drive operational efficiency and process safety outcomes. This project works across the key enablers of digital twin technology; cloud computation, internet-of-things (IoT) sensor networks, advances in machine learning and interactive data visualisation; in order to develop rapid predictive models for engineered multiphase flow systems.
Process engineering within the energy and manufacturing sectors relies on a thorough understanding of multiphase flow systems. Systems comprising liquids, gases and solid particles are complex to control and feature poorly understood dynamics regarding phase formation and transitions. Digital twin technologies will unlock barriers to operational efficiency and industrial process safety through a fusion of simulation and experimentally derived data streams.
A key metric of digital twin performance is the underlying simulation fidelity. In many engineering cases, physics based models outperform purely statistical models; as limited data are typically available for inference, or data may not exist due to the intrusiveness of measurement techniques. Simulation fidelity, however, comes at a cost: slow computation times limit iteration and parameter-exploration opportunities, while high-resource demands restrict computations to energy-intensive high-performance clusters. New hybrid physics/machine learning models will hence unlock real-time digital twin coupling, and allow for computation on low-intensity cloud resources. A tradeoff between simulation realism and relevancy must be navigated in order to deliver solutions: rapid, low-fidelity solutions can be of high decision-making utility, provided that uncertainty is explicitly quantified.
Advances in machine learning can be exploited to calibrate underlying simulations with physical data streams, leading to rapid prediction tools for complex multiphysics scenarios. This project's focus areas include reduced-order modelling, surrogate modelling and active learning strategies, in addition to numerical solvers for continuum mechanics partial differential equation systems. Incorporating physics based constraints and insights, such as energy/mass conservation and empirical closures, into machine learnt models is a priority.
The project will demonstrate value creation and impact within the energy and manufacturing sectors by deploying cloud-native digital twin applications. The project roadmap includes:
Within the energy and manufacturing sectors, digital twin technology will see application in varied multiphase control settings:
[email protected]
"
https://www.turing.ac.uk/events/ai-between-plant-and-agricultural-science,"
Data science and machine learning are central to the development of high-throughput plant and agricultural research and its application to problems of social and environmental resilience, such as food security. The workshop will focus in particular on the structure, management and use of plant data infrastructures, including the choice of semantic structures, the maintenance of systems and their accessibility - issues that are central to the integration of diverse datasets for computational analyses at an environmental scale. Having reliable, meaningful and usable ways to integrate, compare and visualise data from different sources and scientific approaches is essential to data analysis in the emerging field of Environmental Intelligence, which seeks to enhance human resilience to environmental changes through understanding the complex interactions between climate, ecosystems, and human social and economic systems. Plant science makes a distinct contribution to this effort through the development of evidence-based agriculture and governance strategies for food security. These strategies are predicated on having efficient plant data infrastructures for integrating diverse datasets and conducting complex computational analyses, for example on the interrelations between genotype, environment and growth data for crop varieties. Building infrastructures that are sustainable and responsive to the needs of plant and agricultural scientists working in high-throughput research settings continues to pose a significant challenge for achieving food security goals.
This one-day workshop will bring together experts in the plant and agricultural sciences who are working with complex datasets spanning genomic, physiological and environmental data and computational methods of analysis with data scientists interested in the application of cutting-edge technologies to this field. The workshop aims to map future directions for (1) developing and consolidating The Turing Institute’s capabilities in the area of data science for plant science, and (2) integrating plant science within emerging networks of Environmental Intelligence. Particular emphasis will be given to mapping the current needs of the plant and agricultural science community, in order to establish a guiding framework for the efficient and responsive deployment of data science and artificial intelligence resources in those fields. Short research presentations will be followed by an extended discussion format, which will provide a forum for identifying possible collaborations and developing proposals for project applications.
Agenda
 09.30 - Registration
 10.00 - Welcome and introduction    
- Challenges in data linkage and analysis for the plant sciences - Sabina Leonelli (University of Exeter)    
- The Environmental Intelligence initiative - Gavin Shaddick (University of Exeter)    
- chaired by Sabina Leonelli (University of Exeter)  
 10.30 - Presentation session 1: Plant and agricultural science at The Turing
 11.30 - Break
 12.00 - Presentation session 2: Integrating plant data for environmental intelligence
 13.00 - Lunch
 13.45 - Panel session: Between plant and data science – Mapping needs
 15.15 - Break
 15.30 - Breakout session: Future paths – Identifying proposals and collaborations
 16.30 - Summary and closing remarks
 17.00 - Finish

"
https://www.turing.ac.uk/people/business-team/warwick-wood,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/teesside-university,"
"
https://www.turing.ac.uk/people/external-researchers/rolf-bienert,"
"
https://www.turing.ac.uk/research/research-projects/social-information-and-public-opinion,"
Recent election surprises, regime changes and political shocks seem to indicate that the public agenda has become more fast-moving and volatile. One of the likely components of this political turbulence is the recent change in how we acquire and consume information. Yet, there has been no systematic study of the individual bases of political opinion formation. This project seeks to address this gap by studying the relation between social information and political opinion.
To understand the effects of exposure to social information on individual political opinion, this project is drawing on methods from experimental psychology and social data science, trying to quantify the extent to which social information can influence an individual’s political opinion or stance, and whether this influence is stronger for any particular set or type of political issues.
Social media platforms have been shown to inject instability and uncertainty into social life, public opinion, civil society and the policy-making environment. By showing in real time what other people are doing, these platforms create a pool of social information, which can influence users. These feedback influences at work on social media platforms have been shown to introduce instability into cultural markets, and may be having wider effects on what issues the public, media and politicians focus their attention on, and affect policy-making or even contribute to political turbulence.
Although the large-scale consequences of this volatility of public political attention are visible in current political events, the extent to which social information can change an individual’s political opinion or stance is still poorly understood. This project is addressing this challenge by trying to understand in which contexts social information can change an individual’s opinion about the importance of a political issue and by quantifying how large these changes can be.
Understanding the ways in which social information affects the dynamics of political opinion will shed light on the role of social media platforms, leading to a more thorough understanding of the undercurrents of public opinion that have, in recent years, burst to the surface in highly unpredictable ways.
This can inform the creation of early warning signals that may aid in prediction and explanation of public opinion dynamics, improve the opportunities to help policy makers understand and represent the will of their constituents, and guide the design of better online platforms for social interaction.
[email protected]
"
https://www.turing.ac.uk/research/research-programmes/research-engineering,"

Researchers and practitioners face a common need for high quality tools, practices, methodologies, platforms and systems.
Many domains can benefit from the deployment of cutting-edge algorithms and approaches, but these cannot be effectively applied unless realised as usable software libraries, reproducible analyses and workflows, or high performance computational environments.
The Research Engineering Group (REG) contributes skills in research software engineering and data science in support of other programmes, as well as to its own projects. This model of working ensures that the tools they develop are useful and applicable to a wider range of areas. The team supports professional delivery of impactful research across the Turing's programmes, as well as its own research interests.
The team concentrate mainly on:
To keep up to date with the team, follow us on twitter at @turinghut23 or LinkedIn
Header image: Close up of a Bletchley Park Bombe, designed by Alan Turing and his colleagues to decipher German Enigma machine messages in WW2. Source: mendhak, Flickr, Creative Commons.

 
Read about some of our project highlights. See here for a complete list. 
Research Engineering is involved in many projects across the Turing, collaborating both with academics and strategic commercial partners.
Below is the list of our most recent projects, the full list can be found here.
For a full list of blogs and news associated with REG see here.
The most recent events involving REG are shown below. For a complete list see here. 
Below is the list of our most recent publications, the full list can be found here. 
"
https://www.turing.ac.uk/news/publications/exploiting-web-semantic-change-detection,"
Detecting significant linguistic shifts in the meaning and usage of words has gained more attention over the last few years. Linguistic shifts are especially prevalent on the Internet, where words’ meaning can change rapidly. In this work, we describe the construction of a large diachronic corpus that relies on the UK Web Archive and we propose a preliminary analysis of semantic change detection exploiting a particular technique called Temporal Random Indexing. Results of the evaluation are promising and give us important insights for further investigations.
Basili, P. and McGillivray, B. (2018). Exploiting the Web for Semantic Change Detection. In 21st International Conference, DS 2018, Limassol, Cyprus, October 29-31, 2018, Proceedings
"
https://www.turing.ac.uk/blog/counterspeech-better-way-tackling-online-hate,"
Social media is where most of us find information, read the news and discuss ideas – and watch cute cat videos. But unfortunately, online platforms are also used for spreading malicious messages, such as hate speech and harassment. This can cause real harm to people’s wellbeing, and also wider society – online hate has been linked to terrorism and gun violence, for example. According to the Turing’s 2019 report ‘How much online abuse is there?’, 30-40% of people in the UK have witnessed online abuse, and 10-20% of people have been personally targeted.
Traditionally, online hate mitigation has focused on developing AI tools for automatically detecting harmful content, which can be used to moderate content at scale. For instance, out of all the hate speech that Facebook took action on in Q1 2022, 96% was found before users reported it, with automated classification systems playing a key role. However, while content moderation can immediately reduce the number of harmful messages, it can also limit free speech and may not be effective at tackling hate in the long term. Content moderation does little to provide support to victims, build resilience within communities, or change the beliefs of hateful individuals. And, concerningly, content removed on one platform might just be reposted somewhere else.
Counterspeech has been proposed as a more effective approach to challenging online hate. Instead of blocking or removing content, counterspeech directly presents an alternative, polite and non-aggressive response to whatever has been posted. A 2021 study in Proceedings of the National Academy of Sciences (PNAS) found that empathy-based counterspeech can encourage Twitter users to delete their own racist posts. An example of empathetic counterspeech to racist content might be: “I feel sorry for the victims. As a mother, any types of violence are always alarming and disturbing. We should try to make a better world.” (This is a modified version of a real tweet in our project’s dataset.)
Counterspeech can take the form of text, graphics or video, and it offers several benefits over content moderation, such as protecting free speech, supporting victims, and signalling to other social media users that hate is unacceptable. However, creating effective counterspeech requires considerable expertise and time, and many community groups that create counterspeech don’t have the resources to scale up their work. What’s more, there has been relatively little academic work in this area, primarily due to a lack of data about existing counterspeech.
To help address these issues, the Turing’s Online Safety Team, part of the public policy programme, has launched a new project that aims to use AI computer models to automatically detect and generate counterspeech in English. First, we are collecting datasets of abusive social media posts, and their spontaneous responses, so that we can study how online hate is responded to in the wild. This will give us valuable data with which to train our models.
Automatically detecting counterspeech in this data is a difficult task – it tends to be very rare on social media, and uses similar language and grammar to other forms of language. One approach that’s been used in previous studies uses keyword or pattern matching to look for content that is identical to known counterspeech, but this misses any new counterspeech. Our approach will instead use more complex AI models (trained on large pools of labelled data) to evaluate how similar the content looks to known counterspeech. Once these models have been trained to know what counterspeech looks like, they could be used to automatically generate their own examples from scratch, which would help to make addressing online hate radically more easy.
With this project, we aim to advance academic knowledge, support the work of practitioners, experts and activists, and raise awareness of the benefits of having more content on social media platforms (i.e. through counterspeech) rather than less (i.e. through content takedowns). We aim to make all of our work open source, sharing the guidelines, datasets and other resources we create.
We are actively looking for partners to help us with this project, and we welcome any expressions of interest from researchers, civil society organisations or social media platforms. To find out more, please get in touch with Yi-Ling Chung, the project’s lead researcher.
 
Top image: Rodion Kutsaev / Unsplash
"
https://www.turing.ac.uk/people/external-researchers/abdul-rahman-alsewari,"
"
https://www.turing.ac.uk/people/external-researchers/thomas-pasquier,"
"
https://www.turing.ac.uk/research/research-projects/science-cities-and-regions,"
This initiative aims to catalyse the progression of urban analytics into Turing 2.0 with the creation of a cross-cutting technology platform for data science innovation in the science of cities and regions. It will leverage world-leading capabilities for research and innovation; and provide a unique bridge from foundational data science to global challenges which are of extreme importance to provide citizens with future urban designs with enhanced levels of sustainability, prosperity and liveability.
Harmonising components within a single technology layer will encourage greater focus on exploitation of the research engineering capabilities of the Turing and an increased ability to translate fundamental advances in data science and AI into products and tools which address critical real-world challenges. By capitalising on these distinctive assets, Turing engenders a unique resource to support programmes of work with government and commercial partners, and a singular proposition to be leveraged in joint projects with other University partners.
Investment in a cross-cutting resource provides a platform capability which will be leveraged multiplicatively across diverse project streams. An immediate high value return is possible by capitalising on existing research capacity, however this could potentially be scaled back, staged or deferred to manage to a more restrained investment profile.
The work will be delivered by a team which has developed a collegiate and inclusive culture, with high levels of openness and transparency in the production of software and data products.
• Our ambitions are specifically focused towards enhancing the delivery capability of four Urban Analytics missions. (1) New forms of data propel sustainability, equity and efficiency in all forms of Urban Mobility; (2) AI is instrumental to the Land-Use Planning process for buildings, cities and landscapes; (3) Pervasive data are leveraged for healthy, sustainable, and prosperous cities and their populations; (4) Digital Twins are a mature paradigm for the evaluation and implementation of urban policies. These have been articulated and refined through discussion and persistent engagement with our customers, research partners and international peers.
• Outcomes will be measurable: through the adoption of more refined operating models we will seek a minimum of 400% gearing on Turing’s core investment.
• Achievability is demonstrable through established collaborations across every mission with corporates, government, internal programmes (including strategic REG involvement) and university partners.
• The work is enormously relevant to Grand Challenges in Environment, Health and Economy. The importance of a
‘place-based approach to net-zero’ is explicitly recognised in HMG Net Zero Strategy. Connections between place, health and economy are clearly evident in the Levelling Up Missions e.g. in relation to ‘pay, employment and productivity’ in ‘globally competitive cit(ies)’ (Mission 1)’, narrowing the gap in healthy life expectancy (Mission 7), and ‘well-being … improve(ment) in every area of the UK’ (Mission 8). The data science dimension is recognised in the UKRI (ESRC) Strategic Delivery Plan to support ‘world class places’ by providing ‘better data for better policies’ through ‘sustained investment in data infrastructure’.
• The initial timeframe is to advance the city science agenda over 30 months to March 2025. 
• Capability to exploit data – Proportion (count) of stakeholders reporting that they believe they make better decisions in place-based policy-making
• Product innovation – new products developed; uptake of products and tools by research organisations, business and government.
• Skills and awareness – Proportion (count) of ROs and RWOs reporting that they understand and use advanced data science more effectively than previously
• Research quality – invitation to participate in international events, institutional visits, contribution to ISI-rated journals and other peer-reviewed outputs; citations including software and data products as well as scientific articles; impact case studies.
[email protected]
"
https://www.turing.ac.uk/people/researchers/andrew-holding,"
Dr Andrew Holding is a Lecturer in Biomedical Science at the University of York. He was previously Senior Research Associate at Cancer Research UK's (CRUK) Cambridge Institute and a Fellow of Downing College, Cambridge. Andrew originally studied chemistry at the University of Oxford before he went on to apply these skills to investigate how life works at a molecular level. During his PhD at the University of Cambridge, he focused on how nature makes the antibiotics we use to treat antibiotic-resistant infections including MRSA. In 2009, he moved to the MRC Laboratory of Molecular Biology as a Career Development Fellow; here, he investigated how the machinery within the cells interacts to undertake basic processes including the replication of DNA.
Then in 2013, Andrew joined CRUK's Cambridge Institute where he now leads a team focusing on the molecular processes that drive cancer, in particular the role of nuclear receptors in breast cancer and leukaemia. In 2015 Andrew was appointed a Fellow of Natural Sciences at Downing College, University of Cambridge, where he teaches Biochemistry and Molecular Biology and was awarded the inaugural CRUK Rising Star in Public Engagement Prize by CRUK in recognition of his work communicating with public.
Andrew's research project brings together several cutting-edge technologies: combining single cell sequencing and CRISPR gene editing with neural networks to establish a computational model that predicts therapeutic response to combination drug therapies. Each year there are about 750 cases of Acute Lymphoblastic Leukaemia (ALL), predominantly in those of age 0-4, where cure rates are high. However, in older patients and children who relapse, survival is dismal, leading to around 240 deaths annually. Glucocorticoid Receptor (GR) activation by dexamethasone (Dex) has been one of the mainstay therapies for ALL for over three decades. The caveat is drug resistance: 60% of adult patients are unresponsive to therapy at relapse and is therefore a critical problem for survival.
Combinatorial therapies have the potential to bypass resistance. Screening human cancer cells for drug response has been successful in predicting single therapeutic options from a patient's genotype but does not resolve the challenge of identifying particularly efficacious combinations of drugs. Andrew proposes an alternative strategy: first, establishing how cancer cells respond to combination therapies by experimentally targeting combinations of genes in a high-throughput manner; and secondly, integrating the results through machine learning. By predicting therapies that bypass resistance, Andrew's work will play a key role in improving the current five-year survival rate in adults which, at only 40%, is considerably short of public targets, including CRUK's own 75% 10-year survival goal.
"
https://www.turing.ac.uk/events/clinical-ai-summer-school,"
This is a one day training event for Clinicians with little or no expertise in artificial intelligence (AI).
It is organised by the Clinical AI Interest Group
In the morning, we will have AI experts teaching you about the basic concepts of AI and Machine Learning, giving an overview of the ethical considerations when using AI responsibly and discussing the quality, standards, and regulation considerations needed for developing and deploying AI in a clinical setting.  
In the afternoon, we will hear about the state of health AI deployment and then we will go on to have a hand-on session that will give you experience of using AI to diagnose cancer from images. 
The event is in person only and is being held in the Enigma Room of The Alan Turing Institute. 
The address is British Library, 96 Euston Road, London NW1 2DB, London, UK.
Information about how to get to the Turing can be found here: How to get to The Alan Turing Institute | The Alan Turing Institute
Our programme for the day will be:
Nick Fuggle and Alisha Davies
Arron Lacey
 
David Leslie
Clíodhna Ní Ghuidhir
 
Nick Fuggle
 
 
 
 
To apply, please fill in this short application form: https://forms.office.com/e/Du17H96Xhd
Applications need to be submitted by Friday 23 June 2023.
The organisers of the interest group will then assess the applications. Our reviewing process can be seen here: reviewing process document.
Applicants will be informed of the outcome by email by Friday 30 June 2023.
If you have any questions or need any more information, please contact Emma Karoune ([email protected])
"
https://www.turing.ac.uk/research/research-projects/virtualised-hardware-acceleration-data-analytics,"
Analytics systems in defence often deal with high volumes of streaming data, applying complex queries to detect sensitive patterns. To support such work alternative hardware architectures have been exploited due to their support for high throughput, low latency computation. Field programmable gate arrays (FPGAs) offer a programmable hardware platform that allows custom architectures for such search operations to be highly efficient. However, the programming paradigms are low level and cumbersome. Hence, alternative approaches based around flexible architectures and custom compilers are required to allow such platforms to be exploited further.
The NAIL Framework allows multiple hardware accelerators on an FPGA to be integrated within a general computing framework to accelerate a set of workloads that can benefit from parallelisation on FPGA. At present this is proprietary and restricted, with a fixed architecture that can combine a fixed number of static accelerator units, determined in advance, on an FPGA, and a software framework that interacts with this architecture to offer programmers the required interface. NAIL is presently deployed as an accelerator in standard servers.
This project aims to support the broader release of the NAIL Framework, porting to Xilinx devices, support for partial reconfiguration to allow dynamic accelerator loading, and to lay the groundwork for more ambitious evolution, such as to direct-network-attached accelerators. This will allow this framework to find wider use, enable the recruitment of more experienced engineers, and offer the benefits of alternative deployment scenarios and expanded use within the defence community. In addition, this works supports broader aims of exploiting heterogenous architectures to improve performance, efficiency, and flexibility of analytics systems.
"
https://www.turing.ac.uk/research/research-projects/environmental-monitoring-blending-satellite-and-surface-data,"
Satellite sensors can now provide an amazing level of detail of the Earth surface, yet with sparse and imperfect surface sensors to validate them, and due to their relatively short record (a few decades) their usefulness when used on their own is somewhat limited. To make new leaps in understanding environmental change and to improve prediction we must find intelligent ways to combine satellite data with surface sensors and the output from physics-based environmental simulators (e.g., climate models). To bridge these spatial scales and various modalities we are creating a team of scientists and engineers to build and deploy open-source toolkits and examples driven by real-world case studies.
This project aims to lay the foundations for ambitious research programmes to tackle our greatest environmental changes. We will develop and deploy reproducible and interpretable methods to increase scientific understanding, build tools to help environmental measurement planning, and provide the underpinning tools for intelligent real-time monitoring. The project aims to strengthen collaborations across the environmental and machine learning/AI communities by enhancing open-source and user-friendly computing platforms, including Pangeo.
The project will help researchers around the globe to access and make more efficient use out of the vast volumes of environmental data collected by national research facilities, often in somewhat ad hoc and sometimes unplanned ways. This work will support broader scientific communities where sensors are periodically upgraded (e.g., earth observation satellites), where there are sparse sensor networks (e.g., Antarctica, high mountainous regions), or where placement is not by design (e.g., ship-based sensors for ocean monitoring). The research will inform future planning of sensor development, placement and implementation.
"
https://www.turing.ac.uk/work-turing/studentships/enrichment/support,"

The Turing is committed to supporting its students to get the most out of their Enrichment experience and increasing access to the scheme. We are always open to discuss individual circumstances and make reasonable adjustments for students. This page gives information on how we support students on the Enrichment scheme.
On the 25 January at 12:30 we are offering an Enrichment Application Webinar to help students with writing their application. The webinar will feature members of the Academic Recruitment Team at the Turing as well as members of our research community who review applications. We’ll discuss:
This webinar is open to all potential applicants and will be held on Tuesday 25 January 12:30-13:30. Sign up is now open via Zoom. If you can’t make it, don’t worry, we’ll share all the resources plus a recording after the event.
To support the additional costs that come with an Enrichment Placement Award, the Turing offers a stipend of £450 a month. This can be used flexibly to cover relocation costs or commuting from your current home. Unlike previous years, this is now offered as a flat rate to all students on a Placement Award. 
Awards are paid to the student through the university, which is why we suggest you speak to your university before applying
Please contact us at [email protected] or on 0203 862 3578 if you would like to discuss this further.
For information on the Turing's guidelines for Enrichment students on attendance and leave, including parental leave and sick leave, you can view our current guidelines. Please be aware these guidelines will be updated annually so may change before you start your placement. 
As part of our commitment to inclusion, we recognise that some groups face additional barriers to participating in the Enrichment scheme. For this reason we offer an Access Fund to support students to participate who otherwise would not be able to due to a disability, caring responsibility or financial hardship. The fund provides expenses awards of up to £500 per month.
Students with caring responsibilities may for example choose to use the fund to support additional childcare costs, or a student with a disability may use the fund to secure appropriate accommodation or support while on placement.
You can apply for the fund on the application form. Applications will be assessed separately by the Student Services Team and will not be seen by reviewers. For more details please see our FAQs.
We want to ensure all our students are well supported at the Turing. This requires students to share relevant information with us about what they need support with. Please be assured that anything you do share will be treated in confidence. We encourage candidates to let us know about any reasonable adjustments they may require for the application process or their placement. You may already know what you would like to request, or you may like to have a general discussion of your options.
There are opportunities to share this information on your application form and at the onboarding stage if you are offered a place. This information will not be seen by reviewers or panels assessing applications. You can also contact us at any time to discuss adjustments at [email protected] or on 0203 862 3578.
We have dedicated academic services support to provide help and advice throughout your time with us including weekly drop-in sessions. Enrichment students have access to our Employee Assistance Programme which provides a 24-hour telephone helpline among other things.
For research advice academic mentors and members of the Research Engineering Group can be approached for support and advice. The Turing can also offer support in finding accommodation by signposting students to useful sources of advice and accommodation.
"
https://www.turing.ac.uk/people/researchers/andrew-dowsey,"
Andrew Dowsey is Chair of Population Health Data Science at the University of Bristol, a joint post between the Department of Population Health Sciences and Bristol Veterinary School, where he is Research Director for Bristol Veterinary School. His data science expertise lies at the interface of health sciences and engineering, having previously held positions in the Department of Computing and Institute of Biomedical Engineering, Imperial College London (EPSRC Postdoctoral Fellowship), the Institute of Human Development, University of Manchester (Lecturer), and the Department of Electrical Engineering and Electronics, University of Liverpool (Reader), together with visiting appointments at the University of Texas MD Anderson Cancer Centre and the Conway Institute of Biomolecular and Biomedical Research, University College Dublin.
Andrew’s research focus is on the facilitation and acceleration of health sciences research through novel statistical modelling and machine learning methodology, as well as data collection and management platforms. His team works in highly multidisciplinary environments to both support and lead investigations across three strands of research: Profiling the protein and metabolite content of biological fluids and tissues using mass spectrometry proteomics and metabolomics for understanding disease mechanism, and to discover biomarkers for clinical diagnostics; Constructing secure research platforms from population-level agricultural, environmental and health data to inform policy on One Health challenges such as global food security and the transmission of anti-microbial resistance; Intensive longitudinal health, activity and behavioural monitoring at the level of individual animals and groups of animals.
Andrew is leading implementation of the John Oldacre Centre for Sustainability and Welfare in Dairy Production at Bristol Veterinary School, with the aim of create the world’s most intensively monitored livestock cohort at our dairy farm. The Centre will provide blanket 24/7 surveillance of each animal through video tracking, activity wearables, measurements of individual feed and emissions, environmental monitoring, and recording of veterinary assessments. While such infrastructure is key for developing future ‘precision farming’ approaches for optimising farm management in the livestock industry, our Centre is particularly focussed on harnessing this data to tackle fundamental grand challenges affecting human and animal health globally. These include the need to better understand what constitutes and predicts animal resilience, health and welfare, and what factors damage sustainability by increasing the total economic and social burden with respect to competing land use, climate change, and the effects of antimicrobial resistance.
The aim of Andrew’s work at the Turing is to build the foundations for a comprehensive open research data platform for the John Oldacre Centre. This involves several interesting data science challenges around the robust monitoring of individual animals and the integration of heterogenous data streams, with many more opening up once the platform is achieved. In this Turing project, we will work with Dr Tilo Burghardt, Department of Computer Science, University of Bristol, on extending his deep learning methodology for species localisation and individual animal biometrics robust to the movements and interactions of our 200+ Holstein cattle.
"
https://www.turing.ac.uk/events/turing-intel-projects-data-science-and-ai-scale-programme,"
Agenda
There will be a scoping workshop for 3 projects, and we invite all interested members of the Turing community to participate. These translational projects are very well-suited to a collaborative approach with multiple academics interacting with a postdoc or research software engineer, and relevant teams in Intel.
The aim is that research proposals for projects in this area will be formed during the workshop. Members of Intel who drive hardware pathfinding and innovation in these areas will be present, physically and possibly virtually, and will present their views on the project areas for the first 30 minutes of each session.
This is therefore an excellent opportunity to engage with them on these topics. The three projects all involve research providing insight into future Intel architectures for Data Science and Artificial Intelligence, and can broadly be categorized as relating to:
For the second and third of these, very little has been determined; we are looking for principal investigators with their own vision for what should be pursued. The initial project should run for 6-12 months, and the resource available is the equivalent of a postdoc or research software engineer. If the direction of research is promising, we would then seek to extend the scope and duration of the project, and commit additional resources.Please email the Programme Director, Anthony Lee [email protected] with any questions about the workshop. If you are unable to attend but have strong views in these areas, please also email them so that your views can be represented

"
https://www.turing.ac.uk/people/researchers/peter-flach,"
Peter Flach is Professor of Artificial Intelligence at the University of Bristol. His main research is in the area of mining highly structured data and the evaluation and improvement of machine learning models using ROC analysis. He has also published on the logic and philosophy of machine learning, and on the combination of logic and probability. He is author of Simply Logical: Intelligent Reasoning by Example (John Wiley, 1994) and Machine Learning: the Art and Science of Algorithms that Make Sense of Data (Cambridge University Press, 2012).
Professor Flach is the Editor-in-Chief of the Machine Learning journal, one of the two top journals in the field that has been published for over 25 years by Kluwer and now Springer-Nature. He was Programme Co-Chair of the 1999 International Conference on Inductive Logic Programming, the 2001 European Conference on Machine Learning, the 2009 ACM Conference on Knowledge Discovery and Data Mining, and the 2012 European Conference on Machine Learning and Knowledge Discovery in Databases in Bristol. He is a founding board member of the European Association for Data Science (EuADS.org). 
The fundamental question of measurement theory is: how to assign meaningful numbers to objects that are not themselves numbers. Issues of measurement are of particular importance in inductive sciences including data science and artificial intelligence, for example when one assesses the capability of models and learning algorithms to generalise beyond the observed data.
Nevertheless, measurement concepts are underdeveloped in data science and AI, in at least the following senses: (i) a wide-spread under-appreciation of the importance and effects of measurement scales; and (ii) the fact that in most cases the quantity of interest is latent, i.e. not directly observable. This Pilot Project will seek to advance understanding of capabilities and skills of models and algorithms in data science and AI, and how to measure those capabilities and skills. Just as psychometrics has developed tools to model the skills of a human learner and develop standardised (SAT) tests, there is a need for similar tools to model the skills of learning machines and for standardised benchmarks which will allow skill assessment with only a few well-chosen test sets.
"
https://www.turing.ac.uk/research/research-areas/machine-learning,"
"
https://www.turing.ac.uk/research/research-areas/algorithms/data-structures,"
"
https://www.turing.ac.uk/people/guest-speakers/marc-raibert,"
Marc Raibert is Founder and Chairman of Boston Dynamics, a company that creates some of the world’s most advanced dynamic robots, such as BigDog, Atlas, Spot and Handle.  These robots are inspired by the remarkable ability of animals to move with agility, dexterity, perception and intelligence.  A key ingredient of these robots is their dynamic behavior, which contributes to their effectiveness and versatility in the real world.  Before starting Boston Dynamics, Raibert was Professor of Computer Science and Robotics at MIT and Carnegie Mellon.  While at CMU and MIT Raibert founded the Leg Laboratory, a lab that helped establish the scientific basis for highly dynamic robots and that set the stage for the work done at Boston Dynamics.   Raibert is a member of the National Academy of Engineering. 
"
https://www.turing.ac.uk/blog/can-ai-help-us-talk-whales,"
Header image credit: a humpback breaches (c) fernando reyes 
We spoke to Turing Lecturer and nature filmmaker Tom Mustill about his encounter with a whale in California, how AI can help us speak to whales and what developments in AI and animal communication he’s most excited about. 
In 2015 I was kayaking with my friend in Monterey Bay, California, when a humpback whale the size of three T-Rexes leapt out of the ocean and landed on us. A nearby tourist captured the moment by chance. We nearly died, and scientists who analysed the video told us we only survived because the whale saw us and turned to avoid us at the last minute. 
As a result of this encounter I learnt about how artificial intelligence (AI) is being used to decode patterns in animals. Researchers were able to use AI to identify the whale - I now know how old he is, who his mother is and where he was born. I can follow his life every time another tourist takes a photo of him and the AI tool HappyWhale WhaleID identifies him.
As a biologist who has spent a lot of time squinting at rare birds in forests or at pilot whale fins in the sea trying to identify them, this was a revelation to me. I then learnt that other AI tools are being used to find patterns in animal communications, with big expeditions currently working to decode whale speak. I became fascinated by this.
A perhaps unsurprising problem with learning to speak whale is that we are humans! The sea is dangerous and vast and we are badly adapted to listen to their wild lives. Now recording robots, and other devices, can go there for us and listen to their communications without disturbing them. This includes the young ones learning to talk. We can now see the context of how they talk, what is happening, which whales are speaking, and can start to discover patterns that give clues as to whether they have rules in their ‘speech’, like those of our language. 
Another problem humans have with looking for other kinds of language is that we are trapped in our own ways of thinking - we might not be able to see other kinds of language because we can only try to find patterns similar to our own language. This is where AI comes in. It can see patterns we miss and do things that we cannot on vastly bigger scales.
This is what is really exciting. Current expeditions are hoping to record huge samples of wild whale communications - machine learning and other AI analysis can then help us find patterns in these recordings that we cannot perceive. This has already been demonstrated in human languages, where AI has discovered patterns that humans are unable to detect in large datasets of written or spoken language, and translate between them in a totally different way than human translators - this is how Google Translate works.
That there are fish that live in the anus of sea cucumbers! Also that orcas have teamed up with humans to go hunting in Australia, and other orcas had a craze for wearing salmon hats. 
An exciting development is that these tools are becoming more general purpose and available to the public. Anyone can already use AI on their phones to identify birds by their song, and plants from photographs . I don't think it is far-fetched to think that soon we will be able to take a picture of something in nature, like a bird in a tree, and  then find out information such as what species the animal is, who the individual is, what tree they are sitting on, what rock that tree is growing on, how old the animal is and perhaps even what they are chirping. 
I think if quantum computing takes off, this will be huge. If biology is the study of finding patterns in nature, then making pattern-finding tools vastly more effective and powerful will reveal new patterns in nature.
Hope and wonder. I also hope people will learn that there are still many things we don't understand about how animals communicate, that technology is not always bad for nature and that some of these tools can help us heal our broken relationship with other living creatures.

"
https://www.turing.ac.uk/events/data-science-uk-secondary-education-supporting-humanities-and-languages,"
Online learning is expanding rapidly in UK secondaries. Thousands of pupils and teachers login daily to online learning platforms to access content and engage in learning activities, creating unprecedented amounts of data. How can we use such data to help young learners develop strong language and writing skills?
This workshop will explore how data science might build on the rapid expansion of online learning platforms in UK schools to improve the language development and writing skills of students in the humanities and languages in the UK state school sector. Academics, together with teachers and the educational industry will explore how to frame a partnership for data sharing and educational AI applications in order to support language and literacy and improve outcomes in the UK state school sector.
The workshop will include research talks and a panel discussion, followed by break-out sessions where participants consider the areas where big data from schools and writing analytics could help improve learning outcomes in language and literacy as well as questions around ethical data sharing and privacy and research infrastructure. We expect contributions from researchers in natural language processing, machine learning, writing analytics, linguistics and education as well as stakeholders, e.g. the Chartered College of Teaching, the National Subject Association for English as an Additional Language (NALDIC), the Royal College of Speech Therapists and industry providers.
The workshop is open to researchers in data science, language and education, secondary school teachers and managers, government, educational companies, assessment bodies and educational charities.
There are two ways to join the workshop: in-person at Murray Edwards College in Cambridge, or online via Zoom. Registration is essential.
To learn more and register, click here. 
"
https://www.turing.ac.uk/people/researchers/christopher-nemeth,"
Dr Chris Nemeth is a senior lecturer in Statistics in the Department of Mathematics and Statistics at Lancaster University. He is interested in problems pertaining to Bayesian machine learning and computational statistics, with a particular focus on Markov chain Monte Carlo and sequential Monte Carlo techniques. His current research is focused on Bayesian methods for large-scale datasets and tools for online learning. He also works on problems related to Gaussian processes and network modelling. Chris currently holds a UKRI-EPSRC Turing AI Fellowship on Probabilistic Algorithms for Scalable and Computable Approaches to Learning (PASCAL) and formerly an EPSRC Innovation Fellowship on Scalable and Exact Data Science for Security and Location-based Data. Chris is currently the chair of the Computational Statistics and Machine Learning section of the Royal Statistical Society and Associate Editor for the Data-Centric Engineering journal.

"
https://www.turing.ac.uk/people/researchers/chris-barnes,"
"
https://www.turing.ac.uk/people/guest-speakers/percy-liang,"
Percy Liang is an Associate Professor of Computer Science at Stanford University (B.S. from MIT, 2004; Ph.D. from UC Berkeley, 2011) and the director of the Center for Research on Foundation Models. His research spans many topics in machine learning and natural language processing, including robustness, interpretability, semantics, and reasoning. He is also a strong proponent of reproducibility through the creation of CodaLab Worksheets. His awards include the Presidential Early Career Award for Scientists and Engineers (2019), IJCAI Computers and Thought Award (2016), an NSF CAREER Award (2016), a Sloan Research Fellowship (2015), a Microsoft Research Faculty Fellowship (2014), and multiple paper awards at ACL, EMNLP, ICML, and COLT.
"
https://www.turing.ac.uk/people/guest-speakers/xilin-jiang,"
Xilin Jiang is a statistical geneticist working on the genetic architecture of complex diseases and its implications in clinical practice. Specifically, his research focuses on constructing risk factors using many variables that each have a small effect on diseases. The variables include genotype data, proteomic data, gene expression data and hospitalisation data (EHR). Xilin likes to think of disease as a stochastic process and disease risk on the “liability” scale. In statistical terminology, his research involves Bayesian inference for high dimensional data, longitudinal analysis and causal inference.
Xilin gained his DPhil in Genomic Medicine and Statistics from Oxford University, funded by a Rhodes Scholarship and a Wellcome Trust studentship and delivered the scholar address (name for a student speech delivered at the scholarship graduating ceremony) for the Rhodes scholar class of 2017. At the beginning of Pandemic, he worked as a consultant to the Gates Foundation and China CDC. Prior to his DPhil, Xilin worked on neural imaging modelling, analysing both structure and functional MRI data.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/manchester-metropolitan-university,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/international-institute-tropical,"
"
https://www.turing.ac.uk/people/external-researchers/matthew-sperrin,"
"
https://www.turing.ac.uk/people/external-researchers/samia-nefti-meziani,"
"
https://www.turing.ac.uk/news/publications/system-generating-non-uniform-random-variates-using-graphene-field-effect,"
We introduce a new method for hardware nonuniform random number generation based on the transfer characteristics of graphene field-effect transistors (GFETs) which requires as few as two transistors and a resistor. We implement the method by fabricating multiple GFETs and experimentally validating that their transfer characteristics exhibit the nonlinearity on which our method depends. We use characterisation data in simulations of a proposed architecture for generating samples from dynamically selectable non-uniform probability distributions. The method we present has the potential for Gb/s sample rates, is reconfigurable for arbitrary target distributions, and has a wide range of possible applications. Using a combination of experimental measurements of GFETs under a range of biasing conditions and simulation of the GFET-based non-uniform random variate generator, we demonstrate a speedup of Monte Carlo integration by up to 2×. This speedup assumes the analogue-to-digital converters reading the outputs from the circuit can produce samples in the same amount of time that it takes to perform memory accesses.
N. J. Tye, J. T. Meech, B. A. Bilgin and P. Stanley-Marbell, ""A System for Generating Non-Uniform Random Variates using Graphene Field-Effect Transistors,"" 2020 IEEE 31st International Conference on Application-specific Systems, Architectures and Processors (ASAP), Manchester, United Kingdom, 2020, pp. 101-108, doi: 10.1109/ASAP49362.2020.00026.
 
NAN
"
https://www.turing.ac.uk/research/research-projects/bringing-social-city-smart-city,"
Technological developments, such as the rise in GPS enabled devices and Web 2.0 technologies have created social transformations in how we connect and share information through the mass uptake of smart phones and social media platforms. This new generation of mobile technologies work as individual sensors capturing data on a wide range of human behaviours that have been previously hidden. These include data on individual movement, preferences and opinions. Understanding these behaviours is crucial if we are to create a joined up approach to simulating how cities breath and grow.
However, considerable work is required in adapting and developing new technologies from machine learning to extract behaviours which can be embedded into cutting-edge modelling techniques. Creating this bridge between ‘big’ data representing the ‘real’ world, and simulations producing alternative versions of reality is of value to both academics and policymakers looking to develop new solutions to many of the challenges that today’s cities face. To do this we need to understand how factors within the “Social City” (the impact of individual movements and decisions) play out every day in the “Smart City” (data collected from fixed sensors on for example, traffic counts, air pollution or movements of populations).
Cities are complex systems. Understanding how individuals move around and use urban spaces is a challenging area. Combining sensor data (footfall, traffic) with social data (social media) allows researchers to understand who, where and why individuals are in cities. More importantly, it also allows researchers to construct simulations (using agent-based models or ABMs) that replicate how individuals will react under a number of scenarios.
Whilst agent-based models have become a very popular tool, creating accurate behavioural rule sets is difficult due to a lack of individual level data. This data is now available, but considerable work needs to be undertaken to link new and traditional data sets and then to analyse this data for patterns and relationships between individuals that will inform these models.
Understanding the relationships between individuals is a crucial factor to capture to build into accurate ABMs of city applications. This research will use causal inference modelling (from statistical epidemiology) to quantify these relationships and build them into ABMs.
Finally, this research will examine approaches for understanding and quantifying the uncertainty associated with ABMs. The goal in this area is to examine approaches from the physical sciences that will allow social scientists to provide a level of confidence with their simulation outputs (i.e. to understand where the model under and over predicts).
This research has the following aims:
This work is of interest to any sector or industry that is interested in simulating and understanding how individuals move and interact in urban spaces.  In particular, understanding the relationships in populations and the impact on these relationships with the introduction of a new policy (e.g. free transport). This work will also create methods for handling and quantifying uncertainty within simulations.
Ultimately this research could be useful for any stakeholders and industry who are interested in better understanding how cities work, quantifying uncertainty and understanding the implications of implementing different scenarios and policy interventions.
[email protected]
"
https://www.turing.ac.uk/courses,"

Free and open learning resources on Data Science and AI topics.

Course
Course overview This course focuses on incorporating ethical AI principles into the systems design process and is taught using a...
Course
Course overview This course aims to address the priority of transparency in responsible AI. We will study both transparent machine...
Course
Course overview This course is offered via the AI Standards Hub and requires a separate registration. If you have limited...
Course
Course overview This course is offered via the AI Standards Hub and requires a separate registration. This course is designed...
Course
Course overview This expert-level course deepens and further advances the skills in the intermediate-level course. It is taught using a...
Course
Course overview This course is alternatively titled ""How learning to lie with data is essential to prevent AI from being...
Course
Course overview Artificial Intelligence is widely used in sensitive domains such as healthcare, insurance, recruitment and credit scoring. In these...
Course
Course overview This course covers the fundamentals of algorithmic fairness, the most common biases in machine learning, and the importance...
Course
Course overview AI has transformative potential in healthcare by delivering consistent, high-quality care for patients while relieving workload pressures in...
"
https://www.turing.ac.uk/people/research-engineering/carlos-gavidia-calderon,"
Carlos Gavidia-Calderon is a researcher, software developer, and educator. He is currently a Research Software Engineer at The Alan Turing Institute.
He graduated in 2020 from University College London with a PhD in Software Engineering, under the supervision of Earl T. Barr, Federica Sarro and Mark Harman. As a postgraduate researcher, Carlos studied cooperation in software development teams using game-theoretic models. He also has a Masters in Computer Science from Pontificia Universidad Catolica del Peru; with a dissertation on ant colony optimisation algorithms applied to computer vision. Previously, Carlos was an undergraduate student of Systems Engineering at Universidad Nacional de Ingenieria.
As a software engineer, Carlos worked on projects ranging from mobile applications to large-scale information systems. His industry roles included senior software engineer and technical lead. He has also taught courses on programming fundamentals and algorithms to undergraduate engineering students.
At the Alan Turing Institute, Carlos is developing privacy-aware tools for generating synthetic data. He is also actively involved in activities for widening the participation of students from diverse backgrounds in Data Science and Artificial Intelligence.
"
https://www.turing.ac.uk/news/publications/ai-assistants-framework-semi-automated-data-wrangling,"
Data wrangling tasks such as obtaining and linking data from various sources, transforming data formats, and correcting erroneous records, can constitute up to 80% of typical data engineering work. Despite the rise of machine learning and artificial intelligence, data wrangling remains a tedious and manual task. We introduce AI assistants, a class of semi-automatic interactive tools to streamline data wrangling. An AI assistant guides the analyst through a specific data wrangling task by recommending a suitable data transformation that respects the constraints obtained through interaction with the analyst. We formally define the structure of AI assistants and describe how existing tools that treat data cleaning as an optimization problem fit the definition. We implement AI assistants for four common data wrangling tasks and make AI assistants easily accessible to data analysts in an open-source notebook environment for data science, by leveraging the common structure they follow. We evaluate our AI assistants both quantitatively and qualitatively through three example scenarios. We show that the unified and interactive design makes it easy to perform tasks that would be difficult to do manually or with a fully automatic tool.
Petricek, Tomas; van den Burg, Gerrit J. J.; Nazábal, Alfredo; Ceritli, Taha; Jiménez-Ruiz, Ernesto; Williams, Christopher K. I. ""AI Assistants: A Framework for Semi-Automated Data Wrangling"". IEEE Transactions on Knowledge and Data Engineering 2022
2 MB
"
https://www.turing.ac.uk/people/honorary-fellows/mark-briers,"
Mark Briers a Honorary Fellow of The Alan Turing Institute, and was formerly Programme Director for the defence and security programme. Prior to joining Turing, Mark worked in the defence and security sector for over 16 years, directing research programmes in the area of statistical data analysis, and leading large teams to drive impactful research outputs. He completed his PhD in 2007 at Cambridge University where he developed Sequential Monte Carlo based techniques for state-space filtering and smoothing. In recent months, Mark has been providing independent advice to scope the technical development and help to oversee modelling and analytics of the NHS Test and Trace app.
He is an Honorary Senior Lecturer at Imperial College London, where he teaches methodological techniques for use in a Big Data environment and conducts research into statistical methods for cyber security, and he is a Council Member at the Royal Statistical Society. He is an Industrial Fellow alumnus of the Royal Commission for the Exhibition of 1851. Mark is a co-investigator on the EPSRC funded programme grant Computational Statistical Inference for Engineering and Security.
Mark's publications are listed on Google Scholar.
"
https://www.turing.ac.uk/research/interest-groups/visualization,"
Visualization research and innovation has become critical to data science, it bridges the gap between digital data and human cognition. It is also emerging as an important methodology for helping visualize how machine learning and AI systems arrive at decisions, while clearly illustrating any bias in those decisions. The visualization interest group (VizTIG) meets regularly at the Turing, inviting internal and external partners to discussions, organising workshops and symposium, and contributing to national strategy on visualization research and innovation.
Visualization research and innovation covers a wide range of activities in image synthesis intended to provide timely insight into datasets. It has application across all of the Turing's challenge areas whenever a data result needs to be understood by people.
The theoretical basis for visualization methods is varied but three topics have a strong influence on the field: computer science, applied statistics and human psychology. Computer science provides an understanding of efficient algorithms and data structures, applied statistics provides analysis and summarization methods and human psychology helps define the limits of human perception and cognition.
Visualization has emerged in data science and AI as a fundamental technology enabling human understanding of complex data and automated decision processes. There remains a challenge for visualization methods to keep pace with the scale and complexity of these activities. This needs research and innovation to create and deliver visualization tools that can continue to deliver technical, economic and social benefit.
VizTIG brings together a community of researchers and innovators in visualization for data science and AI that have few similar forums in which to meet. A key task is to map the landscape for visualization in the UK and highlight new challenges to focus the group's other activities around. These will include input to strategic planning, seminars, workshops and an annual symposium.
Challenges: Big data provides the challenge of ever increasing dataset size that needs to be visualized for humans whose abilities are staying the same. AI provides the challenge of not simply portraying automated decisions but also explaining complex automated outcomes to people.
Example output: As one example of managing data scale the 'TeraScope' output from the 'Automating data visualisation' seedcorn project uses interactive trillion pixel images to enable visualization of urban data at scale from the whole city to a single desk in a building. See the related paper here.
Challenges: It is not just end users of data pipelines that can benefit from visualization. The engineers and scientists who build and tune data science and machine learning systems could be better supported with tools that help: examine and prepare data for models; monitor and understand the model training process; compare the qualities of resultant models; explain the choices underpinning, and uncertainty of final model outcomes.
Challenges: The limits of the HVS are exploited widely in image compression and internet standards such as JPEG and MPEG. Visualization methods make some direct use of results from HVS research, but there is a key challenge to use more evidence to help produce clearer and faster to comprehend visualization. Part of this is to also extend our understanding of the HVS in the context of visualization tasks.
We are happy to hear from any academic, public sector, NGI or commercial body that has an interest in visualization and would like to suggest activities or provide input to VizTIG.
Please contact Nick Holliman and the interest groups team in the first instance at [email protected] and [email protected].
293.7 KB
"
https://www.turing.ac.uk/people/researchers/andy-smith,"
Andy is a Research Data Scientist in the Turing's Research Engineering Group.
Before joining the Turing, he worked for a small charity who provide mapping and information management support to humanitarian operations for major international emergencies. Over the years, his roles included everything from fixing the printers to leading teams in post-disaster environments, juggling both the operational and analytical sides of the work.
Prior to that, he completed his PhD at Newcastle University, exploring flood risk in the UK, modelling the rainfall scenarios likely to cause the most widespread and damaging floods.
Given the choice Andy will always opt for projects with a geospatial component, but he enjoys the interdisciplinary nature of the work in REG.
 
"
https://www.turing.ac.uk/people/guest-speakers/karen-elliott,"
Dr Karen Elliott is a Senior Lecturer in Enterprise/Innovation (FinTech) at Newcastle University Business School. Her passion surrounds ‘FinTech’; a major growth industry in the UK and a burgeoning sector in the North East region. Karen bridges an industrial background/academic career and recently named as ‘Standout#35 Women in FinTech Powerlist 2019 by Innovate Finance’. She co-leads with Prof van Moorsel, a £1.2m EPSRC Research Project (FinTrust) in 2018 collaborating with Durham Based nationally renowned Atom Bank on optimising FinTech services combined with exploring identity verification for financial inclusion funded by the Bill & Melinda Gates Foundation. FinTrust examines the issue of trust, gender diversity, security and ethical AI investigating increased automation and trust engineering to deliver consumer transparency. 
"
https://www.turing.ac.uk/news/supercharging-sustainable-development-new-policy-priority-tool-turing-backed-undp,"
All over the world, countries are striving to meet the United Nation’s Sustainable Development Goals (SDGs) by 2030 – an ambitious agenda by any measure. These SDGs address the myriad global challenges faced by humanity, such as inequality, access to healthcare and education, climate change, and of course the COVID-19 global pandemic which has changed our world in ways previously unimagined.
Whatever the country, sustainable development is a transformational process impacted by targeted government funding and resources. Progress towards the 17 SDGs are monitored by the collection of data on over 200 “development indicators.” A new collaboration between researchers in the UK and Mexico has resulted in a suite of analytical tools that can successfully model the impact of a variety of policy decisions on development indicators which show progress towards the SDGs.
Today, Wednesday 27 May, the Turing publishes an impact story detailing this new AI approach, called ‘Policy Priority Inference’ (PPI), explaining how it is being championed by the United Nations Development Programme (UNDP) and is already in practice across Latin America, with ambitions to bring it to other global regions. It is increasingly critical that policymakers correctly prioritise public expenditure to ensure that these multi-dimensional global goals are met, especially as new challenges loom large on the horizon.
ESRC/Turing Fellow Omar Guerrero from University College London, with his research partner, Professor Gonzalo Castañeda of the Center for Research and Teaching in Economics in Mexico are pioneering a new approach with potential to supercharge the effectiveness of government-backed sustainable development to the benefit of billions of people—and the planet itself.
Guerrero explains how the tool can be used today to keep governments on track despite setbacks from the pandemic: “Recently, governments around the world have had to destine substantial resources in order to fight the COVID-19 pandemic; preventing them from achieving their original goals. This is extremely important for the 2030 Agenda of the SDGs, to which all UN Member States have committed.
PPI can support reaching the SDGs by helping answering questions such as: How long will it take to reach the SDGs? How can governments reallocate their public expenditure in order to minimise delays? What countries/goals will experience the longest delays? What synergies between SDGs should be promoted?”

Prioritising issues for maximum impact is an enormous challenge for governments. The range of development policy options is countless, often with unanticipated inefficiencies that waste resources. And, crucially, there are complex interdependencies between policies that should be taken into consideration (e.g. investing in industrialisation tends also to produce negative outcomes for the environment, while investing in public transport might also boost education outcomes because more children become able to access school.)
Previously, economists focused on GDP as a measure of development, but this is a blunt, unidimensional measure and is ill-equipped to monitor progress on the 17 SDGs and over 200 development indicators today. Modelling these sorts of complex scenarios is exactly the sort of ‘wicked’, long-running policy challenge the Turing’s Public Policy programme is committed to working on with policymakers around the world and that cutting-edge data science and AI technology can make a huge impact on.
PPI builds on a behavioural computational model, taking into account the learning process of public officials, coordination problems, incomplete information, and imperfect governmental monitoring mechanisms. The approach is a unique mix of economic theory, behavioural economics, network science and agent-based modelling.
“The results of this project show the potential the Policy Priority Inference model has for providing governments with concrete information on how to increase the effectiveness of public spending and accelerate the achievement of development goals,” says Annabelle Sulmont, Public Policy Project Coordinator for the UNDP office in Mexico. “The model also provides a common language that enables its implementation in other parts of the world, and facilitates sharing and comparing results across regions and countries.”
“Government expenditure data will take this technology to a whole new level,” says Guerrero. And PPI is not only about government, Guerrero stresses, but also about accountability. “We want to bring these tools to NGOs too, because this is useful for them to assess the actions of governments. NGOs can check if governments are prioritising the right policies.”
Media enquiries
Beth Wood
Press and Communications Manager
The Alan Turing Institute
T  +44 (0)20 3862 3390
M +44 (0)75 3803 8168[email protected]Vanessa Hidalgo
United Nations Development Programme
T +1 646 3389462[email protected]
Notes to Editors:
"
https://www.turing.ac.uk/eduroam-turing,"

The Institute is a participating organisation in eduroam(UK). ‘eduroam’, which stands for education roaming, is an international initiative that allows members of educational establishments to gain internet access at other member organisations easily and securely using credentials from their home organisation. Find out more about eduroam.
The Turing qualifies as both a home and visited organisation for eduroam services:
Home organisation
This allows Turing users who are visiting an institution which is a member of eduroam (including eduroam(UK)) to access the network using their Turing credentials; normally in the format of [email protected]
Visited organisation
This allows visitors from other member institutions to the Turing to access the network using the credentials provided by their home institution. If your institution isn’t a participating organisation you will need to use the guest wifi.
To do this follow the instructions below for the device you have:
"
https://www.turing.ac.uk/research/research-projects/computational-models-meaning-change-ancient-greek,"
Words mean different things depending on time, context, and people (think of the recent new social media meaning of 'tweet'), which represents a challenge with any unstructured datasets. Computational research has made great advances to find meaning change in language using textual data, but has not worked on ancient languages nor has engaged humanists so far, who can offer invaluable expertise in designing and validating these systems. This highly interdisciplinary project is the first one to focus on an ancient language and has taken the first steps towards building computational models of meaning change that engage classicists.
Bayesian computational models of meaning change (which infer temporal meaning representations as probability distributions over words) have been proposed to find meaning change in texts (Frermann & Lapata 2016). This project incorporates expert-driven knowledge (specifically on the genre of texts) to further improve the models.
The choice of Ancient Greek has several reasons. Its scholarship provides excellent validation data (we know the outcomes) and external knowledge bases (for example of genre of texts). Its words have particularly many different meanings. There are top-quality transcribed texts (no need to correct OCR errors), which enables applications to born-digital texts. Greek has its own language family, unlike Latin (Romance) and English (Germanic); confounding factors from languages of the same family do not apply to Greek, making this a more controlled environment and an ideal testbed for applications to modern languages.
This work is very relevant to humanities scholarship for the investigation of word and concept change. It can also be applied in the context of historical semantic search of large historical text collections to make it possible for users to look for words with different meanings in different historical periods.
"
https://www.turing.ac.uk/events/nature-reviews-physics-machine-learning-research-industry,"
Machine learning is no longer restricted to data analysis. It is currently used in theory, experiment and simulation. This is a sign that AI is becoming pervasive in all traditional aspects of research. However, are theorists, experimentalists and computational scientists aware of each other’s problems and the solutions developed to tackle them? Are researchers working in different areas of physics aware of developments in other areas? Industry is creating powerful AI tools that are expected to revolutionise science but how can academic researchers make the most out of them? This session explores how industry-academia collaborations work, how scientists can access industry-developed tools and how they can best benefit from them while feeding back their needs to shape future developments.
Chair - Professor Tony Hey 

In this event we will hear from Professor Anima Anandkumar, Dr. Tie-Yan Liu and Professor Max Welling.
Dr. Tie-Yan Liu:  AI-for-Science – the next wave of artificial intelligence
In previous decades, AI has achieved notable success in computer vision, speech recognition and natural language understanding. However, mimicking human vision, speech and language capabilities is just a shallow aspect of AI. It neglects the fact that we, as human beings, are unique because of our courage and ability to discover and change the world. AI-for-Science aims to build powerful tools to help natural scientists to better discover and change the world. Specifically, AI-for-Science assumes that the physical world can be theoretically characterised by fundamental scientific equations, usually at a very large scale. It also acknowledges that there is always a gap between theory and reality, and the evidence of the gap can be found in experimental data. No one has the capability to efficiently solve all those complex scientific equations, analyse huge amounts of experimental data, or create a closed loop between them. This is exactly where AI could play a disruptive role. As a showcase of such disruptions, I will introduce several research projects at MSR AI4Science, including Graphormer, an AI model for molecular dynamics simulation, DeepVortexNet, a neural PDE solver for fluid dynamics, SciGPT, an AI language model to automatically extract knowledge from scientific literature, and LorentzNet, an equivariant AI model to detect new particles from large-scale jet data. After introducing these works, I will also discuss some future trends of AI-for-Science research. 
Professor Max Welling: Boosting scientific discovery using machine learning
The tools of AI, ML and DL are starting to transform the way we do science. In particular, we are seeing the start of a new paradigm where data generation through ab initio simulation of physical processes is recycled into machine learning models, which then accelerate these ab initio simulation models. For instance, when performing MD simulations of molecules, one would need quantum mechanical calculations to calculate the contribution of electrons. However, the information from many such simulations can be stored in so-called machine-learned force fields which can in turn accelerate the MD simulation. Analogous paradigm shifts are emerging for DFT calculations and PDE solving to name a few. In my presentation, I will talk about how AI is expected to impact the natural sciences beyond just data analysis and in reverse how the tools from the natural sciences are also making a positive impact on ML research.


"
https://www.turing.ac.uk/people/external-researchers/walt-johnson,"
"
https://www.turing.ac.uk/people/guest-speakers/roshni-modhvadia,"
Roshni is a Researcher (Public participation & research) at the Ada Lovelace Institute. She is currently working on understanding public attitudes towards data and AI technologies.
Before joining Ada, Roshni was a researcher at BookTrust, a UK-based children’s reading charity. Her work explored the impact of representation on children’s reading habits.
Roshni has an MPhil in Psychology and Education from the University of Cambridge.
"
https://www.turing.ac.uk/research/research-projects/radiation-detectors-and-machine-learning,"
Scintillator detectors capture signals from multiple types of incident radiation and it's necessary to discriminate between these different types with both high accuracy and high computational efficiency. Data-driven approaches can learn to accurately discriminate between complex noisy signals and uncover the true underlying distribution of the data, and numerous frameworks have been developed to generate high performance models. Developing a successful method would produce a better signal-to-noise ratio than existing methods while keeping the latency costs low enabling more efficient neutron scintillator detectors for facility science, which has application to a wide variety of physical, biological, and material sciences.
Header image: Science and Technology Facilities Council
Scintillator detectors capture incident radiation to produce an excitation in the form of emitted light which is then converted to an electronic signal which can be digitised and recorded. Neutron scintillator detectors detect both incident neutron and gamma particles. The gamma particles are generally considered noise which one wishes to ignore, while neutrons provide the signal from which one can infer structural properties of matter. The discrimination of the neutron pulse shape from the gamma pulse shape is non-trivial and is further complicated by limited information at lower energies.
The aims of this project are to explore different computational approaches to solving the Pulse Shape Discrimination (PSD) problem for neuron-gamma scintillator detectors with application to ISIS Neutron and Muon Source. The desired outcome is to produce a technique which provides a better signal-to-noise ratio without creating additional latency overheads. 
An additional outcome will be to create a close collaboration between the Detector Technology group and the Scientific Machine Learning group based at Rutherford Appleton Laboratory to enable better cross facility engagement. 
 
Accurate and efficient pulse shape discrimination allows us to achieve a better signal-to-noise ratio in the detection of neutron events. A more accurate and efficient detector ultimately leads to a reduced count time, facilitating an increase in efficiency for experiments carried out a neutron sources such as ISIS Neutron and Muon Source.
"
https://www.turing.ac.uk/people/external-researchers/gabriela-ciocarlie,"
"
https://www.turing.ac.uk/people/researchers/aleksander-domanski,"
Dr Aleks Domanski is a Senior Research Associate at the University of Bristol and the Francis Crick Institute, London. Aleks was awarded a PhD by the University of Edinburgh in 2014 following a collaborative Wellcome-National Institutes of Health sponsored project on the network basis of distorted sensory processing in Autism, carried out jointly in Bethesda (USA). He moved to Bristol University for postdoctoral research in 2015 and studied the neural dynamics supporting animal decision making and learning. In 2020, funded by a Turing Biomedical Data Science Award, he joined the Neurophysiology section at the Crick as a visiting scientist to develop new computational methods to understand the role of multisensory integration in predictive coding in the brain. He holds joint appointments at the two institutions.
Aleks’ research focuses on understanding the collective dynamics of coupled sparse networks, and in developing mathematical, analysis and modelling frameworks for understanding network responses to perturbations. His work has applications in understanding the neural circuits controlling animal behaviour, quantifying biodiversity and inter-species interactions and monitoring urban environments. Additionally, he develops smart sensor networks for collecting data, both in the laboratory (e.g. wearable EEG) and for field observations (e.g. automated animal behaviour quantification, urban analytics). 
"
https://www.turing.ac.uk/people/spotlights/maria-liakata,"

My work focuses on Natural Language Processing (NLP). This is the field of Artificial Intelligence (AI) that is developing computational methods that can process human language to perform different tasks. These tasks could include summarising documents automatically or identifying the accuracy of information on social media.
Specifically, my team at Queen Mary University of London and the Turing are developing NLP methods to identify and understand changes in individuals' behaviour. This behaviour  is demonstrated through their language and other content they produce while interacting with others online or by using different devices.
We hope to create time-sensitive sensors from language and other heterogeneous digital content created by individuals. This will help create tools that can be used by individuals, as well as clinicians, to monitor mental health conditions.
How hard it is to work with mental health data. The challenges involved range from the size of datasets and the data collection process to how and whom you can share data and models with. As AI researchers we need to learn how to work with sensitive data so we can improve our understanding and monitoring of mental health conditions.
Being awarded a prestigious EPSRC/UKRI Turing AI Fellowship and becoming a full Professor of Natural Language Processing at Queen Mary University of London.
The Data Science for Mental Health Special Interest Group was set up to bring people from different disciplines (such as Maths, Psychiatry, Machine Learning) and backgrounds (academic, industry, general public) together to discuss topics on mental health research. We are motivated by a vision to develop data science methods that can provide a uniform approach to analysing and addressing a range of different mental health conditions.
Spending time with my four year old. Raising children is a wonderful and formidable project - no matter how many books you read on the subject and how well you know the theory. I try to sneak in some time for exercise (mainly walking and Pilates) to counterbalance all the hours spent sitting in front of my laptop and to help me keep up with my child's energy.
"
https://www.turing.ac.uk/media/news/intel-turing-programme-helps-detect-cancer-cells-accurately-hospital-artificial-intelligence/,"
"
https://www.turing.ac.uk/people/former-researchers/giovanni-colavizza,"
Giovanni is Assistant Professor of Digital Humanities at the University of Amsterdam and a Visiting Researcher at The Alan Turing Institute, where he was a Co-investigator on the Living with Machines project and convenes the AI for Arts special interest group.
He did his PhD at the Digital Humanities Laboratory of the EPFL in Lausanne, working on methods for text mining and citation analysis of scholarly publications, and is co-founder of Odoma, a start-up offering customised machine learning techniques in the cultural heritage domain.
Giovanni is primarily working on AI for cultural heritage and on the public understanding of science.
Prior to joining the Turing, Giovanni has been a researcher at Leiden University (CWTS), the Leibniz Institute of European History in Mainz, and the University of Oxford. He studied computer science (BSc) and history (BA, MA) in Udine, Milan, Padua and Venice in Italy.
"
https://www.turing.ac.uk/events/turing-lecture-learning-how-learn-efficiently,"

We are looking forward to welcoming Nando de Freitas, Principal Scientist at DeepMind.

Training a large neural network with lots of data and subsequently deploying this model to carry out specific tasks, such as speech recognition, machine translation, game playing, image recognition, image and text generation, text-to-speech, and lipreading has been incredibly fruitful.  
Instead of focusing on few tasks with massive amounts of data, this talk will focus on training neural networks to solve many tasks with few data each. The objective is not to learn a fixed-parameter classifier, but rather to learn a “prior” neural network that can be adapted rapidly to solve new tasks with few data. The output of training is no longer a fixed model, but rather a fast learner. That is, the goal is to build tools that learn. 
18:00-18:30 - Registration
18:30-18:35 - Welcome and introduction - Mark Briers (The Alan Turing Institute, UK)
18:35-19:25 - Learning how to learn efficiently - Nando de Freitas (Google DeepMind, UK)
19:25-19:40 - Q&A - Nando de Freitas and Mark Briers
19:40-20:30 - Drinks reception
 
Some technical knowledge required
King's Place
90 York Way, London N1 9AG
"
https://www.turing.ac.uk/people/feargus-pendlebury,"
Feargus is a PhD cybersecurity student with the Systems Security Research Lab at King's College London and the Information Security Group at Royal Holloway, University of London where his research explores the limitations of machine learning when applied to the security setting. He is supervised by Prof. Lorenzo Cavallaro and Prof. Johannes Kinder.
Prior to joining the Turing, Feargus was a PhD intern at Facebook where he worked with the Abusive Accounts Detection team to develop novel techniques for detecting adversarial behaviour on its social media platforms.
He is also the author and maintainer of TESSERACT, a framework and Python library for performing sound ML-based evaluations without experimental bias.
Feargus' research focuses on the challenges of deploying machine learning (ML) systems in hostile environments. While ML has shown to be effective in lab settings there are a host of complications that arise when used in the wild: e.g., adaptive attackers that seek to evade and poison classifiers; an evolving environment that results in concept drift; poor explainability that limits the use of ML as part of a larger analysis pipeline.
To date Feargus has demonstrated that previous work has vastly overestimated the performance of ML classifiers in security and shown that large-scale attacks against malware detectors are a reality by generating adversarial examples with automated software transplantation.
His work at the Turing continues to explore problems of dataset shift and adversarial ML while also exploring new techniques for the robustness, recovery, and interpretability of detection models.
"
https://www.turing.ac.uk/contact-us/join-our-mailing-lists,"

The main Turing newsletter is a lively compilation of our latest journalism: news stories and blogs from the Institute. The monthly newsletter also signposts to some of our new research papers, publications and case studies – with an emphasis on science, innovation and impact.
If you are having trouble viewing the form above please click here.

Sign up above.

 
The Events and Engagement newsletter is our lively insight into how to get involved in upcoming Turing activities, from our fascinating events to our vibrant public engagement projects. This monthly newsletter is your guide on how to interact and engage with our world-leading AI and data science experts and the wider Turing community.
Sign up above for our events newsletter.

Our applied skills programme allows the Turing community to apply their academic knowledge to real-world data science questions from industry, government, and the third sector.
For updates on the latest applied skills programmes, sign up for our regular newsletter to keep up to date with news, virtual events, and opportunities at a local and national level.

 
We have a number of ways for researchers to get involved with the Turing's work.
Sign up above for up-to-the-minute news on research opportunities.
"
https://www.turing.ac.uk/people/researchers/terry-lyons,"
Professor Terry Lyons is the Wallis Professor of Mathematics; he was a founding member (2007) of, and then Director (2011-2015) of, the Oxford Man Institute of Quantitative Finance; he was the Director of the Wales Institute of Mathematical and Computational Sciences (WIMCS; 2008-2011). He came to Oxford in 2000 having previously been Professor of Mathematics at Imperial College London (1993-2000), and before that he held the Colin Maclaurin Chair at Edinburgh (1985-93).  
Prof Lyons’ long-term research interests are all focused on Rough Paths, Stochastic Analysis, and applications – particularly to Finance and more generally to the summarsing of large complex data. More specifically, his interests are in developing mathematical tools that can be used to effectively model and describe high dimensional systems that exhibit randomness. Prof Lyons is involved in a wide range of problems from pure mathematical ones to questions of efficient numerical calculation.
"
https://www.turing.ac.uk/research/interest-groups/media-digital-age,"
Our focus is the theme of media, defined as the main means of mass communication. The importance of addressing this topic is highlighted by the prominent role of the media in political, economic, scientific, and social discourse. With a drastic shift towards digital communication, individuals and organizations are able to almost instantly disseminate information to a large audience with little-to-no regulation, creating both new challenges and opportunities.
The purpose of the interest group is to bring together academics studying different aspects of the media and practitioners representing different sides of the media (e.g., journalists, fact-checkers and platforms), in order to explore several important themes and the wide range of efforts in today’s mediasphere. We will aim to create a community of individuals working in/on media and become a platform for interdisciplinary communication that facilitates concrete collaborations within and outside of academia.
Digital media is vast, and the nature and sources of data within it are extremely varied (e.g., text, audio, images and videos on news websites and social media platforms). Media-related data is inherently time-dependent and there is often a lack of sufficiently large labeled ground truth data for many questions of interest (e.g., news credibility at the level of articles or claims). The variety of media data raises important questions around the generalisability of scientific tools across platforms, languages and data formats, and the interactive aspect of media brings up profound questions on detecting causality.
Research challenges within digital media have been approached from many different angles, including natural language processing, network science, machine learning, statistics, computer science, social science, and political science. In this interest group, we hope to bring together researchers across these different communities and, in collaboration with practitioners, to define and address concrete real-world challenges.
Key aims of this interest group are
Our objectives will be to form close ties with key players in the media landscape, to develop a holistic understanding of the state-of-play, and to contribute to improving the reliability and robustness of the information sphere we live in today.
We will organise a series of workshops (two per year) around narrowly defined media-related themes. Workshops will involve both UK-based and international speakers and participants. The interest group co-organisers will aim to publish a yearly report highlighting key workshop takeaways.
Our workshops will favour depth over breadth by involving field experts on targeted domains, enable knowledge transfer between academics and practitioners (e.g., through mediated group discussions and panels), and explore different communication mediums (e.g., surveys and blog posts). Between workshops, we will aim to organise related and informal meet-ups, training sessions, and “hackathons” (e.g., through the Data Study Group).
For an up-to-date list of our interest group events, please click here.
Click here to request sign-up and join
Marya Bazzi[email protected]
Elena Kochkina[email protected]
Karl Aberer, École Polytechnique Fédérale de LausanneRabab Ahmad Alkhalifa, Queen Mary University of LondonMadhav Chinnappa, Google and the Reuters InstituteJohn Dougrez-Lewis, University of WarwickGabriele Pergola, University of WarwickMarzieh Saeidi, Facebook ResearchQiang Zhang, UCLArkaitz Zubiaga, Queen Mary University of LondonChristoph Bregler, Google
"
https://www.turing.ac.uk/news/publications/wrangling-messy-csv-files-detecting-row-and-type-patterns,"
Data scientists spend the majority of their time on preparing data for analysis. One of the first steps in this preparation phase is to load the data from the raw storage format. Comma-separated value (CSV) files are a popular format for tabular data due to their simplicity and ostensible ease of use. However, formatting standards for CSV files are not followed consistently, so each file requires manual inspection and potentially repair before the data can be loaded, an enormous waste of human effort for a task that should be one of the simplest parts of data science. The first and most essential step in retrieving data from CSV files is deciding on the dialect of the file, such as the cell delimiter and quote character. Existing dialect detection approaches are few and non-robust. In this paper, we propose a dialect detection method based on a novel measure of data consistency of parsed data files. Our method achieves 97% overall accuracy on a large corpus of real-world CSV files and improves the accuracy on messy CSV files by almost 22% compared to existing approaches, including those in the Python standard library. Our measure of data consistency is not specific to the data parsing problem, and has potential for more general applicability.
Van den Burg, G. J. J., Nazabal, A., and Sutton, C. (2019). Wrangling messy CSV files by detecting row and type patterns. Data Mining and Knowledge Discovery, 33(6), pp. 1799-1820.
 
"
https://www.turing.ac.uk/people/researchers/alan-wilson,"
Sir Alan Wilson is Director, Special Projects of The Alan Turing Institute, with responsibility for leading specific research and partnership projects. He was CEO of the Institute between 2016-2018. From 2018-2021 he has directed the Turing-UKRI Programme, AI for science and government, and now continues to work on other research programmes, including learning machines, the economics of cities and the Living with machines project. He is a member of the Board of Directors of the Banking Standards Board.
He is a Cambridge Mathematics graduate and began his research career in elementary particle physics at the Rutherford Laboratory. He turned to the social sciences, working on cities, with posts in Oxford and London before becoming Professor of Urban and Regional Geography in Leeds in 1970. He was a member of Oxford City Council from 1964-1967. In the late 1980s, he was the co-founder of GMAP Ltd, a University spin-out company. He was Vice-Chancellor of the University of Leeds from 1991 to 2004 when he became Director-General for Higher Education in the then DfES. After a brief spell in Cambridge, he joined UCL in 2007.  He was Professor of Urban and Regional Systems in the Centre for Advanced Spatial Analysis at University College London up to 2018.
From 2007-2013 he was Chair of the Arts and Humanities Research Council; from 2013-2015, Chair of the Lead Expert Group for the Government Office for Science Foresight Project on The Future of Cities; from 2013-2018, Chair of the Home Office Science Advisory Council; and from 2017-2019, Chair of the Ada Lovelace Institute.
He is a Member of Academia Europaea, an FBA, an FAcSS and an FRS. He was knighted in 2001 for services to higher education.
His research field covers many aspects of mathematical modelling of cities and the use of these models in planning. These techniques are now in common use internationally – including the use of the concept of entropy in building spatial interaction models – summarised in Entropy in urban and regional modelling (re-issued in 2011 by Routledge). These models have been widely used in areas such as transport planning, demography and economic modelling.
His recent research is on the applications of dynamical systems theory in relation to modelling the evolution of urban structure in both historical and contemporary settings. This led to the laying of the foundations of a comprehensive theory of urban dynamics described in Complex spatial systems (2000).
He has published over 200 papers and his recent books include The science of cities and regions (2012), his five volume Urban modelling (2012, edited), Explorations in urban and regional dynamics (2015, with Joel Dearden), Global dynamics (2016, edited) and Geo-mathematical modelling (2016, edited). He has a particular interest in interdisciplinarity and published Knowledge power in 2010; he writes the Quaestio blog.
"
https://www.turing.ac.uk/people/guest-speakers/devi-sridhar,"
Devi Sridhar is a Professor at the University of Edinburgh Medical School and holds a Personal Chair in Global Public Health. She is the founding Director of the Global Health Governance Programme and holds a Wellcome Trust Investigator Award.
She was previously Associate Professor in Global Health Politics and a Fellow at Wolfson College, Oxford University and a Postdoctoral Research Fellow at All Souls College, Oxford University. She was also a visiting Associate Professor at LMU-Munich and guest lecturer at the Harvard School of Public Health and the Public Health Foundation of India.
Her books include ‘Governing Global Health: Who Runs the World and Why?’ (OUP, 2017) and ‘the Battle against Hunger: Choice, Circumstance and the World Bank’ (OUP, 2007) and she has published in Nature, Science, the New England Journal of Medicine, the Lancet and the British Medical Journal.
She served on the board of Save the Children UK, on the World Economic Forum Council on the Health Industry and co-chaired the Harvard/LSHTM Independent Panel on the Global Response to Ebola. She holds a DPhil and MPhil from Oxford as a Rhodes Scholar and a B.S. from the University of Miami in the Honors Medical Program.
Her work is concentrated in three areas: international health organizations, financing of global public health and developing better tools for priority-setting.
"
https://www.turing.ac.uk/people/researchers/michael-wooldridge,"
Michael is Director of Foundational AI Research. His work at the Turing involves developing the AI programme, focusing in particular on multi-agent systems. He is also a Professor of Computer Science at the University of Oxford. He has been an AI researcher for more than 30 years, and has published more than 400 scientific articles on the subject.
 
Michael's main research interests are the in field of multi-agent systems, and his current focus is on developing techniques for understanding the dynamics of multi-agent systems. His research draws on ideas from game theory, logic, computational complexity, and agent-based modelling. 
Michael is a Fellow of the Association for Computing Machinery (ACM), a Fellow of the Association for the Advancement of AI (AAAI), and a Fellow of the the European Association for AI (EurAI). From 2014-16, he was President of the European Association for AI,  from 2015-17 he was President of the International Joint Conference on AI (IJCAI), and from 2007-09 he was President of the International Foundation for Autonomous Agents and Multi-Agent Systems (IFAAMAS). In 2006, he received the ACM SIGART Autonomous Agents Research Award, given annually to an individual whose research has been particularly influential in the field of autonomous agents and multi-agent systems. He has been the recipient of the AAMAS ""best paper"" award three times (in 2008, 2009, and 2011). 
"
https://www.turing.ac.uk/people/researchers/kaushik-jana,"
Kaushik is working in the Data Centric Engineering project. He is also affiliated with the Statistics Section of Imperial College London. He received a PhD in Statistics from Indian Statistical Institute. 
 
 He has methodological interests amongst other things in
"
https://www.turing.ac.uk/news/publications/advancing-data-justice-research-and-practice,"
The Advancing Data Justice Research and Practice (ADJRP) project aims to widen the lens of current thinking around data justice and to provide actionable resources that will help policymakers, practitioners, and impacted communities gain a broader understanding of what equitable, freedom-promoting, and rights-sustaining data collection, governance, and use should look like in increasingly dynamic and global data innovation ecosystems. In this integrated literature review and annotated bibliography we hope to lay the conceptual groundwork needed to support this aspiration. The endeavour to broaden current visions of what data justice is (and what it could become) involves not only building on the considerable insights that have accrued since the inception of the field less than a decade ago. It also involves identifying where the study of data justice has—thus far—fallen short of engaging with and integrating the perspectives and wisdom of those significantly impacted by the subject matter it broaches. It involves distinguishing where limited fields of vision in the current academic literature, gaps in disciplinarily anchored understandings, and listening deficits in scholarship and policymaking, have cramped the analytical and normative scope of its concerns, conclusions, and proposed solutions. 
Leslie, David, Katell, Michael, Aitken, Mhairi, Singh, Jatinder, Briggs, Morgan, Powell, Rosamund, Rincón, Cami, Chengeta, Thompson, Birhane, Abeba, Perini, Antonella, Jayadeva, Smera, & Mazumder, Anjali. (2022). Advancing Data Justice Research and Practice: An Integrated Literature Review. The Alan Turing Institute in collaboration with The Global Partnership on AI. https://doi.org/10.5281/zenodo.6408304
This report was commissioned by the International Centre of Expertise in Montréal in collaboration with GPAI's Data Governance Working Group, and produced by the Alan Turing Institute. The research was supported, in part, by a grant from ESRC (ES/T007354/1), Towards Turing 2.0 under the EPSRC Grant EP/W037211/1, and from the public funds that make the Turing's Public Policy Programme possible.
"
https://www.turing.ac.uk/people/researchers/ruth-ahnert,"
Ruth Ahnert is Principal Investigator on the flagship Turing project 'Living With Machines', and a Professor of Literary History and Digital Humanities at Queen Mary University of London. She gained her PhD from the Department of English at the University of Cambridge, but more recently her work has focused on the intersection between literary history and data science. She has held fellowships and grants funded by the AHRC, Stanford Humanities Center, the Folger Shakespeare Library, and the National Endowment of the Humanities (US).
In addition to her Turing-based work she is also Co-I on the AHRC-funded 'Networking the Archives: Assembling and Analysing Early Modern Correspondence'. With Elaine Treharne she is editor of the Stanford University Press book series Stanford Text Technologies.
Ruth's research at the Turing is dedicated to the 'Living with Machines' project. This project is a bold proposal for a new research paradigm. In this ground-breaking partnership between the Turing, the British Library, and partner universities (the University of Cambridge, the University of East Anglia, the University of Exeter, and Queen Mary University of London), historians, data scientists, geographers, computational linguists, and curators have been brought together to examine the human impact of industrial revolution.
It is widely recognised that Britain was the birthplace of the world's first industrial revolution, yet there is still much to learn about the human, social, and cultural consequences of this historical moment. Focusing on the long nineteenth century (c.1780-1918), the Living with Machines project aims to harness the combined power of massive digitised archives and computational analytical tools to examine the ways in which technology altered the very fabric of human existence on a hitherto unprecedented scale.
The central theme - the mechanisation of work practices - speaks directly to present debates about how society can accommodate the revolutionary consequences of AI and robotics in what has become known as the fourth industrial revolution. To understand the fraught co-existence of human and machine, this project contends that we need research methods that combine technological innovation and human expertise.
"
https://www.turing.ac.uk/people/researchers/nick-barlow,"
Nick is a Senior Research Software Engineer at The Alan Turing Institute. He has a PhD in particle physics from the University of Bristol, and worked as a postdoc for the University of Manchester on the BaBar experiment at the Stanford Linear Accelerator Center, and then for the University of Cambridge on the ATLAS experiment at the Large Hadron Collider at CERN.
In both of these experiments, he was involved in both the exploitation of the huge datasets generated by the colliders, and with the software infrastructure facilitating the analysis.
He then worked as a software developer at the National Cyber Crime Unit, helping to develop their capability to process and analyse large and complex datasets.
"
https://www.turing.ac.uk/blog/selfish-reason-share-research-data,"
As many of us adapt to the benefits of remaining connected with our peers and colleagues digitally, new evidence suggests that scholarly papers might get increased citations by staying digitally connected to the research data that support their results.
To promote open, reproducible research, many journals have started to encourage or even mandate that researchers share their research data and provide statements about the availability of their data in their published papers. These Data Availability Statements (DAS’s) in published papers (see Table for examples) provide a way to study if and how researchers share their data, and if data sharing correlates with citations of research.
Previous studies have explored researchers’ data-sharing practices and their associations with citation counts in specific journals, or in specific research disciplines only. We analysed more than half a million papers across multiple journals and research disciplines, to explore the following questions:
We analysed published articles from the PubMed Open Access collection to help answer these questions, and made all our code and data available for replication.
Using an automated approach designed for the study, we classified different kinds of data availability statements, according to their main categories (illustrated in Table 1). The outcome is important as statements that contain a link to research data available in a public repository (category 3) are considered to be preferable over all other types of statements. However, depositing data in a repository may be more time-consuming for researchers than other approaches.
""These Data Availability Statements in published papers provide a way to study if and how researchers share their data, and if data sharing correlates with citations of research.""
We found that data availability statements are now very common in published papers. We also found that journal mandates to include these statements are effective, with big increases in the number of statements in 2014 and 2015, when PLOS (Public Library of Science) and BMC (BioMed Central), respectively, mandated them. When journals only encourage that authors provide a statement, only a small percentage of papers include them. In 2018 93.7% of 21,793 PLOS articles and 88.2% of 31,956 BMC articles had data availability statements. Yet, data availability statements containing a link to data in a repository are just a fraction of the total. In 2017 and 2018, 20.8% of PLOS publications and 12.2% of BMC publications provided DAS containing a link to data in a repository. Figure 1 gives an overview of data availability trends over time in PLOS journals, while Figure 2 exemplifies distinct approaches to data sharing from two BMC journals.
 
 
 
 
We also find that papers that link to data in a repository can have up to 25.36% (± 1.07%) higher citation impact on average, using a citation prediction model. While our results show that higher citations to a paper are correlated with linking to data in a repository, we cannot be certain this is the cause for the higher citations. However, this result may be encouraging for researchers, journals, publishers, funders and policymakers who are interested in data sharing and reproducible research. It suggests there might be a further incentive—beyond increasing transparency and reproducibility of results—to authors to make their data available using a repository.
There might be a variety of reasons for this effect. More efforts and resources are put into papers sharing data, thus this choice might be made for better quality articles. It is also possible that more successful or visible research groups have also more resources at their disposal for sharing data. Sharing data likely also gives more credibility to an article’s results, as it supports reproducibility. Finally, data sharing encourages re-use, which might further contribute to citation counts.
Researchers are concerned that there are insufficient resources and incentives to share their research data, and that more effort is required to publish data when publishing papers. However, this extra effort is really an investment rather than a cost. It is an investment in more reliable and reusable research for the scientific community, and this new evidence suggests it could also be a reasonable, “selfish” investment in researchers’ own reputations.
Read the authors’ paper in PLOS ONE: The citation advantage of linking publications to research data
Access the data in the following repository.
"
https://www.turing.ac.uk/research/research-projects/digital-twin-worlds-first-3d-printed-steel-bridge,"
The 3D printed bridge is a cutting-edge architectural, engineering and data science experiment, pushing the boundaries of conventional infrastructure manufacturing processes and incorporating a state-of-the-art sensor network to monitor structural behaviour. This project is an interdisciplinary research endeavour for exploration and innovation in multiple scientific fields, drawing together experts from data sciences and statistics, engineering, materials science, industrial design, computer science and philosophy of technology, to collaborate on a project with revolutionary potential.
Printed by Dutch 3D printing company MX3D and developed as a collaboration between a number of industrial and academic partners, in addition to its unique construction, the 12 metre-long stainless steel bridge has been equipped with a sensor network designed and installed by Arup, Imperial College London, Autodesk, University of Twente and Force Technology, in collaboration with the Turing’s data-centric engineering programme, sponsored by the Lloyd’s Register Foundation.
Once the bridge is placed across the Oudezijds Achterburgwal canal in central Amsterdam it will immediately become a living laboratory. As part of the fabric of the local urban infrastructure, everyone that crosses over the bridge will generate data that will inform the development of a digital twin of the bridge to measure, monitor and analyse the performance of the structure.
The Alan Turing Institute’s involvement with the bridge began with the engagement of researchers on the data-centric engineering programme applying a data-centric approach to test the 3D printed stainless steel used to construct the bridge to ensure its safety, using statistical techniques in conjunction with materials science.
The novel 3D printing technique pioneered by MX3D represents a radical shift from conventional manufacturing, by making it possible to build new innovative large structures. Placing material only where it is essential makes for much stronger, more economical and unique structures. However, there are questions to be addressed about the steel’s material properties, due to its inherent variability and how to guarantee safety standards for its manufacture and continued monitoring.
Researchers on the data-centric engineering programme performed detailed experiments to probe the material properties of 3D printed stainless steel and to analyse the sensor data gathered using novel statistical methods, leading to improved understanding of this advanced material. The load tests showed the bridge was capable to hold a 19.5 ton load, which was well over its ultimate design load. These experimental tests provided critical evidence to secure a permit from the City of Amsterdam for bridge placement.
The data from the sensors is being fed into an intelligent digital twin, which will learn from live data, including strain, displacement, vibration data and environmental factors such as air quality and temperature collected from the bridge while in operation. The digital twin builds on mathematical models and abstractions already widely used in structural engineering and blends them with recent statistical and machine learning techniques. The digital twin of the bridge will be a living computer model which is continuously learning to imitate the physical bridge with growing accuracy as the data comes in.
The performance and behaviour of the physical bridge will be tested against its digital twin, which will provide valuable insights for the design and certification of future 3D printed structures. It will also enable the current 3D bridge to be modified to suit any required changes, ensuring it is safe and secure for pedestrians.
Beyond the digital twin of the bridge, the overarching vision of the data-centric engineering programme is to develop the essential mathematical foundations and software engineering needed for intelligent digital twins for all kinds of engineering applications.
To enable these digital twin studies, the Turing is developing a data access platform that will integrate with Autodesk’s software to enable researchers to access the large quantities of sensor data stored on cloud computing servers. The Turing is hosting the bridge data for the full two-year period covered by the bridge’s current operating permit and has conducted a thorough ethics review of the project to ensure that the scientific goals of the project do not compromise the public's privacy. Using a custom data platform, the Turing supports researchers who require access to the sensor data stored in its secure cloud.
The project has engaged with the University of Twente’s BRIDE (BRIdging Data in the built Environment) project to use the bridge, its design and instrumentation to gather insight into how the city’s population engages with the bridge. This will subsequently inform how designers, technologists, and citizens can utilise rapid urban manufacturing and IoT technologies for designing urban space. By developing an understanding of the interactions between people, place, activity and technology that contribute to a sense of ‘cityness’, the project will analyse how smart technologies such as those used to create and monitor the bridge might contribute to designing 'cityness'. Ultimately this aims to promote a feeling of ownership of the public space by the community.
Researchers have also developed a smaller-scale model of the 3D printed bridge, complete with sensor network and digital twin. This ‘mini-bridge’ acts as a demonstrator that can be toured to public venues to demonstrate the concept of a digital twin.
Read articles in The Times, The Engineer, and MyScience.

Video courtesy of Neil Bowdler.
"
https://www.turing.ac.uk/people/external-researchers/bob-andrews,"
"
https://www.turing.ac.uk/people/researchers/camila-rangel-smith,"
Camila is a Research Data Scientist at The Alan Turing Institute. She holds a PhD in Particle Physics from Université Paris Diderot where she worked on the ATLAS experiment at the Large Hadron Collider at CERN. During her PhD she participated on the discovery of the Higgs Boson particle announced by CERN in 2012. She continued working on ATLAS as a postdoc for Uppsala University where she focused on searches for physics beyond the Standard Model of Particle Physics.
Right before joining the Turing, she worked as Data Scientist in the EdTech sector developing innovative products focused on the assessment process in education.
She is passionate about impactful research  that create public value. Currently she is currently contributing to the Artificial Intelligence for Data Analytics (AIDA) project. 
"
https://www.turing.ac.uk/news/publications/method-emerging-empirical-age-structures-agent-based-models-exogenous-survival,"
For many applications of agent-based models (ABMs), an agent's age influences important decisions (e.g. their contribution to/withdrawal from pension funds, their level of risk aversion in decision-making, etc.) and outcomes in their life cycle (e.g. their susceptibility to disease). These considerations make it crucial to accurately capture the age distribution of the population being considered. Often, empirical survival probabilities cannot be used in ABMs to generate the observed age structure due to discrepancies between samples or models (between the ABM and the survival statistical model used to produce empirical rates). In these cases, imputing empirical survival probabilities will not generate the observed age structure of the population, and assumptions such as exogenous agent inflows are necessary (but not necessarily empirically valid). In this paper, we propose a method that allows for the preservation of agent age-structure without the exogenous influx of agents, even when only a subset of the population is being modelled. We demonstrate the flexibility and accuracy of our methodology by performing simulations of several real-world age distributions. This method is a useful tool for those developing ABMs across a broad range of applications.
"
https://www.turing.ac.uk/people/researchers/david-beavan,"
David (he/him) is Lead Research Software Engineer in the Research Engineering Group (REG) at The Alan Turing Institute, and Research Affiliate at the University of Edinburgh Centre for Data, Culture & Society (CDCS).
He is currently:
Trained as a software engineer, with a digital humanities research background, he has been REG Co-Investigator for Accelerating AI in the Arts and Humanities. He also led the Research Engineering Group's work on Data-centric Engineering projects such as AI for Control Problems, Vehicle Grid Integration and the development of the Data Safe Haven Classification Web App. David has led REG's involvement in the Turing Research and Innovation Cluster in Digital Twins (TRIC-DT) shaping it and its ways of working. He is the funder of the award winning Turing Data Stories, an open community creating and curating data stories.
He is a member of the UKRI Peer Review College, reviewing for both AHRC and ESRC. David has served the Digital Humanities community as an elected member of the European Association for Digital Humanities (EADH), and was a former Co-organiser of the  Humanities and Data Science Turing Interest Group. Prior to joining the Turing, David was Associate Director for Research at the UCL Centre for Digital Humanities (UCLDH) and Research Manager for the UCL Faculty of Arts & Humanities. He has worked on large-scale projects of international and national importance, such as the Scottish Corpus of Texts and Speech and its sibling projects, including the Corpus of Modern Scottish Writing, while at the University of Glasgow.
"
https://www.turing.ac.uk/work-turing/internships/algorithms-automatic-detection-new-word-meanings-social-media,"

Developing computational methods for identifying the emergence of new word meanings using social media data, advancing understandings of cultural and linguistic interaction online, and improving natural language processing tools.
Students on the project: Farhana Ferdousi Liza and Philippa Shoemark
Project Supervisors
Barbara McGillivray, Turing Research Fellow, University of Cambridge
Dong Nguyen, Turing Research Fellow, University of Edinburgh
Scott Hale, Turing Fellow, University of Oxford
This project focuses on developing a system for identifying new word meanings as they emerge in language, focussing on words entering English from different languages and changes in their polarity (e.g., from neutral to negative or offensive). An example is the word kaffir, which, starting from a neutral meaning, has acquired an offensive use as a racial or religious insult. The proposed research furthers the state of the art in Natural Language Processing (NLP) by developing better tools for processing language data semantically, and has impact on important social science questions.
Language evolves constantly through social interactions. New words appear, others become obsolete, and others acquire new meanings. Social scientists and linguists are interested in investigating the mechanisms driving these changes. For instance, analysing the meaning of loanwords from foreign languages using social media data helps us understand the precise sense of what is communicated, how people interact online, and the extent to which social media facilitate cross-cultural exchanges. In the case of offensive language, understanding the mechanisms by which it is propagated can inform the design of collaborative online platforms and provide recommendations to limit offensive language where this is desired.
Detecting new meanings of words is also crucial to improve the accuracy of NLP tools for downstream tasks, for example in the estimation of the “polarity” of words in sentiment analysis (e.g. sick has recently acquired a positive meaning of ‘excellent’ alongside the original meaning of ‘ill’). Work to date has mostly focused on changes over longer time periods (cf., e.g., Hamilton et al. 2016). For instance, awful in texts from the 1850s was a synonym of ‘solemn’ and nowadays stands for ‘terrible’.
New data on language use and new data science methods allow for studying this change at finer timescales and higher resolutions. In addition to social media, online collaborative dictionaries like Urban Dictionary are excellent sources for studying language change as it happens; they are constantly updated and the threshold for including new material is lower than for traditional dictionaries.
The meaning of words in state-of-art NLP algorithms is often expressed by vectors in a low-dimensional space, where geometric closeness stands for semantic similarity. These vectors are usually fed into neural architectures built for specific tasks. The proposed project aims at capturing meaning change on a fine-grained, short time scale. We will use the algorithm developed by Hamilton et al. (2016), who used it to identify new meanings using Google Books. We will train in-house vectors on multilingual Twitter data collected from 2011 to 2017.
Through this process we will identify meaning change candidates and evaluate them against the dictionary data by focusing on analysing the factors that drive foreign words to enter the English language and to change their polarity. In doing so, we will shed light on the extent to which the detected meaning changes are driven by linguistically internal rather than external (e.g. social, technological, etc.) factors.
"
https://www.turing.ac.uk/people/guest-speakers/inken-von-borzyskowski,"
Inken von Borzyskowski is Assistant Professor of Political Science at University College London, having previously been on the faculty at Florida State University. She received her Ph.D. in International Relations, Methodology, and Comparative Politics at University of Wisconsin-Madison.
She is originally from Berlin where she received a Postdoctoral research fellowship and previously an M.A. at Free University Berlin. She has been a research fellow at the European University Institute in Florence, at the International Foundation for Electoral Systems (IFES) in Washington, D.C., and a consultant at the U.S. Institute of Peace (USIP).
Her research focuses on the domestic politics of international relations with an emphasis on international organizations and their effect on domestic conflict and elections. Specifically, her research falls into three areas: international democracy assistance; the causes and consequences of election violence; and international organizations’ membership politics. In her first book, she develops and tests a theory of how international organizations can influence election violence by changing election credibility.
Her research is published in International Studies Quarterly, British Journal of Political Science, Journal of Peace Research, Review of International Organizations, and Cornell University Press.
"
https://www.turing.ac.uk/people/former-researchers/andrew-elliott,"
Andrew received his PhD/DPhil from the University of Oxford under the supervision of  Professor Felix Reed-Tsochas and Professor Gesine Reinert as part of the Systems Approaches to Biomedical Sciences DTC sponsored by e-therapeutics. His thesis focused on subsampling in biological networks, exploring the structure induced by biologically motivated network sampling using computational and analytical approaches. Previously, he completed a BSc in Mathematics from Imperial College London.
Andrew joined The Alan Turing Institute as a postdoc to work on a project with Accenture to develop new approaches that improve the identification of anomalies in networks, with applications in fraud detection.
Andrew is currently Lecturer in Statistics at the University of Glasgow.
Andrew is interested in the development of scalable network-based algorithms to solve various real-world problems, such as anomaly detection and core-periphery detection. This includes improving and extending existing methods, developing new methods to solve a wider class of problems, and applying well-understood methodologies to gain insights into new application areas.
He is interested in applying these algorithms to a wide variety of problems, from fields including biology, social sciences and physical sciences. Most recently, Andrew has worked on several projects looking at applications in financial systems, cybersecurity and urban networks.
"
https://www.turing.ac.uk/people/researchers/anna-zanchetta,"
Anna is a Turing Research Fellow affiliated with the Science for Cities and Regions Programme. In this role, she aims to investigate and generate tools for promoting environmental sustainability and active modes of transport, in collaboration with government offices and local councils.
She joined the Institute in 2021 as a Post Doc Research Associate within the Urban Analytics Programme, working on the early phase of the national scaling-up of a regional pandemic model, what then became SPC (Synthetic Population Catalyst). Later she joined a pilot project in partnership with the Geospatial Commission on land use characterisation to support sustainable urban development, where she led the research on computing potential accessibility by different means of transport.
Her background is Physics of the Atmosphere and Geomatics, with and MSc and PhD both from Bologna University in Italy. Anna holds a multi-year experience in the Middle East region, where she was involved in activities ranging from water resources and environment management in humanitarian aid projects, to remote sensing and desertification studies in arid and semiarid areas.
Anna’s interest is in using geospatial data to tackle inequalities and to promote environmental sustainability.
She is particularly keen in generating code for reproducible and open science, following the Turing Way principles.
"
https://www.turing.ac.uk/people/external-researchers/johanna-hutchinson-0,"
Dr. Johanna Hutchinson is the UK Health Security Agency’s (UKHSA) Director of Analytics & Data Science and Data IQ’s third most influential person in Data (Data IQ 100 The Most Influential People in DATA 2021).
Johanna moved to DHSC in April 2020 and joined the Joint Biosecurity Centre, now part of UKHSA, to build the growing data capability and promote transparency. She is a keen advocate of increased use of data and analysis in public services and has always been a key player in promoting diversity in the workforce.
 
"
https://www.turing.ac.uk/people/guest-speakers/kate-crawford,"
Kate Crawford is a leading scholar of the social implications of artificial intelligence. Her work focuses on understanding large-scale data systems in the wider contexts of history, politics, labor, and the environment.
She is a Research Professor at USC Annenberg, a Senior Principal Researcher at MSR-NYC, and she currently holds the inaugural Visiting Chair for AI and Justice at the École Normale Supérieure in Paris.
Kate’s work also includes collaborative projects and visual investigations. Her project Anatomy of an AI System with Vladan Joler won the Beazley Design of the Year Award, and is in the permanent collection of the Museum of Modern Art in New York. Her collaboration with the artist Trevor Paglen produced Training Humans – the first major exhibition of the images used to train AI systems. Their investigative essay, Excavating AI, won the Ayrton Prize from the British Society for the History of Science.
Crawford's latest book, Atlas of AI, was published by Yale University Press in May 2021, and has been described as ""timely and urgent"" by Science.
"
https://www.turing.ac.uk/people/researchers/domenic-di-francesco,"
Domenic is a chartered mechanical engineer who has previously worked for consultancies in the energy industry. He completed his PhD in 2021, and his thesis investigated the application of partial pooling (hierarchical and gaussian process) Bayesian models in value of information analysis, with application to structural integrity management. Since joining the Alan Turing Institute, he has continued to work on various methods in statistical decision analysis, as well as industrial projects including guidance for engineers on the use of computational statistics, and verification of digital twins.

"
https://www.turing.ac.uk/people/researchers/janis-wong,"
Dr Janis Wong is a Postdoctoral Research Associate on the Ethics theme of the Public Policy Programme. She has an interdisciplinary PhD in Computer Science from the Centre for Research into Information, Surveillance and Privacy (CRISP), University of St Andrews, where her PhD thesis on co-creating data protection solutions through a commons was awarded the Stefano Rodotà Award by the Council of Europe for best data protection thesis. She also holds an MSc in Computing and Information Technology from the University of St Andrews and an LLB Bachelor of Laws from the London School of Economics.
Janis is interested in the legal and technological applications in data protection, ethics, governance, and stewardship. In 2020-21, Janis was a Research Fellow at the Open Data Institute, focusing on data protection and governance in online teaching, learning analytics, and educational technology. In addition to working with universities and the public sector, she is a frequent expert contributor to public media and works closely with private sector organisations to turn theory into practice. She currently sits on the Data Ethics Steering Group as part of The RED Foundation to apply data ethics practices into the real estate sector.
Janis' research focuses on data protection, stewardship, and governance as it relates to artificial intelligence, ethics, and public policy. She draws on her expertise in data protection and information regulations, data stewardship and data governance, human-computer interaction (HCI), and science and technology studies (STS) to address power imbalances and stakeholder tensions when designing, building, and deploying innovative technologies.
Janis uses both quantitative and qualitative methodologies in her research, where her past research includes topics such as data protection rights, privacy-enhancing technologies (PETs), online learning and educational technologies, the impact of social media on the physical and mental health of young people, research ethics frameworks, and collaborative design.
Janis was awarded the Stefano Rodotà Award 2023 from the Council of Europe Committee of Convention 108, with her PhD thesis on data protection awarded first place and an invitation to present her work at the Committee of Convention 108 Plenary in Strasbourg.
Janis is an active speaker and enjoys communicating artificial intelligence, ethics, and privacy-related topics to both academic and non-academic audiences.
A selected list of invited talks and public appearances include:
 
"
https://www.turing.ac.uk/person/408,
https://www.turing.ac.uk/people/former-doctoral-students/andrew-burnie,"
Andrew Burnie is studying for a PhD in Computer Science at UCL. His research applies open-source tools (e.g. R and Python) to analyse the linkage between changes in social media discussions and phasic shifts in the cryptocurrency price movement across time. His work has been presented at ACM SIGIR and published in Royal Society Open Science and Ledger. His cryptocurrency classification framework was included with written evidence provided by Eversheds Sutherland LLP in the UK Parliament Digital Currencies Inquiry. Previously, he was a data scientist at Hitachi Consulting and at ERS insurance (a Lloyd’s syndicate), where he set up the data science team. He has a BA in Economics and Management Studies (Cambridge University) and an MSc Finance, culminating in a publication (Burnie and Mchawrab, 2017).
His research examines the association between events and concerns expressed in social media posts and phasic shifts in the cryptocurrency price movement across time. This applies natural language processing, neural networks (word2vec), non-parametric statistical hypothesis testing, time series analysis, social media analysis, causal inference and sentiment analysis.
"
https://www.turing.ac.uk/news/majority-british-public-support-laws-and-regulations-guide-use-ai-according-new-nationwide,"
More than 60% of the British public support ‘laws and regulations’ to guide the use of AI, according to a new national survey by the Ada Lovelace Institute and The Alan Turing Institute, with substantial input from LSE’s Methodology Department, published today (Tuesday 6 June).
The nationally representative survey of over 4,000 adults in Britain comes at a time when conversations around AI regulation and the need to mitigate risks are heightening. The survey highlights the complex and nuanced range of attitudes towards the use of AI across different contexts. 
‘AI’ can be difficult to define, subject to multiple interpretations and often poorly understood, which is why the survey asked about specific and clearly described uses of AI, from facial recognition and targeted advertising to driverless cars and robotic vacuum cleaners.
The survey found that the public see clear benefits for many uses of AI, particularly technologies relating to health, science and security. When offered 17 examples of AI technologies to consider, respondents thought the benefits outweigh concerns for 10 of these. For example, 88% of the public say AI is beneficial for assessing the risk of cancer, 76% can see the benefit of virtual reality in education and 74% think climate research simulations could be advanced using the technology. 
The survey also showed that people often think speed, efficiency and improving accessibility are the main advantages of AI. For example, 82% think that earlier detection is a benefit of using AI with cancer scans and 70% feel speeding up border control is a benefit of facial recognition technology.
However, attitudes do vary across different technologies. Almost two thirds (64%) are concerned that workplaces will rely too heavily on AI for recruitment, rather than using professional judgement, and 61% are concerned that AI will be less able than employers and recruiters to take account of individual circumstances.
Public concerns extend beyond use of AI in the workplace. People are most concerned about advanced robotics, for example 72% express concern about driverless cars and 71% about autonomous weapons. Over three quarters (78%) worry that the use of robotic care assistants in hospitals and nursing homes would mean patients missing out on human interaction and over half (57%) are concerned that smart speakers gather personal information that could be shared with third parties.
Awareness of AI technologies also varies greatly depending on context. 93% are aware of the use of AI in facial recognition for unlocking mobile phones, but only 19% are aware of the use of AI for assessing social welfare eligibility
The survey also asked about attitudes towards the governance of AI more generally. When asked what would make them more comfortable with the use of AI, almost two thirds (62%) chose ‘laws and regulations that prohibit certain uses of technologies and guide the use of all AI technologies’ and 59% chose ‘clear procedures for appealing to a human against an AI decision’.
The Ada Lovelace Institute and The Alan Turing Institute have conducted this new survey to understand how people experience AI as well as their awareness of these technologies, their perceived benefits and concerns and how attitudes differ across different groups of people.
By informing AI researchers, developers and policymakers about the concerns and benefits that the public associate with AI, this research can help to maximise the potential benefits of AI.
Andrew Strait, Associate Director at the Ada Lovelace Institute, said: ‘AI technologies are developing faster than ever and more organisations, in both the private and public sector, are expanding their use of AI. However, it is important that companies and policymakers are aware of public expectations and concerns.
‘Our research provides a detailed picture of how the public perceive the use of AI across a range of contexts. We hope that it will help AI companies and policymakers understand and respond to the public’s nuanced attitudes towards AI and its regulation.’
Professor Helen Margetts, Programme Director for Public Policy at The Alan Turing Institute and Principal Investigator, said: ‘We conducted this survey to better understand people’s attitudes to AI technologies, at a time when AI has become entwined with many aspects of daily life. It’s important to delve into people's perceptions of the possible benefits and concerns associated with the various uses of AI. The survey showed that for the majority of technologies, people saw more benefits than concerns. But their views of these technologies were highly nuanced, in that they could see benefits and concerns simultaneously.
‘Studies like this can be helpful in considering the development and deployment of AI, especially with the advent of newer generations of AI such as ChatGPT. People's clear support for the regulation of AI showed how important it is to get the governance right, to ensure that the uses of these technologies embody fairness and transparency, and that people can benefit from them safely.’
"
https://www.turing.ac.uk/people/guest-speakers/keren-weitzberg,"
Keren Weitzberg is a tech and migration researcher with 10+ years of experience in East Africa conducting fieldwork in cross-cultural, multilingual settings. Keren works at the intersection of science and technology studies, migration studies, and critical race studies, examining problematics related to mobility, digital identity, biometrics, and fintech. She has been published in a variety of forums, ranging from top academic presses to popular blogs, and has worked closely with a range of civil society organizations, non-profits, and artists. Her first book, We Do Not Have Borders: Greater Somalia and the Predicaments of Belonging in Kenya (2017) was a finalist for the 2018 African Studies Association Book Prize and named one of Quartz Africa’s favourite books of 2017. Her recent work has included a scoping project for Amnesty International called ‘Defending the Rights of Refugees and Migrants in the Digital Age’, a forthcoming moving-image work on the UK hostile environment with filmmaker Edwin Mingard, and projects on biometrics in the humanitarian sector and counterterrorism industry for Privacy International.
"
https://www.turing.ac.uk/events/turing-lectures-what-are-your-chances,"
Cynthia Dwork is renowned for placing privacy-preserving data analysis on a mathematically rigorous foundation. A cornerstone of this work is differential privacy, a strong privacy guarantee, frequently permitting highly accurate data analysis. 
In machine learning, algorithms produce scores, which are often interpreted as probabilities: ""What is the probability that this student will graduate within four years?""; ""What is the chance that this loan will be repaid?”; ""What is the probability that this tumour will metastasise under the given course of treatment?"". Speaking intuitively, the goal in machine learning and AI is to produce these probabilities from training data, i.e. from evidence in the form of labelled examples: descriptions of individuals (inputs to the prediction algorithm), together with their outcomes (loan repaid or not).
The problem here is that we cannot even say what a ‘probability’ is! A 50% probability of heads for a coin means that if you flip it 1,000 times you’re very likely to get about 500 heads. But when you’re talking about an individual, you can’t run her though college, rewind her, run her through college again, rewind her, and see the fraction of those runs in which she graduates in four years. And without knowing what an ‘individual probability’ is, how can we design an algorithm to produce it? How do we know when we have succeeded?
This talk describes Outcome Indistinguishability, an approach to forecasting based in complexity theory, developed with Michael Kim, Omer Reingold, Guy Rothblum and Gal Yona.
"
https://www.turing.ac.uk/people/guest-speakers/rob-mullins,"
Robert Mullins is a Reader in Computer Architecture in the Department of Computer Science and Technology at the University of Cambridge. Robert is interested in the design of many core processors, hardware accelerators for machine learning and open-source chip design. He is a founder and director of lowRISC a not-for-profit company focused on engineering high-quality open-source hardware. He was a founder and trustee of the Raspberry Pi Foundation.
"
https://www.turing.ac.uk/people/researchers/chris-hicks,"
"
https://www.turing.ac.uk/news/publications/sgx-lkl-library-os-running-java-applications-intel-sgx-enclaves,"
SGX-LKL is a library OS designed to run unmodified Linux binaries inside SGX enclaves. It uses the Linux Kernel Library (LKL) (https://github.com/lkl/linux) to provide mature system support for complex applications within the enclave. A modified version of musl (https://www.musl-libc.org) is used as C standard library implementation. SGX-LKL has support for in-enclave user-level threading, signal handling, and paging. System calls are handled within the enclave by LKL when possible, and asynchronous system call support is provided for the subset of system calls that require direct access to external resources and are therefore processed by the host OS. The goal of SGX-LKL is to provide system support for complex applications and managed runtimes such as the JVM with minimal or no modifications and minimal reliance on the host OS.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/daphne-jackson-trust,"
"
https://www.turing.ac.uk/research/research-projects/computational-social-science-social-data-bias,"
Computational social science (CSS) provides a powerful approach to analyse and predict the social world, yet a widely known concern with this approach to understanding social phenomena is possible bias across all stages of the process. One underrepresented area is the application of social science theory and research to help to address social bias when building CSS tools and techniques. To provide a synthesis of the current state of knowledge across the fields of social and computational science on social bias we will conduct a scoping review and a series of workshops. We aim to publish the findings to help to raise awareness of some of the concerns and possible solutions to the issue of social bias in CSS.
Beyond the current work we hope to continue the project and to build a community of interested parties enabling further problems to be investigated and for bias to be investigated in more detail.
Computational social science refers to the attempt to use computational approaches to analyse and predict social phenomena.
We consider ‘social bias’ as knowingly or unknowingly making judgments on certain groups of individuals, which typically relates to race/ethnicity, gender, or sexuality.
Social science theories refer to those developed in disciplines such as psychology, sociology, or politics, to explain actions or behaviours. 
 
The work will review and summarise what is known and still unknown in the area of social bias in CSS; offering critical evaluation of the current situation, identifying ways to address the highlighted issues, and outlining priorities for future research. In addition, the workshop events will bring together individuals in the Social Sciences and CSS communities and begin to build a network for this field. Ultimately, this work will help to drive forward thinking and encourage greater input from social science theory within the development of CSS techniques and tools. In doing so, the findings will identify and outline opportunities to further draw on social science to build fair and equitable AI systems—something that is crucial for society given that this bias can have severely negative consequences for groups of people, especially minority and underrepresented groups.
The work has applications across government and industry where CSS is implemented—the findings will have implications for the design, development, and use of CSS. There are also possible benefits for academia; a comprehensive review of the current issues of social bias in CSS will indicate areas for improvement and highlight priorities for future research.
"
https://www.turing.ac.uk/events/ai-biomedicine-and-healthcare,"
Biomedical AI is revolutionising the way we diagnose, treat, and prevent diseases. In particular, there is an increasing focus on multi-modal data integration for various applications, including the use of omics, imaging and clinical data. New deep learning techniques, including transformers, diffusion models, and generative pre-trained algorithms are also being increasingly explored for biomedical AI applications. There are however several open challenges, and it is therefore important to discuss the recent progress in the field.
The workshop will broadly cover methodological, theoretical and applied research in biomedical AI, with a focus on AI safety and interpretability, which are still unsolved challenges. It will include a mixture of talks from experts in the field and early-career researchers, presenting various case studies.
Register here
"
https://www.turing.ac.uk/news/publications/data-quality-guarantee-credible-caching-device-selection-mobile-crowdsensing,"
Mobile crowdsensing systems (MCSs) present a flexible and economical alternative to traditional infrastructure-based large-scale sensing through the recruitment of personal mobile devices as data sources. As this becomes a popular sensing approach, it will impact the capacity of typical centralized cellular communication infrastructures widely adopted by MCS applications and any costs accrued. Following the trend toward edge processing, mobile edge caching offloads data and services from the system core to reduce service latency and bandwidth occupation. However, in the MCS case the edge devices are owned by the general public and are therefore more vulnerable to data or calculation manipulation by the user. We now better understand sensor data and user trustworthiness but have no way to determine which devices can also be trusted (i.e., act as a credible caching device). In this article, we treat the quality of sensing data reported by each user as an indication of their possibility of providing credible caching services. Specifically, we conduct a comprehensive study of the data quality problem with regard to cache-enabled MCSs, and develop an incentivization method to encourage users to actively provide high-quality data. That is, quality-aware behavior evaluation is core to the credible caching device selection process. Results of extensive simulations based on realworld data verify the effectiveness of our design. We also highlight several promising research directions that remain open for further elaboration.
Cong Zhao, Shusen Yang, Ping Yan, Qing Yang, Xinyu Yang, Julie McCann, ‘Data Quality Guarantee for Credible Caching Device Selection in Mobile Crowdsensing Systems’, in IEEE Wireless Communications, 25(3): 58-64, June 2018.
"
https://www.turing.ac.uk/blog/we-asked-chatgpt-create-some-christmas-cracker-jokes-heres-what-happened,"
If you’ve been following the news recently, you’ve probably heard of ChatGPT. This AI chatbot, created by US research laboratory OpenAI, is making headlines with its ability to hold natural-looking conversations and write essays, product reviews, GSCE exam answers and even articles about itself.
But how does ChatGPT fare with creating jokes? Humour is one of the most complex and nuanced aspects of human communication, and a challenge for even the best bot. So, as the festive season gets underway, we decided to put ChatGPT’s punning abilities to the test by asking it to create some Christmas cracker jokes.
First, we asked it to make up some jokes about Christmas trees and Brussels sprouts. Here are a couple of its efforts:
Why was the Christmas tree late for work?
Because it was waiting for its ornaments!
Why did the Brussels sprout get invited to all the best parties?
Because it was always the life of the Brussels!
These are funny, but only because they make no sense. In fact, you could call them anti-jokes. You can see what ChatGPT is trying to do here – working an element of the joke’s setup into the punchline – but it’s not quite there.
Could we teach ChatGPT how to tell better jokes? One of us (Lydia) is an improv comedian in The Oxford Imps, and regularly teaches new performers how to create puns. Happily, ChatGPT is a keen pupil:
Lydia: I would like to help you learn a way to make jokes. Would you like to try?ChatGPT: Sure, I would be happy to learn how to make jokes. Can you provide some guidance on how to do so?Lydia: First you need to take your keyword to start with. Let’s start with ‘elf’. Next you need to think of a word that sounds like ‘elf’. Can you think of any?
ChatGPT was able to identify that ‘alph’ sounds like ‘elf’, and used this to create its own joke:
Why was the elf bad at math?
Because he was an alph-a-bet!
This would make sense, if only ChatGPT had said ‘elf-a-bet’ instead! The chatbot needed more guidance, so Lydia advised that the joke’s punchline needs to contain ‘elf’. This led to the following joke:
Why did the elf never get sick?
Because he had an elf-thly immune system!
This almost works! Replace ‘elf-thly’ with ‘elf-y’ and we would be there. This was as good as we got with the ‘words that rhyme with elf’ jokes but, after giving ChatGPT some more advice, Lydia next asked it to come up with a joke that played on the word ‘bauble’. This resulted in its most human-like joke:
Why did the Christmas tree get mad at the elf?
Because the elf burst its bauble!
OK, it’s not quite Peter Kay, but this is a nice play on the phrase ‘burst its bubble’, and it’s as good as some of the Christmas cracker jokes we’ve had in the past. What’s especially interesting about ChatGPT is that we could teach it things as we would a person, and it took our advice on board, producing progressively better results.
That said, ChatGPT’s jokes aren’t quite at the level of human ones, and we are seeing some of AI’s current limitations in this area. Essentially, chatbots are very good at parroting existing jokes (when we first asked ChatGPT for Christmas jokes, it gave us examples that could already be found online) but they lack a full understanding of what makes jokes funny in the first place, as is beautifully illustrated by this exchange:
Lydia: Why is this funny: “What do you get if you cross a snowman and a shark? Frostbite.”ChatGPT: This joke is funny because it creates a surprising and unexpected contrast between two things that are normally very different: a snowman and a shark. The idea of a snowman and a shark crossing paths is absurd and unlikely…Additionally, the punchline uses wordplay, with the word ‘frostbite’ being used both as a way to describe the cold weather that a snowman might experience and as the name of a medical condition that a shark might get if it were exposed to cold water.
Chatbots’ abilities at creating human-like text are truly impressive, but without a firm grasp of the subtleties of humour, they will continue to struggle with creating jokes. AI is most successful when it collaborates with humans, and it will need plenty more coaching before it can rival real-world comedians.
In the meantime, we’re going to leave you with some Christmas jokes created especially for this blog by the (human) improvisers in The Oxford Imps. Enjoy!
Why did the astronaut go to the South Pole?
To go where snowman had been before!
Why did I spend all Christmas in the cellar?
I was mulling over the wines.
How did the Christmas cheeseboard get a date?
He pulled a cracker!
Why was Sherlock Holmes at our house for Christmas?
There’d been a myrrh-der.
What do you use to lock away the Christmas dinner before it is ready?
A tur-key!
 
The authors would like to thank The Oxford Imps for their puns, and Giacomo Nanni and Anne Lauscher for helping with ideas.
Top image: Jesper Jensen / Shutterstock
"
https://www.turing.ac.uk/people/spotlights,"

"
https://www.turing.ac.uk/people/former-researchers,"

Our Researcher list is optimised to use JavaScript. Without Javascript enabled our filter and search function, which helps you find Turing researchers by speciality, title and institution, can’t function. Please consider allowing Javascript to run on this page or use the site wide search function. Contact [email protected] if you have any issues.
2018 Doctoral Students
2017 Doctoral Students
2016 Doctoral Students
2021 Enrichment Students
2020 [email protected] Students
2019 Enrichment Students
2018 Enrichment Students
2017 Enrichment Students
2016 Enrichment Students
"
https://www.turing.ac.uk/people/aditi-shenvi,"
Aditi is a third year PhD student in the Mathematics for Real-World Systems doctoral training centre at the University of Warwick. She is supervised by Professor Jim Smith (Turing Fellow). As part of her PhD research, she works on developing flexible and interpretable graphical models for public health applications such as modelling of recurrent events. Before starting her PhD, she earned a masters in Interdisciplinary Mathematics from the University of Warwick and a bachelors in Pure Mathematics with minors in Statistics and Economics from the University of Mumbai. 
Aditi will be an enrichment student at the Turing from January to June 2020. She is interested in using her time here to form new collaborations, and to work with them to gain a deeper mathematical understanding of interpretable artificial intelligence tools and to explore their applications in health, medicine and related domains.
Aditi is primarily interested in model development and Bayesian inference. In her PhD research so far, she has developed a dynamic graphical model class integrating the technologies of chain event graphs (a generalisation of bayesian networks) and semi-markov processes customised to modelling adverse recurrent events in heterogeneous populations. Another aspect of model development that interests her is model selection. Her current research involves exploring properties of this graphical model family and of scoring functions which can be exploited to develop scalable model selection algorithms. She is also keen to improve her understanding of belief propagation algorithms, missingness mechanisms and causal reasoning. 
Her PhD work has also found applications in the domains of defence and security through existing collaborations at the Turing. During her time at the Turing, she hopes to explore more such parallels in her work with other domains.  
"
https://www.turing.ac.uk/people/guest-speakers/amuku-isaac,"
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series,"

The Turing-Roche strategic partnership was established in summer 2021. The goal of this five-year partnership in advanced analytics between the two organisations is our North Star: 'enabling the generation of insights to better understand patient and disease heterogeneity and its relevance to clinical outcomes at an unprecedented level of precision in order to improve clinical care'. Our partnership will cover multiple activities to investigate how and why diseases affect people differently and why they may respond differently to treatments.
This knowledge share series aims to bring together members of both Roche and Turing's networks, as well as the wider scientific community, to showcase partnership updates and research, knowledge share and hear different academic and industry perspectives on data science topics to gain insight and help build new connections and collaborations.
This series will be loosely based around areas related to our North Star but we also want to explore other areas of data science that are of mutual interest to Roche, The Alan Turing Institute and beyond – so everyone is welcome.
Join the Turing-Roche Slack Workspace. 
If you are interested in being a speaker at one of our events or want to give feedback on a knowledge share event, then please do get in touch with Vicky Hellon, Community Manager for the partnership ([email protected])
 
"
https://www.turing.ac.uk/people/former-doctoral-students/alex-mansbridge,"
Alex started his doctoral studies at The Alan Turing Institute in October 2017. He is registered at UCL. After graduating with a masters degree in Mathematics, he worked for 18 months in a risk modelling position at a major Danish Bank. Following this, he worked for two years as a senior data scientist at Under Armour Connected Fitness. Here, he worked with an accomplished team of data scientists to implement deep learning algorithms across Under Armour's suite of fitness apps, with the aim of improving the health and well being of millions of users worldwide.
Alex’s research interests lie at the intersection of deep learning and natural language processing. His research is focused on tackling problems in the field of question answering, through the use of memory augmented neural networks. He is interested in how the concept of external memory can be used to improve the performance of neural networks, and enable them to handle information from complex graphical data structures more effectively.
"
https://www.turing.ac.uk/news/publications/distributional-semantic-methodology-enhanced-search-historical-records,"
In this paper we present a methodology based on distributional semantic models that can be flexibly adapted to the specific challenges posed by historical texts and that allow users to retrieve semantically rel- evant text without the need to close-read the documents. We focus on a case study concerned with detecting smell-related sen- tences in historical medical reports. We demonstrate a process for moving from generic domain label input to a more nu- anced evaluation of the semantics of smell in a set of sentences extracted from this corpus, and then develop a machine learn- ing technique for compounding scores on a variety of modelling parameters into more effective classifications
McGregor, S. and McGillivray, B. (2018). A Distributional Semantic Methodology for Enhanced Search in Historical Records: A Case Study on Smell. Proceedings of the 14th Conference on Natural Language Processing (KONVENS 2018) Vienna, Austria,  September 19-21, 2018. Austrian Academy of Sciences Press.
"
https://www.turing.ac.uk/people/spotlights/vaishak-belle,"

I work on AI and machine learning. Broadly, I’m motivated by the autonomous agency of machines - in the sense of how we can make them learn faster, more accurately model elements of the world, and what this might tell us about our own cognition. Quite a bit of my work involves thinking about models of space, time, dynamics, beliefs, and then finding ways in which these models could be partially learnt from data. Having said that, my work has recently focused on making machine learning interpretable and understanding how ethical behaviour and responsible decision-making could be codified. 
I was initially inspired, as I’m sure many others are, by science fiction. When I first entered the field, however, I was surprised: on the one hand, there weren’t many rigorously defined models of human level cognition, and on the other, much of the mathematically principled parts of the field were concerned with narrow problems around predictions and labels based on collected data. Of course, there were many impactful applications of this work, but I was interested in the more basic problems of artificial cognition. 
This got me interested in mathematical logic. In this area there is some attempt to tackle problems around cognition, however there was very little work on acquisition and learning from data. I then dabbled in machine learning, before ultimately began looking at how the best of both worlds (logic and learning) could be combined. 
The main goal is to have a principled, and rigorous, account of artificial cognition, which is not defined solely by the ability to classify images, such as cats and dogs, but in terms of a commonsensical understanding of the world. This means, among other things, the ability to communicate, to reason, to manipulate and to contextualise decision making in a way that is open-ended or unrestricted.
Of course, since all of this is partly inspired by human cognition but doesn’t necessarily replicate the architecture/functionality in a biologically plausible way, the question of how insights from the two fields can influence each other remains to be seen. 
I will mainly focus on interpretability in machine learning. Although the topic of explainability has interesting connections to fields such as social sciences and human-computer interaction, there is a more urgent need to inspect and scrutinise the boundaries of decision making in machine learning models. I’ll be focusing mostly on how we might approach this, including some recent work we’ve done to categorise these approaches, and future steps involving causality and human-readable model learning. 
To not be so worried about doing your own thing and go where your interests take you - even if you’d find yourself on unsure footing!
I enjoy travelling and backpacking, running, literature and of late, looking after my infant daughter. 
"
https://www.turing.ac.uk/people/external-researchers/jacob-davey,"
Jacob Davey is a Senior Research Manager at ISD where he leads on research into the far-right and hate crime. His work involves mapping far-right extremism and the development of new tools for tracking hate speech globally. In this role Jacob has advised national and local policy makers on right wing extremism. Jacobs recent reports have focussed on tracking the internationalisation and mainstreaming of right-wing extremism, and online interventions for countering extremism.
"
https://www.turing.ac.uk/people/guest-speakers/sam-doody,"
Sam Doody is the Synthetic Aperture Radar (SAR) system Team Leader at Airbus Defence and Space.  He is primarily involved in the domain of spaceborne active and passive microwave sensing, covering detailed design, build, test and operation of radar SAR instruments.  His spaceborne experience covers the implementation of Sentinel-1 and NovaSAR instruments along with many novel concept studies.
Sam also has been involved in SAR system development, trials and experimentation on both manned and unmanned aircraft and drones including on-board processor development.  He is also an active member in both the UK and EU active and passive microwave communities.
"
https://www.turing.ac.uk/research/interest-groups/interpretation-modelling-and-verification-information,"
The consumption of information by individuals, and the assurance that information is accurate, is crucial to the well-being of societies. Misinformation/disinformation and the related misperception (the incorrect perception of policies, phenomena, etc.) is not new, but while in the past, information took time to be spread (giving people more time to verify information), the Internet and in particular social media platforms have enabled information to spread faster than anyone, or any system, can verify its veracity. Such spread can accentuate problems such as the emergence of echo chambers, online incivility, opinion polarization, radicalisation, all of which can have consequences for people’s mental health, democracy, and the fabric of society itself. This Turing Interest Group (TIG) will focus on bringing together researchers of all backgrounds with an interest of leveraging data (such as social media data) together with current AI approaches to process information, leading to the development of solutions to tackle issues of misinformation. The TIG aims to bring the issue to the forefront of research discussion and search for solutions that can help people deal with the quantity and quality of information to which they are exposed.
The interpretation, modelling and verification information is a research area that is cross and inter-sectional drawing from expertise of various fields including data science and artificial intelligence but also, literacy, education, communication studies, psychology, to name a few. Humans are exposed to an amount of information that is beyond anyone’s cognitive ability leading to an information ecosystem that is prone for inaccurate information being spread without verification. 
The amount of information being generated in the world, coupled with an infrastructure that enables the rapid spread of any piece of information, causes the handling of information a challenging task. The general population are unable to know what to trust; what is true; how to interpret information being published; how to verify accuracy of information; know what the consequences of information spread are; to name a few. This TIG aims at understanding this world of information spread and look at how data science and artificial intelligence approaches can help the general population to handle information being produced and at the rate it is being produced. Success for this TIG can be measured in two distinct ways:
Information understanding which includes issues related to misinformation and misperception by the population can be considered one of the problems of our lifetime as it affects security, democracies, politics, policies, public satisfaction, healthcare, to name a few.
Click here to request sign-up and join
[email protected]
"
https://www.turing.ac.uk/news/publications/rethink-government-ai,"
Margetts, H., & Dorobantu, C. (2019). ""Rethink government with AI."" Nature, 568, 163-165. https://doi.org/10.1038/d41586-019-01099-5
2.1 MB
"
https://www.turing.ac.uk/research/research-projects/turing-way-handbook-reproducible-data-science,"
Reproducible research is work that can be independently verified. In practice, it means sharing the data and code that were used to generate published results – yet this is often easier said than done. The Turing Way is a handbook to reproducible, ethical and collaborative data science. We involve and support a diverse community of contributors to make data science accessible, comprehensible and effective for everyone.
Our moonshot goal is to 'make reproducibility too easy not to do'!
In the ideal case, all published results should be independently verifiable and suitable for other researchers to build upon. For this to happen, the data and code that support the publication need to be made available in an easy-to-use and open format. Sharing these research outputs means understanding data management, library sciences, software development and continuous integration techniques: skills that are not widely taught or expected of academic researchers and data scientists. The Turing Way's goal is to provide all the information that researchers and data scientists in academia, industry and the public sector need at the start of their projects to ensure that they are easy to reproduce at the end. Our resources are designed to support researchers as they develop their projects, apply coding skills and handle data, with an aim to produce work that will be regarded as gold-standard examples of trustworthy and reusable research.
Launched in 2019 as a lightly opinionated guide to data science, The Turing Way has since expanded into a series of guides on Reproducible Research, Project Design, Communication, Collaboration and Ethical Research. Each guide offers chapters on a range of topics covering best practices, guidance and recommendations. These chapters have been co-authored by contributors who are students, researchers, educators, community leaders, policy-makers and professionals from diverse backgrounds, lived experiences and domain knowledge.
The Turing Way is an open collaboration and community-driven project. The Turing Way's mission is to involve everybody in data science and research infrastructure roles: the developers of the code (research engineers, postdocs and doctoral students), their supervisors and the business team members who coordinate these projects. Everyone who contributes to this book, no matter how small or big their contributions are, is recognised in this project as a contributor and a community member.
We recognise that the burden of requirement and new skill acquisition can be intimidating to individuals who are new to data science. Therefore the format of The Turing Way chapters is kept modular for the reader to dip in and out of, depending on their level of experience in the various topics. The project will help to answer questions that researchers don't always ask: ""How do I ensure that my code's existing functionality doesn't change as I extend the codebase?"", ""How do I make my project easy for someone else to run?"", and many more.
Senior team members – Turing fellows, program directors and managers – will be catered for with key points tailored towards managing reproducible research projects highlighted for each topic covered. The project will build and curate checklists for what can be done to ensure all project outputs are reproducible. A chapter on Binder will be of interest to supervisors who want to regularly review their students' code and will include the technical details of how to set up a BinderHub that will be useful for research software engineers.
The Turing Way book currently hosts 200 live pages across 50 chapters that have been co-created by over 300 contributors.
A recent version of the book can be cited as: The Turing Way Community. (2021). The Turing Way: A handbook for reproducible, ethical and collaborative research (1.0.1). Zenodo. https://doi.org/10.5281/zenodo.5671094
To hear about events and monthly project updates, sign up to the newsletter.
The Turing Way hosts chapters on research topics cetegorised across the following community-produced guides:
Since 2019, we have developed reusable illustrations collaboratively with The Turing Way Book Dash participants using the scribing service of Scriberia. These have been shared for community use on Zenodo and have been downloaded nearly 10000 times to use in different Open Source materials. A new set of illustrations are released after each Book Dash event that can be reused under CC-BY 4.0 license with the appropriate citation: “This image was created by Scriberia for The Turing Way community and is used under a CC-BY 4.0 Licence. Zenodo. DOI: 10.5281/zenodo.3332807.
The Turing Way community is dedicated to making collaborative, reusable and transparent research accessible. That means investing in the socio-technical skills required to work in a team, to build something more significant than any individual could deliver alone. As an openly developed project, we welcome any and all questions, comments and contributions via the GitHub repository. The Turing Way members regularly offer guidance, review support and mentoring for facilitating contributions by new members.
We have also collaborated with Open Life Science (OLS) to support our members to receive mentoring and training on applying open research principles in the work within The Turing Way.
In 2020 and 2021, multiple projects in the Turing and The Turing Way community successfully graduated from OLS. To support this collaboration in 2021, the Turing Online Training Grant was offered to the Open Life Science in association with The Turing Way. In 2022, The Turing Way team is collaborating with the Training Team to adopt Open Life Science training model in delivering The Turing Way resources via a peer-mentoring programme, as well as co-create a Data Science Educators' Programme.
Contact the project lead Kirstie Whitaker, co-lead Malvika Sharan and community manager Anne Lee Steele.
More ways to connect: bit.ly/turingway
"
https://www.turing.ac.uk/research/research-programmes/data-centric-engineering/data-centric-engineering-challenges,"

Major infrastructure systems such as railways, power plants and supply chains are vital to our way of life. Increasingly these are becoming intense generators of data, and their safe operation is dependent on data science.
We are applying modern data-centric engineering techniques to perform predictive modelling of London Underground trains, designing sensor operation for water distribution networks, and analysing large-scale usage of roads across different modes of transport.
These projects and others in development are all contributing to operating more resilient infrastructure systems, robust against shocks or unexpected incidents and preventing future ‘black sky’ events.
This challenge area is led by Professor Julie McCann, Professor of Computer Systems at Imperial College London.
Understanding and anticipating the impact of rare and high-consequence events in complex engineering systems such as aircraft, power stations or cyber networks, is critical for safety and security in the modern world.
We are using algorithms and statistical techniques to perform health monitoring on vital railway bridges, operate predictive maintenance on gas turbines, and predict rare events in transport systems. We are also modelling extreme events in cyber networks to detect malicious attacks on critical infrastructure.
These projects allow us to operate old and new infrastructure in smarter, safer ways, making best use of data to preserve the operation of vital systems.
This challenge area is led by Professor Emma McCoy, Professor of Statistics at Imperial College London.
Sensors and monitoring equipment are being increasingly employed in engineering applications, from complex computer networks to detecting pedestrian traffic in urban environments. The data being generated presents enormous opportunities to transform both system design and control.
However, to take full advantage of modern technology we need systems that are designed for data, and designed by data. The placement of sensors, the operation of autonomous systems and the way humans and machines interact must all be informed by data-centric engineering.
This challenge area will address fundamental questions of optimal data collection and design optimisation in uncertain environments.
This challenge area is led by Professor Jennifer Whyte, Laing O’Rourke / Royal Academy of Engineering Professor of Systems Integration at Imperial College London.
Rigorous theoretical research and practices are essential for delivering reliable and robust data science solutions within engineering applications.
The objective of the mathematical foundations theme is to facilitate and develop links between the theoretical researchers involved in the programme and the applied projects pursued within it. The aim is to to build a bridge between theory and applications across the programme.
Theoretical research is being conducted in areas including probability theory and mathematical statistics, for translation and application to data science and engineering.
This challenge is led by Professor Alex Mijatovic, Chair in Statistics at the University of Warwick.
It is vital that future generations of engineers appreciate the utility of data and understand how to correctly and astutely deploy modern data analysis techniques.
The programme for data-centric engineering aims to actively promote and develop education in data science. This includes dissemination of the programme’s research outputs among universities and industrial partners.
The education strategy of the programme is currently being developed, and we welcome input from interested parties.
The education theme is led by Professor Omar Matar, Vice-Dean (Education), Faculty of Engineering at Imperial College London.
"
https://www.turing.ac.uk/research/research-projects/quantitative-urban-analytics-quant,"
QUANT is a land use transportation model simulating the location of employment, population and transport interactions designed for predicting the impact of changes to and on these activities. The model predicts the impact of changes in the pattern of urban development as measured by employment, population, transport and physical land use constraints. It simulates such changes using physical models based on gravitational principles which determine location and interactions as a function of travel costs. Its outputs are highly visual for the model is operated in a visual web-based environment delivering many different urban solutions in a matter of seconds so that policy makers and professionals can evaluate a wide range of scenarios associated with future urban plans.
The model simulate spatial interactions which are captured over 3 modes of travel - road, rail and bus - in such a way that the they compete for patronage based on a comparison of the different travel times associated with the underlying networks for each mode. For example if we add a new rail line, this will (usually) reduce the time and thence be more competitive relative to the other modes whose travel times remain the same. This will attract users from road and bus to  travel on rail.
In formal terms, the models that we use to simulate transport are subject to constraints on  employment but they essentially predict where employment resides. The many competitive effects between transport modes and the relative locations of different volumes of population and employment are worked out by the model which in typical systems fashion involve a structure where everything relates to everything else directly but mainly indirectly. The model is highly visual as it is web-based. It resides on a server and using any client - from mobile phone to desktop workstation, it can be accessed and then run in real time. The upper number of users is not known but as yet it is not optimised for many users accessing the system all at the same time. It opens up a separate thread for each distinct user. The following block diagram shows the structure and visual interface.
 
The model is web-based and available free for anyone who wishes to evaluate planning scenarios in any part of the country. After the model has been calibrated (that is, tuned to replicate the existing situation at the base year), a user can define a scenario on-the-fly and then evaluate it through new input data which a user can enter into the model on screen, then running the model which delivers solutions in a matter of seconds. The aim is to empower policy makers and town planning professionals with tools that are big enough to replicate their systems of interest but are available from any place at any time across the web. As the model runs in seconds, a user can generate a sequence of scenarios which can then be used in the selection of a best plan. Planners in practice lack tools to simulate urban futures and although this is a relatively simple sketch planning too, we consider it would have achieved success if it is eventually adopted by  planners in practice. This is a long term aim and it will require much more targeted investment in digital technologies in practice than possible currently, but the project provides a demonstration and a direction. The zoning system for England-Scotland-Wales for employment, population and vector flow averages is shown as  
 

 
QUANT is being applied to large-scale scenarios associated with new urban development and transport which we list as follows in the order that we have modelled their impact.
1. The development of the new east west rail line in London and the South East called Crossrail 1. This rail line involves tunnelling under central London and it joins Reading in the west to Shenfield in the east. We have simulated the modal shifts from road and bus to rail in this scenarios and these shifts plus changes in accessibility are reflected  in house price rises which are already factored in.
2. The development of HS2 - the high speed line from London Euston to Birmingham and thence onwards to Manchester. Recently we have tested the change in this line which involves cutting off the eastern leg to Nottingham-Leeds, which was proposed at the end of 2021.
3. The impact of the third runway extension at Heathrow airport which is in the Greenbelt. Basically the Davies Commission has predicted the growth of 50,000 new jobs if the runway is built and we have tested this on various scenarios involving changes to the Greenbelt in this area.
4. The impact of the line proposed from Cambridge to Oxford via Milton Keynes (in the CAMKOX corridor) known as East-West Rail. This is a highly controversial growth area. 
5. We are currently developing methods for evaluating multiple scenarios defining future solution spaces for different outputs from the model, involving simple machine learning techniques. 
To give some idea of what the model can be used for, we show the impact of Crossrail 1 below. The line is in essence a high speed 'tube' line called the Elizabeth Line to be opened by the end of 2022. As tube in London is heavy rail in any case, it dovetails with National Rail with a link to Heathrow, so that travellers are now able to get from the main London airport to Kings Cross-St. Pancras much more easily and quickly than the current fast trains into London Paddington. The following illustrations show a) the rail line itself b) the improvements in travel times on rail through southern England and c) the growth (red) and decline (blue) in population along the line caused by shifts from road and bus to rail and relocation of population to optimise their relative accessibilities.
 
a)                                    
b)
 
c)

The project so far has been in 2 stages - starting in February 2020 with an end date of January 2021 and then from February 2021 to end January 2022. Currently we are bidding for a third year of funds from Turing. In the first stage we extended the initial QUANT model which had been funded by the Future Cities Catapult to include Scotland. We also used it to link with the LUISA model developed by Ying Jin at the Martin Centre, University of Cambridge where we interfaced the model with output area zones, solving various problems caused by this change of scale. In the second stage, Richard Milton has been working with scenario optimisation and this work is yet to be written up. A major paper has now been published on the model and various elements of QUANT have been used to simulate the RAMP epidemic model developed in the Turing Urban Analytics stream: see 
 
"
https://www.turing.ac.uk/people/guest-speakers/ali-boushehri,"
Ali Boushehri is a PhD student at Roche, in collaboration with Helmholtz Munich, working in the intersection of explainable AI, computer vision and single-cell analysis. Specifically, Ali is investigating how explainable AI and computer vision can be used in drug discovery and development. Before that, he worked as a data scientist for four years, working on a variety of machine learning related tasks. 
"
https://www.turing.ac.uk/events/ai-art-0,"
The workshop brings together stakeholders in AI art, including universities, artists, curators, gallerists, and companies in the AI and Creative Industries sectors.
The event will consist of talks and roundtables. As a result, a formal network will be established, which will expand on a 1-5 basis (each participant will invite 5 new participants for the next round), with a webpage, social media, and dedicated communication channels. This network will foster collaboration in promoting AI in art at all cycles of the production and distribution cycle (from recommenders to content generation and post-production), facilitate an open debate on ethical and legal aspects of AI technology and its socio-economic impact, and in training responsible professionals through internships and apprenticeships.
In order to generate a coherent community and meet the goals of this workshop, registration for this event will be invitation-only.
"
https://www.turing.ac.uk/events/events-around-turing-network,"

To showcase the breadth of activity taking place across the UK's AI and data science landscape, please find below a list of events hosted by universities from around the Turing University Network. If you have questions or would like to attend an event, please follow the relevant link and contact the university directly.
In most cases, these events are developed, managed and hosted by the individual universities. The Alan Turing Institute has no direct involvement in their organisation, or responsibility for their agendas, content or overall delivery. Where there is an exception, this will be noted on the individual event page.
We are pleased to announce that the Institute is currently inviting applications from universities across the UK to join the Turing University Network. Applications are being accepted via an open call, all UK universities with an interest in data science and AI are invited to apply. Full details are available on the Turing University Network Call 2023 page.
The Alan Turing Institute endeavours to keep health and safety at the forefront of all events. 
While it’s no longer a legal obligation, face coverings and hand sanitiser is available at our events. If you are displaying any COVID symptoms or have received a positive test, please do not attend the event. 
If you have any concerns regarding attendance at an event due to health and safety, please contact [email protected] and member of the team will get back to you.
 
"
https://www.turing.ac.uk/people/guest-speakers/tabitha-goldstaub,"
Tabitha is chair of the UK government's AI Council and co-founder of CogX. A serial entrepreneur, Tabitha was the co-founder of video distribution company Rightster (IPO 2011). An advocate for increased diversity in tech, Tabitha is the author of How to Talk to Robots – a girls' guide to a world dominated by AI. She's also a co-founder of Future Girl Corp and is an advisor to TeensInAI and on the board of Tech UK.
"
https://www.turing.ac.uk/people/guest-speakers/alice-pease,"
Alice is a passionate geoscientist broadening her skillset into digital & data architecture technology project management. Her 8-year career has so far spanned technical disciplines, continents, onshore and offshore wellsites, multi-national energy operators and now the public sector in energy transition R&D.
Alice has demonstrable experience in well log data acquisition and interpretation. Such interpretations have fed into high-value decisions while drilling high-pressure/high-temperature wells on the UK Continental Shelf. She has also provided petrophysical evaluations for the purposes of carbon storage.
In her current role at the Net Zero Technology Centre, Alice is responsible for the management of digital & data architecture technology development projects that are funded by the Centre. She is also involved in the technical assessment of technology proposals submitted to the NZTC. Alice manages projects that include R&D towards a proof-of-concept subsea AI body of knowledge (SAIBOK) that will demonstrate the potential for machine learning in subsea inspection. Alice works closely with technology developers, industry partners and academia to ensure projects reflect the business case and operate within the framework governing the use of public funds.
Alice is an Associate Member of the Energy Institute, holds a BSc in Geology-Petroleum Geology and an MSc in Integrated Petroleum Geoscience from the University of Aberdeen. She completed the Maersk International Technology and Science programme in 2016, which included an executive MBA at the University of California – Berkeley, and qualified with IFP Training as a Subsurface Geoscience Operations professional in 2019. Alice is presently undertaking project management certificates and is enthusiastic about mentoring the next generation of STEM professionals.
"
https://www.turing.ac.uk/research/research-projects/free-energy-landscapes-big-data,"
Machine learning is used in a range of data science tasks, but it's necessary to explain the natural geometric structure of landscapes arising in machine learning to derive efficient algorithms.
Many important challenges in data science can be reduced to the problem of minimising a high dimensional function, typically for the parameters of a neural network. In practice, given the vast numbers of parameters involved (for deep networks this can be millions or more), the available data is often sparse (it may even be smaller than the number of parameters) and a full exploration is never possible.
Much work is therefore needed to explain the natural geometric structure of landscapes arising in machine learning and to use this knowledge of the landscape to derive efficient algorithms.
In close analogy to the problems of data science, simulations in chemistry and physics often involve modelling the 'energy landscape' which describes the relative weight of different states, for example describing the configurations of atoms. The complexity and high-dimensionality of the underlying system often make exploration of the most probable states difficult.
This challenge has given rise to the development of the Monte-Carlo (MC) method, and molecular dynamics (MD). The development of molecular sampling methods has gone hand in hand with the design of algorithms for optimisation, for example the protein folding problem is usually formulated as a global optimisation problem. Optimisation also plays a key role in the refinement of experimental (e.g. spectroscopy) data and the search for structural motifs for new materials.
Molecular dynamics-like sampling methods can often be adapted to data science, for example helping circumvent problems due to over fitting in parameter inference. Identification of suitable variables becomes the most important task in practice and then problems can be reduced to calculating barriers in a 'free-energy' landscape.
This project is focusing algorithm development to the task of choosing suitable variables for molecular dynamics-like sampling methods, as this task is critical to the efficiency of these methods.
In molecular sampling one powerful recent approach to automatic determination of variables has been developed recently based on diffusion maps, an idea that in fact originated in harmonic analysis (and which provides a systematic procedure for manifold learning).

Within the scope of the project a software toolkit called TATi has been developed that allows for analysing the loss manifolds of neural networks in the field of machine learning. TATi stands for Thermodynamic Analytics Toolkit, meaning that sampling methods such as Langevin samplers have been used that are based on concepts of temperature, heat, and energy from statistical physics. The goal is to bring answers to the questions why neural network training works so well. Neural networks operate in very high-dimensional spaces and their cost functions are generally non-convex. Therefore, it is not straight-forward to understand why training does not get stuck in ""bad"" local minima. Answers to this question have the potential to accelerate training and to allow for more efficient networks.
MNIST is one of the major prototypical datasets in machine learning. It consists of a dataset of 70,000 grey-scale images of hand-written digits, 28 by 28 pixels in size. The goal is to correctly associate each image with its digit. A very simple neural network, the single-layer perceptron, for this problem already has 7850 degrees of freedom. Large state-of-the-art networks easily have one million degrees of freedom. In order to visualise the loss manifold, the project researchers look at the covariance matrix obtained from sampling the loss manifold. In the figures shown the eigenvectors of the covariance matrix associated with the first and strongest eigenvalue and the 64th smaller eigenvalue have been used. Moreover, the figures depict a typical optimisation trajectory whose end point, the local minimum, is chosen as the origin.
The visualisation shows a large funnel. If one zooms into the minimum region, depicted by the red square, i.e. move deeper into the funnel from left to right, then it's possible to see that the walls of the funnel are no longer smooth but that they have many tiny bumps, i.e. local minima, especially at its bottom. This corresponds well with theoretical predictions for this type of network. With TATi the project aims to dig a lot deeper, looking at many different loss functions and networks in order to generalise results and make predictions.
 
[email protected]
"
https://www.turing.ac.uk/events/turing-lecture-building-dynamic-robots,"
123.4 KB
In conversation with Marc Raibert of Boston Dynamics. 
Event format:
Lecture - 20 minutes
In conversation with Sethu Vijayakumar (including Spot mini-demo) - 30 minutes
Audience Q&A - 30 minutes
Public opinion of robots has seen a positive turn since the global lockdown. Their use in hospitals, as social distancing assistance and personal companions have given us more reason than ever to welcome them into our lives, our homes and our public services.
In this lecture, we’ll hear from the man behind one of the biggest robotics companies in the world, Boston Dynamics, whose robotic dog, Spot, has been used to encourage social distancing in Singapore and is now getting ready for FDA approval to be able to measure patients’ vital signs in hospitals.
Making robots move dynamically in order to do versatile tasks more safely, efficiently and economically sustainably has been one of the holy grails of robotics. We will explore why this is a difficult challenge, both in terms of creating appropriate hardware as well as developing robust, reactive planning and control systems.
Professor Sethu Vijayakumar, Turing Programme Director for AI (Robotics and Autonomous Systems), will explore these questions in conversation with one of the indisputable leaders in this field, Marc Raibert, Founder and Chairman of Boston Dynamics.
Boston Dynamics makes some of the world’s most advanced robots, including Spot, BigDog, Atlas, and Handle. Boston Dynamics specializes in dynamic robots that travel where other robots can’t go, handle larger payloads with smaller footprints, and move faster to get work done more quickly.
Making robots dynamic opens up a broader range of behaviour than static or trajectory-driven robots. But they are also more challenging to create because the robot hardware needs to have higher performance characteristics and the control systems need to predict future behaviour in addition to sensing ongoing behaviour.
Raibert will tell the audience about the latest robots being worked on at Boston Dynamics, including robots being deployed today, new robots being developed for commercial applications tomorrow, and R&D robots that lead to future innovations.
We'll also attempt a live demonstration of a robot being operated across continents (Edinburgh to Boston) and take a look at some of the advanced robots at Edinburgh Centre for Robotics.
 

The Edinburgh Centre for Robotics (ECR) is a £120m joint venture between Heriot-Watt University and the University of Edinburgh, supported by EPSRC, Industry and the Universities.
It captures the expertise of 50 principle investigators of international standing from 12 cross-disciplinary research groups and institutes across the School of Engineering and Physical Sciences and the Department of Computer Science at Heriot-Watt University, and the Schools of Informatics and Engineering at the University of Edinburgh.
The Centre includes an EPSRC Centre for Doctoral Training (CDT) in Robotics and Autonomous Systems which trains innovation– ready postgraduates, the Robotarium, a £8m national capital equipment facility and the £35m ORCA Hub.
"
https://www.turing.ac.uk/people/researchers/polly-hudson,"
Polly Hudson is a Senior Research Fellow at The Alan Turing Institute and project lead for the Colouring Cities Research Programme. She was  previously a Senior Research Fellow at the Centre for Advanced Spatial Analysis University College London, where she developed the Colouring Cities concept, and has also been a Visiting Fellow at the Kellogg Centre for the Historic Environment, University of Oxford.
Polly trained as an architectural historian, and worked initially in historic building restoration, museum and exhibition design, and community planning. In 1996 she set up the Building Exploratory charitable trust in London as a prototype for creative, multidisciplinary knowledge sharing centres about building stocks where the first iteration of Colouring London was built and tested. Polly is a member of the ‘Computer Vision and Digital Heritage’ and ‘Facilitating Participation in Data Science ‘special interest groups at Turing and sits on the UK’s Digital Twin Hub Community Council. She has also worked on advisory committees and boards for the Department of Culture, Media and Sport, English Heritage, The Royal Institute of British Architects, and the National Lottery.
"
https://www.turing.ac.uk/people/turing-ai-fellows/philip-torr,"
Professor Philip Torr did his PhD (DPhil) at the Robotics Research Group of the University of Oxford under Professor David Murray of the Active Vision Group. He worked for another three years at Oxford as a research fellow, and still maintains close contact as visiting fellow there.
He left Oxford to work for six years as a research scientist for Microsoft Research, first in Redmond, USA, in the Vision Technology Group, then in Cambridge founding the vision side of the Machine Learning and Perception Group. He then became a Professor in in Computer Vision and Machine Learning at Oxford Brookes University, where he has brought in over one million pounds in grants for which he is PI. Recently in 2013, Philip returned to Oxford as full professor where he has established the Torr Vision group.
He won several awards including the Marr prize (the highest honour in vision) in 1998. He is a Royal Society Wolfson Research Merit Award Holder. Recently, together with members of his group, he has won several other awards including an honorary mention at the NIPS 2007 conference for the paper 'P. Kumar, V. Kolmorgorov, and P.H.S. Torr, An Analysis of Convex Relaxations for MAP Estimation', in NIPS 21, Neural Information Processing Conference, and (oral) Best Paper at Conference for 'O. Woodford, P.H.S. Torr, I. Reid, and A.W. Fitzgibbon, Global Stereo Reconstruction under Second Order Smoothness Priors', in Proceedings IEEE Conference of Computer Vision and Pattern Recognition, 2008 . More recently he has been awarded best science paper at BMVC 2010 and ECCV 2010.
He was involved in the algorithm design for Boujou released by 2D3. Boujou has won a clutch of industry awards, including Computer Graphics World Innovation Award, IABM Peter Wayne Award, and CATS Award for Innovation, and a technical EMMY. He then worked closely with this Oxford based company as well as other companies such as Sony on the Wonderbook project.
He is a director of new Oxford based spin out OxSight, and Chief Scientific Advisor for Five AI.
He was elected Fellow of the Royal Academy of Engineering (FREng) in 2019, and Fellow of the Royal Society (FRS) in 2021 for contributions to computer vision. In 2021 he was made Turing AI world leading researcher fellow.
"
https://www.turing.ac.uk/people/guest-speakers/jennifer-ngadiuba,"
Dr. Jennifer Ngadiuba is Associate Scientist with Wilson Fellowship at the Fermi National Accelerator Laboratory, the leading facility for particle physics research in the United States. She is specialised in the application of AI to particle physics towards more intelligent detector systems, data reduction and data analysis strategies. This ensures the efficient extraction of the most fundamental physics information from the multitude of data collected at the Large Hadron Collider (LHC), the world’s highest-energy particle physics experiment located at the CERN laboratory (Switzerland-France).
Jennifer co-founded the fast machine learning organisation, a research collective of physicists, engineers and computer scientists interested in deploying machine learning algorithms for unique and challenging scientific applications. Dr. Ngadiuba is also applying modern anomaly detection techniques to advance the quest for elusive new physics at the LHC.
"
https://www.turing.ac.uk/events,"

View events from around the Turing University Network.

The Alan Turing Institute endeavours to keep health and safety at the forefront of all events. 
While it’s no longer a legal obligation, face coverings and hand sanitiser is available at our events. If you are displaying any COVID symptoms or have received a positive test, please do not attend the event. 
If you have any concerns regarding attendance at an event due to health and safety, please contact [email protected] and member of the team will get back to you.
 
View our Events terms and conditions (PDF)
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/modelling-statistical-methods-theory,"
"
https://www.turing.ac.uk/news/publications/gnar-edge-model,"
In economic and financial applications, there is often the need for analysing multivariate time series, comprising of time series for a range of quantities. In some applications such complex systems can be associated with some underlying network describing pairwise relationships among the quantities. Accounting for the underlying network structure for the analysis of this type of multivariate time series is required for assessing estimation error and can be particularly informative for forecasting. Our work is motivated by a dataset consisting of time series of industry-to-industry transactions. In this example, pairwise relationships between Standard Industrial Classification (SIC) codes can be represented using a network, with SIC codes as nodes, while the observed time series for each pair of SIC codes can be regarded as time-varying weights on the edges. Inspired by Knight et al. (2019), we introduce the GNAR-edge model which allows modelling of multiple time series utilising the network structure, assuming that each edge weight depends not only on its past values, but also on past values of its neighbouring edges, for a range of neighbourhood stages. The method is validated through simulations. Results from the implementation of the GNAR-edge model on the real industry-to-industry data show good fitting and predictive performance of the model. The predictive performance is improved when sparsifying the network using a lead-lag analysis and thresholding edges according to a lead-lag score.
https://arxiv.org/abs/2305.16097
1 MB
"
https://www.turing.ac.uk/news/alan-turing-institute-responds-publication-governments-ai-regulation-white-paper,"
The government has published its white paper on artificial intelligence (AI) regulation setting out the UK’s approach to regulating AI. 
 
The white paper, from the Department for Science, Innovation and Technology, provides a roadmap for how the UK’s sector-specific approach to AI regulation will work in practice. The Turing believes this approach, that draws on the strength of the UK’s existing regulators, is crucial to the effective regulation of AI.  
 
Five principles will guide the use of AI in the UK, as part of a new national blueprint for regulators to steer responsible AI innovation and maintain public trust in this technology.  
 
The government has long recognised the crucial role of standards in encouraging responsible innovation. And as detailed in the white paper the AI Standards Hub, an initiative led by The Alan Turing Institute in partnership with BSI and NPL, will play an important role in the government’s vision for AI governance.   
 
The white paper also aims to create a central function to help monitor developments and identify risks. A recent Turing report supports the government recommendation for central capacity building and greater coordination across regulators to meet the challenge of regulating activities transformed by AI. 
 
Organisations and individuals working with AI can share their views on the white paper as part of a new consultation launching today.  The government's emphasis on stakeholder engagement and the recognition of the need for an iterative approach to AI regulation are crucial elements, which will enable the UK to finetune its approach.
Sir Adrian Smith, Institute Director at The Alan Turing Institute, said: “This white paper is good news for how AI is used and regulated in the UK. We’re very pleased to see that the government’s approach builds on the strength of existing regulators. A principle-based, sectoral approach is crucial to ensure that AI is regulated in the most effective and efficient way. 
“As the national institute for AI, we look forward to contributing to the consultation and working alongside the government to fulfil its ambition for the UK to be a powerhouse for responsible AI.” 
"
https://www.turing.ac.uk/events/public-engagement-data-science-and-health,"
Everyone is welcome to this online event, but the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn about the challenges of defining multiple long term conditions and strategies that can be applied to their respective areas of work.
Here’s a description of the session:
Trustworthy data science depends on engagement with patients and the public- the communities who are affected by data science and digital tools used in healthcare and health research.
Datasets, digital tools and analytics need to be designed for – and with – diverse populations, so they don’t only work for some groups at the expense of others.
Our speaker from Wellcome Trust will share three examples of different approaches to engaging with patients and the public:
Register in advance for this event
"
https://www.turing.ac.uk/people/guest-speakers/elizabeth-dolan,"
Elizabeth is a PhD Candidate, at the University of Nottingham’s Digital Economy Horizon Centre of Doctoral Training and N/LAB research centre for the use of big data and machine learning, specialising in data donation and digital health.  Elizabeth is investigating using shopping data in medical research, specifically asking: how can personal transactional data be collected and analysed for the purposes of health research in a way that is acceptable to society, and works for infectious and chronic disease?
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/high-dimensional-inference,"
"
https://www.turing.ac.uk/people/business-team/tracey-peterson,"
"
https://www.turing.ac.uk/people/enrichment-students/lukas-franken,"
Lukas is a PhD student working at the intersection of machine learning and energy systems. He started his doctoral studies at the University of Edinburgh in Summer 2022 after completing a Master's degree in Physics at the University of Cologne, Germany. During his Master's, Lukas worked on quantum computing and was active as a student researcher at the Fraunhofer Institute IAIS, where he worked on the theory of deep learning.
Lukas is mainly but not exclusively interested in energy systems and long-term storage technologies. In that context, Lukas applies machine learning in various capacities: First, some of the most affordable and efficient storage technologies are very slow to release their energy. Hence, they are commonly used as part of district heating systems where being able to predict demand can be greatly beneficial in their operation. It turns out that such a district's design affects how easy it is to predict that demand.
Another aspect, in which machine learning is relevant in energy systems comes with its potential to reduce the computational overhead of energy system simulations. Due to their complexity, such simulations are usually run on software that takes a long time to execute and acts as a black-box. However, it is common that between simulation runs, only a small number of input parameters are changed while only a single or a small number of output variables are of interest. Hence, given a set of simulation runs, a machine learning model can learn the relationship between input parameters and output variables, replace the software in future runs and thereby greatly reduce runtime.
"
https://www.turing.ac.uk/people/turing-research-fellows/anna-freni-sterrantino,"
Anna Freni Sterrantino joined The Alan Turing Institute in June 2021 as a Senior Research Associate in the Shocks and Resilience project. Her work focuses on developing new theory and methods for understanding causality and dynamics in complex interconnected systems under conditions of uncertainty.
Her background is in Spatio-Temporal Bayesian modelling and in environmental epidemiology. After completing her PhD in Methodological Statistics from the University of Bologna, she held postdoc positions at the Environmental Health Agency (Italy) and at the Small Area Health Statistics Unit  (Imperial College London).  
In addition to her academic research, Anna is a certified coach (ILM Level 5) and collaborates with the Imperial Postdoc centre to promote women in science. She is also the Program Chair at the International Society for Applied Bayesian (ISBA) Section on Biostatistics and Pharmaceutical Statistics.
"
https://www.turing.ac.uk/people/researchers/sukankana-chakraborty,"
Sukankana is a Research Assistant at The Alan Turing Institute where she works as a part of the Shocks and Resilience project. In this role, she aims to understand how information (or misinformation and harmful content) spreads on online platforms and its impact on policy interventions. Prior to this, she was a PhD student at the University of Southampton where she studied approaches to optimise network-based interventions in social networks.
"
https://www.turing.ac.uk/events/event-type/seminar,"
"
https://www.turing.ac.uk/events/nature-reviews-physics-science-age-large-language-models,"
Rapid advances in the capabilities of large language models (LLMs) and the broad accessibility of tools powered by this technology have led to both excitement and concern regarding their use in science. LLMs can now generate scientific text that is often undistinguishable from human-written one. The scientific community is yet to come to terms with this new capability and fully understand its positive and negative impact. There are many open questions regarding the implications for science assessment, communication and the scientific method itself.

 
In this panel discussion experts in AI ethics and policy will explore these issues and ultimately ask what will science look like in the age of LLMs and what will be the role of the human scientist. The panel will be moderated by Professor David Leslie Director of Ethics and Responsible Innovation Research at The Alan Turing Institute.
 
The discussion will touch upon three main themes...
1.  Accuracy, reliability and accountability
2.  Explainability, missingness and bias
3.  Science assessment and peer review
The panelists will discuss open questions which emerge from each of the themes listed above.
This event features a 40-minute panel discussion followed by a 20-minute Q&A session.
Register for free here.
"
https://www.turing.ac.uk/people/adam-johansen,"
Professor in Statistics, University of Warwick and Group Leader of the data-centric engineering programme at the Turing
Adam’s research focuses upon methodological and theoretical aspects of simulation-based algorithms.
 
 
"
https://www.turing.ac.uk/news/publications/core-periphery-structure-directed-networks,"
While studies of meso-scale structures in networks often focus on community structure, core--periphery structures can reveal new insights. This structure typically consists of a well-connected core and a periphery that is well connected to the core but sparsely connected internally. Most studies of core--periphery structure focus on undirected networks. We propose a generalisation of core-periphery structure to directed networks. Our approach yields a family of core-periphery block model formulations in which core and periphery sets are edge-direction dependent. We mainly focus on a particular core--periphery structure consisting of two core sets and two periphery sets which we motivate empirically. To detect this directed core-periphery structure we propose four different methods, with different trade-offs between computational complexity and accuracy. We assess these methods on three benchmarks and compare to four standard methods. On simulated data, the proposed methods match or outperform the standard methods. Applying our methods to three empirical networks -- a political blogs networks, a faculty hiring network, and a trade network -- illustrates that this directed core--periphery structure can offer novel insights about the underlying dataset.
Elliott, A., Chiu, A., Bazzi, M., Reinert, G., & Cucuringu, M. (2019). Core-Periphery Structure in Directed Networks. arXiv preprint arXiv:1912.00984.
"
https://www.turing.ac.uk/people/researchers/raul-santos-rodriguez,"
Raul Santos-Rodriguez is a Turing AI Fellow and Associate Professor in Data Science and AI at the Department of Engineering Mathematics, University of Bristol. Prior to his current role, he held positions both in industry and academia. He worked as a lead data scientist developing large-scale recommender systems in industry, as a Research Fellow at the Universitat de Valencia and as a postdoctoral researcher at the University of Bristol. His research interests lie around the foundations of human-centric machine learning, data science, artificial intelligence and their application to healthcare, media analysis, music information retrieval and more.
"
https://www.turing.ac.uk/people/guest-speakers/james-lambert,"
"
https://www.turing.ac.uk/people/doctoral-students/ioana-bica,"
Ioana is a second year Doctoral student at the Department of Engineering Science, University of Oxford. She is supervised by Professor Mihaela van der Schaar and co-supervised by Professor Pietro Lio. She has previously completed a BA and MPhil in Computer Science at the University of Cambridge where she has specialised in machine learning and its applications to biomedicine. Ioana has worked on research projects involving integration of transcriptomics and epigenomics data using cross-modal neural networks, variational autoencoders for modelling gene expression data and neural network interpretability.
Ioana’s doctoral research focuses on developing machine learning methods aimed at improving healthcare and advancing medical research. She has worked on building methods for causal inference and individualised treatment effect estimation from observational data. In particular, she has developed machine learning methods capable of estimating the heterogeneous effects of time-dependent treatments, thus enabling us to determine when to give treatments to patients and how to select among multiple treatments over time. Recently, Ioana has started working on methods for understanding and modelling clinical decision making through causality and inverse reinforcement learning.
Peer-reviewed conference publications (* denotes equal contribution). Ioana has presented their work at all the conferences for which they were the first author on the paper.
1. Ioana Bica, Ahmed M. Alaa, James Jordon, and Mihaela van der Schaar. ""Estimating counterfactual treatment outcomes over time through adversarially balanced representations.” in International Conference on Learning Representation (2020).
2. Ioana Bica, Ahmed Alaa, and Mihaela Van Der Schaar. ""Time Series Deconfounder: Estimating treatment effects over time in the presence of hidden confounders."" in International Conference on Machine Learning (2020).
3. Ioana Bica*, James Jordon*, and Mihaela van der Schaar. ""Estimating the Effects of Continuous-valued Interventions using Generative Adversarial Networks."" Advances in Neural Information Processing Systems (2020).
4. Daniel Jarrett*, Ioana Bica*, and Mihaela van der Schaar. ""Strictly Batch Imitation Learning by Energy-based Distribution Matching."" Advances in Neural Information Processing Systems (2020).
5. Jeroen Berrevoets, James Jordon, Ioana Bica, and Mihaela van der Schaar. ""OrganITE: Optimal transplant donor organ offering using an individual treatment effect."" Advances in Neural Information Processing Systems (2020).
6. Can Xu, Ahmed Alaa, Ioana Bica, Brent Ershoff, Maxime Cannesson, and Mihaela Schaar. ""Learning Matching Representations for Individualized Organ Transplantation Allocation."" in International Conference on Artificial Intelligence and Statistics (2021).
7. Ioana Bica, Daniel Jarrett, Alihan Hüyük, and Mihaela van der Schaar. "" Learning ""What-if"" Explanations for Sequential Decision-Making"" in International Conference on Learning Representations (2021).
8. Daniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, and Mihaela van der Schaar. ""Clairvoyance: A Pipeline Toolkit for Medical Time Series.” in International Conference on Learning Representations (2021).
9. Ioana Bica*, Daniel Jarrett*, Mihaela van der Schaar. “Invariant Causal Imitation Learning for Generalizable Policies” in Advances in Neural Information Processing Systems (2021).
10. Zhaozhi Qian, Yao Zhang, Ioana Bica, Angela Mary Wood, Mihaela van der Schaar. “SyncTwin: Treatment Effect Estimation with Longitudinal Outcomes” in Advances in Neural Information Processing Systems (2021).
11. Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. “Time-series Generation by Contrastive Imitation” in Advances in Neural Information Processing Systems (2021).
12. Alex J. Chan, Ioana Bica, Alihan Huyuk, Daniel Jarrett, and Mihaela van der Schaar. ""The Medkit-Learn (ing) Environment: Medical Decision Modelling through Simulation."" in Advances in Neural Information Processing Systems Datasets and Benchmarks Track (2021).
Peer-reviewed journal publications:
13. Ioana Bica, Ahmed M. Alaa, Craig Lambert, and Mihaela van der Schaar. ""From real‐world patient data to individualized treatment effects using machine learning: Current and future methods to address underlying challenges."" Clinical Pharmacology & Therapeutics (2020).
14. Pedro Baqui*, Ioana Bica*, Valerio Marra, Ari Ercole, and Mihaela van Der Schaar. ""Ethnic and regional variations in hospital mortality from COVID-19 in Brazil: a cross-sectional observational study."" The Lancet Global Health, (2020).
15. William R.Zame, Ioana Bica, Cong Shen, Alicia Curth, Hyun-Suk Lee, Stuart Bailey, James Weatherall, David Wright, Frank Bretz, and Mihaela van der Schaar. ""Machine learning for clinical trials in the era of COVID-19."" Statistics in Biopharmaceutical Research (2020).
Workshops organized:

- Machine Learning for Healthcare (ML4H) Workshop at NeurIPS 2020.
- AI for Public Health Workshop at ICLR 2021.
- Machine Learning for Healthcare (ML4H) Symposium co-located with NeurIPS 20201.
"
https://www.turing.ac.uk/taxonomy/term/1277,"

Machine learning is no longer restricted to data analysis, currently being used in theory, experiment and simulation ⁠– a sign that AI is becoming pervasive in all traditional aspects of research. But are theorists, experimentalists and computational scientists aware of each other’s problems, and the solutions developed to tackle them? Are researchers working in different areas of physics aware of developments in other areas?
Nature Reviews Physics and The Alan Turing Institute are tapping into international and local expertise to try to identify the different classes of problems that machine learning can help physicists solve and the key issues that physicists and machine learning scientists should be aware of. In doing so we are hoping to create a framework to facilitate understanding and the exchange of ideas. Through a series of talks followed by discussions we aim to identify the most exciting developments in using AI in different areas of physics. A seminar series organised by Nature Reviews Physics and The Alan Turing Institute.



Nature Reviews Physics and The Alan Turing Institute are in the unique position to be able to reach out to the broader physics and AI communities and bring together international and local expertise. This partnership offers the opportunity to harness community insight and benefit from editorial guidance to produce streamlined discussions and high-quality summary content.
This innovative partnership encourages collaboration between fields, institutions and businesses. The aim being to solve mutual issues, benefitting all. Find out more about ASG.
This series is of interest to physicists from all fields who are using or intending to use AI in their research. It is equally relevant to AI scientists who are developing new machine learning tools and who would want to know what types of problems physicists are trying to solve. The talks and follow-up discussions are non-technical so that the content is accessible to non-specialists. 
Each episode within the series is saved on the Turing YouTube page.
Watch the seminar series here
If you have any further questions about the events, please contact [email protected].


"
https://www.turing.ac.uk/blog/elon-musk-wants-reduce-content-moderation-twitter-concerning-it-sounds,"
Over the last two decades, Elon Musk has become a household name for creating and leading some of the most successful companies in the world, including Tesla, SpaceX and PayPal. He now has his sights set on Twitter, and plans to turn the already successful social media platform into a tech juggernaut, growing user count by 200% over the next three years and bringing in five times more revenue by 2028.
Large companies are bought and sold all the time, and consumers aren’t generally concerned about who the companies' owners are. But this controversial deal – if it goes ahead – gives Elon Musk access to a very powerful way of influencing the world.
Musk is known for sharing his thoughts online, uncensored, so we already have a good sense of what he plans for Twitter. He has promised to scale back content moderation to protect free expression and to restore former US President Donald Trump’s account. But should we be worried about this more relaxed approach to moderation?
Content moderation is how social media platforms ensure that content follows their rules and guidelines, and is safe for users to view. Companies like Twitter usually use a mix of algorithms and people to review content reported by users, and algorithms alone to proactively search for violating content. Moderation is vital to reduce abusive and toxic messages on social media – but it’s not perfect. Content moderation processes won’t catch everything and, in some cases, moderation removes content that should be allowed to stay. They also miss lots of genuinely toxic content and activity, which can be incredibly difficult to identify. For instance, many fake accounts and bots on Twitter do not just have long alphanumeric strings for names (e.g. ‘werkulderkle29192’), egg icons for profile pictures, and retweet scam ads every 15 minutes. They are also sophisticated operators which mimic real human behavior, even acting innocuously for months until they do something nefarious.
The law might seem like an obvious starting point for online safety (this is, after all, what Elon Musk has pushed for!) – but in reality, turning to the law raises more problems than it solves. Few laws have been passed specifically to address online safety, and those that do have only been brought in very recently, such as the 2021 Online Safety Act in Australia or 2017’s NetzDG in Germany. In the UK, the forthcoming Online Safety Bill has the admirable aim of ensuring that offences offline are also illegal online; but it still has not yet been passed into law. And a fundamental problem is that it builds on pre-existing laws and so could potentially replicate the same gaps, limitations and practical challenges. For instance, the 1986 Public Order Act is the main way in which hate is tackled in offline settings in the UK – but it still does not consider misogyny to be a hate crime, despite widespread violence and abuse against women. These issues are compounded by the fact that platforms are inherently global, yet laws are set by each country – and, as such, the provisions in the Online Safety Bill for the UK are different to those outlined in Australia, New Zealand and America (and all other territories introducing online safety laws). Put simply, most platforms would find it almost impossible to just “apply the law”. 
But, in turn, the big problem with platforms going beyond the law is that deciding what can be seen, said and shared online has been left to a tiny number of Silicon Valley executives. And at a time when most platforms are beefing up their online trust, safety and integrity, Elon Musk is using that power to propose a radical shake-up – to dramatically downsize the amount of moderation that it applies, raising concerns that Twitter could become more like free speech niche sites such as Gab, 4chan and Truth Social. 
Scaling back content moderation may at first, understandably, sound concerning. But the flipside of less top-down content moderation is that users could be empowered to control what content they view. If Twitter develops a market for what political scientist Francis Fukuyama calls ‘middleware’ (technology that allows users to choose their own software to identify and handle harmful content), we could witness a huge increase in choice through personalised trust and safety. This would take editorial power away from a very small number of large, wealthy technology platforms and give it to users, enabled by a diverse range of competitive firms that would allow people to tailor their experience online.
But this, again, presents challenges. First, the average social media user is going to find it difficult and time-consuming to evaluate the different middleware tools and choose the right one for them. Second, and perhaps most importantly, there just aren’t that many providers to choose from. Although there is mounting investor interest in the growing market of 'safety tech' providers, and trust and safety trade organisations are appearing, the market is still emerging. If the Musk deal goes through and if he does reduce content moderation (and those are two big ifs!) then there is a lot to do to ensure that Twitter doesn’t just become even more toxic.   
And, of course, even if the middleware market is created then we may see a range of new problems appear. It could increase polarisation of opinions; leave people unprotected; create more digital inequalities based on ability to pay; and we may see insidious effects of people opting not to remove potentially harmful – or what might be considered ‘grey area’ – content. And there’s no guarantee that other platforms would follow Twitter, potentially limiting the full impact and growth of the market.
But… that aside, this may well be the best option in online safety that hasn’t yet been tried. Without more diversity and options, we will continue to have top-down moderation from platforms that alienates users, is rarely explained, and routinely makes serious mistakes. 
Dr Bertie Vidgen is Head of Online Safety at The Alan Turing Institute and co-founder of Rewire, a tech startup building socially responsible AI for online safety. 
 
Top image: Rokas Tenys / Shutterstock
"
https://www.turing.ac.uk/people/external-researchers/victor-meirinhos,"
"
https://www.turing.ac.uk/people/guest-speakers/andrew-strait,"
Andrew is an Associate Director at the Ada Lovelace Institute and is responsible for Ada’s work addressing emerging technology and industry practice.
Andrew has spent the last decade working at the intersection of technology, law and society. Prior to joining Ada, he was an Ethics & Policy Researcher at DeepMind, where he managed internal AI ethics initiatives and oversaw the company’s network of external partnerships. Previously, he worked as a Legal Operations Specialist at Google where he developed and implemented platform moderation policies on areas such as data protection, hate speech, terrorist content and child safety.
"
https://www.turing.ac.uk/people/external-researchers/oriol-bosch-jover,"
Oriol Bosch is a fourth-year PhD candidate in Social Research Methods supervised by Professor Patrick Sturgis and Professor Jouni Kuha. He is also a Research Assistant at The Alan Turing Institute and a Non-Resident Researcher at the Research and Expertise Centre for Survey Methodology (RECSM).
As a methodologist, Oriol focuses on understanding how to better collect and analyse attitudinal and behavioural data for the social sciences. He specializes in topics related to web and mobile surveys and the use of digital trace data and sensors to enhance or substitute surveys. His work, published in journals such as Social Science Computer Review or the Journal of the Royal Statistical Society, has explored the measurement quality of survey scales in online surveys using MTMM experiments; the generational divides between participants in terms of survey behaviour and data quality; and the impact on data quality of using novel data types to answer survey questions such as images, voice memos and emojis.
For his PhD, he is focusing on understanding how social scientists can best collect information about citizens’ online behaviours using web trackers, e.g., apps that can track the URLs and apps through that individuals visit. Through a combination of theory and traditional survey and computational methods, his research explores how to quantify and minimize metered data errors, while comparing them with the ones of surveys.
Oriol is also involved or has been involved in several international research projects, such as the ERC’S WEB DATA OPP project, TRI-POL, GenPopWeb2 network, Netquest Research Lab and the CROss-National Online Survey (CRONOS). Apart from his academic background, he frequently works as a consultant for for non-profit organisations and public bodies. So far he has collaborated with the Wellcome Trust, the Social Care Institute for Excellence and MoneyHelper, helping them better design and analyse socially relevant surveys.
"
https://www.turing.ac.uk/events/data-study-group-september-2019,"
Telenor
Our next Data Study Group will be held Monday 2 September - Friday 6 September 2019 at The Alan Turing Institute in London.
Applications for this Data Study Group are now closed. 
Why apply?
The Turing Data Study Groups are popular and productive collaborative events and a fantastic opportunity to rapidly develop and test your data science skills with real-world data. The event also offers participants the chance to forge new networks for future research projects, and build links with The Alan Turing Institute – the UK’s national institute for data science and artificial intelligence.
It’s hard work, a crucible for innovation and excellent science, and a space to develop new friendships.
Reports from a previous Data Study Group are available on the Outcomes section of the April 2018 Data Study Group pages.
Our challenges and data sets are provided by partner organisations for researchers to work on over the week.
The organisations and challenges leading the Data Study Group this September are:
Please see below for further details on each challenge
The skills that we think are particularly relevant to the challenges for this Data Study Group are listed under each challenge description below.  Please note, the lists are not exhaustive and we are open to creative interpretation of the challenges listed. Diversity of disciplines is encouraged, and we warmly invite applications from a range of academic backgrounds and specialisms.

The Alan Turing Institute will cover travel costs in alignment with our expenses policy. We will also provide accommodation for researchers not normally London-based. Accommodation for researchers who are from a London university or research institute may be available for those who travel from outside London to work. Expenses for international applicants is capped at £200, which includes any costs of visa. Lunch and dinner is provided for participants during the week. 
Preference will be given to applicants who are available for the full duration of the week.
The Alan Turing Institute is committed to increasing the representation of female, black and minority ethnic, LGBTQ+, disabled and neurodiverse researchers in data science. We believe the best solutions to challenges result when a diverse team work together to share and benefit from the different facets of their experience. You can review our equality, diversity and inclusion (EDI) statement online. 

We have been working with GSMA and through them we are presenting four challenges:
Telenor
Green Radio: Dynamic power saving configuration for mobile networks
Mobile networks waste energy by keeping too many radio-cells turned on when demand is low during off-peak. This challenge is about automating next-day power saving schemes for each individual cell tower in a country, based on current load and expected demand profile in the area. The solution should optimise power saved while avoiding negative impact on the user's network experience.  
Dataset: Time-series with two years of hourly load for about 1,000 sites in urban and rural areas of a European country. 
Useful skills: Machine learning, time series analysis, reinforcement learning, data science
Turkcell
Real-time jammer detection, identification and localization in 3G and 4G networks
In urban areas, jammers cause interruption on mobile 3G and 4G communication networks, which leads to severe service quality deterioration. This situation causes customer complaints and time and labour costs during detection studies. Based on the provided dataset, including some network service quality indicators and jammer geographical location, the participants are invited to investigate methods for real-time detection of jammer presence, identification of jammer type and its location. 
Useful skills: good data analysis and geographical data visualization skills; experience training and tuning statistical, machine learning and deep neural network models with libraries/frameworks such as sci-kit learn, TensorFlow, or similar
 
STC
Bandwidth allocation for mobile users: a solution for rural and urban areas
We want to build algorithms to dynamically allocate bandwidth over mobile networks in order to improve quality of service and speeds for specific customers in rural and urban areas. It is common for mobile phone users to purchase plans which include dedicated bandwidth. However, some of these customers may not consume all their bandwidth or they may sometimes require more allocation. STC will provide an extensive and rich dataset, which will include data about customer profiles, network consumption and mobility patterns for both an urban and a rural case study. 
Useful skills: modelling with structured data records, supervised learning, time series prediction, probabilistic prediction.
 
Telus
Understanding the influences of network measures on customer perception of network reliability
Telus regularly conducts surveys with customers to gather feedback and identify service improvement opportunities to prioritise investments in a way that reflects customers’ needs. We are therefore interested in understanding how our network and the customer experience, while using our network, influence the results of this survey. More specifically, we would like to understand (i) how accurately the customer’s experience of reliability on our network can be predicted, and (ii) what are the main drivers of network performance to influence our customer’s rating of their experience. This challenge will include a wide cross-section of anonymised data from network sources (e.g. usage metrics, KPIs etc.), non-network sources, as well as completed customer feedback questionnaires.
Useful skills: predictive/supervised modelling and time series modelling.
How to get involved as a researcher
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/time-series,"
"
https://www.turing.ac.uk/news/publications/autonomous-network-defence-using-reinforcement-learning,"
In the network security arms race, the defender is significantly disadvantaged as they need to successfully detect and counter every malicious attack. In contrast, the attacker needs to succeed only once. To level the playing field, we investigate the effectiveness of autonomous agents in a realistic network defence scenario. We first outline the problem, provide the background on reinforcement learning and detail our proposed agent design. Using a network environment simulation, with 13 hosts spanning 3 subnets, we train a novel reinforcement learning agent and show that it can reliably defend continual attacks by two advanced persistent threat (APT) red agents: one with complete knowledge of the network layout and another which must discover resources through exploration but is more general.
Foley, Myles, et al. ""Autonomous Network Defence using Reinforcement Learning."" Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security. 2022.
"
https://www.turing.ac.uk/partnering-turing,"

Our long-term vision is for the Turing to be the centre of a connected network that brings together academic expertise from the UK’s world-class university sector with industry, government, and third sector partners and collaborators to research, catalyse, apply, and drive global impact in data science and AI. The Turing networks will encourage direct connections between universities, partners and other communities we reach and provide an important benchmark for the success of the Institute in driving the data science and artificial intelligence agenda in the UK.
We collaborate with industry to work on real life use cases for data science, with universities to engage with a broad pool of talent and expertise and with public bodies to ensure data science is effectively and safely employed for public good.
We work with a wide range of organisations to help deliver our mission of using data science and artificial intelligence to change the world.
Tackle the thorniest challenges. We bring together different disciplines to develop solutions to hard methodological and real-world problems in the field of data science and artificial intelligence and in accordance with our charitable objects.
Generate public benefit from our work, through open publication, driving societal and economic impact. We want the results, outputs and outcomes of our work to spread beyond single projects, across domains and sectors, yielding positive transformation.
Uphold the highest ethical standards. All our research is subject to ethical review and the Institute demonstrates leadership in data and AI ethics. 
Make more progress together. We bring different research disciplines, diverse researchers, data scientists and engineers together to do what no single entity can do alone. 
Support the adoption and further development of our outputs and outcomes. We want the theories, methods, tools, practices, systems and guidance developed through the Institute to add value to the UK national and international ecosystem. We support our partners to use our findings, build on them, and embed the learnings. We prefer the co-design of collaborative research over consultancy or contract research. 
Work with real-world data. We already work with open data and synthetic data. Our partnerships provide privileged access to confidential, sensitive and proprietary data. 
Activate the enthusiasm, curiosity and commitment of your people and ours.
The Alan Turing Institute works with a wide number of organisations, including government, charities, companies and universities to undertake world-leading research to change the world for the better.
"
https://www.turing.ac.uk/research/research-areas/machine-learning/deep-learning,"
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures,"
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-manchester,"
"
https://www.turing.ac.uk/events/turing-lecture-secrets-and-lies,"
Dr Hannah Fry is an Associate Professor in the Mathematics of Cities at the Centre for Advanced Spatial Analysis at UCL where she studies patterns in human behaviour. Her research applies to a wide range of social problems and questions, from shopping and transport to urban crime, riots and terrorism. Her critically acclaimed BBC documentaries include Horizon: Diagnosis on Demand? The Computer Will See You Now, Britain’s Greatest Invention, City in the Sky (BBC Two), Magic Numbers: Hannah Fry’s Mysterious World of Maths, The Joy of Winning, The Joy of Data, Contagion! The BBC Four Pandemic and Calculating Ada (BBC Four). She also co-presents The Curious Cases of Rutherford and Fry (BBC Radio 4) and The Maths of Life with Lauren Laverne (BBC Radio 6). Hannah is the author of Hello World. How to be human in the age of the machine.
@FryRsquared

This exclusive talk will not be filmed so don't miss it.
18:00-18:30 – Registration
18:30-18:35 – Welcome and introduction
18:35-19:25 – Turing Lecture
19:25-19:40 – Q&A
19:40-20:30 - Meet and greet
 
Image by Angel-Kun from Pixabay
1 Wimpole St
London, W1G 0LZ
"
https://www.turing.ac.uk/research/research-projects/trustworthy-digital-infrastructure-identity-systems,"
Governments around the world are committed to supporting the roll out of national digital IDs, but there are privacy and security implications associated with scaling these systems at a national level.
Responsible implementation of ID services is a critical enabler for financial inclusion; it enables access to services and enactment of civil rights. According to the World Bank, more than one billion people are currently living without an official digital identity.
The Alan Turing Institute is joining a vibrant community of NGOs, charities, private sector providers, universities and think tanks addressing global identity challenges in the digital age.
Questions of trust are based around the complex interplay of socio-technical considerations, requiring multi-disciplinary expertise. The ‘trustworthiness’ of digital IDs is characterised by multiple inter-related dimensions that include security, privacy, ethics, resilience, robustness and reliability. These dimensions are required to provide the knowledge, tools and guidance needed to implement privacy-preserving, secure identification systems
This initiative has received initial funding through a grant from the Bill & Melinda Gates Foundation.
The project aims to enhance the privacy and security of national digital identity systems, with the ultimate goal to maximise the value to beneficiaries, whilst limiting known and unknown risks to these constituents and maintaining the integrity of the overall system.
The project is comprised of three major workstreams:
This is a multi-disciplinary project bringing together skills from diverse fields such as threat modelling and red teaming, dynamic cyber security and real-time governance, risk modelling, cloud solutions/ID-as-a-service, privacy enhancing technologies (PETs) and security policy development. Please see related opportunities here.
 
The Alan Turing Institute's Trustworthy Digital Identity Interest Group brings together a vibrant, global community of over 100 practitioners and researchers. They meet regularly to host vital discussion and investigate the evolving technical and social risks, as well as to contribute towards the development of more trustworthy systems. 
Learn more and apply to join. 

You can find more content on The Alan Turing Institute’s YouTube page. See below for a list of playlist from our recent events and workshops: 
Trustworthy Digital Identity Interest Group
Trustworthy Digital Identity Workshop - December 2022
Trustworthy Digital Identity International Conference - September 2022
 
"
https://www.turing.ac.uk/people/guest-speakers/bruce-guthrie,"
Bruce Guthrie is Professor of General Practice at the University of Edinburgh where he is the Director of the Advanced Care Research Centre and PI for the AIM-CISC programme. He is a health services researcher using mixed quantitative and qualitative methods to understand and improve the quality and safety of healthcare, with a focus on multimorbidity, polypharmacy and prescribing quality and safety. He has led a number of projects in the field of multimorbidity including HDR-UK work examining how multimorbidity is measured in the research literature, and additionally collaborates with colleagues in the UK and internationally. To ensure implementation of research findings in the NHS, he has served on a number of national committees including chairing the Guideline Development Group for the NICE Multimorbidity clinical guideline, and being a member of the NHS Scotland Polypharmacy Working Group.
"
https://www.turing.ac.uk/people/external-researchers/nadim-kobeissi,"
Dr. Nadim Kobeissi currently runs Symbolic Software, a Paris-based applied cryptography consulting office. As its director, he manages a small team that offers security audits, cryptographic protocol design and formal verification services. Since its founding in 2018, Symbolic Software has completed and delivered over 200 software and cryptographic audits for clients all around the world. Nadim is also the author of Verifpal, new software for verifying the security of cryptographic protocols. Building upon contemporary research in symbolic formal verification, Verifpal’s main aim is to appeal more to real-world practitioners, students and engineers without sacrificing comprehensive formal verification features.
In 2018, Nadim defended his Ph.D. thesis, Formal Verification for Real-World Cryptographic Protocols and Implementations, at Inria Paris, after 3½ years of research with team PROSECCO. In 2018 and 2019, Nadim also designed and taught the computer security course at New York University's Paris campus, where he served as an adjunct professor.
@kaepora
"
https://www.turing.ac.uk/news/professor-adam-sobey-lead-turing-lloyds-register-foundation-data-centric-engineering-programme,"
Professor Adam Sobey has been appointed to lead The Alan Turing Institute-Lloyd’s Register Foundation’s growing programme in data-centric engineering (DCE).
Based on findings from Lloyd’s Register Foundation’s Foresight Review on Big Data in 2015, the Data Centric Engineering Programme was established at The Alan Turing Institute. Since its inception, the programme championed international research in data and digital systems, and has helped bring together global thought leaders to ensure advances in mathematics, computer science and big data can be applied to make the world a safer place.
As Programme Director, Professor Sobey will be responsible for leading the research programme, building on its core challenges, and working with industry to deploy data-centric engineering techniques to real-world problems.
Professor Sobey is a Professor in the Maritime Engineering group at the University of Southampton, and previously Group Lead for Marine and Maritime in the DCE programme of The Alan Turing Institute. He also held a Non-Executive Director role at Theyr Ltd, a leading Voyage Optimisation Software and MetOcean provider. Adam is also a former Lloyd’s Register Foundation funded PhD student, who has gone on to deliver innovative safety solutions in the maritime industry and beyond.
Since 2017, the DCE programme has made a strong scientific impact with notable projects including the 3D printed bridge in Amsterdam, and Project Odysseus, papers in 400 scientific publications, delivering and supporting more than 100 projects, and securing several international agreements, from Finland to Australia.
Professor Sobey, who is understood to be the world’s first Professor of Data-Centric Engineering, at University of Southampton, says, ‘I am delighted to join the Turing-Lloyd’s Register Foundation team as Programme Director, and I look forward to growing its leading international programme on data-centric engineering. The field of data-centric engineering has made rapid progress, enabling us to improve our ability to understand and improve the safety of engineered systems, and transforming the way new infrastructure projects, and complex machinery is designed and delivered. I look forward to working with colleagues and our network of partners to accelerate the Turing’s scientific and real-world impact in this area.’
The Alan Turing Institute’s Chief Scientist, Mark Girolami, says, ‘The Turing’s DCE programme has successfully established data-centric engineering as a new academic discipline, built capacity within it and delivered outstanding impact. This has been achieved as a direct result of Turing’s partnership with Lloyd’s Register Foundation. With Adam at the helm, we look forward to an exciting new phase for the programme supporting our Institute’s new strategy, and in line with key national priorities.’
Lloyd’s Register Foundation’s Chief Executive, Ruth Boumphrey, says: 'It’s fantastic that someone with Adam’s skills and expertise can lead the Data-centric Engineering programme into this new ambitious phase in its history. I have confidence that Adam is perfectly suited to accelerate the translation of Data-Centric Engineering into real world impact, and improve the safety of critical infrastructure around the world.'
"
https://www.turing.ac.uk/sitemap.xml,
https://www.turing.ac.uk/research/interest-groups/entrepreneurship,"
The Entrepreneurship Interest Group has two main goals. Firstly, we aim to provide networking, inspiration and support for people interested in turning their research into an entrepreneurial venture. Secondly, we aspire to help researchers in the entrepreneurship field to discuss and discover research areas in need of more development as well as current methodological innovations in the field.
The main aims of the group are to:
Challenges: Taking a successful research idea and applying it real world applications is a daunting process. This theme focuses on supporting people interested in ‘spinning out’ their research.
Challenges: The world of industry is quite different to the academic space, with its own set of unique challenges. This theme focuses on providing people with the knowledge to understand how their research fits in to the bigger picture of industry.
Challenges: Research in the field of entrepreneurship often relies on rather old-fashioned sources of data and methodologies. This theme focuses on discussing how new data sources and methods such as Causal Inference and Natural Language Processing can advance our understanding of how, why, and where entrepreneurship develops.
Challenges: Entrepreneurship in AI and ML is mostly pursued by well defined demographic categories in well defined spatial setting. This theme focuses on encouraging and showcasing entrepreneurial opportunities by actors generally considered as marginalised.
Click here to join us and request sign-up
Joseph Early[email protected]
Giulia Occhini[email protected]
"
https://www.turing.ac.uk/events/masterclass-intel-training-day,"
​
Speakers: James Cownie (Intel, UK) and Andrew Mallinson (Intel, UK) Date: Monday 15 May 2017 Time: 11:00 – 16:00 Email: The Alan Turing Doctoral Scheme to attend. Some sections will be live-streamed, for more details see the programme outline below. Recordings will be made available on our YouTube channel after the event.
The Alan Turing is delighted to announce the masterclass, a day of lectures exploring and learning about the Intel cluster. As outlined below, the day will include two lectures in the morning and three in the afternoon led by Jim Cownie and Andrew Mallinson.
11:00 - 12:00 - Why are our machines the way they are? James Cownie (Intel, UK)  This lecture will describe the physical realities that underlie our hardware designs, historic performance trends, and their implications for the way that we will need to write our programs to achieve high performance now and in the future. By the end of it, you should understand why we don't give you the very fast single processor scalar machine with huge memory capacity and bandwidth that you would like to have. This lecture will be live-streamed.
12:00 - 13:00 - Introduction to the Intel® Architecture  Andrew Mallinson (Intel, UK) This lecture will introduce the Intel® architecture and describe the different components of the processor (vectors, registers, processors, caches, ...) and the system (network, storage). It will explain why you need to know about these if you want to achieve the highest performance from your code.
14:00 - 15:00  - Introduction to Programming Models, Libraries and Tools James Cownie (Intel, UK)  This lecture will introduce the different programming models and programming languages that can be used to achieve performance as well as the software libraries and tools which you can use to help you to achieve that noble objective. This lecture will be live-streamed.
15:00 - 15:30 - Introduction to The Alan Turing Institute Cluster Andrew Mallinson (Intel, UK) This lecture will describe the hardware that is in the Turing cluster being installed in Edinburgh, the interesting options for research which it makes available, and how to gain access to it.
15:30 - 16:00 - Intel Training Syllabus for The Alan Turing Institute and General Discussions  Andrew Mallinson (Intel, UK)  This lecture will present the future training courses which Intel is proposing to deliver to Turing researchers. It will also include some time for a general discussion regarding Turing research and training priorities, with feedback being used to influence future training course offerings.  
Bios: Jim Cownie is an ACM Distinguished Engineer and Intel Principal Engineer. He has been involved with parallel computing since starting to work for Inmos in 1979. Along the way, he owned a chapter in the MPI-1 standard and has worked on parallel debuggers and OpenMP implementations. Andrew Mallinson is a senior application engineering within Intel’s Developer Relations Division. His focus is on working with scientific researchers to enable them to fully utilise Intel’s processing technologies, in particular, the Intel® Xeon Phi™ processor. Andrew holds a PhD from the University of Warwick and his research interests include Partitioned Global Address Space (PGAS) programming paradigms and their use within irregular and data intensive applications within the field of data science.  
​

"
https://www.turing.ac.uk/research/research-projects/online-harms-observatory,"
The Online Harms Observatory leverages our innovative research to help policy makers, regulators, security services and other stakeholders better understand the landscape of online harms. Initially, it will focus on online hate, extremism and misinformation. It aims to create a stepchange in how we understand the cross-cutting threats posed by this nexus of toxic content, with plans to expand to other harms in the near future. The Observatory was officially launched in March 2022 at AI UK, the UK’s national showcase of artificial intelligence.
This project is delivered in partnership with the Department for Digital, Culture, Media & Sport.
The first tracker in the Observatory focuses on the abuse faced by football players in the men’s Premier League. Ofcom commissioned an in-depth report off the back of this first set of dashboards. Key findings from the 2021/2022 season include:
Read the full report
Areas being explored for our next trackers include abuse directed at MPs, female journalists and women playing in the Premier League.
The Online Harms Observatory is supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1 and EPSRC Grant EP/W006022/1, particularly the “Criminal Justice System” theme within those grants and The Alan Turing Institute. The Observatory is being delivered in partnership with the department for Digital, Culture, Media & Sport.
If you have questions about the Online Harms Observatory, or would like more information about the Online Safety Team’s research, reach out to Pica Johansson ([email protected]).
"
https://www.turing.ac.uk/people/external-researchers/paul-cripps,"
Dr Paul Cripps is part of the Cyber & Information Systems Division at Dstl. Paul has worked extensively with geospatial information across a range of sectors, including the civil service, academia, third sector and commercial consultancy in each case combining technological horizon scanning with a pragmatic take on sectoral requirements, user needs and capabilities. His doctoral research investigated the fusion of geospatial and semantic information to support innovation in research methods in the humanities.
"
https://www.turing.ac.uk/research/interest-groups/machine-learning-radio-frequency-applications,"
For many years machine learning approaches have been successfully applied to numerous detection and classification tasks from image processing to voice separation and text recognition. However, it is only recently that similar techniques have been applied to the processing of radio frequency (RF) signals and the electromagnetic environment (EME).The EME is becoming more congested, contested and complex. This can be seen by:
It is therefore becoming increasingly difficult for a human in the loop to handle the flood of information. This is resulting in the adoption of deep learning approaches for the detection, classification, identification and transmission of signals.
The development of new technologies for the automated, real-time processing and analysis of radio frequency data requires domain specific expertise that is spread across multiple organisations and disciplines. This special interest group aims to build a community of machine learning (ML) for RF researchers and to run a series of theme lead workshops covering the applications and challenges in this domain.
Machine learning (ML) for RF degradation and resilience
Many of the characteristics of RF signals that are exploited to enable long range imaging, transmission and communication without direct line of sight, create a new set of challenges and opportunities for ML algorithms intended to learn and monitor activity.  Examples of this include RF propagation effects from multipath in urban environments and diffraction from high water vapour content in the atmosphere. Similarly, the development of covert capability such as passive radar and low probability of intercept waveforms mean ML algorithms need to be resilient to a wide range of dynamic ranges, interference and low signal to noise ratios. Both military and commercial radars are exhibiting ever increasing levels of agility across multiple parameters and over short timescales. Such signals provide a challenge for electronic surveillance receivers attempting to detect, cluster, separate and identify radars in a contested and congested EME. Processing techniques relying on a-priori knowledge of expected signals in the environment will be limited in their performance, and as such this provides an opportunity for the application of novel ML approaches to the aforementioned processes.
The modern agility of radars provides both a challenge for detection but an opportunity for the application of novel approaches for spectrum sharing and waveform distribution and design. Traditionally the spectrum was managed by operating comms systems within a fixed bandwidth. ML approaches, e.g., in cognitive radios and radars, are now being used to adaptively change transmission parameters to improve spectrum utilization, optimize channel conditions and enable adaptive routing between multiple nodes and networks (Deepwave, 2021).  To meet the demand for automatic network recognition and to build resilience in hostile environments, we need to be able to detect and classify overlapping RF signals from multiple sources operating over ever-increasing frequency bandwidths.
Machine learning techniques are increasingly being explored for protection against jamming and deception. It provides the means to see anomalies and unusual patterns. Being able to counter jamming requires the ability to detect the signal and automatically adapt to it. This could be by adapting your waveform or moving to another part of the EME. ML for jamming and deception detection requires an understanding and improved awareness of the operational EME. Spoof detection requires algorithms capable of identifying and distinguishing features often based on higher-order statistics and thus lends itself to ML. As radar systems gradually move towards using ML techniques themselves, waveform structure, timing and agility may all be used to concurrently optimise probability of detection while avoiding interception by an adversary. As such ML may be the only feasible concept for exploiting such signals.
Machine learning resilience in contested environments necessitates strong verification and validation of algorithms that requires drawing from a large community of experts. Developing universal test sets of data is crucial for benchmarking codes and explainability of methodologies. It is critical for user confidence and wider adoption that we move away from using ML algorithms as a black box, explore new methods for explainability of network performance, and start to encode uncertainties in our decision making and predictions. In training ML algorithms, the importance of pre-processing and choice of features and embeddings can often be overlooked compared to the choice of ML architectures and hyperparameter fine-tuning. It is important when testing algorithms to identify which parts of a new algorithm contribute to better performance as well as having a universal set of metrics to use for testing.
To tackle the scarcity of tagged real datasets synthetic dataset creation is in many cases being used to augment datasets. This is of particular relevance in defence, where complete databases of signals may not be available. As such development of, validation and verification of sufficiently large, variable and realistic datasets consisting of both real and synthetic data is of particular interest. Generating realistic RF datasets that incorporate the interactions between multiple sensors and consider interference is a big challenge. To auto-generate datasets that are representative of different types of real data we also need automatic methods for feature extraction which reflect aspects such as characteristic parameter ranges, and skews of distributions. We then need to find ways to map these features onto RF functional IDs and to understand how we can use features to identify and explain phenomena causing signal interactions with the environment.
Multi-source signal fusion and distribution
Multi-sensor distributed systems measure parameters independently then use signal processing techniques to combine observations. Distributing signals across multiple sensors can make operations more covert, increase platform agility, allow rapid switching between modalities and help to solve trade-offs between platform performance and Size Weight and Power (SWAP). Being able to integrate observations from multiple sensors can improve accuracy, reliability, and detectability, reduce ambiguity, increase spatial-temporal ranges, enhance resolution, increase the dimension of target observations, and help to resolve multipath, and improve SNR (Kong et al, 2020).
Enhanced integration of multi-platform systems operating in an agile and real-time way requires novel multi-source signal fusion and distribution techniques. ML techniques are being explored for rapid, efficient, automatic allocation, transmission and reception of signals across multiple platforms. Distributed systems need very accurate position and timing information. Whilst GPS and atomic clocks can help to maintain good coherence, signal processing is still currently used post acquisition to make a number of corrections. This is an example of the kind of operations which may begin to be replaced by ML algorithms to improve coherence, and perform timing and positioning corrections and adjustments in real time. How we acquire and integrate data from multi-user distributed sensors and use them to cross validate each other has many solutions in the realm of embedded hardware and software.
RF embedded hardware and software
A drive towards real-time distributed processing at the edge with reduced human in the loop is pushing solutions towards embedded hardware and software approaches. Hybrid computing architectures, and software defined radios for ML applications are rapidly advancing areas of technology from embedded control, to autonomy and Artificial Intelligence (AI). The strong coupling between hardware and software in the RF domain and the use of purpose-built deep learning accelerators will need to be exploited to meet future requirements for data retrieval and transmission as well as considerations of SWAP. For signal detection it will be desirable to adjust the amount of power investment to make it proportional to the level of interest in a particular signal, and we will be looking to determine whether a signal is interesting as early as possible (Mullins, R., 2021). Multi-purpose RF sensors with ML capability using embedded hardware and software will be used to detect RF signals including Wi-Fi, Bluetooth and cellular to exploit the order of magnitude mark up in speed compared to conventional techniques.
In the case of multiple sensors, we will be looking to control and adapt the power consumption, parameters and precision of each sensor to optimise our use of the available power. There will be opportunities to co-design sensors, pre-processing and neural networks (Mullins. R, 2021). We are beginning to see frameworks designed to generate efficient neural network accelerators perform automatic transferral of machine learning architectures to FPGAs (Mullins, R., 2020). This has multiple applications notably for improved situational awareness. Strategies for early exit from inference at different stages in network architectures are beginning to be explored (Laskaridis, S. et al, 2020). In-network computing is being used to offload standard applications to network devices to increase throughput by processing data as it traverses the network (Zilberman, N., 2020). In-network data processing on wireless sensor nodes can be used to collect data at multiple distributed sources and aggregate it on the way to its final destination (Leung.K, 2020). There is great potential for the use of ML for data aggregation and resource optimisation and allocation. Dynamic hardware adaptation is already enabling in-orbit satellite updates and partial reconfigurations. Autonomous, unmanned vehicles will require automatic algorithm updates to embedded hardware to meet changes in the environment, cross platform modifications and advances in technology often on legacy hardware.
Developing efficient ML solutions on smaller platforms requires the reduction of models, dynamic compression, compact representations and knowledge distillation using techniques such as pruning of networks, improving performance in lower precision modes, dimensionality reduction, and sparse layer representations. We need to have a good understanding of when COTs solutions are fit for purpose and situations where we require custom specialised hardware. There are a number of choices to be made about what processing should be done in hardware, what to do in software, where to perform computations at the edge and when to push back to the cloud. The answers to some of these questions are in many cases strongly linked to requirements for data security and anonymization.
Application specific ML for RF
ML for RF covers a wide range of scales in terms of distances, frequencies, and applications.  Small scale passive systems are used for monitoring health and in a COVID world and beyond wireless IoT technologies dominate our day-to-day home lives. Signals intelligence, electronic warfare and communications are increasingly seeing the need to develop new approaches to automate the detection, classification, and identification of signals, from urban scale analytics to larger scale signals intercept on airborne platforms for situational awareness. At an earth observation scale Interferometric Synthetic Aperture Radar (InSAR) is being used to automatically extract features in the difference in phase between satellites. This is being used to detect earthquakes, monitor subsidence, and track ice flows to monitor the effects of climate change. All of these processes cover a range of frequencies from oscillations on the scale of an atom to the size of a football pitch. Our ability to successfully deploy ML algorithms at such a wide range of scales depends on our ability to successfully adapt solutions to domain specific applications.
References
Deepwave digital, 2021, deepwavedigital.com/software-products/spectrum-sensing
Kong, L et al 2020 Int. J. Extrem. Manuf. 2 022001
Laskaridis, S., Venieris, S. I., Kim, H., Lane, N. HAPI: Hardware-Aware Progressive Inference, arXiv:2008.03997, DOI: 10.1145/3400302.3415698
Leung, K. The Alan Turing Institute Edge Computing for Earth Observation Workshop Abstracts, 2020
Mullins, R. The Alan Turing Institute Edge Computing for Earth Observation Workshop Abstracts, 2020
Mullins, R. email correspondence 2021
Rosen, J. Fleets of radar satellites are measuring movements on Earth like never before, ScienceMag.org 25 Feb 2021
Zilberman, N. The Alan Turing Institute Edge Computing for Earth Observation Workshop Abstracts, 2020
The most recent interest group meeting was on 10 January. The focus of this meeting was machine learning for communications applications. With increased reliance on Internet of Things (IoT) devices and more complex communications in the form of MIMO rollout of 5G and development of WiFi6 machine learning (ML) approaches are being widely adopted in communications. In this meeting we found out more about advances in this domain including machine learning approaches for network management and operation, passive communications network topology reconstruction, radio frequency fingerprinting, and ML solutions for developing a 6G network with low latency, high data rate and capacity, secure communications and reliable data connectivity.  
You can view the agenda here.
Please contact Victoria Nockles if you have any questions.
[email protected]
External researchers
Richard Walters, Durham UniversityMatthew Ritchie, UCL Michael Woollard, UCL
Robert Mullins, University of Cambridge 
Kin Leung, Imperial College London Daniel Andre, Cranfield University
Michail Antoniou, University of Birmingham 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/greater-london-authority,"
www.london.gov.uk
"
https://www.turing.ac.uk/news/filed-under/how-to,"
"
https://www.turing.ac.uk/research/research-projects/shocks-and-resilience,"
The COVID-19 crisis has highlighted how vulnerable societies and governments are to shocks. This vulnerability is exacerbated by the propensity to design policy for narrow silos relating to singular policy areas and government departments, without adequate consideration of the interdependencies between them and the interconnected nature of local and global societies. The pandemic has brought into focus the fact that resilience in one policy area (e.g. health) can come at the cost of resilience in another (e.g. the economy). The overall aim of this large-scale, 2-year research project is to develop a better understanding of resilience in interconnected health, social, and economic systems and to use this understanding to identify robust policy measures.
This project is supported entirely by public funds, through Wave 1 of the UK Research and Innovation Strategic Priorities Fund, under EPSRC Grant EP/T001569/1.


 
The project brings together multidisciplinary expertise from across the Turing community, including in health, public policy, economics, and urban analytics. We work across discipline lines to develop a rigorous understanding of societal responses to shocks and a clear strategy for how to engender policy resilience. To achieve our aims, we require reliable, consistent, real-time, fine-grained data sources, as well as integrative, highly-granular models that bring together policy areas and cross organisational boundaries.
The Shocks and Resilience project consists of five work packages:
The aim of this work package is to use the current pandemic as an initial exemplar of a specific shock. Our primary focus is to develop a coupled epidemiologic and socio-economic model of COVID-19 dynamics and its societal effects. A crucial part of this package is the identification and curation of data sources that can be used to monitor the impact of policy interventions and inform subsequent models. Alongside data collection and curation, we are building an epidemiological model that allows for variations in social patterns as well as societal and economic factors, such as inequality, to affect the dynamics of spread.
This work package focuses on developing new theory, methods, tools, and practices for understanding causality and dynamics in complex interconnected systems under conditions of uncertainty. A crucial part of this package is discovering ways to allow for feedback that is able to adapt and update as new data arrives. We aim to develop rigorous new statistical theory as well as computational methodology that allows for the incorporation of physical, economic, and biological principles into machine learning algorithms.
The aim of this work package is to develop spatial modelling methods that can be integrated within the epidemiologic-socio-economic models in order to tackle policy questions that are relevant at the sub-national level (e.g., regional and local authorities). We are producing methodologies that offer ‘what if’ scenario modelling in relation to spatial variations in policy regulations, such as the local relaxation or reintroduction of social distancing rules, or local controls over business, leisure, and education. 
This work package explores the role of social media in challenging resilience in socio-economic systems.  It will do so by investigating how the dynamics of social media activity shape patterns of diffusion and consumption of information and disinformation, which in turn shape public attitudes, reaction to shocks (such as terrorist attacks, pandemics or natural disasters) and compliance with government guidance and policy interventions.  We will use computational social science methodologies to collect and model social media data, looking for potential drivers of instability, such as rapid spread of disinformation, scaling up of social or political mobilisation, lurches in opinion and shifting attitudes to interventions such as vaccines or policy change.  We will combine different computational approaches, including (but not limited to): network science; statistical models; and  unsupervised machine learning. We will use the insight we gather on the role of social media activity in driving instability (particularly during or after shocks) to provide behavioural insight to policy-makers working on crisis management and the design of (for example) government guidance and public awareness campaigns.
This work package distills general lessons learnt from the other work packages to develop a rigorous understanding of what resilience means in complex, interconnected socio-economic systems. We combine insight from different modelling approaches to explore the multi-faceted nature of resilience, such as: network science; agent-based models; compartmental ordinary/stochastic differential equation models; continuum/partial differential equation models; statistical models (e.g. hidden Markov processes); and microdynamics simulations. We will apply these models to a number of case studies which we are developing in partnership with policy-makers, tackling specific policy questions that arise as a result of the COVID-19 crisis.
This work package focuses on ensuring that policy-makers play a key role in informing our research. We are consulting with representatives from various government departments and agencies to understand what their main questions are, especially related to COVID-19. We are using the input from these conversations to design case studies and to steer our data and modelling work.
The primary aim behind us pursuing this large-scale programme of work in shocks and resilience is to produce data, methodologies, and tools that help policy-makers make better informed choices. Our hope is that our research, steered by constant feedback from policy-makers, will make governments, economies, and societies more resilient.
The research meets governments' need for a data-driven understanding of policy resilience and for integrated models that allow policy-makers to assess how various interventions affect health and socio-economic outcomes. Throughout the project, we aim to produce regular policy reports accompanied by open source code, data repositories, and online tutorials.
The project will help local and national governments understand the wider implications of different policy options. It will also enable them to prepare operationally for the challenges brought by future policy interventions related to the COVID-19 crisis, upcoming repercussions, and future shocks. 
[email protected]
"
https://www.turing.ac.uk/research/research-projects/ethical-assurance-digital-mental-healthcare,"
The use of data-driven technologies in mental healthcare, such as machine learning or AI, poses a series of well-known ethical, social, and legal risks for matters such as data privacy, explainability of automated decisions, and respect for mental integrity. Providing assurance that these matters have been addressed responsibly requires a participatory approach that includes affected stakeholders within the design, development, or deployment of the respective technology. This project will work with a range of stakeholders to understand what issues matter most to them, and how to develop a justifiable method of assurance that helps promote trust and confidence in digital mental healthcare.
The method of assurance that this project will rely upon is known as argument- based assurance. This is a process of using structured argumentation to provide evidence-based justification to another party that a system or product will operate as intended within a well-defined environment. Argument-based assurance is an established method of governance in safety-critical areas, and facilitates trustworthy communication between developers and stakeholders.
Key to this method is the development of an assurance case, which serves as a formal and visual means for representing how a particular goal has been achieved, by reference to a set of claims that establish central properties about the system or project. In turn, these claims are warranted by reference to clearly documented evidence that can help stakeholders assess the overall justifiability of the assurance case (or, argument).
Argument-based assurance has a range of supporting tools and mechanisms to enabled the development and review of assurance cases. For instance, there are models and argument patterns that serve as guides and templates for building an assurance case. In addition, there are best practices for how to frame an assurance case so that it supports a wide range of functions, such as anticipatory reflection, risk identification and minimisation, and accessible and transparent communication.
This project will expand these tools and methods such that they can also provide assurance for key ethical goals, such as sustainability, accountability, fairness, and explainability. Because each goal presupposes a core set of normative values, it is vital that ethical assurance is an inherently participatory activity. Therefore, our project will also demonstrate how to support stakeholder engagement and participation and link these activities with the building of an assurance case.
 
There already exist a wide range of standards for the assurance of a product or system’s safety, security, or reliability (e.g. NCAP standards for car safety). These standards of assurance are both important governance mechanisms and also valuable means of communicating and establishing trust between different stakeholder groups.
However, similar standards or frameworks for ethical properties, such as sustainability, accountability, fairness, and explainability are, by comparison, not sufficiently nor widely established or accepted.
By showing how a method of ethical assurance can support and enhance the governance of a project involving a digital mental healthcare technology, this project will develop both a theoretical framework and practical mechanism that can assist the design, development, and deployment of the technology.
"
https://www.turing.ac.uk/people/guest-speakers/kin-k-leung,"
Kin K. Leung received his M.S. and Ph.D. degrees from University of California, Los Angeles. He worked AT&T Bell Labs and its successor companies in New Jersey for 19 years. Since 2004, he has been the Tanaka Chair Professor in the Electrical and Electronic Engineering (EEE), and Computing Departments at Imperial College. He is the Head of Communications and Signal Processing Group in the EEE Department at Imperial. His research focuses on networking, protocols, optimization and modeling of wireless broadband, sensor and ad-hoc networks. He also works on multi-antenna systems and cross-layer optimization of these networks.
He received the Distinguished Member of Technical Staff Award from AT&T Bell Labs in 1994, and was a co-recipient of the 1997 Lanchester Prize Honorable Mention Award.  He was elected as an IEEE Fellow in 2001.  He received the Royal Society Wolfson Research Merits Award from 2004 to 2009 and became a member of Academia Europaea in 2012. Along with his co-authors, he also received best paper awards at major conferences, including the IEEE ICC 2019, ICDCS 2013 and PIMRC 2012. He was a member (2009-11) and the chairman (2012-15) of the IEEE Fellow Evaluation Committee for Communications Society. He has served on the editorial boards for 10 leading technical journals.
"
https://www.turing.ac.uk/people/guest-speakers/emily-shuckburgh,"
Professor Emily Shuckburgh is Director of Cambridge Zero at the University of Cambridge and Reader in Environmental Data Science at the Department of Computer Science and Technology. She is a mathematician and climate scientist and a Fellow of Darwin College, a Fellow of the Cambridge Institute for Sustainability Leadership, an Associate Fellow of the Centre for Science and Policy and a Fellow of the British Antarctic Survey.
She leads the UKRI Centre for Doctoral Training on the Application of AI to the study of Environmental Risks (AI4ER). Until April 2019 she led a UK national research programme on the Southern Ocean and its role in climate (ORCHESTRA), and was deputy head of the Polar Oceans Team and head of the Data Science Group at British Antarctic Survey. In the past she has worked at École Normale Supérieure in Paris and at MIT.
She is a fellow of the Royal Meteorological Society and co-chair of their Climate Science Communications Group. She has also acted as an advisor to the UK Government on behalf of the Natural Environment Research Council. In 2016 she was awarded an OBE for services to science and the public communication of science. She is co-author with HRH The Prince of Wales and Tony Juniper of the Ladybird Book on Climate Change.
"
https://www.turing.ac.uk/aicd,"
Advances in intelligent autonomous agents, such as those based on Deep Reinforcement Learning (DRL), have demonstrated super-human capabilities across a range of simulated and game-based tasks. Recent ground-breaking performances include enhancing the speed of fundamental mathematical operations, and defeating the world champion of the popular multiplayer real-time strategy game Dota 2.
These breakthroughs have been made possible by new developments in DRL, allowing intelligent agents (IAs) to identify winning strategies despite imperfect information, highly complex action and observation spaces, and immense game trees. Until now, the computer security and privacy research communities have been largely focussed on conventional (un)supervised machine learning. While this type of AI is great at classification, for example identifying malicious computer binaries or anomalous network traffic, it does not natively support learning from interaction. DRL and related techniques offer a mechanism for planning strategically that we intend to show, through this project, can transform our understanding of, and capacity to attack and defend, computer systems and networks.
The centre is led by principal investigators Vasilios Mavroudis and Chris Hicks who are computer security researchers seeking to fundamentally transform the way in which we secure digital systems through the development and application of cutting edge, deep-learning based approaches to intelligent agents. Our current focus areas are as follows:
Autonomous cyber operations and network defence
AI for Systems Security
Adaptive fuzzing and state-machine learning
Cryptographic ciphers, protocols and their implementations
Although dedicated to solving security and privacy problems rather than any one method (i.e., DRL) for doing so, we are currently researching the following DRL techniques:
Foley, M., Wang, M., Zoe M., Hicks C., and Mavroudis, V., 2022, October. Inroads into Autonomous Network Defence using Explained Reinforcement Learning, Conference on Applied Machine Learning for Information Security
Foley, M., Hicks, C., Highnam, K. and Mavroudis, V., 2022, May. Autonomous Network Defence using Reinforcement Learning. In Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security
Many thanks to our funder and research partner DSTL.
[email protected]
"
https://www.turing.ac.uk/events/ai-tax-audit-and-fintech-workshop,"
The use of digital technologies in tax and financial services has come far over the last ten years while there has been little attention on how it empowers and impacts a diverse society. The workshop aims to match digital innovation in tax administration with principles and mechanisms to protect both users and the reliability of outputs.
The workshop will bring together experts in tax administration, fintech, economic policy design, machine learning, AI, early career researchers and PhD students. It is co-organized by Queen Mary University of London (Centre for Commercial Law Studies, Institute of Tax Law), University of Exeter (Tax Administration Research Centre (TARC)) and the University of Surrey (Surrey Institute for People-Centred Artificial Intelligence, Surrey Law and Technology Hub, Sustainable and Explainable Fintech Centre, Surrey Business School).
Register now
"
https://www.turing.ac.uk/people/external-researchers/alex-krasodomski-jones,"
"
https://www.turing.ac.uk/events/audience-type/cross-disciplinary,"
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/uncertainty-quantification,"
"
https://www.turing.ac.uk/news/publications/uncovering-wireless-blackspots-using-twitter-data,"
Blackspots are areas of poor signal coverage or service delivery that leads to customer complaints and loss in business revenue. Understanding their spatial–temporal patterns at a high resolution is important for interventions. Conventional methods such as customer helplines, drive-by testing, and network analysis tools often lack the real-time capability and spatial accuracy required. The potential of utilising geo-tagged Twitter data to uncover blackspots is investigated. Lexicon and machine-learning natural language processing techniques are applied to over 1.4 million Tweets in London to uncover blackspots for both pre-4G (2012) and post-4G (2016) roll out.
It was found that long-term poor signal complaints make up the majority of complaints (86%) pre-4G roll out, but short-term network failure was responsible for most complaints (66%) post-4G roll out.
Guo, W.; Zhang, J.: 'Uncovering wireless blackspots using Twitter data', Electronics Letters, 2017, 53, (12), p. 814-816, DOI: 10.1049
Guo W, Zhang J
"
https://www.turing.ac.uk/people/researchers/prajamitra-bhuyan,"
Prajamitra Bhuyan is a Lecturer in Mathematical Data Science at the Queen Mary University of London. He is engaged with data-centric engineering programme at the Alan Turing Institute. Before joining Queen Mary University London, he held a postdoctoral position at Imperial College London and a National Postdoctoral Fellowship in India. He obtained PhD degree in Statistics from Indian Statistical Institute. He has several years of industry experience as a Data Scientist in the Analytics sector providing training and business solutions to global clients across industry verticals. 
His primary research interests lie broadly in statistical data science and methodology, motivated by real-life challenges arising from complex systems, social science and public policy. In particular, his doctoral work deals with computational and inferential issues in time-dependent stress-strength interference. In postdoctoral research, he worked on modeling and analysis of incomplete longitudinal data with missingness and zero-inflation. Currently, he is working on causal inference and its application in transport networks. He is also engaged in cross-disciplinary work, focusing on data-analytic settings in sports and environmental sciences.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/bill-melinda-gates-foundation,"
Bill & Melinda Gates Foundation.
"
https://www.turing.ac.uk/people/researchers/emma-griffin,"
Emma Griffin is a Professor of Modern British History at the UEA. She is the author of several books, the editor of the Historical Journal, and a Fellow of the Royal Historical Society.
Emma was educated at London and Cambridge and is now based at the University of East Anglia. She is the author of four books, most recently Liberty's Dawn: A People's History of the British Industrial Revolution, published by Yale UP in 2013, as well as many articles, essays and reviews in both academic and non-academic publications. Her current project is focused on women and work in Victorian Britain and explores the emotions of family life and the origins and consequences of low female pay.
Emma is a Co-I on the Living with Machines project. Her particular interest in the project is exploring how data science can help scholars to make sense of the vast amount of historical data available for Victorian Britain. The inaugural census of 1801 took a head count of Britain’s population, and subsequent censuses held at ten-yearly intervals, gathered increasingly intricate information about where individuals lived, where they had been born, the people they lived with, and the jobs they performed.
Along with the registration of births, marriages, and deaths, this provides us with billions of data points – an unparalleled trove of information about the lives of people during the world’s first industrial revolution. Most of the sources that the project will work with are well known to historians, but historians don’t by training have the skills to work with very large datasets. Much of Emma's contribution to the project involves collaborating with data-scientists to develop effective ways of probing this vast collection of records.
"
https://www.turing.ac.uk/people/guest-speakers/michelle-roberts,"
Michelle Roberts is the Physical Activity and Health Programme Lead at the Richmond Group of Charities, joining in 2017 to lead their work to support people with multiple long term health conditions to move more.  Prior to this Michelle delivered various community physical activity programmes across the UK.
"
https://www.turing.ac.uk/research/research-projects/economics-nowcasting,"
The exploitation of novel data streams (for example from card payments) will allow rapid and real-time model estimates to be made available to stakeholders. As the quantity of data available for Office for National Statistics (ONS) to process increases, there needs to be comparable development in the methods of exploiting this data. The project aims to create a model to 'nowcast' consumer spending by category and geography.
This project will exploit the recent advances in the signature methodology using controlled differential equation models, which give a generic approach to efficient use of multimodal and irregularly sampled (or missing) time series data. This approach has been shown to be very effective in related problems for astronomy, medicine, finance and human-computer interaction, and is well suited to dealing with the complex systems under consideration.
Challenge 1 focuses on specific economic applications. The challenge is to produce high-frequency near to real-time estimates of consumer spending and consumer income at very detailed geographic level. This requires combining detailed consumer expenditure data with other data sources on economic activity and developing the best possible near to real-time model estimates for high dimensional outcomes.  
A related challenge is the production of high-frequency measures of inflation, in particular the CPI. Real-time spending data and large-scale data on the prices of goods are available from ONS. The challenge is to link these two types of data, in order to develop improved price indices, estimate demand, and evaluate the impacts of tax, health and other policies on household welfare and spending.
 Challenge 2 seeks to understand how disparate, irregularly sampled time series can be understood and exploited in an official statistics context. The project aims to build robust and widely applicable methods for nowcasting fine-grained economic data, using passive or indirect observations of related variables.  
By creating a signature model for Nowcasting, the project aims to provide new economic indicators to assist policy makers with:
Tony Zemaitis[email protected]
"
https://www.turing.ac.uk/people/guest-speakers/rosalind-goodfellow,"
Rosalind Goodfellow is Deputy Director for Policy at the Geospatial Commission, Cabinet Office. The Geospatial Commission was established in 2018 by the government as an independent, expert committee responsible for setting the UK’s geospatial strategy and coordinating public sector geospatial activity. Previously, Rosalind has held a number of roles in the UK government including leading the national implementation of the General Data Protection Regulations (GDPR) and modernising protective security delivery for UK government departments. Rosalind holds an Executive Masters from London School for Economics and Political Science in Public Policy.
"
https://www.turing.ac.uk/people/researchers/alain-zemkoho,"
Alain is a Lecturer within the Southampton School of Mathematical Sciences, where he is affiliated to CORMSIS - Centre for Operational Research, Management Science & Information Systems and the Operational Research Group. Prior to joining the University of Southampton, he was a Research Fellow at the University of Birmingham and had previously worked as a Research Associate at the Technical University of Freiberg, where he also completed his PhD in Mathematics. He is Fellow of the Institute of Mathematics and its Applications, a Member of the EPSRC Mathematical Sciences Early Career Forum (2017 - 2019), a regular member of the Society for Industrial and Applied Mathematics (SIAM), the Mathematical Optimization Society (MOS), and The Operational Research Society. 
Alain's research interests are in continuous optimization, with special focus on bilevel optimization and its applications to transportation, data analysis, forecasting, and phase retrieval. He has developed optimality conditions and stability/sensitivity analysis results for such problems and is currently developing numerical algorithms that can capture the optimistic and pessimistic features of bilevel optimization problems.
"
https://www.turing.ac.uk/about-us/equality-diversity-and-inclusion/women-data-science-and-ai/resources,"
"
https://www.turing.ac.uk/people/guest-speakers/cecilia-nunn,"
Cecilia is a software engineer and responsible innovation professional. As Responsible AI Manager at Digital Catapult, she works with businesses and start-ups to put ethics into practice and teach tech teams how to implement it into their products. She's managed multi-million-pound international digital projects for leading technology companies such as Google and multiple UK nonprofits. Mission-driven by programming, design and the use of tech to transform society, she's extremely passionate about getting more women into STEM and teaches software engineering on the Code First Girls course. Having originally come from a humanities background in international relations, she blends this experience with her more recent technical expertise to try and bridge the gap between ethicists and engineers.  
"
https://www.turing.ac.uk/news/publications/anomaly-detection-networks-application-financial-transaction-networks-0,"
A. Elliott, M. Cucuringu, M. M. Luaces, P. Reidy, and G. Reinert, Anomaly detection in networks with application to financial transaction networks, 2019
3.2 MB
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/human-computer-interface,"
"
https://www.turing.ac.uk/events/future-test-evaluation-defence-ai-and-autonomy,"
Apply to attend now closed
The testing and evaluation of semi-autonomous systems in defence already produces significant challenges, from getting unmanned aerial systems in the air to seeking the right levels of assurance for decision-support mechanisms and human-machine teaming.
As technology moves towards fully autonomous systems, and then ones which incorporate AI and machine learning, how can the right levels of assurance be sought? How can fully-autonomous systems be safely allowed onto ranges for testing, be it land, sea or air? The nature of the pressures and responsibilities defence faces creates unique and often more pronounced challenges.
This workshop will bring together leaders from across government, industry and academia in order to understand and begin to tackle these challenges; challenges which will only grow over time as technologies evolve and become more complex.
By drawing on a diverse range of experiences the workshop will explore common challenges and approaches to future AI and autonomous T&E and V&V, bring together the key stakeholders within the community and start to solve these questions.
Registration from 10:00, workshop starts at 10:30


"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/transport-london,"
tfl.gov.uk
"
https://www.turing.ac.uk/people/researchers/lingyi-yang,"
"
https://www.turing.ac.uk/events/data-study-group-march-2023,"
Introduction
Stage 1: The Precursor Stage (part-time)
Event Stage (full-time)
Applicants should be able to commit to the duration of the event. The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected]ing.ac.uk to discuss any reasonable adjustments.
Challenges
The challenges are:
What are Data Study Groups?
These are intensive 'collaborative hackathons' hosted at the Turing, which bring together organisations from industry, government and the third sector, with talented multi-disciplinary researchers from academia.
Organisations act as Data Study Group 'Challenge Owners', providing real-world problems and datasets to be tackled by small groups of highly talented, carefully selected researchers.
Researchers brainstorm and engineer data science solutions, presenting their work at the end of the week.
How to apply
Applications are being accepted through Flexi-Grant – apply now to avoid disappointment.
Apply now
Why apply?
The Turing's Data Study Groups are popular and productive collaborative events and a fantastic opportunity to rapidly develop and test your data science skills with real-world data. The event also offers participants the chance to forge new networks for future research projects and build links with The Alan Turing Institute – the UK’s national institute for data science and artificial intelligence.
It’s hard work, a crucible for innovation and a space to develop new ways of thinking.
Read reports from previous Data Study Groups to see challenges and outcomes.
FAQs
What if I am already part of the Turing community?
If you are employed by one of the Institute’s 13 university partners, please contact your University Liaison Manager – list available here – to make them aware of your application. They can provide support, answer questions and involve you as part of the Turing community at your university from now on.
If you are employed at a university that received a Turing Network Development Award, please contact your Award lead – list available here (scroll to the bottom of the page) – to make them aware of your application.
More FAQs for Data Study Group applicants.
Find out more
Learn more about being a DSG participant including FAQs
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
"
https://www.turing.ac.uk/people/former-enrichment-students/merve-alanyali,"
Merve Alanyali is a data scientist at LV=. She was previously a doctoral researcher at Warwick Business School and research associate at The Alan Turing Institute.
Her research focuses on analysing large open data sources with concepts from image processing and machine learning to understand and predict human behaviour at a global scale. Her work has been featured by television and press worldwide including coverage in the Financial Times and Bloomberg Business. At Warwick Alanyali held a Chancellor’s International Scholarship. She was awarded a double Masters degree in Complex Systems Science by the University of Warwick, UK and Chalmers University of Technology, Sweden and draws on an interdisciplinary background in computer science, complex systems and behavioural science.
"
https://www.turing.ac.uk/events/methodological-challenges-translation-data-and-evidence-policy-during-covid-19,"
A virtual event, hosted by The JBC-Turing-RSS Laboratory.
Introduced by Dr Johanna Hutchinson, Director of Data and Data Science at the Joint Biosecurity Centre, and by Professor Peter Diggle, Technical Director of the Turing-RSS Laboratory.
Albert Icksang Ko is Professor of Epidemiology and Medicine and Department Chair of Microbial Diseases at the Yale Schools of Public Health and Medicine
In this visiting lecture, Professor Ko will discuss the challenges in developing data-driven policy during the COVID-19 pandemic.
He will draw on his experience as lead scientific advisor to the US state of Connecticut and discuss the barriers to framing policy questions that can be addressed by data and evidence, to obtaining actionable data during an emergency response, and to communicating evidence to decision makers.
Finally, he will use the example of post-licensure vaccine effectiveness studies during the current COVID-19 resurgence in Brazil to illustrate the gaps between data collection, analytic methods and policy and identify lessons learned for present and future health challenges.

"
https://www.turing.ac.uk/people/guest-speakers/anna-thomas,"
Anna Thomas is the Co-Founder and Founding Director of the Institute for the Future of Work (‘IFOW’), an independent research and development institute exploring the impacts of technology on work and working lives. Anna initiated and established the UK’s Future of Work Commission, the All Party Parliamentary Group on the Future of Work and the Pissarides Review into Work and Wellbeing. Formerly, Anna was a barrister specialising in equality and labour law and Head of Future of Work, and a special advisor on digital policy, for the Labour Party. She has also advised on a wide range of technology policy across departments and the Centre for Data Ethics and Innovation, including on the Review into Bias in algorithmic decision-making. Anna is a Fellow of the Institute for Policy Research, Royal Society of Arts, Institute of Employee Ownership and is a member of the ESRC funded Digital Futures at Work programme. 
"
https://www.turing.ac.uk/people/researchers/melanie-smallman,"
Melanie Smallman is a lecturer in Science and Technology Studies at UCL and Co-Director of the UCL Hub for Responsible Research and Innovation. Melanie's research focuses on how responsibility and ethics is understood and enacted in research and how these issues affect public perceptions and social acceptance of technologies. Specifically, she is interested in the role of technologies in increasing inequality and how these wider social impacts can be taken account of within ethical frameworks.
Her research utilises large datasets and computational approaches to qualitative data analysis and she is interested in developing these techniques further. Previously, Melanie ran science policy communication consultancy Think-Lab and spent eight years as an adviser within the UK Government. She is a former Fellow in Science, Technology and Society at the Harvard Kennedy School for Government and has a PhD in Science and Technology Studies at UCL.
AI and data science offers huge opportunities, but also raises important issues around ethics, responsibility and social acceptability. There is also growing evidence that digital and data technologies are helping create growing economic inequalities. As a Fellow at The Alan Turing Institute and a member of the Data Ethics Group, Melanie Smallman's research will examine these issues, in particular looking at the role of AI and data science in growing inequalities and how these issues can be taken account of within ethical frameworks. At the same time, Melanie draws upon data science and computational approaches to qualitative data analysis for her research.
During her time at the Turing, Melanie also hopes to collaborate with data and computational scientists to develop computational and AI approaches to humanities and social research further
"
https://www.turing.ac.uk/people/researchers/adam-johansen,"
Adam Johansen is Professor in the Department of Statistics at the University of Warwick. He studied at Cambridge where he read an M.Sci. in Natural Sciences (Physical) followed by a Ph.D. in Information Engineering. He was a Brunel Fellow in the Department of Mathematics at the University of Bristol, before moving to Warwick in 2008.
 
Adam's research interests centre around statistical computation, particularly Monte Carlo methodology.
"
https://www.turing.ac.uk/people/research-fellows/eric-meyer,"
Eric T. Meyer’s work focuses on shifts in work, knowledge creation, and interactions when digital technologies replace their previously non-digital counterparts. His research in this area has included studies of data sharing in dementia research, the use of digital images in biology, digital information practices in the sciences, social sciences, arts and humanities, and uses of web archive data. He is currently leading research on automation and computerisation in the health sector.
He has three main interests that he plans to develop further as part of the Turing. First, the ability to detect emerging research areas and disciplines using novel techniques related to mining published material, such as journals and scanned books, but also web archives such as those held at the British Library. Second, he is leading research on automation in the health sector to better understand tasks in primary care, and then build models that will help understand the potential for automation. The third area is blockchain as it relates to art, working with art organisations around protecting artists’ intellectual property rights, enhancing their financial security, and dealing with complex artists’ estates.
"
https://www.turing.ac.uk/people/researchers/christopher-burr,"
Dr Christopher Burr is a Senior Researcher in Trustworthy Systems (TPS Programme) and Head of the Innovation and Impact Hub (Turing Research and Innovation Cluster in Digital Twins).
 Comment
He specialises in the trustworthy and ethical design, development, and deployment of data-driven technologies. He is also interested in exploring and understanding how data-driven technologies affect our decision-making and social institutions (e.g. factors that undermine trust in algorithmic systems).
He is the Head of the Innovation and Impact Hub for the Turing Research and Innovation Cluster in Digital Twins (TRIC-DT). The Innovation and Impact Hub team are responsible for supporting and advancing the TRIC-DT’s goals of democratising access to digital twin technology by providing open and reproducible computational and social tools, and also building a multidisciplinary community of practice in digital twinning.
He is also a project lead for the Turing Commons—a platform, community, and repository of resources that are designed to build skills and capabilities across all areas of society (e.g. digital and data literacy). This project is a key skills and training initiative within the Turing, and seeks to build openly accessible resources to improve awareness and understanding of the ethical and societal issues surrounding data-driven technologies, such as machine learning and artificial intelligence.
In 2023, Dr Burr will lead the Trustworthy and Ethical Assurance of Digital Healthcare project—a collaborative project with the Assuring Autonomy International Programme (University of York) to build on and harmonise existing research and work in trustworthy and ethical assurance, including the development of open and reproducible tools that help project teams meet ethical and regulatory best practices in health research and healthcare for a range of digital and data-driven technologies.
Between 2021 and 2022 he was the Principal Investigator for the Trustworthy Assurance of Digital Mental Healthcare project, funded by the UKRI’s Trustworthy Autonomous Systems Hub. The goal of this project was to develop a method for assuring ethical goals and claims associated with the design, development, deployment, and use of data-driven technologies in mental healthcare.
Prior to starting at The Alan Turing Institute, he held postdoctoral research posts at the Oxford Internet Institute, University of Oxford (2018–19), working as part of the Data Ethics Lab, and the Department of Computer Science, University of Bristol (2017–18). He completed his PhD in Philosophy of Cognitive Science at the University of Bristol in 2017, funded by a European Research Council scholarship.
He has previously advised and worked with organisations including the Office for Artificial Intelligence, Council of Europe, Nuffield Council on Bioethics, Ada Lovelace Institute, Centre for Data Ethics and Innovation, the Department of Health and Social Care, and has conducted interviews with media organisations including the Observer, New York Times, BBC, and Vox.
"
https://www.turing.ac.uk/people/researchers/mark-hobbs,"
Mark Hobbs is a Postdoctoral Research Fellow at the University of Exeter and The Alan Turing Institute, where he works under Professor Tim Dodwell in the data-centric engineering group. He gained his PhD in the Department of Engineering at the University of Cambridge (Christ's College), where his research focussed on the development of a numerical model to predict the failure behaviour of quasi-brittle materials. The model was developed using the peridynamic theory of solid mechanics, an integral formulation of continuum mechanics that allows for the natural inclusion of fracture behaviour.
More generally, he is interested in the emerging field of 'learning to simulate' - the application of modern machine learning methods to the field of computational mechanics. This is motivated by the need to reduce the computational expense of costly simulations, and to infer model parameters with a quantified level of uncertainty. Mark has broad research interests in peridynamics, computational mechanics, high-performance computing, simulation, model validation, and uncertainty quantification.
Mark is currently working on the development of PeriPy, a lightweight, open-source and high-performance package for peridynamic simulations written in Python. The development of this package is motivated by the need for fast analysis tools to achieve the large number of simulations required for 'outer-loop' applications, including sensitivity analysis, uncertainty quantification and optimisation. The developed code utilises the heterogeneous nature of OpenCL so that it can be executed on any platform with CPU or GPU cores.
The GPU implementation provides a significant performance gain (4-7 times faster) over a similar OpenMP implementation. Mark is open to academic and industrial collaborations, and he is particularly interested in the simulation of non-deterministic problems. He would be very interested to hear about experimental or numerical problems that exhibit different failure modes due to small changes (or inherent randomness) in the initial conditions.
"
https://www.turing.ac.uk/people/researchers/omar-guerrero,"
Omar Guerrero is the Head of Computational Social Science Research and leads the Policy Modelling Theme at the Turing's public policy programme. He is an economist by training, and has a PhD in Computational Social Science (CSS) from George Mason University. Previously, he worked at University College London and at the University of Oxford, in the Saïd Business School and in the Institute for New Economic Thinking.
Omar researches on different aspects of CSS, including methodology, large-scale data analysis, and different modelling flavours. He also enjoys translating this research into useful tools to help solving problems of public interest, leading him to work with various governments and international organisations around the world. At the moment, he is working on topics related to labour mobility, sustainable development, public administration, technological change, housing markets, and wealth inequality. If you wish to learn more or get involved do not hesitate in getting in touch.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/neuroscience,"
"
https://www.turing.ac.uk/research/research-programmes/tools-practices-and-systems,"
Data science and artificial intelligence are becoming ever more prevalent in the UK with common needs and challenges arising. Solving these challenges requires novel tools, practices and systems which can unlock advances across the wider sector and accelerate innovation.
The tools, practices and systems (TPS) programme at the Turing represents a cross-cutting set of initiatives which seek to build open source infrastructure that is accessible to all, and to empower a global, decentralised network of people who connect data with domain experts.
The programme will:
The programme builds upon work done under the Turing’s AI for science and government integrated research programme, supported by the UKRI Strategic Priorities fund.
TPS also supports the Turing's Ethics Advisory Group to implement guidance developed by social scientists and philosophers in the public policy programme.
TPS is closely aligned with the Research Engineering Group at the Turing.
 
Header image created by Scriberia for The Turing Way community and used under a CC-BY licence.
Open-source toolkits represent a hugely valuable resource for a wide range of sectors and industries. TPS will work closely with developers, stakeholders and end-users to support the creation and maintenance of essential open-source software libraries. Key widely-used products supported by the TPS programme to date include a machine learning framework for the Julia programming language, and a toolbox for time series analysis in data science.
 
Effective science relies on reproducibility. TPS will continue to promote and embed practices which ensure that data science research is carried out in a reproducible way from inception to completion. The Turing Way guide for reproducible research is used around the world to ensure that published results can be interrogated and extended with ease.
 
Secure infrastructure is essential for research and innovation that deals with high-sensitivity data. The Data Safe Haven initiated under TPS is now deployed around the UK – this secure research environment gives data scientists the scale and power of the cloud to predict personalised outcomes without sacrificing the privacy of people represented in the dataset. TPS also prioritises development of high-performance computing infrastructure which can optimise potential of scientific research.
 
TPS looks to co-create solutions in a multidisciplinary and mutually respectful environment. We are committed to demonstrating the creativity and transformative vision that can only be delivered by a diverse group of experts in a welcoming and collaborative space. Researchers and practitioners will be trained and mentored to transparently communicate their insights, and programme participation and outputs will be promoted openly to reach a broad community.
The crisis of reproducibility in science is well known. The combination of ‘publish or perish’ incentives, secrecy around data and the drive for novelty at all costs can result in fragile advances and lots of wasted time and money. Even in data science, when a paper is published there is generally no way for an outsider to verify its results, because the data from which the findings were derived are not available for scrutiny. Such science cannot be built upon very easily: siloed science is slow science.
That’s one of the reasons funders and publishers are beginning to require that publications include access to the underlying data and analysis code. It’s clear that this new era of data science needs a new cultural and practical approach, one which embraces openness and collaboration more than ever before. To this end, a group of Turing researchers have created The Turing Way – an evolving online 'handbook' on how to conduct world-leading, reproducible research in data science and artificial intelligence.
Read the full impact story
Please contact Programme Lead Kirstie Whitaker at [email protected] with any enquiries.
"
https://www.turing.ac.uk/people/doctoral-students/daniel-wilson-nunn,"
Daniel started his doctoral studies at The Alan Turing Institute in October 2016. Daniel is registered at the University of Warwick. He graduated from the University of Warwick with an integrated master’s degree in mathematics and statistics. His areas of interest are rough path theory and machine learning, with particular application to sound.
Rough path theory is a method of analysis of SDEs driven by highly oscillatory processes, such as Brownian motion. Recent developments in the theory of rough paths have opened the doors for ideas and concepts developed for studying rough paths to be used in a wide variety of applied situations, including the recognition of Chinese handwriting and financial modelling. Combining the rich theory of rough paths to the rapidly growing field of machine learning and big data, the aim of Daniel’s project is to develop tools for sound analysis, recognition and compression.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/british-airways,"
"
https://www.turing.ac.uk/about-us/impact/open-minds-how-open-source-tools-are-broadening-horizons-data-science,"
Too often, the codes and software systems that AI and data-driven technologies are based on remain behind closed doors, for company profit or competitive advantage. That way, we all miss out on the best of what these technologies have to offer. Now, though, a raft of projects funded by The Alan Turing Institute’s AI for science and government (ASG) research programme is demonstrating how adopting the most open and collaborative approaches is leading to tools that are more easily adapted to solving diverse problems.
In the North Sea, the Cefas Endeavour research vessel, a floating laboratory, now uses software developed via the ASG programme to study zooplankton – microscopic marine animals including tiny crustaceans and worms that drift through the oceans, feeding everything higher up the food chain. Whilst they are barely visible to the naked eye, closer inspection reveals their various forms; telling one type from another is key to keeping tabs on our oceans’ health. Counting and categorising zooplankton by eye is labour intensive and slow, so scientists at the government’s Centre for Environment, Fisheries and Aquaculture Science (Cefas) wanted to automate the process, using computer software to identify plankton from images taken by the Endeavour’s new high-speed camera system. But their computational expertise was limited.
“We didn’t know how to improve the algorithm ourselves to make it more effective,” explains Sophie Pitois, a marine ecologist at Cefas. So when she found out about the Turing’s Data Study Groups, expert ‘hackathons’ that bring together data scientists from different institutions to work on real-world data science challenges, she was immediately on board.
It was a fruitful collaboration: within a fortnight, the assembled team built a classifier that could take an image and correctly say what type of plankton it showed over 90% of the time. The new software uses artificial neural networks – algorithms inspired by the way the human brain works – trained on 58,000 images of plankton hand-labelled by Cefas, and is capable of counting and classifying 50 plankton per second. It’s a “step change” in performance, according to Robert Blackwell, a Cefas scientist who took part.
The group tested its algorithm during the Data Study Group in December 2021, using an early-stage, open-source tool called scivision, a platform for exploring computer vision data designed by ASG-funded researchers. By May 2022, the new software was at sea, and Cefas is now developing the system so that scientists can access plankton data to guide and adapt sampling, via the research vessel, as it’s happening.
“This is what we’re working towards,” says Pitois. “You press that button, the instruments start collecting the information, process it at the same time, and someone in an office somewhere can visualise how things are evolving.” What’s more, she adds, all the code the team wrote is open source and could be modified to identify a multitude of “very small things in the seas”, potentially transforming how scientists study our oceans. One possible application is in classifying the trillions of tiny pieces of plastic (‘microplastics’) that pervade our oceans.
The team behind another open-source tool, Raphtory, is planning its own Data Study Group early next year. Raphtory analyses how connections in any network – like a social network or transport network – evolve over time. Effectively a time machine for network data, it allows users to travel back to any point in a network’s history, so they can, for instance, analyse misinformation about COVID-19 on Twitter over the last two years, or the shifting interactions within social media platforms popular among the alt-right. The upcoming Data Study Group will bring together prospective new Raphtory users – along with their datasets – to help identify new and creative ways to use it.
Since the project’s inception in 2017, the team has focused on making Raphtory open source and free for all. But the researchers are increasingly taking a double-pronged approach thanks to the recent launch of a Turing spin-off, Pometry, that provides tailor-made versions of the software for users with specific needs. As CTO and founder Ben Steer notes, Pometry can tackle ever more diverse and challenging problems by building on the existing platform.
In the cryptocurrency space, Pometry has provided custom tools to help blockchain analysis companies quickly detect ‘bad guys’ operating within complex webs of crypto-transactions. The time travel element is critical, as it’s only by looking at entire transaction histories that users who unknowingly accept tainted currency can be differentiated from criminals who propagate lengthy chains of fraudulent transactions and should therefore be blacklisted or prosecuted. “What we’re enabling those companies to do,” explains Steer, “is massively scale up the analytics they’re running, and provide a much clearer and, a lot of times, more accurate answer.” Increasingly, these kinds of analytics are necessary for companies dealing in cryptocurrencies to help them comply with financial regulations.
Very recently, Pometry started working with nPlan, a company that uses AI in construction scheduling. Here, the tool views complex construction projects as networks of interdependent processes – its benefit is in extracting more detailed insights from past projects to inform future ones. “We started our collaboration in September 2022 and have already seen great progress on the initial problem,” says nPlan’s Research Director, Vahan Hovhannisyan.
Meanwhile, another set of ASG-funded tools is finding diverse uses in engineering, manufacturing and beyond. Pranay Seshadri, a Data-Centric Engineering Group Leader at the Turing, has developed computational tools that are now being used by Rolls-Royce to help streamline design processes for its jet engines. Though the tools weren’t initially openly available, Seshadri realised that sharing the code and making it more adaptable could benefit people in widely different industries and research spaces.
“We want [the projects we work on] to be open source,” he says. “But perhaps more than that, we want a certain generalisability, in the sense that we don’t want to develop tools just for one company, or indeed one sector.”
Now downloaded over 52,000 times as the equadratures toolbox, Seshadri’s algorithms help simplify the complex computer models used in manufacturing and engineering. However, their ability to zero in on the most important variables in any complex system makes them valuable for other types of model, too, for example, in helping to identify the most important environmental threats to coastal marshlands as part of a United States Geological Survey project.
To offer support to diverse users of equadratures, Seshadri has given introductory workshops attended by over 120 people, including at University of Cambridge and University of Warwick, as well as for companies including Siemens and McLaren. But he’s also keen that the toolbox exists as something people can pick up and use without support, or modify if they want to. So there’s now an online forum, where users can share feedback, and code.
If developing open tools is about placing agency into the hands of users, then a perfect example is the innovative, ASG-funded project Colouring Cities. Designed as a knowledge exchange platform for buildings data, it is boldly setting out to conquer the world, one city at a time. Project lead Polly Hudson, a Senior Research Fellow at the Turing, started by collecting buildings data for London and putting it on a permanent, data-rich map. The idea is now being adopted and tested by buildings researchers internationally, from Colombia to Indonesia, thanks to the project’s open-source code.
The platform joins up data – everything from building age and historical usage to up-to-date information about energy efficiency – that is often fragmented and difficult to access in the buildings space, and maps it to individual building footprints. Of course, open maps like Google Maps already provide the same zoom-in capability, but they don’t provide data on the variables needed to tackle the tricky research problems faced by local policy makers. Problems like: how do we minimise materials and emissions when updating and improving our city’s buildings? Colouring Cities also uses automated approaches to supplement the data – these can, for example, spot high streets based on the characteristics of building footprints attached to them. Through crowdsourcing, the results can then be verified by people living near those streets.
The Australian offshoot of this project has already spread to all the country’s major cities, with a focus on transforming urban planning. Meanwhile, Colouring Athens plans to collect data that will contribute to mitigating risk from natural disasters and climate change, according to coordinator Athina Vlachou at the National Technical University of Athens. In Germany, the Leibniz Institute of Ecological Urban and Regional Development is developing Colouring Dresden in collaboration with the Association of German Architects, and local partners including the state library. “We believe that the data is equally relevant for policy makers, urban planners, civil society, and city residents who, through their active participation in research, better understand Dresden’s buildings and their cultural value, and actively participate in solutions for a better future in the city,” says researcher Robert Hecht, suggesting that the data could be used for research that informs, for example, planning for more energy-efficient buildings. Colouring Dresden has received funding from the German Federal Ministry of Education and Research.
It takes some big picture thinking to envisage what this project, once fully-fledged, could achieve, and Hudson believes this kind of thinking is only possible outside of traditional research settings. “You have to do this kind of research in a more multidisciplinary, collaborative institution,” she says, “where you can answer very big questions by producing resources that enable potentially hundreds of thousands of people to begin to connect stuff they couldn’t connect.” That, she adds, is why Colouring Cities is so at home at the Turing.
To get the most out of AI and data science, we need to be doing more than just using open-source code: we need to be opening our minds about how we work. That means creating collaboratively, choosing open-science principles, and seeking to tackle more diverse problems that can benefit us all. Each of these innovative ASG projects broadens the possibilities, and raises the bar, for what can be accomplished through innovative, open AI and data science.
 
Header illustration: Jonny Lighthands
"
https://www.turing.ac.uk/people/external-researchers/owen-daniel,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/office-national-statistics,"
The Office for National Statistics (ONS) is an impartial department and exercises its functions free of political control.  We produce the numbers that matter most – on the economy and business, people, population and communities.
We provide reliable, quality, independent information on a wide range of issues from understanding changes to the labour market through to Covid-19 and its impacts. We act quickly, setting up new surveys and analytical programmes to help inform policy decisions, such as tracking the spread of the Coronavirus and its effects on people’s lives and the UK economy.  We provide high quality statistics, data and analysis, vital public assets, that are central to democratic society.
We are driven by four core principles:
Radical – in taking opportunities to innovate in the creation and ethical use of data for public benefit while safely protecting its anonymity 
Ambitious – in setting out to answer the critical questions on the economy and society the public needs answers to, and informing the key decisions taken by citizens, businesses and civil society.
Inclusive – in ensuring our statistics reflect the lives and experiences of every member of our society so that everybody counts.  
Sustainable – in delivering a unique service to the highest international standards while providing value for money.  We seek to collaborate with partners, working  together to deliver quality stats in a timely manner.
In June 2021 the ONS and The Alan Turing Institute launched a new strategic partnership to produce close to real time economic statistics to help track changes in the networks underpinning the economy while preserving privacy.   
The collaboration, which will initially run for two years, between the UK’s National Statistics Institute and the National Institute for data science and artificial intelligence will see ONS economists, analysts and data scientists working closely with a team of Turing researchers. 
The first three projects set for delivery are:   
Understanding economic networks
This project will utilise a variety of cutting-edge data science techniques to provide new insights about transactions between firms in near real time, allowing the ONS to better understand the impact of seasonal patterns and major events such as the COVID-19 pandemic or Brexit on the UK economy.  
Economic nowcasting
By rapidly bringing together a range of new data, we aim to create economic models in close to real time that track changes in retail prices, household spending and income at a detailed local level, allowing us to measure the pulse of the economy.    
Synthetic data and privacy preservation
This project will develop tools to allow the sharing of private datasets with a wider range of stakeholders, while preserving privacy. This can be done using synthetic data generators which offer a private way to generate data, whilst preserving statistical features in the original data set. Applying this methodology to sensitive data held by ONS would allow greater flexibility for collaboration between ONS and researchers in the wider community and government. 
"
https://www.turing.ac.uk/news/new-exhibition-considers-human-impact-rapid-technological-change-19th-century,"
A new exhibition at Leeds City Museum, Living with Machines: Human stories from the industrial age is exploring how rapid changes in technology in the nineteenth century altered life and work in the area.
The exhibition is inspired by the Living with Machines research project, a collaboration between the British Library, The Alan Turing Institute and five partner universities which aims to develop new computational and historical methods for working with library collections at scale.
Opening on Friday (29 July), Living with Machines is the first large-scale exhibition developed in partnership between the British Library and Leeds Museums & Galleries.
Early examples of Leeds machinery, including an enormous textile loom, sit alongside a diverse array of material including newspapers, paintings, workers’ protest ballads, propaganda and football ephemera.
Items such as an early example of fast fashion - still in its original packaging - and the ground-breaking pattern marking machine which inspired hopes of a shorter working day, paint a relatable picture of society as advancing technology changed what it was to live and work in a fast-paced city.
Contemporary responses, offering reflections on the parallels between mechanisation in the 19th century and advances in AI and digital technology – sometimes called “the fourth industrial revolution” – are woven throughout the display.
Mia Ridge, co-curator of Living with Machines, British Library lead for the Living with Machines research project, and digital curator at the British Library, said: “It’s been fascinating to look at the history of mechanisation, at a time when society is once again trying to understand what its new forms - AI, data science, machine learning, algorithms - mean for how we live and work. What’s emerged is that some questions – what should we embrace, what should we rethink? – haven’t changed.
“This exhibition is the result of so many layers of collaboration – between Living with Machines researchers, the British Library, The Alan Turing Institute, the public, and finally, between the British Library and Leeds Museums & Galleries. It’s packed with creative responses to mechanisation, alongside relatable human stories and machines that evoke times of change.”
The Living with Machines research project is a 5-year collaboration between curators, data scientists, linguists, historians, other specialists and the public. Over 2,000 people have contributed to the project via crowdsourcing work, analysing how machines were reported on in historical newspapers from the British Library’s collection. The data they created helped shape the exhibition and future questions of the research project. It is funded by the Arts and Humanities Research Council (AHRC) via UK Research and Innovation’s (UKRI) Strategic Priorities Fund.
Professor Ruth Ahnert, Principal Investigator on the Living with Machines project said: “There has never been a more relevant time to explore the impact of rapid technological change on people and society. The Living with Machines project, from which the exhibition arises, has been collaborative effort between data scientists, humanities scholars and the public – it’s exciting to see this exhibition come together to address many of the questions and topics uncovered in our research.”
Header image: fully restored and working dobby loom early 1900s. (c) sunny bank mills and leeds museums galleries
"
https://www.turing.ac.uk/research,"

We are doing this through basic research into theoretical scientific problems, and through applying our cutting-edge science to real-world problems, and with recognition of the important legal, ethical and societal implications of these technologies.
We channel our research around a number of ambitious challenges which represent areas in which AI and data science can have a game-changing impact for science, society, and the economy. These challenges will not be led by the Turing alone, but depend on significant collaboration and partnerships.
The Institute's research spans a huge range of disciplines and expertise. See the (still expanding) list of research areas and explore snapshots of projects, people, events and more.
A grand challenge-led approach for Turing’s science and innovation agenda provides the common focus, clear direction and a mandate to prioritise activity, as well as the inspiration and motivation for diverse communities and capabilities to come together in support of these organising principles. In alignment with national priority areas, we have selected the following areas of focus for grand challenges:
The ‘digital society’ could help to address systemic productivity issues. It could also lead to the provision of more inclusive, fair and robust services. Using our unique position and established expertise, we can tap into the enormous potential for data science and AI to drive a new generation of public services which are efficient, smart and responsive to both complexity and uncertainty. Similarly, advances could improve resilience and economic security through real-time monitoring of the economy, its financial systems, and supply chains.   
The Turing will draw and expand on its established capability and partnerships in public policy and data-driven economics to drive this work forward.  We will focus on the impact of digitisation on business, finance, the measurement of the economy and cross-cutting issues of standards, regulation, ethics and policy. 
To drive advances in the Grand challenge areas, multi-faceted core capabilities are required to provide the underpinning infrastructure, this will involve: 
Core capabilities
Building our foundational capability in data science and AI in support of the above Grand Challenges.
Core capabilities
Expanding our work in the development and provision of open-source infrastructure that is accessible to all.
Core capabilities
Growing our core research software engineering capability to continue to contribute skills in research software engineering and data science in support of national priorities.
Core capabilities
Expanding our capability through Research Application Managers and Community Managers to support translation of our communities’ science and innovation activity.
Together, The Alan Turing Institute and its partners have invested more than £26m in digital twin research and innovation across a range of projects, including developing foundational theory and applications in the engineering, environmental and social sciences. This represents one of the largest and most exciting portfolios of academic digital twin research and innovation in the UK. To build on this activity, we are now establishing a new Turing Research and Innovation Cluster in Digital Twins.
Find out how we are harnessing the power of digital twins
Learn more about the Turing Research and Innovation Cluster in Digital Twins
Significant programmes of strategic research, typically led by a Programme Director or Directors and with involvement from industry, public sector or third sector partners.
Focused scientific study, led by Turing researchers, working with other academics and often industry collaborators, towards defined objectives. Turing research projects aim to be multidisciplinary and multi-institutional, delivering both scientific and real-world impact. 
Institute researchers have set up interest groups to drive collaboration in shared topics in data science. Contact group organisers to get involved or find out more.
Explore our latest research publications, software, and policy submissions in response to government and parliamentary calls for evidence
Intensive two-week events where a team of experts initiate work on tackling foundational challenges in data science and artificial intelligence
Find out about the data ethics initiatives taking place at the Turing.
"
https://www.turing.ac.uk/news/publications/motif-based-spectral-clustering-weighted-directed-networks,"
Clustering is an essential technique for network analysis, with applications in a diverse range of fields. Although spectral clustering is a popular and effective method, it fails to consider higher-order structure and can perform poorly on directed networks. One approach is to capture and cluster higher-order structures using motif adjacency matrices. However, current formulations fail to take edge weights into account, and thus are somewhat limited when weight is a key component of the network under study. We address these shortcomings by exploring motif-based weighted spectral clustering methods. We present new and computationally useful matrix formulae for motif adjacency matrices on weighted networks, which can be used to construct efficient algorithms for any anchored or non-anchored motif on three nodes. In a very sparse regime, our proposed method can handle graphs with five million nodes and tens of millions of edges in under ten minutes. We further use our framework to construct a motif-based approach for clustering bipartite networks. We provide comprehensive experimental results, demonstrating (i) the scalability of our approach, (ii) advantages of higher-order clustering on synthetic examples, and (iii) the effectiveness of our techniques on a variety of real world data sets. We conclude that motif-based spectral clustering is a valuable tool for analysis of directed and bipartite weighted networks, which is also scalable and easy to implement.
Underwood, W. G., Elliott, A., & Cucuringu, M. (2020). Motif-Based Spectral Clustering of Weighted Directed Networks. arXiv preprint arXiv:2004.01293.
"
https://www.turing.ac.uk/people/researchers/aldo-faisal,"
Professor Aldo Faisal is the Professor of AI & Neuroscience at the Dept. of Computing and the Dept. of Bioengineering at Imperial College London. In 2021 he was awarded a prestigious 5-year UKRI Turing AI Fellowship.
In 2019, Aldo became the founding director of the £20Mio UKRI Centre for Doctoral Training in AI for Healthcare and leads the Behaviour Analytics Lab at the Data Science Institute, London. Aldo works at the interface of Machine Learning, Neuroscience and translational Biomedical Engineering to help people in diseases and health.
He currently is one of the few engineers worldwide that lead their own clinical trials to validate their technology. In this space, his works focus on Digital Biomarkers and AI for medical intervention (Makin et al, Nat Biomed Eng; Komorowski et al, NatMed, 2018; Gottessmann et al NatMed, 2019). His work has received several prizes and awards, including the $50,000 Research Discovery Prize by the Toyota Foundation.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/samsung,"
"
https://www.turing.ac.uk/people/guest-speakers/meag-doherty,"
Meag has spent the past 10+ years growing teams and technologies for public benefit at places like Mass General Brigham, Centers for Medicare & Medicaid Services and the National Institutes of Health (NIH). Meag currently leads a team working on building up a research community around NIH's All of Us Research Program. 
 
In her volunteer time, Meag mentors open source community builders with Open Life Sciences and co-organises events for underrepresented technologists in Washington, D.C. 
Meag has a B.S. in Health Policy and Management from Providence College and holds a Software Engineering Immersive Certificate from General Assembly. Meag is also a 2022 Software Sustainability Fellow.
 
"
https://www.turing.ac.uk/people/former-doctoral-students/francesco-cosentino,"
Francesco is a DPhil student at University of Oxford, supervised by Prof. Alessandro Abate and Prof. Harald Oberhauser. Before joining the Turing he developed expertise in the financial industry as FX risk analyst. He holds a Bachelor and an M.Sc. in Mathematics from University of Turin, Italy, with focus on Probability, Finance and Insurance, and an M.A. in Financial Economics from Collegio Carlo Alberto, Turin.
He is interested in stochastic systems from a computer science and mathematical point of view. His research lies between mathematics and theoretical computer science, the best result he would reach is to do formal verification of stochastic environments in continuous time and space (SDE), to study and quantify (compute the probability of) safety-critical properties.
He is attracted by quantitative financial applications, which are relevant thanks to the dynamical setting of the problems, e.g. optimal execution, market microstructure.
"
https://www.turing.ac.uk/people/research-fellows/stephen-hansen,"
Stephen Hansen is an Associate Professor of Economics at the University of Oxford and a Fellow of University College. He received his BSc Econometrics and Mathematical Economics and PhD in Economics from the London School of Economics. Previously he was Assistant then Associate Professor at Pompeu Fabra University in Barcelona, Spain.
Stephen is primarily interested in organisational economics, with a particular focus on central bank design. He has worked on projects that examine the optimal structure of monetary policy committees, the impact of reputational incentives on policy dynamics, and the behavioural effects of transparency. Recently he has begun to incorporate unstructured text data into his research, which has led to a cross-over interest in statistical learning for high-dimensional discrete data.
"
https://www.turing.ac.uk/sessions/are-foundation-models-the-right-foundation-for-ai/,"


"
https://www.turing.ac.uk/news/ai-has-critical-role-play-helping-tackle-climate-crisis-new-turing-white-paper-finds,"
Data science and artificial intelligence (AI) are two of our most powerful assets in addressing the climate crisis, according to findings from a new white paper by The Alan Turing Institute’s AI for science and government (ASG) programme published today (Friday 24 March 2023).
But the paper’s authors say that the use of data science and AI are currently under-recognised and under-explored in climate science research.
They suggest four key recommendations for how data science and AI can be better harnessed to tackle climate change:
The authors hope that these recommendations will provide researchers and funders with a roadmap for future climate science research. The recommendations are based on findings from projects in the ‘Environment and sustainability’ theme within the ASG programme, developed in partnership with the Engineering and Physical Sciences Research Council (EPSRC).
The ASG projects profiled in the paper (which is released as the Turing announces environment and sustainability as one of the grand challenges in its new strategy) demonstrate the diversity of environmental research at the Turing. They encompass software for automatic plankton classification, a framework for forecasting Arctic sea ice change, a model for simulating the health impacts of extreme heat, and a digital twin that is being used to optimise the world’s first underground farm.
Dr Scott Hosking, one of the authors and a Senior Research Fellow at the Turing, said: “Data science and AI are essential tools in helping us to understand, mitigate and adapt to the climate emergency. If used in the right way, they can offer huge societal benefit. We believe that collaboration across sectors and domains is crucial to ensure that research outputs are ready to use in real-world situations.”
Professor Ben MacArthur, Director of the ASG programme, said: “The climate crisis is arguably one of the biggest threats facing humanity. This paper provides a clear pathway for harnessing the potential of data science and AI to help safeguard the future of the planet. It also demonstrates the crucial role that the ASG programme has had in breaking boundaries between environmental research and data science and AI.”
Read the paper:Tackling climate change with data science and AI
 
Top image: Marcel Poncu
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/gchq-ministry-defence,"
The Institute has launched a partnership with GCHQ and embarked on strategic relationships with the Ministry of Defence and two associated departments: the Defence Science and Technology Laboratory (Dstl) and Joint Forces Command.The defence and security programme, developed from this partnership, is interested in advancing data science methodologies and techniques, and in the direct application of data science to real world scenarios.
"
https://www.turing.ac.uk/people/guest-speakers/dr-steve-macfeely,"
Steve MacFeely is the Director of Data and Analytics at the World Health Organization. He is also Adjunct Professor at the Centre for Policy Studies at University College Cork in Ireland and the Director of the IASE International Statistical Literacy Program. 
He is co-chair of the Committee of the Chief Statisticians of the UN System (CCS-UN), chairs the Advisory Board of the Statistical Journal of the IAOS, and is a member of the statistical advisory panel to the UNDP Human Development Index. He is an elected member of the International Statistics Institute.
He was a co-lead on the Data Strategy of the Secretary-General for Action by Everyone, Everywhere 2020 – 2022, and a lead author of the 2020 System-wide Roadmap for Innovating UN Data and Statistics. 
Before joining WHO, Steve was the Head of Statistics and Information at UNCTAD. Prior to joining the UN, he was the Deputy Director-General at the Central Statistics Office (CSO) in Ireland and Programme Director of the joint CSO - Institute of Public Administration ‘Professional Diploma in Official Statistics & Policy Evaluation’. 
 
"
https://www.turing.ac.uk/news/publications/towards-quantitative-research-framework-historical-disciplines,"
The ever-expanding wealth of digital material that researchers have at their disposal today, coupled with growing computing power, makes the use of quantitative methods in historical disciplines in- creasingly more viable. However, applying exist- ing techniques and tools to historical datasets is not a trivial enterprise (Piotrowski, 2012; McGillivray, 2014). Moreover, scholarly communities react dif- ferently to the idea that new research questions and insights can arise from quantitative explorations that could not be made using purely qualitative ap- proaches. Some of them, such as linguistics (Jenset and McGillivray, 2017), have been acquainted with quantitative methods for a longer time. Others, such as history, have seen a growth in quantitat- ive methods on the fringes of the discipline, but have not incorporated them into the mainstream of scholarly practice (Hitchcock, 2013).
McGillivray, B., Wilson, J. and Blanke, T. (2019). Towards a quantitative research framework for historical disciplines. In Proceedings of the Workshop on Computational Methods in the Humanities 2018, Lausanne, Switzerland, June
"
https://www.turing.ac.uk/people/external-researchers/erica-palmerini,"

"
https://www.turing.ac.uk/events/gamechangers-diversity-stem,"
76.2 KB
Read the event report here:

Watch the video here:

 
Spaces at the Gamechangers hackathon are limited. The organisers will select up to 40 participants based on their prior experiences working towards promoting the inclusion of traditionally underrepresented groups, and/or improving their STEM working environment for all.
For more information, please visit the Gamechangers website.
 
This event is kindly sponsored by:

Across STEM industries and academic institutions there are many challenges: professional isolation, sexual harassment and bullying and difficulties balancing work and family. These sorts of experiences disproportionately affect women, people from ethnic minorities, the LGBTQ+ community and disabled people.
There are currently many projects to increase the uptake of STEM subjects by school/undergraduate students. However there are very few evidence based, large scale projects to improve the environment within these areas.
This event hopes to provide an opportunity for like-minded people across a variety of STEM based fields to network, design and develop projects to 1) increase diversity in STEM and 2) improve the working environment. The event will include invited talks and panel discussions, alongside sessions for participants to work collaboratively on projects. There will be time for unconference presentations by participants, as well as ample time for networking. The second day will close with pitches, where participants present their ideas to funders and industry leaders to obtain funding to take the projects forward.
 
Keynote talks:
Why we must engage with AI now - Tabitha Goldstaub (CognitionX, UK)
The reality is that AI adopts the biases of its creators: humans. If existing misogyny and unconscious gender bias is codified into the next wave of technology, the underrepresented are exposed to a less fair, more unequal future. How can we change the conversation and the technology to ensure all humans flourish not just the few?
How to make a fuss: achieving solutions efficiently - Dr Emma Chapman (Royal Astronomical Society, UK)
In this talk, Emma will discuss the most effective methods that she and The 1752 Group have used for achieving change within institutions and the wider culture, from learning to send a polite but firm email and engaging activist support, to knowing when to give up in a constructive way.
Being LGBTQ+ in STEM - Dr Alfredo Carpineti (IFLScience, UK)
What is it like being LGBTQ+ in a STEM field in this day and age? What are the challenges, what is being done, and what our hopes for the future are.
Championing equality, diversity, and inclusion in STEM: Ozak's Perspective - Dr Ozak Esu (Institution of Engineering, UK)

"
https://www.turing.ac.uk/people/researchers/judy-wajcman,"
Judy Wajcman is Emeritus Professor of Sociology at the London School of Economics (LSE). Until 2022, she held the Anthony Giddens Chair in Sociology at the LSE. Prior to joining the LSE in 2009, she was the Professor of Sociology in the Research School of Social Sciences at the Australian National University. She has held posts in Cambridge, Edinburgh, Manchester, Sydney, Warwick, most recently holding the Mellon Fellowship at the Center for Advanced Study in the Behavioral Sciences at Stanford University (2017-18).
She was President of the Society for Social Studies of Science (2009-2011) and is currently a Visiting Professor at the Oxford Internet Institute. In 2013, she was the recipient of the William F. Ogburn Career Achievement Award of the American Sociological Association. Professor Wajcman has an honorary doctorate from the University of Geneva, and she is a Fellow of the British Academy. In 2018, she received the Oxford Internet Institute’s Lifetime Achievement Award ‘in recognition of her contributions to the field of the social study of science and technology'.
She is a member of the AI100 Standing Committee.
Judy has published widely in the fields of science and technology studies, feminist theory, work and organisations. She is probably best known for her analysis of the gendered nature of technology. Her books include The Social Shaping of Technology, Feminism Confronts Technology, Managing Like a Man: Women and Men in Corporate Management, TechnoFeminism, The Politics of Working Life, The Sociology of Speed, and Pressed for Time: The Acceleration of Life in Digital Capitalism, which was awarded the 2017 Ludwik Fleck prize by the Society for the Social Studies of Science. Her work has been translated into Chinese, French, German, Greek, Korean, Japanese, Portuguese, Spanish and Russian. Her current research is about the impact of digital technologies on the experience of time in everyday life.
"
https://www.turing.ac.uk/people/business-team/donna-brown,"
"
https://www.turing.ac.uk/turing-university-network,"

The Turing University Network provides all UK universities with an interest in data science and AI the opportunity to engage and collaborate both with the Institute and its broader networks.
The network is a key component of the Institute Strategy aimed at changing the world for the better with data science and AI. The network supports the Institute in achieving its three ambitious goals: advance world-class research and apply it to national and global challenges, build skills for the future, and drive an informed public conversation. It enables the Institute’s role as a national convenor in the data science and AI landscape and creates opportunity for meaningful collaboration where interests align across research and innovation, skills and engagement.
The network is open, with minimal barriers to both entry and bureaucracy, and provides enablers such as mechanisms to connect across the ecosystem, interesting problems and routes to impact. Through Turing-coordinated activity, the network provides a structure for UK universities to facilitate better connections and collaboration with one another and showcase their data science/AI activity. It also provides universities with an opportunity to engage with various Turing departments and teams.
36 universities from the previous Turing university partners and Turing Network Development Award recipients have joined the pilot network and a call inviting applications from all UK universities is now live.
Members of the Turing University Network nominate a Turing liaison as the primary contact person at the university. Further information and contact details are available here.

 
We are pleased to announce that the Institute is currently inviting applications from universities across the UK to join the Turing University Network. Applications are being accepted via an open call, all UK universities with an interest in data science and AI are invited to apply. Full details are available on the Turing University Network Call 2023 page.
Universities interested in joining the network are invited to contact the Academic Engagement team for an introductory discussion.
To showcase the breadth of activity taking place across the UK's AI and data science landscape, please see our list of events hosted by universities from around the Turing University Network.
Institute schemes and initiatives are not limited to those in the Turing University Network and researchers at universities outside the network are encouraged to consider all available opportunities:
"
https://www.turing.ac.uk/work-turing/why-work-turing,"

The Institute is based at the British Library in central London, close to major rail stations St Pancras International and Kings Cross. The vibrant area is host to restaurants, shops, and dozens of research organisations making up the Knowledge Quarter.
The Turing office is designed to promote collaboration and creative thinking, with varied working areas, including casual seating and writable surfaces.
Employees at the Turing enjoy a number of benefits, including generous annual leave and pension scheme.
To change the world for the better, we need to better reflect the diverse nature of the world.
"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/graph-theory,"
"
https://www.turing.ac.uk/people/sam-nutt,"
Sam leads LOTI’s research efforts and serves as the first ever Data Ethicist for LOTI boroughs. As a researcher, he currently leads work on innovative participation methods and the future of work. As data ethicist, he provides hands on support and leadership for London boroughs, to help them become more responsible innovators with data. Prior to joining LOTI, Sam was a Public Sector Innovation Analyst at the Observatory of Public Sector Innovation (OPSI) in the OECD.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/earlham-institute,"
"
https://www.turing.ac.uk/people/doctoral-students/dominic-danks,"
Dom Danks was a final-year PhD student under the supervision of Christopher Yau. He began his doctoral studies at The Alan Turing Institute and the University of Birmingham in September 2019. His background lies in the fields of physics and machine learning, having studied for an MSci in Theoretical Physics at the University of Birmingham and an MSc in Computational Statistics and Machine Learning at UCL. He is particularly excited by the prospect of improving healthcare outcomes using modern statistical techniques. His doctoral research therefore focuses on developing statistical and machine learning methodologies which have applications within biomedical research.
Dom’s research focuses on the development and application of health-relevant probabilistic machine learning methods. His research interests within machine learning are broad, but he largely focuses on generative modelling, survival analysis and constrained machine learning. During his PhD he has worked on numerous projects with varying degrees of methodological and clinical emphasis. Most recently he has been developing methods to understand complex multimorbidity and treatment response as part of the NIHR AIM-OPTIMAL project.
"
https://www.turing.ac.ukhttps//www.turing.ac.uk/research/research-programmes/public-policy/programme-articles/project-explain,"
AI and machine learning technologies are helping people do remarkable things. From assisting doctors in the early detection of diseases and supporting scientists who are wrestling with climate change to bringing together diverse groups from around the globe through real-time speech-to-speech translation, AI systems are enabling humans to successfully confront an ever-widening range of societal challenges.
This progress has, however, brought with it a new set of difficulties. Many machine learning applications, such as those in natural language processing and computer vision, complete their assigned tasks by identifying subtle patterns in large datasets. These systems accomplish this by linking together many hundreds, thousands—or sometimes even millions—of data points at a time. Humans don’t think this way and because of this have difficulty understanding and explaining how these sorts of AI systems reach their results.     
This gap in AI explainability becomes crucial when the outcomes of AI-assisted decisions have a significant impact on affected individuals and their communities. If an AI system is opaque then there is no way to ensure that its data processing is robust, reliable and safe. Similarly, in cases where social or demographic data are being used as inputs in AI decision-support systems—for instance, in domains such as criminal justice, social care, or job recruitment—the employment of ‘black box’ models leaves designers and deployers no way to properly safeguard against possibilities of lurking biases that may produce inequitable or discriminatory results.
Over the last year, The Alan Turing Institute and the Information Commissioner’s Office (ICO) have been working together to discover ways to tackle these difficult issues. The ultimate product of this joint endeavour—the most comprehensive practical guidance on AI explanation produced anywhere to date—has now been released for consultation. The consultation runs until 24 January 2020, with the final guidance due to be released later in the year.
The project underpinning this work, Project ExplAIn, came about as a result of Professor Dame Wendy Hall and Jérôme Pesenti’s 2017 independent review on growing the AI industry in the UK. This was followed in 2018 by the Government’s AI Sector Deal, which tasked the ICO and the Turing to “…work together to develop guidance to assist in explaining AI decisions.”
In February 2019, two five-day-long citizens’ juries on AI explanation were staged in Coventry and Manchester. These were designed to elicit public preferences about what people expect from explanations of AI-assisted decisions. The juries used a deliberative format with the assistance of expert witnesses, who provided jurors with background information about the technical, legal and ethical dimensions of AI explainability. The juries were followed by three roundtables, where the feedback from the citizens were presented to and then discussed by a range of academic and industry stakeholders, from data scientists and researchers to data protection officers, C-suite executives and lawyers. The results of these public engagement activities as well as extensive desk research have provided the basis for the guidance.
Increasingly organisations are using AI to help them make decisions. Where they are processing personal data to do this, they have to comply with certain parts of the General Data Protection Regulation. Moreover, where their AI-assisted decisions raise possibilities of discrimination against protected characteristics such as age, disability or race, organisations must comply with the 2010 Equality Act.  
But beyond this, an organisation’s capacity to explain its AI-assisted decisions to those affected by them builds trust among the public. It also improves the transparency and accountability of internal governance processes by having an informed workforce that can then maintain oversight of what these systems do and why. Society benefits too, as the priority of designing explainable AI models can improve their reliability, safety, and robustness. It can also help surface the existence of potential issues of bias within these AI systems and in the data they use, which can then be addressed and possibly mitigated.
Wherever organisations use personal data to make AI-assisted decisions, they should be able to explain those decisions to the people affected by them. The guidance we have produced provides an accessible overview of the key principles, concepts and tools that can help organisations provide explanations in practice.
At the heart of the guidance is a series of related questions: What makes for a good explanation of decisions supported by AI systems? How can such explanations be reliably extracted and made understandable to a non-technical audience? How should organisations go about providing meaningful explanations of the AI-supported decisions they make? What do the people affected by these decisions deserve, desire and need to know?
The main focus of the guidance is the need to tailor explanations to the context in which AI systems are used for decision-making. This vital contextual aspect includes the domain or sector in which an organisation operates, and the individual circumstances of the person receiving the decision.
The guidance also stresses a principles-based approach to the governance of AI explanations. We present four principles of explainability that provide ethical underpinnings for the guidance and that steer the practical recommendations contained in it:
Building off these principles, we identify a number of different explanation types, which cover various facets of an explanation, and will often be used in concert with each other:
For organisations, the emphasis is on how to set up and govern the use of AI systems to be suitably transparent and accountable, and that they prioritise, where appropriate, using inherently explainable AI models before choosing less interpretable models, such as ‘black box’ systems. We outline the art of the possible in these considerations, to help the governance and technical teams in organisations think about how to extract explanations from their AI systems.
When delivering an explanation to the individual affected, there are a number of contextual factors that will inform what they should be told first, and what information to make available separately. We call this ‘layering’ explanations, which is designed to avoid information overload. These contextual factors are:
Our Ethics Fellow David Leslie and the public policy programme have incorporated the state of the art in the responsible design and implementation of interpretable AI systems into this guidance. They have also drawn from the results of the Project ExplAIn citizens’ juries and existing frameworks such as the Turing’s own Understanding Artificial Intelligence Ethics and Safety to provide the strong ethical foundations that underpin it.
The guidance is intended to be a useful and inclusive tool, so the ICO and the Turing welcome comments from members of the public, experts, and practitioners who are developing and deploying AI systems. You can find details on responding to the consultation here.
About the authors: David Leslie is the Ethics Fellow in the public policy programme at The Alan Turing Institute. Helena Quinn is Senior Policy Officer at the Information Commissioner’s Office. She is currently seconded to the Information Commissioner’s Office from The Alan Turing Institute.
 

This infographic video produced by Fable Studios consists of an introduction to 'Explaining Decisions Made with AI', a guidance co-produced by the Information Commissioner’s Office and The Alan Turing Institute. The video provides basic information about the importance of explainable AI. It includes an introduction to the four principles of AI explainability and a description of the six explanation types which are meant to assist organisations with delivering understandable explanations to relevant stakeholders. The purpose of the video is to provide an accessible entry point to the guidance and to direct towards the complete version of the guidance, to learn more about how to implement it in practice. 

"
https://www.turing.ac.uk/people/researchers/gesine-reinert,"
Prof. Gesine Reinert is a Research Professor at the Department of Statistics, Oxford, and Fellow at Keble College, Oxford (2000 – present).  
Her current and main research interests are in network statistics and to investigate such networks in a statistically rigorous fashion. Often this will require some approximation, and approximations in statistics are another of her research interests. Stein’s method is an excellent method to derive distances between the distributions of random quantities, and is one that Prof Reinert has required some expertise in over the years. Moreover she uses theoretical insights to develop machine learning methods for the analysis of network data, to tackle problems in finance, economics, and computational biology.
"
https://www.turing.ac.uk/events/audience-type/technical,"
"
https://www.turing.ac.uk/people/doctoral-students/corinne-cath,"
Corinne Cath is a doctoral student at the Oxford Internet Institute (OII) and a cultural anthropologist. 
Prior to joining the Turing Institute, she worked at human rights NGO ARTICLE 19 and as a policy advisor for the US House of Representatives in Washington D.C.  She has an MSc in Social Science of the Internet from the University of Oxford, an MA in Conflict Studies and BA in anthropology from the University of Utrecht.
Her research covers engineering culture, Internet governance, and the social significance of technologies like artificial intelligence. For her doctoral project, she focuses on public interest advocacy in engineering organizations. She is working on these issues under the guidance of Professor Luciano Floridi and Professor Victoria Nash. 
Early 2018, she published as first editor a Special Edition with the Royal Society on the governance of artificial intelligence (AI).
At the Turing, she co-convenes the Protocol Governance interest group. 
When she is not at her standing desk at the Turing, she can be found online @C_Cath (T) or ccath_incc (Insta). 
"
https://www.turing.ac.uk/people/former-doctoral-students/shouto-yonekura,"
Shouto studies at the Department of Statistical Science, at University College London. His current research interests are statistical methods for stochastic differential equations with discrete observations.
Shouto’s current research interests include studying model selection methods for diffusion-type models with discrete time observations. Although empirical studies using diffusion-type models have been exponentially increasing, there are quite a few studies on model selection methods for these models. This is  basically because the likelihood functions of diffusion-type models with discrete time observations are intractable. To overcome this difficulty, Shouto’s research seeks to develop Sequential Monte Carlo methods (a special case of Feynman-Kac Formulae) to make it possible to construct model selection methods for diffusion-type models with discrete time observations. He is also interested in Sequential Monte Carlo methods for parameter estimation.
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/dstl-challenge-machine-learning-understanding-code-quality,"

As systems get bigger, software becomes more complicated. As an illustration of this, the Boeing 787 aircraft carries 7 million lines of code. Managing this and checking for code errors has become increasingly difficult and the Defence Science and Technology Laboratory (Dstl)’s challenge for Turing researchers was to find a machine-learning solution that can help improve tools for understanding code quality. The direct benefit of this would be greater control of the growth of bugs in new big systems, which in turn, if applied to the aeroplane example above would mean lower risk of something going wrong, with potentially catastrophic results.
Current static analytic and formal method solutions are ‘noisy’, in the sense that when applied, up to 100,000 errors maybe identified which need to be manually sifted through to identify the most serious errors. They are also costly. Machine learning tools on the other hand, are more sensitive and able to prioritise problems in order of severity.
Before coming to the Data Study Group, John, from Dstl, already had ideas about the potential of what machine learning could achieve. He says: “I knew that machine-learning systems have the capability to detect and learn from patterns in data and that this could be applied to detecting bugs in code. I was hoping that this idea could be built on and that Turing researchers would help me to take it further.”
John says that what at the beginning seemed impossible now appears to be a real possibility. He says: “I didn’t really come with any expectations. I just hoped the researchers would be enthusiastic about engaging with the problem, pull it apart and tackle it with different approaches.”
""I don’t think one person or discipline in isolation could have achieved this [solution]""
'John', Defence Science and Technology Laboratory (Dstl)
The researchers were drawn from backgrounds ranging from machine learning, computer science and deep learning, which meant they all had different ideas about how the challenge could be tackled. John says, “It was great to get that range of experience all in one place. They bounced ideas off each other and this led to the beginnings of a solution. I don’t think one person or discipline in isolation could have achieved this.”
He explains, “This is an area that is sparse in research. There is a lot of potential in this field but not much to see across the academic literature, so we are not yet in a position to expect a full solution. But if we can begin to drive the research, we can begin to drive the solutions.”
Reflecting on the highlights over the five days, he says, “Being able to sit in with the team as they worked was a privilege. Also the speed with which they could jump from problems to solutions was great to see.”
He adds: “I am confident that machine learning and data science will eventually provide the tools to solve the problem of code quality. We knew that technically it is a difficult problem to solve and that it would require a lot of work. However, machine learning tools have the capacity to learn over time which would then enable us to build better future systems more quickly.”
In terms of advice to those who may be interested in collaborating with the Institute in future, John says, “In advance of a Data Study Group prepare the data and the techniques as best you can. It was astonishing to see what’s been achieved, the hours put in and the progress the researchers were able to make, but they only really had three full days to work on the challenge. Good preparation of the data in advance would have allowed more time.”
"
https://www.turing.ac.uk/blog/clouds-and-blackberries-how-web-archives-can-help-us-track-changing-meaning-words,"
The meaning of words changes all the time. Think of the word ‘blackberry’, for example, which has been used for centuries to refer to a fruit. In 1999, a new brand of mobile devices was launched with the name BlackBerry. Suddenly, there was a new way of using this old word. ‘Cloud’ is another example of a well-established word whose association with ‘cloud computing’ only emerged in the past couple of decades. Linguists call this phenomenon ‘semantic change’ and have studied its complex mechanisms for a long time. What has changed in recent years is that we now have access to huge collections of data which can be mined to find these changes automatically. Web archives are a great example of such collections, because they contain a record of the changing content of web pages.
But how can we automatically detect in a huge web archive when a word has changed its meaning? A common strategy is to build geometric representations of words called word embeddings. Word embeddings use lots of data about the context in which words are used so that similar words can be clustered together. We can then do operations on these embeddings, for example to find the words that are closest (and most similar in meaning) to a given word. It’s a useful technique, but building embeddings takes a lot of computing power. Having access to pre-trained embeddings can therefore make a big difference, enabling those in the scientific community without sufficient computational resources to participate in this research.
A team of researchers from The Alan Turing Institute and the Universities of Bari, Oxford and Warwick, in collaboration with the UK Web Archive team based at the British Library, has now released DUKweb, a set of large-scale resources that make pre-trained word embeddings freely available. Described in this article, DUKweb was created from the JISC UK Web Domain Dataset (1996-2013), a collection of all .uk websites archived by the Internet Archive between 1996 and 2013. (This dataset is held and maintained by the UK Web Archive, which has been collecting websites since 2005, initially on a selective basis and since 2013 at a whole domain level.) DUKweb contains 1.3 billion word occurrences and two types of word embeddings for each year of the JISC UK Web Domain Dataset. The size of DUKweb is 330GB.
Researchers can use DUKweb to study semantic change in English between 1996 and 2013, looking at, for instance, the effects of the growth of the internet and social media on word meanings. For example, if the word ‘blackberry’ is used mostly to refer to fruits in 1996 and to mobile phones in 2000, the 1996 embedding for this word will be quite different from its 2000 embedding. In this way, we can find words that may have changed meaning in this time period.
The figure below (from Tsakalidis et al., 2019) shows four words whose contexts of use have changed in the last couple of decades: ‘blackberry’, ‘cloud’, ‘eta’ and ‘follow’.
The resources that underpin DUKweb are hosted on the British Library’s research repository, and are available for anyone in the world to download, reuse and repurpose for their own projects. This repository is part of the BL’s Shared Research Repository for cultural heritage organisations, which brings together the research outputs produced by participating institutions, and makes them discoverable to anybody with an internet connection. Providing a stable, dedicated location to hold heritage datasets in order to share them with a wider research community has been one of the key drivers in the implementation and development of this repository service. We are grateful to the British Library’s Repository Services team for supporting this collaboration between the UK Web Archive team and the Turing by making the content for DUKweb available.
Read the paper:DUKweb: diachronic word representations from the UK Web Archive corpus
 
Top image: Jeremy Bezanger / Unsplash
"
https://www.turing.ac.uk/people/researchers/theo-damoulas,"
Theo Damoulas is Professor of Machine Learning and Data Science with a joint appointment in Computer Science and Statistics at the University of Warwick. He is also a Turing Fellow of The Alan Turing Institute and affiliated with NYU as an Exchange Assistant Professor at the Center for Urban Science and Progress (CUSP). He holds a prestigious 5-year UKRI Turing AI Acceleration Fellowship on developing the machine learning foundations for digital twins.
Probabilistic machine learning and Bayesian statistics with an emphasis on the study and integration of various forms of structure and inductive biases (e.g. structured priors, spatiotemporal dependencies, dynamics, compositions, physical laws, flows, causality) while advancing robust and scalable approximate inference methodologies.
"
https://www.turing.ac.uk/people/researchers/stephen-mcgough,"
Dr Stephen McGough is a Senior Lecturer in the School of Computing at Newcastle University. He studied Mathematics at Durham University as an undergraduate (1993) before studying his MSc (1996) and PhD (2000) at Newcastle University in the School of Computing. Stephen has previously worked at Imperial College London (2000-2009), University College London (2009), Newcastle University (2009-2013) and Durham University (2013-2017).
Stephen has a keen interest in machine learning and specifically deep learning which has developed as a consequence of his prior interests in mathematics and parallel computing. His interests in optimisation has led to a focus on the optimisation of deep learning networks. Although deep learning has shown, in many cases, super-human performance, this often requires vast computational resources. This is not only due to the vast amounts of data which need to be used for training, but also the need to identify the 'best' deep learning network to use. Stephen's research in this area is in how we can automatically identify the 'best' deep learning network to achieve optimal results without having to expend vast amounts of computing time.
"
https://www.turing.ac.uk/people/former-doctoral-students/andreas-grammenos,"
Andreas was born in Athens, the capital of beautiful Greece in the late 80’s and formally obtained his Degree in Electronic & Computer Engineering from Technical University of Crete in 2015. He joined University of Cambridge as a first year PhD student in October 2016.
Andreas’ thesis revolves around the challenges involved when dealing with data at a massive scale which are primarily sourced from mobile devices. Finally, his general research interests include the broad field of Data Analytics which is an interesting intersection of computer science, systems programming and mathematics.
"
https://www.turing.ac.uk/research/research-projects/synthetic-data-and-privacy-preservation,"
Datasets are often stored in silos spread across organisations and are not easy to share with outside entities (e.g. academic community) or with different departments within organisations. The roadblocks for that sharing are privacy constraints and regulatory requirements.
Synthetic data generators (SDGs) enable users to share and link data, to work with data in safe environments, to fix structural deficiencies in data, to increase the size of the data, and to validate machine learning systems by generating adversarial scenarios.
This project aims to produce state of art data generators for both structured and unstructured datasets, as well as metrics for evaluation the utility and privacy of synthetic datasets across multiple use cases.
This project will draw on recent methodological developments in network modelling and the application of the signature method for data description. Combining these developments with classical and deep data generation processes ensure that the data generated is not just accurate but also efficient and explainable. Many SDGs can also be made private using differential private mechanisms at the optimisation and model parameter level. Unlike de-identification methods like data masking, shuffling, and encryption, SDGs minimize the scope for adversaries to recover personal information. SDGs use algorithms that preserve the original data's statistical features while producing new data points, without the one-to-one matching that is seen with de-identification methods. 
Synthetic data has many possible use cases such as increasing the size of the data, fixing structural deficiencies, or enabling researchers to test machine learning algorithms functionality without access to the full datasets.
Alongside this, synthetic data has the potential to enable easier access to synthetic versions of sensitive datasets, democratising research and allowing greater sharing of data between (and within) organisations.
Working with the Office for National Statistics (ONS), we seek to create state of the art synthetic data generators, alongside metrics for assessing the utility and privacy of synthetic data to bolster data sharing within the ONS, and across the research community engaging with them.
The potential applications of SDGs are numerous and range from simple synthetic datasets for software development, to allowing researchers access to synthesized versions of incredibly sensitive datasets, whilst providing data controllers with reassurance regarding privacy concerns.
This project is concerned with building up a useful framework for generating synthetic data, as well as assessing its privacy and utility. Doing this in collaboration with the ONS enables for the adoption of this framework in researcher safe havens across the ONS and other government departments.
Tony Zemaitis[email protected]
"
https://www.turing.ac.uk/people/guest-speakers/johan-ordish,"
Johan leads Software Group within the Innovative Devices Division of the Medicines and Healthcare products Regulatory Agency (MHRA). Software Group are responsible for most aspects of regulating software as a medical device (AI included). He was a co-author of Good Machine Learning Practice for Medical Device Development, has been involved in the development of various reporting standards for AI such as DECIDE-AI, and represents the UK in several international fora such as the AI Medical Device Working Group at the International Medical Device Regulators’ Forum. Johan is also an Honorary Associate Professor in the College of Medical and Dental Sciences, University of Birmingham, and an Associate at Hughes Hall, University of Cambridge.
"
https://www.turing.ac.uk/people/guest-speakers/michael-dandrea,"
Michael is a Principal Data Scientist at Genentech in the Pharma Development Informatics division where he works on leveraging AI for clinical trial informatics. Some of his previous relevant experience includes developing the EHR data course for Udacity’s AI for Healthcare nanodegree program and working as a Lead Data Scientist for McKesson’s Change Healthcare. He has a Masters in Computing in Education from Columbia and MBA from Cornell as well as a BA in Psychology with a Pre-Med concentration from the University of New Hampshire.
"
https://www.turing.ac.uk/research/research-projects/data-science-climate-resilience-east-africa,"
The impacts of climate change are most strongly felt by subsistence farmers who are seeking to mitigate these effects by planting trees. This project aims to combine satellite measurements with social data on activity and participation. It aims to develop improved and transparent metrics for assessing the impacts of tree planting and strategies for expansion. 
The project uses the cloud based Google Earth Engine which allows anyone to access and analyse a catalogue of satellite imagery and geospatial datasets. This project is mainly interested in indicators of plant productivity and drought, for example NDVI, which measure the amount of chlorophyll in an image, or NDWI, which measures plant water content and serves as a proxy for plant water stress.
The specific satellite data being used comes from LandSat which gives a low-resolution but long-term survey of the areas of interest (primarily East Africa) and Sentinel which gives high-resolution (up to 10m) images for directly studying individual farms.
By combining these indices together with the very detailed organisational data of the project's partners at TIST (The International Small Group and Tree Planting Program), it's possible to calibrate models that infer tree cover from satellite images, as well as study landscape-scale changes occurring due to TIST and Kenyan NRT (Northern Rangelands Trust) conservation practices over their histories.
This analysis can also be combined with forest cover change datasets to understand any secondary effects of TIST's activities on the landscape, such as increases in forest cover near to TIST groves.
The high temporal resolution of the satellite data will allow the development of novel tools for assessing the impacts of tree planting and conservation farming, using indicators of plant productivity, vegetation structure and drought indices. These computational methods potentially provide cheap and effective ways to monitor progress in tree growth, resilience, conservation, SDGs as well as enhancing field surveys and land management. 
The project partners at TIST will occasionally seed an area by bringing existing members to speak with locals about the benefits of the program. Past experience suggests that the gender balance of these meetings has a significant impact on their outcome. This work aims to identify significant social and environmental factors affecting the success of these seeds.
 
Improved and transparent metrics for assessing the impacts of their activities will be of direct value to tree planting and conservation organisations, supporting their growth and making the benefits of participation clear to members and funders. For example, the project is looking at areas managed by the Kenyan Northern Rangelands Trust (NRT) an association of community conservancies. An understanding of how and where community grazing plans are impacting grassland resilience will be invaluable to NRT, and will help communities to develop sustainable and drought-resilient grazing practices. 
The International Small-Group and Tree Planting Program (TIST), a farmer led tree planting scheme are the main partners in this work. Their members are concerned about drought and soil erosion, so an increased understanding of the impact of TIST's tree planting will be invaluable in retaining members and growing the organisation. An understanding of the social factors at play in TIST: for example how information spreads, how the gender make-up of the groups affects their success, how the organisation grows, will likewise be invaluable and give important lessons for the future of both TIST and similar schemes in other countries.
"
https://www.turing.ac.uk/people/external-researchers/farina-farrier,"
"
https://www.turing.ac.uk/work-turing/studentships/enrichment,"

Now in its 8th year, the Enrichment Scheme is designed for students looking to enhance and broaden their research within the Turing's community.
Applications for the 2023 Enrichment scheme have now closed.
For information on how to prepare a competitive Enrichment application, please view the slides used in the application session from the 25 January 2022.
Our two open events have now taken place. We recorded the in-person sessions and will send to students who cannot make the event.
You can now watch the open event on the Turing's YouTube channel. 
As the Enrichment scheme enters its 8th year, the Turing announces that for the 2023/24 academic year we will only be offering the following option:

The Placement Award offers candidates the opportunity to undertake a 6 or 9 month placement at the Turing HQ in London. This award carries an expectation that holders will engage in person at the office for a minimum of 2 days per week (subject to the office being open and government health guidelines). 
Up to 55 awards will be offered and will become active in October 2023 or January 2024 at the candidate’s choice. A Placement award of £472.50 per month will be provided to facilitate relocating or commuting to the Turing HQ.

For 2023 the Community Award will not be offered as we have for 2022. Following analysis, it has been decided to use the 2022 cohort as a pilot scheme; outcomes and impact will be monitored throughout the 2022/2023 academic year. A decision will be made in summer 2023 about whether the Community Award will be offered again for the 2024 Call.
 
“The opportunities for engagement and networking the Turing Enrichment Scheme provided were second to none. From “Lunch and Learn” seminars to taking part in the AI UK conference (as a presenter, facilitator, and panel discussant), there were so many experiences that substantially enhanced my studies. Altogether, through the Enrichment Scheme I was able to receive world-class training, I expanded my (research) horizon, and became part of the extraordinary community the Turing is fostering.”Isabella Deutsch, University of Edinburgh, 2021 Enrichment student.

 
The Enrichment scheme is designed to give students the opportunity to enhance, refresh, and broaden their research with the Turing’s community and in recognition of their place within the UK’s growing data science and AI research community. Placement Award holders receive funding to physically access the Institute’s facilities whilst also building both online communities and facilitating other activities such as attending training courses, going to conferences and visiting collaborators.

The award should support and enhance the current work towards the PhD thesis and be supported by the student’s supervisor from their home university*. Students usually begin their engagement with us in their second or third years of a typical doctorate to further the work they are undertaking for their research project. Students must be in the active research phase of their studies for the duration of the award. There is no ‘typical’ Enrichment student, and we welcome students from a broad range of disciplines. *Home university is where the PhD will be awarded from.
Enrichment students have the opportunity to find new collaborators for their research or related work. Enrichment Awards allow students to join a cohort from across the UK, as well as the range of researchers already active at the Turing. Collaboration and networking are encouraged at the Institute through interest groups, seminars, events and workshops and engagement with the Turing’s research programmes. 
We encourage our Enrichment students to learn and apply new methodologies to enrich their research. Learning is supported through training activities offered by the Institute and elsewhere across the research landscape. Students will be able to apply to use the computing resources available through the Turing and will have access to academic mentors while at the Turing. Example training activities include:

·    Research Software Engineering with Python:
·    AI Ethics and Governance
·    Research Data Science
Enrichment students have an opportunity to develop their research independence as they engage on programme. With support from their home supervisors to spend time away from their usual work environment, students are able to engage in new research groups and with new ideas which can be beneficial to future research output.
Considering the ethical implications of research is a core part of all the work at the Turing. All Enrichment students are encouraged to examine the ethical issues in their own projects and discuss them with others. Students are encouraged to engage with Turing Research Ethics (“TREx”) as well as contribute to ongoing discussions regarding fairness and transparency through projects such as The Turing Way, our open-source guide and gold-standard in developing reproducible data science projects. 
 
“In addition to developing my core research skills by attending Turing courses such as the excellent RSE programme, I have gained a huge amount of insight and social support from being in a vibrant physical space filled with other research students. This informal mixing and sharing of ideas has helped me to further ground my research in the wider AI field, which will benefit me greatly when writing up my thesis over the coming year.”
Tom Bewley, University of Bristol, 2021 Enrichment Student
There is no typical Enrichment student. We welcome applications from students who will embrace the opportunity to enrich their own research through the Turing environment. We encourage applications from a broad range of academic disciplines and backgrounds, especially those whose research spans multiple disciplines and applications and aligns with the Institute’s research areas. Students should consider how they would best utilise the Enrichment scheme to support their current research and contribute to their doctoral work. You can see examples of the research our Enrichment students have conducted and what they got out of their placements in our yearbooks from our 2019/20 cohorts.
Students will remain enrolled at their university during the Enrichment scheme and must have the support of their supervisor and university.
Potential applicants should review the Call Document for full eligibility and assessment criteria before preparing their application.  
Introduction and opening remarks - Professor Sir Adrian Smith, Institute Director

Keynote speaker - Kirstie Whitaker, Programme Director for Tools, Practices and Systems 

Enrichment student panel discussion 

How to develop a competitive application - Kit Windows-Yule, Turing Fellow
Application mentor panel discussion 

 
 
 
 
 
Applications for the 2023 Enrichment scheme have now closed.
The Alan Turing Institute is committed to championing equality, diversity, and inclusion. If you have questions about the Enrichment scheme or require any adjustments to the application process please contact our team on [email protected] or on 0203 862 3578.
For more information about how we process applicants' data please see our transparency notice.
Enrichment scheme call document
Guidance for Universities and Supervisors
FAQs
 
"
https://www.turing.ac.uk/research/interest-groups/robust-machine-learning,"
Safe and robust AI forms a priority area in the UK government's AI roadmap, and within the AI programme at Turing. We aim to improve the reliability of AI tools developed in academic settings, and to provide a bridge to industry by building benchmarks informed by industry applications of AI tools, and coordinating public challenges around these benchmarks. The interest group will host a platform to discuss these ideas and invite external speakers to foster new collaborations between industry and academia.
Safe and robust AI forms a core theme in the AI programme at the Turing. Together with the Turing's mission to foster collaborations between industry and academia, the interest group will act as a catalyst in creating new collaboration opportunities.
Click here to join us and request sign-up
[email protected]
"
https://www.turing.ac.uk/people/researchers/mihaela-van-der-schaar,"
Professor van der Schaar is John Humphrey Plummer Professor of Machine Learning, Artificial Intelligence and Medicine at the University of Cambridge and a Turing Fellow at The Alan Turing Institute in London, where she leads the effort on data science and machine learning for personalised medicine. She is an IEEE Fellow (2009). She has received the Oon Prize on Preventative Medicine from the University of Cambridge (2018).  She has also been the recipient of an NSF Career Award, 3 IBM Faculty Awards, the IBM Exploratory Stream Analytics Innovation Award, the Philips Make a Difference Award and several best paper awards, including the IEEE Darlington Award. She holds 35 granted USA patents.
The current emphasis of her research is on machine learning with applications to medicine, finance and education. She has also worked on data science, network science, game theory, signal processing, communications, and multimedia. Prior to her academic career, she was a Senior Researcher at Philips Research in the Netherlands and USA; from this work she holds 33 patents.
"
https://www.turing.ac.uk/research/research-projects/predicting-data-centre-system-failures,"
There is a growing academic and commercial interest in the measurement and analysis of hardware failures in pivotal data centres and high performance computing (HPC) systems. Recent error detection and failure diagnosis frameworks, which use data about how much computing resource is being used in combination with failure logs, have shown increased accuracy over using failure logs alone. In collaboration with Intel, this project is aiming to develop new frameworks to better detect, diagnose, and predict system errors and failures.
Computing systems for modern day HPC and data centres are rapidly changing as new technologies and software grow. These computing systems are capable of generating a massive amount of often unstructured data of various different types. Therefore, it is crucial to find the right types of data and analyse it rapidly in order to detect, diagnose, and predict system errors and failures efficiently. This is an important, challenging task for improving the reliability and uptime of computing systems, and it’s importance is demonstrated in the increasing number of large-scale failure analysis research being published.
A significant body of research has also shown the value of failure logs for managing failures. Recent error detection and failure diagnosis frameworks, which use data about how much computing resource is being used in combination with failure logs, have shown increased accuracy over using failure logs alone.
As an example, when there are correlations between the use of system resources for both computing processes and memory allocation, and these activities occur at the same time as memory errors, it indicates that memory allocation activities are associated with the generation of memory errors. Therefore, it’s possible to use the monitoring, or ‘counters’, of the correlated activities of computing process and memory allocation, to assess the state of memory allocation in a system, and then use any associated memory errors to identify which applications are causing these errors.
A. Pelaez, A. Quiroz, J.C Browne, E. Chuah, M. Parashar, ""Online Failure Prediction for HPC Resources using Decentralized Clustering"", in Proceedings of 21st IEEE International Conference on High Performance Computing (HiPC), 2014, doi
N. Gurumdimma, A. Jhumka, M. Liakata, E. Chuah, J.C. Browne, ""Crude: Combining resource usage data and error logs for accurate error detection in large-scale distributed systems"", in Proceedings of 35th IEEE Symposium on Reliable Distributed Systems (SRDS), 2016, doi
This project involves studying the nature and characteristics of system errors and failures, developing new data-processing methodologies, and implementing tools for testing on actual large cluster systems. The knowledge that will be gained from the study can then be used to develop error recovery strategies. The reports that will be generated by these data-processing methodologies can then be used to support data centre systems administrators in system diagnosis (and failure prediction). In addition the tools that will be implemented have the potential to be used in automating diagnostics workflows.
Specifically, this project is producing a framework for analysing and reporting error propagation patterns and degrees of success and failure of error recovery protocols. The framework uses both failure logs and resource use data in its analysis. It has the potential to be adapted for application to any cluster system or supercomputer that generates resource use data and failure logs.
The framework has been applied to resource use data and failure logs on three different large HPC systems operated by the Texas Advanced Computing Center. The analyses generated by the framework have revealed many interesting insights into patterns of memory allocation and memory leaks, communication and file-system I/O errors, and chipset and memory errors.
The framework will continue to be tested on the HPC systems at the Texas Advanced Computing Center as well as at other data centres that operate HPC systems that generate resource use data and failure logs.
Published work on online failure prediction using a decentralized clustering approach (https://doi.org/10.1109/HiPC.2014.7116903) to detect anomalies in resource usage logs showed that 20% of the features in resource usage data give the best results for predicting node failures.'
For more information, please contact The Alan Turing Institute
[email protected]
"
https://www.turing.ac.uk/people/researchers/neil-dhir,"
Neil Dhir recently completed his D.Phil. in Engineering Science at the University of Oxford. He returned to London upon joining The Alan Turing Institute, having completed his M.Eng. in aeronautical engineering at Imperial College London. Previously, Neil started the company Kamin AI.
 
Neil's research interests include causal inference, the sensor scheduling problem, information geometry and more general experimental design. He is particularly interested in those topics under the umbrella of dynamical systems theory.
As the world is fast gaining a certain proclivity towards causal inference, Neil's focus is on developing general-purpose inference algorithms which can tackle the (most) common challenges faced in causal learning and operations research.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/roche,"
The Alan Turing Institute – Roche strategic partnership – Generating insights into disease, patient, and outcome heterogeneity using advanced analytics
Find out more about Roche
"
https://www.turing.ac.uk/people/spotlights/magda-osman,"

The project is called ‘The Ethics of Digital Immortality and Digital Bodies’ and was in collaboration with FUEL who are an arts producer (shows, festivals, films, installations, podcasts, apps and books). FUEL had recently produced a play called ""Dead body in Taos"" which inspired the research I then became involved in. 

The play is about the ethical, legal and emotional implications of using artificial intelligence (AI) to become digitally immortal (the process of uploading, storing, or transferring a person's personality into a digital entity or cyberspace). Some of the other public engagement activities from this project included podcasts, and brilliant visuals to help make the scientific concepts accessible and engaging.
The themes of the play really get to the core of what makes us human, and explore our relationship with next generation technologies – I thought that this was a great opportunity to engage with people on these topics. 
The research idea was simple: why not ask people questions relating to how much they know about digital immortality, and the technological advances we are making in this area. For example, how much more information on digital immortality do people want to know? And would knowing more lead them to change their views? Do people want laws to be in place to prevent digital immortality from happening? 
So, we asked two groups of people these questions, some that hadn’t seen the play or engaged with the podcasts, and others that had. We found that by asking people to think about the topic of digital immortality through the activities we created, more people were interested in research, more were concerned, and also more called to introduce policies around the topic.  
Getting a chance to work with arts organisations has been inspiring and challenging. It’s been a real test of what I know, and has made me think about what I don’t know. I think opportunities like this are good for any researcher that wants to sharpen their skills and learn to communicate their ideas better. Hopefully the impact of this project is to show that there are a number of ways of generating research, and that the research itself can also be a vehicle for public engagement.  
Magda is speaking in the session 'How to make an impact: Public participation stories' at AI UK 2023 - register for your online ticket.
"
https://www.turing.ac.uk/research/research-projects/tackling-health-inequalities,"
Improving population health and reducing inequity is a critical issue that requires systemic interventions. All around the world, declines in life expectancy, widening inequalities in health, food and fuel insecurity, climate change, and restricted government budgets compound to make public health one of the greatest challenges that policymakers must face. As a systemic problem, public health cannot be tackled in isolation from other policy dimensions; hence, a holistic approach across public policy areas towards achieving good health outcomes for all is required. The World Health Organisation is leading a programme to support international governments to place wellbeing at the heart of economic recovery, underpinned by the principle that public investment and spending should be guided by the extent to which a policy, service or organisation can improve population and societal wellbeing.
To advance this initiative, we propose a comprehensive framework based on the Policy Priority Inference methodology to assess the impact of financial interventions on large sets of health and socio-economic indicators across population sub-groups. Our approach accounts for wider contextual drivers of decision-making, the complexity of wellbeing as a multidimensional concept, and the interdependencies between policy domains, while including consideration of equity. This project has the potential to revolutionise the way in which we consider the relationship between the economy and health across sectors and deliver a valuable tool for future health policy in an area of international importance.
"
https://www.turing.ac.uk/people/guest-speakers/simon-thompson,"
Simon is a professor of Health Informatics and the senior technical academic for all data research investments in Swansea University. He is a systems architect with many years’ experience in the NHS and academia. He is the Chief Technology Officer of a large team of talented and diversely skilled software developers, data innovation specialists and enterprise architecture engineers. Simon leads the innovation, design, development and implementation of all SeRP technical system developments. He is responsible for delivering the technical workstream for Swansea University as part of multiple UK networks of health and administrative data research centres of excellence and international collaborations.
"
https://www.turing.ac.uk/research/research-projects/quantifying-technological-sophistication,"
Technological sophistication drives economic growth and diversification. We develop methods to quantify the degree of sophistication of entire industries and to understand how various inputs come together in a production process to generate novel outputs. Our tools build on recent advances in information theory and machine learning, and provide policymakers with insights to make strategic decisions when promoting investments in specific industries.
"
https://www.turing.ac.uk/people/enrichment-students/nouran-khallaf,"
Nouran Khallaf is a PhD student at the University of Leeds. At the Center of Translation studies, funded by a scholarship from the Newton-Mosharafa Fund, British council. She researches Text Simplification methods using tools and theory from natural language processing and machine learning. Nouran. She is particularly interested in natural language processing, data mining, corpus linguistics, and software development. She is also broadly involved in several projects related to corpus collection and annotation, as well as technologies for language learning. Nouran holds an MS in computational linguistics from the University of Alexandria, and a BA in phonetics and linguistics from the same university.
Her project is aimed at a robust Natural Language Processing (NLP) tool to simplify Arabic complex text. Text Simplification (TS) is a Natural Language Processing (NLP) task that aims to reduce the linguistic complexity of the text while maintaining its meaning and original information. The importance of TS involves: (i) designing and simplifying the language curriculum for both second and first language learners, by making texts easy to read for novice second language learners and assisting first language users with cognitive impairments and low literacy language levels; (ii) being a fundamental pre-process in NLP applications such as text retrieval, extraction, summarization, categorization, and translation. The system will also help teachers in inclusion schools as it simplifies the syllabus and aids them in the teaching process for both normal and disabled children without the need for trained teachers to deal with Autistic children. Also, it will help learners of Arabic and a foreign language since this tool will assist them to understand Arabic complex texts leading them to master the Arabic language.
Moreover, this tool would act also as a sub-assisted application to simplify the Arabic text before translating it. In addition, the PhD system would be widely used in schools to simplify the syllabus aiding both students and teachers and enhancing the learning process for normal and language-disabled students.  This leads to the creation of a new generation of language-disabled people who are considered productive and influential persons in society and not a burden on it.
"
https://www.turing.ac.uk/people/researchers/omar-matar,"
Vice-Dean (Education), Faculty of Engineering, Imperial College London and Strategic Leader for the Education theme of the Turing's programme in Data-centric Engineering.
Omar is also Professor of Fluid Dynamics in the Department of Chemical Engineering.
His research interests include multiphase flows and computational fluid dynamics, with applications in oil and gas, manufacturing, and nanotechnology.
"
https://www.turing.ac.uk/research/interest-groups/multi-agent-systems,"
Multi-agent systems (MAS) are a core area of research of contemporary artificial intelligence. A multi-agent system consists of multiple decision-making agents which interact in a shared environment to achieve common or conflicting goals. MAS research spans a range of technical problems, such as how to design MAS to incentivise certain behaviours in agents, how to design algorithms enabling one or more agents to achieve specified goals in a MAS, how information is communicated and propagated among agents, and how norms, conventions and roles may emerge in MAS. A vast array of applications can be addressed using MAS methodologies, including autonomous driving, multi-robot factories, automated trading, commercial games, automated tutoring, etc.
The MAS interest group at The Alan Turing Institute provides a central meeting point for UK-based researchers in universities, industry, and defence who are interested in MAS research. We maintain a UK-wide MAS mailing list through which list members can communicate relevant opportunities and events. We organise regular events (spotlight presentations, UK-MAS symposia) and maintain a virtual MAS labs map shown below.
 
 
Watch interest group organisers Stefano Albrecht and Michael Wooldridge discuss MAS work in more detail in the below videos.


The principle activities of the MAS group are the following:


The group organises a Multi-Agent Systems Talk Series which takes place once a month to give UK-based labs an opportunity to present their MAS research and initiatives. Sign up to our mailing list to receive updates about upcoming talks. If you would like to give a talk, please contact the organisers.
Trustworthy Autonomous Systems
Coordinated Self-Play to Ad-Hoc Teamwork in Bleeding Edge
18 November, 14:00
Rory Greig
Freyr Arinbjarnar
The new Special Issue on Multi-Agent Systems Research in the United Kingdom has now been published in the AI Communications journal. This volume contains 14 contributed articles from UK-based labs, detailing their research in multi-agent systems, developed methods, and open problems.Read the guest editorial, and access the full edition here.
The original call for contributions can be found here.
Is human intelligence inherently social?
Will AI require analogues of human social skills, such as cooperation, coordination, and negotiation?
Should machines use human languages to communicate with each other?
How do norms and languages emerge?
How can an autonomous agent interact safely and reliably with other agents and humans to achieve its goals?
How can research in multi-agent systems effectively translate into industry?
Join the mailing list and members group
"
https://www.turing.ac.uk/people/guest-speakers/anima-anandkumar,"
Anima Anandkumar is a Bren Professor at the California Institute of Technology and Director of ML Research at NVIDIA. She was previously a Principal Scientist at Amazon Web Services. 
Anima has received several honours such as Alfred. P. Sloan Fellowship, the NSF Career Award, the Young Investigator Awards from DoD and Faculty Fellowships from Microsoft, Google, Facebook and Adobe.
She is part of the World Economic Forum's Expert Network and is passionate about designing principled AI algorithms and applying them in interdisciplinary applications. Her research focus is on unsupervised AI, optimisation and tensor methods.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/cardiff-university,"
"
https://www.turing.ac.uk/news/turing-white-paper-outlines-key-issues-be-addressed-achieve-full-potential-digital-twins,"
Important social, technical and institutional issues must be addressed if the full potential of digital twin technologies is to be realised, according to a new white paper published today (Thursday 18 May) by The Alan Turing Institute’s AI for science and government programme (ASG). 
Digital twins are virtual representations of physical objects and could help researchers, industry and policymakers to tackle pressing challenges such as decarbonisation and infectious diseases. 
They can be used to simulate real-life situations and events to see what might happen if we were to make certain adjustments in real life, offering insights and helping people make decisions.  
Digital twins can take the form of a single structure, like that of the world’s first 3D printed steel bridge, a system, like a transportation network, or an ecosystem – a combinations of multiple types of interconnected digital twins, such as a representation of the national health system. 
The authors say that by creating an ecosystem of these digital twins, greater progress can be made towards tackling some of the most important global challenges including future pandemics and climate change. 
They make three key recommendations to address the existing issues:  
Elevate cross-disciplinary activities and spaces for digital twins to create opportunities for sparking new ideas and connections, sharing tools and methods and fostering collaboration.  
Invest in open infrastructure, with a focus on data and technical standards to ensure that research developments are available for use by others working on similar problems in academia, government or industry. 
Prioritise tools for building trust in and understanding of digital twins to anticipate and address potential concerns relating to bias or privacy infringement. 
These recommendations will also help to inform the new Turing Research and Innovation Cluster in digital twins which launched earlier this year and aims to provide open and reproducible computational and social tools for digital twin development. 
Professor Mark Birkin, paper author and Programme Director for Urban Analytics, said: “Building ecosystems of digital twins can provide invaluable insights for complex global challenges. This paper sets out important recommendations to enable the creation of vital digital twins ecosystems so that we can build robust technology that empowers researchers, engineers, and decision makers to understand and tackle large scale challenges.” 
Professor Mark Girolami, Chief Scientist at The Alan Turing Institute, said: “Digital twins can help us to develop crucial capability for understanding and managing complex systems, like transportation and healthcare. With the right investment and collaboration opportunities, creating an ecosystem of digital twins could empower researchers, industry and policymakers to tackle some of the biggest and most complex challenges facing our planet today.” 
 
"
https://www.turing.ac.uk/people/researchers/sylvia-richardson,"
Professor Sylvia Richardson is Director of the MRC Biostatistics Unit and holds the Chair of Biostatistics in the University of Cambridge since 2012. Prior to this, she was Directeur de Recherches at the French National Institute for Medical Research. In 2000, she moved to the UK to take up the Chair of Biostatistics at Imperial College. She was awarded the Guy Medal in Silver from the Royal Statistical Society in 2009. She is a Fellow of the Institute of Mathematical Statistics, of the International Society for Bayesian Analysis and was elected Fellow of the Academy of Medical Sciences in 2016.
She has worked extensively on statistical methodology and applications to the health sciences. Her main interest is centred on modelling and computational aspects of Bayesian statistics applied to complex-highly structured biological and epidemiological data sets. Her recent research has focussed on the analysis of large data problems such as those arising in genomics and on developing methods and scalable algorithms for clustering, sparse regression and large scale hierarchical analysis of high dimensional biomedical and multi-omics data.
"
https://www.turing.ac.uk/people/programme-management/arielle-bennett,"
"
https://www.turing.ac.uk/research/research-projects/extreme-event-prediction-and-monitoring,"
Automated monitoring of large-scale complex-systems, such as cyber and transportation networks, requires characterising and modelling the extreme events that can occur in these networks, such as malware attacks and road closures. Such systems are 'multivariate', i.e. their performance is dependent on multiple interacting variables. Through the availability of new data sets from various sources on these systems, the project aims to characterise extreme events more coherently, and develop new methodology to model them.
Joint modelling of multivariate (multiple variable) extremes across time, space, and networks is crucial for characterising extreme behaviour in complex transportation and cyber systems.
Safety regulations typically require engineers to design complex systems such that they can cope with certain extremes, and such regulations are typically formulated in terms of so called 'return levels'. The 'n-year return level' is the level that is so extreme it is expected to occur only once in every n years. Estimation of return levels in multivariate settings is a crucial task in transportation and cyber systems. The presence of non-stationary data (i.e. where the means and variances of the data change over time) in these applications is another challenge.
The availability of various covariate information (related, independent variables) in the cyber and the transportation applications can be exploited to help better characterise extreme events and dependent variables. This in turn helps to estimate return levels more accurately. Also the availability of large data sets enables the development of data-driven methodology for estimating highly extreme return levels with quantified uncertainty.
The main aim of the project is to develop novel methodology, with theoretical guarantees, that models extreme events arising from large-scale and complex cyber and transportation networks.
The successful application of these methodologies will help in the systematic understanding of extreme behaviours in the complex systems mentioned and will provide improved prediction and uncertainty estimation for spatial and temporal anomalies.
These models and methods have the potential to address the highly challenging task of conducting automatic monitoring and mitigation of high consequence events in cyber and transportation networks, enhancing heir security and performance. 
This project is part of the Data-centric engineering programme's Grand Challenge of 'Monitoring Complex Systems'.
The new methodology developed in this work, along with that being conducted in the other project contributing to the Grand Challenge of 'Monitoring Complex Systems', could benefit cyber security, the transportation industry and could help to mitigate environmental hazards in complex engineering systems.
"
https://www.turing.ac.uk/people/business-team/jane-formston,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-liverpool-0,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/intel,"
"
https://www.turing.ac.uk/people/former-doctoral-students/nicole-peinelt,"
Nicole is a PhD candidate at the Department of Computer Science at the University of Warwick, with a special interest in Natural Language Processing (NLP). She is the organiser of the weekly NLP reading group at The Alan Turing Institute. Prior to undertaking her PhD, Nicole studied Computational Linguistics and Chinese language in Germany, China and Taiwan.
Nicole's current research focuses on deep learning models for natural language understanding problems, such as semantic similarity, paraphrase detection and question answering. During her PhD, she identified common dataset biases, developed more robust evaluation metrics and designed novel deep learning models. She is particularly interested in applications to real-world problems.
Publications:
"
https://www.turing.ac.uk/news/publications/resolving-places-past-and-present-toponym-resolution-historical-british,"
Newspapers and their metadata are richly geographical, not only in their distribution but also their content. Attending to these spatial features is a prerequisite in newspaper research. Following other projects to have geoparsed place names in newspapers, we describe our approach to linking historical geospatial information in text to real-world locations which 1) adopts an expansive definition of what counts as a locatable entity; 2) uses knowledge bases derived from contemporaneous sources; and 3) leverages contextual information to disambiguate hard-to-locate places. This method depends on combining historical and non-historical resources and the paper discusses the potential benefits for humanities research.
Coll Ardanuy, Mariona & McDonough, Katherine & Krause, Amrey & Wilson, Daniel & Hosseini, Kasra & Strien, Daniel. (2019). Resolving places, past and present: toponym resolution in historical british newspapers using multiple resources. GIR '19: Proceedings of the 13th Workshop on Geographic Information Retrieval. 1-6. 10.1145/3371140.3371143. 
"
https://www.turing.ac.uk/research/research-areas/programming-languages/visualisation-programming-languages,"
"
https://www.turing.ac.uk/research/research-areas/social-data-science/data-science-government-politics,"
"
https://www.turing.ac.uk/people/guest-speakers/dr-anthony-j-avery-obe,"
Tony Avery is National Clinical Director for Prescribing, and a GP and Professor of Primary Health Care at the University of Nottingham. He is also a National Institute for Health Research (NIHR) Senior Investigator . He was awarded an OBE in the 2022 New Year Honours list. He is passionate about ensuring the safe, effective and appropriate use of medicines and has worked in partnership with healthcare professionals and patients over 30 years to drive forward research and policy development in prescribing and patient safety. He has led a number of major studies investigating the frequency, nature and causes of prescribing safety problems in the NHS. He has also developed effective methods for tackling hazardous prescribing, most notably the pharmacist-led, IT-based intervention called PINCER, which has now been rolled out nationally to general practices in England.
Tony’s work recognises the vital role that medicines have in treating illness and helping people live with long-term conditions, while acknowledging that prescribing of a medicine is not always the best solution. He is committed to ensuring health care professionals and patients have the information and support they need for shared decision making about whether a prescription is needed and, if so, how to balance the effectiveness and safety of medicines alongside the costs to the patient, the NHS and the environment.
"
https://www.turing.ac.uk/research/research-projects/alan-turing-institute-roche-strategic-partnership,"
The Turing's health and medical sciences programme has long-term interests in personalised healthcare and treatment heterogeneity – the analysis of how medical treatments can affect different people in different ways. The programme is seeking to generate insights which improve the understanding of patient and disease heterogeneity and its relevance to clinical outcomes.
The goal of this partnership is to establish a world-leading collaboration in advanced analytics between Roche and the Turing, focused on enabling the transformative benefits of personalised healthcare to become a reality for patients around the world. Publication of methods and algorithms will follow the principles of open science to ensure that they are reproducible and interoperable. 
Read the news story: The Alan Turing Institute launches a strategic partnership with Roche to generate insights into disease, patient, and outcome heterogeneity using advanced analytics
Learn more about Roche

This expanded strategic partnership was launched following a successful Data Study Group at the Turing in April 2019, and a 12-week research project later the same year, explored in the case study Turing and Roche: Towards tailor-made lung cancer treatment.
More information on the partnership can be found in this short video from our event hosted in July 2021. 

The Turing-Roche partnership is running for five years and will cover multiple activities with the 'North Star' of developing new data science methods to investigate large, complex, clinical and healthcare datasets to better understand how and why patients respond differently to treatment, and how treatment can be improved. Current partnership projects are listed below.
Missing data are a common problem that arise in many fields and can significantly complicate analysis. While there are established ways to handle data that is missing at random, it is often the case that missing readings are structured in some way. Dealing with such structured missingness is substantially more challenging yet occurs commonly in healthcare (and other) contexts. Consequently, there is a need to develop rigorous ways to understand structured missingness and develop tools to handle it appropriately. 
In November 2021 we ran a series of virtual workshops on the topic of structured missingness, with the aim of convening a community of researchers interested in developing new tools and methods, to learn from heterogenous data with structured patterns of missing readings. You can read more about the workshops in the report in the downloads section below. We then ran a competitive funding call in this area to complement the workshops and fund projects in this area. We funded two 18-month projects to Professor Ginestra Bianconi and Dr Robin Mitra and their teams which you can read more about here. These projects will be starting in September 2022 and we will be sharing regular updates from the projects here and also through our Slack Workspace and Knowledge Share events, as well as open publications of methods, algorithms and results.
An blog about our partnership and this area of research was published on Genomics England's site Mind the Gap: Stories of Health Data Equity. The site publishes stories on different ideas, initiatives and perspectives to showcase the challenges of data diversity. You can check out our piece here.
Furthermore, the partnership also has published a perspectives piece on structured missingness in Nature Machine Intelligence. A product of the collaborative workshops mentioned above, the publication explores challenges that address and mitigate effects of structured missingness and where current research could go from here- that as well as creating tools to tackle structured missingness, gaining a more holistic understanding of it could convey important information about the data itself. You can find the publication here.
We are exploring the research theme of predictive modelling in healthcare and how this branch of advanced analytics can potentially predict and help improve patient outcomes and beyond. In October 2022 we ran a collaborative workshop with researchers to explore this theme which you can read about downloads section. We also ran an associated funding call and awarded funding for an 18-month project to Dr Brieuc Lehmann and his team- read more about this here.
We are also hiring two Senior Postdoctoral Research Associates in this area to join the team, working on one of two projects on Conformal Prediction or Geometry of Deep Learning. The job advert has now closed and we will announce the successful candidates in due course.
Our knowledge share series aims to bring together members of both Roche and Turing's networks and the wider scientific community to showcase partnership updates and research, knowledge share and hear different academic and industry perspectives to gain insight and help build new connections and collaborations. Find our monthly events and how to register here.
Sign up to The Alan Turing Institute-Roche strategic partnership mailing list to receive regular updates from the strategic partnership team and community. Mailings include information about Turing-Roche projects and opportunities, jobs, events and funding calls from both The Alan Turing Institute and Roche. If the link above does not work for you (i.e blocked by Roche's firewall) then please sign up via this form.
Request to join our Slack Workspace to connect with the partnership team and the wider community.
If you have any questions about the Turing-Roche partnership please contact [email protected] or Vicky Hellon, Community Manager for the partnership at [email protected].
 
Structured Missingness Workshops Report 2021
Predictive Modelling Workshop Report 2022
The Turing-Roche Partnership Advisors notify the team about advanced analytics challenges and opportunities of interest and work with them on select projects.
"
https://www.turing.ac.uk/people/spotlights/drew-hemment,"

I'm currently working on The New Real, a joint project by Edinburgh Futures Institute, the AI and Arts interest group at the Turing and Edinburgh's Festivals. Through art and artificial intelligence (AI), our project is exploring transformative ideas relating to climate action, how our data is used and how truth is constructed online. We hope to create surprising, joyful and inclusive experiences inspired by the latest thinking in AI.
I believe AI arts can transform the ways we view creativity and cultural expression, and inform new concepts and paradigms for AI. It has the potential to reframe problems and provide contexts for real-world applications that can help fuel innovation in technology and science.
We see great potential for a holistic and experiential approach to explainability in AI, as well as a new research agenda on future intelligent experiences. Artists can help us to look at problems in AI in a holistic way, rather than as solely technical concerns.
The highlight has to be demonstrating the value the arts can bring to technology R&D through FutureEverything, the digital arts festival I founded in 1995 and directed for 25 years. I've been very lucky and able to carry out many incredible projects around the world – such as when I was commissioned by the Singapore Government to conceive and direct the flagship digital culture component of Singapore’s 50th anniversary celebrations.
It would be our Turing 2.0 project, The New Real Observatory, a complete 'experiential AI' system developed with, and for, artists and climate advocates. It combines a data stream and forecasting pipelines for selected climate features, with AI processing engines to manipulate images, words, sounds and numbers using the climate data and forecasts as the exploratory parameters. This enables the artists to expose the operation of a machine learning algorithm, and explore the link between global-scale data and what we call “ground truth” or reality.
This project builds on the recent AWEN: A Walk Encountering Nature project which is available online for people around the world to experience. 
I think art can make better machines. However, I also think that art created with machines can help us better appreciate the limits of humans.
I can usually be found sitting on a beach in Scotland near where I live, or cycling over the hills nearby.
Want to find out more? Visit www.newreal.cc for more on The New Real, and join their session at AI UK.
"
https://www.turing.ac.uk/people/guest-speakers/steven-l-brunton,"
Dr. Steven L. Brunton is an Associate Professor of Mechanical Engineering at the University of Washington. He is also Adjunct Professor of Applied Mathematics and Computer science, and a Data Science Fellow at the eScience Institute. Steve received a B.S. in mathematics from Caltech in 2006 and a Ph.D. in mechanical and aerospace engineering from Princeton in 2012.  His research combines machine learning with dynamical systems to model and control systems in fluid dynamics, biolocomotion, optics, energy systems and manufacturing.  He received the Army and Air Force Young Investigator Program (YIP) awards and the Presidential Early Career Award for Scientists and Engineers (PECASE).
Steve is also passionate about teaching maths to engineers as co-author of three textbooks and through his popular YouTube channel, under the moniker “eigensteve”.
"
https://www.turing.ac.uk/news/filed-under/engaging-with-turing,"
"
https://www.turing.ac.uk/about-us/impact,"
​
The Alan Turing Institute works with academic and industry partners on a diverse range of research projects. Below is a selection of stories demonstrating their far-reaching, real-world impact.
Learn more about the impacts from our major five-year integrated research programme AI for Science and Government (2018-2023), funded by EPSRC.
"
https://www.turing.ac.uk/news/publications/enhancing-domain-specific-supervised-natural-language-intent-classification-top,"
Natural Language Understanding (NLU) systems are essential components in many industry conversational artificial intelligence applications. There are strong incentives to develop a good NLU capability in such systems, both to improve the user experience and in the case of regulated industries for compliance reasons. We report on a series of experiments comparing the effects of optimizing word embeddings versus implementing a multi-classifier ensemble approach and conclude that in our case, only the latter approach leads to significant improvements. The study provides a high-level primer for developing NLU systems in regulated domains, as well as providing a specific baseline accuracy for evaluating NLU systems for financial guidance.
Jenset, G. B. and McGillivray, B. (2019). Enhancing Domain-Specific Supervised Natural Language Intent Classification with a Top-Down Selective Ensemble Model. Machine Learning and Knowledge Extraction. 2019, 1(2), 630-640 https://doi.org/10.3390/make1020037
"
https://www.turing.ac.uk/research/interest-groups/omics-data-generation-and-analysis-group,"
The group aims to bring together Turing Fellows who have a research interest in genomics, proteomic, transcriptomic, metabolomic technologies to share best practice in experimental design and develop new methodologies to interpret these data through the application of machine learning, AI and novel computational and mathematical methods.
Modern omic methods enable the generation of large amount of biological data with ever decreasing cost from a single experiment. However, badly designed experiments can result in worthless data. This interest group aims to ensure that these challenges are resolved before the first experiment has begun.
With the advent of whole-genome sequencing, single-cell sequencing and the advances in mass spectrometry methods, good experimental design and data analysis methods are more important than ever. Our objective is, therefore, to share best practice at the interface between data sciences and experimental biologists to ensure reproducible science and improve research outcomes. This meets the Turing goal to ""advance world-class research and apply it to real-world problems.""
Challenges: Experiments are easily confounded by other variables, and poor experimental design may prevent answering the key research questions.
Example output: Publication of best practice methods.
Challenges: Machine learning is incredible powerful; however, in the biomedical contact the results of model may be complex to interpret and implement the results.
Outcome: Strategies for design and interpreting machine learning strategies for real world impact.
Click here to request sign-up and join
Andrew Holding [email protected]
Ben MacArthur [email protected]
 
Heather Jackson, Imperial College LondonMichal Zulcinski, University of LeedsLaurent Gatto, Université catholique de LouvainSrijit Seal, University of CambridgeVasiliki Rahimzadeh, Stanford UniversityFotios Drenos, Brunel University LondonZhi Yao, LifearcAtif Khan, Newcastle University (WCMR, CDT for Big Data & Newcastle AI Lab)Jack Kelly, University of ManchesterKonstantinos Thalassinos, University College LondonRob Ewing, University of SouthamptonAnas Rana, University of BirminghamBecki Green, King’s College LondonNazanin Kermani, Imperial College LondonNathan Skene, Imperial College LondonSarvesh Nikumbh,  Imperial College London & MRC London Institute of Medical SciencesJean Baptiste Cazier, University of BirminghamBrian Schilder, UK Dementia Research Institute at Imperial College LondonFotios Drenos, Brunel University LondonAndrew Owen, University of BirminghamAladdin Ayesh, De Montfort University, LeicesterJames Timmons, William Harvey Research Institute, Barts & The London School of Medicine & Dentistry, Queen Mary University of London
 
"
https://www.turing.ac.uk/people/researchers/michael-katell,"
Dr Mike Katell is an Ethics Fellow in the Turing’s Public Policy Programme and a Visiting Senior Lecturer at the Digital Environment Research Institute (DERI) at Queen Mary University of London. He is a technology policy scholar and philosopher of technology whose research focuses on the social justice and human rights implications of data and digital infrastructures. Dr Katell is also a co-director of the Critical Platform Studies Group (CritPlat), an independent research collective whose work focuses on surfacing structures of power and domination reflected in technology-related research practices and in the production and allocation of information goods.
"
https://www.turing.ac.uk/people/business-team/alaric-williams,"
Alaric is Programme Manager on the Defence and Security programme, overseeing the activity in partnership with GCHQ, Dstl and JFC.
 
Please get in touch at [email protected] 
"
https://www.turing.ac.uk/taxonomy/term/1301,"
"
https://www.turing.ac.uk/people/researchers/francois-xavier-briol,"
Dr. François-Xavier Briol is a lecturer in statistical science at University College London and a group leader in data-centric engineering at The Alan Turing Institute. He leads a project on the fundamentals of statistical machine learning, which develops novel methodology to enable the use of large-scale engineering models. For more details, you may instead want to have a look at his personal website.
Dr. François-Xavier Briol is interested in developing novel methodology and theory at the interface of computational statistics, machine learning and applied mathematics. Through The Alan Turing Institute, he is collaborating on a number of research projects which are focused on statistical machine learning methodology specifically designed to tackle problems in the field of telecommunications and climate modelling. 
"
https://www.turing.ac.uk/blog/digitisation-data-science-and-humanities-museums-post-pandemic-world,"
The Turing’s humanities and data science interest group reflects on how the humanities and data science have collaborated to offer innovative digital outputs in museums during the pandemic and beyond.
As museums continue to open their doors, we’re all aware that they've had a particularly hard time during the pandemic. However, some have managed to wow us in the last year with new and innovative digital outputs that have provided us with new ways of engaging with their collections. So, how did they do this? The secret ingredient for many was humanities research with a pinch of data science. 
For years, museums, libraries and other heritage organisations (for shorthand, we’ll call them museums in this piece) have been putting their collections online. But in the pre-pandemic world, the high costs of digitisation and associated metadata work was slightly resented. Why did it cost so much to do? Who were the users? Digitisation projects fell out of favour with funders, as they were seen as the bread-and-butter activities of these institutions which should be covered by core funding – and not the kinds of activity that research or philanthropic funders should invest in. 
Museum spaces were closed to visitors, readers, researchers - and curators! The physical access (most of us) took for granted pulled the online reach of these amazing collections into sharp focus. Since March 2020, online museum collections have not just been the preserve of a small cadre of keen researchers, but a home schooling resource for teachers and students. They have also been the sole source of content for museum producers trying to create engaging online content, an archive for a wider range of researchers, and the only access to the objects under their care. 
The institutions who achieved this before the pandemic (and this, of course, is not an exhaustive list) were able to build on their rich stores of digitised materials. This includes Amsterdam’s Rijksmuseum’s Rijksstudio, and Rotterdam’s Het Nieuwe Instituut’s continued work on creative and academic interpretations of the archive. Also, The National Library of Scotland’s Data Foundry already provided a wealth of machine-readable data as well as scholarship opportunities to work with them. 
Digitised collections became the bedrock of museum access, creativity and research in 2020. Collections such as the Metropolitan Museum of Art could be mined and analysed at a distance. The availability of such open datasets has accelerated new and exciting research on using museum collections. The Gaelic Automatic Speech Recognition project, for example, has developed technologies to enable the rapid, semi-supervised transcription of sound archive material in Scottish Gaelic. Working with the School of Scottish Studies Archives and Tobar an Dualchais / Kist o Riches, the project team is enabling new, online dissemination opportunities for these organisations to help mitigate the effects of the Covid lockdown. This will increase digital footfall, enhance their cultural and research value, and facilitate their creative distillation (e.g. via fiction and oral storytelling).
The Turing’s humanities and data science special interest group published a white paper in 2020 (in those pre-pandemic halcyon days), concluding that humanities skills are more valuable than ever in a world that is increasingly digital, digitised and data-rich. The last year has shown us that a bedrock of digitised collections creates opportunities for humanities data to be shared, remixed, researched and enjoyed, well beyond national boundaries. Contrary to the pre-pandemic trajectory, the last year has demonstrated the enormous value of digitisation to create places where humanities and data science meet - for museums, researchers, government, and all of us.
"
https://www.turing.ac.uk/node/7927,"
The relationship between government and media is often intricate, complicated, and highly dependent on institutional context. Emerging large-scale data such as news content and open government spending datasets provides us with new information to understand this relationship. In this project, we combine this type of information with machine learning and econometrics to disentangle the various nuances of how the media influences policymaking and how policymakers influence the media. The project identifies generates novel evidence that sheds new light on this problem, and that can be used to design better regulatory frameworks when aiming to safeguard democracy and free press.
"
https://www.turing.ac.uk/blog/using-machine-learning-improve-reliability-wireless-communication-systems,"
The COVID-19 pandemic has made us all increasingly aware of our reliance on wireless communications via our phones, laptops and other devices. Thanks to our existing infrastructure, students are able to attend school or university online, a major section of the workforce can do its job remotely, and we can connect regularly with our loved ones. That being said, the authors of this blog won’t have been the only ones to have experienced an unstable Wi-Fi connection in the middle of a call! This is not surprising – our wireless systems can often be unreliable.
Why is that the case? Well, wireless systems make use of radio waves (a type of electromagnetic radiation) to carry information between devices, and the way these signals are transmitted can differ widely between settings. This creates challenges for those who are designing these systems.
Take, for example, the Wi-Fi signal in your home: the journey that the signal takes will vary greatly, depending on the location of your router and your device. The signal can only travel from your router to the device by passing through anything that’s blocking its way, and it will also bounce off surfaces that it encounters – and all of this will affect the signal. Of course, Wi-Fi is only one of many ways in which we use wireless signals to communicate. Other examples include our car’s GPS receiving directions from a satellite, talking on the phone in a moving train, or even NASA sending messages from Earth to Mars. In short, there’s a huge variation in the physical environments and obstacles encountered by these signals.
Clearly, accounting for these differences is essential to ensuring we have reliable communication systems. To do so, scientists and engineers are developing ever more complex mathematical models of how wireless signals interact with their environment. These models usually apply to a wide range of distances and environments, but need to be calibrated with data when adapted to individual scenarios. The process of calibrating a model with data can be thought of as turning the knobs on an old radio until you receive the best possible signal. In this analogy, the old radio is our model, and listening to the radio corresponds to collecting data. Our strategy for turning the knobs is the calibration method, and the number of knobs corresponds to the number of parameters in the model.
Calibration has been a significant technical challenge to date because existing methods usually depend on the specific model being calibrated. This means that a new calibration method needs to be developed every time a new model is proposed, which significantly slows down the innovation process. Fortunately, this is where statistics and machine learning can come in handy. In recent work, we proposed an algorithm based on approximate Bayesian computation (ABC) which is able to calibrate models that have vastly different mathematical structures. This was done by combining engineering expertise and machine learning tools to create a method which can automatically and reliably detect subtle differences in wireless signals, rather than directly making use of the mathematical structure of the model being calibrated.
As a result, our new method has the potential to significantly speed up the innovation process for wireless communication systems. We have decoupled the model from its calibration process, thus alleviating the need to develop new calibration methods for each new model. Engineers will therefore be able to more quickly develop accurate models of how signals are transmitted, and hence improve our communication systems.
About the project
This project is part of an ongoing collaboration between researchers at The Alan Turing Institute’s data-centric engineering programme, University College London, Aalborg University and the Finnish Centre for Artificial Intelligence (FCAI). It builds on the work of the fundamentals of statistical machine learning project, and is part of ongoing collaborations between the Turing and the FCAI.
The project was initially planned to take place during a six-month research visit by Ayush Bharti at the Turing in early 2020. Unfortunately, this didn’t work out due to the pandemic, but thankfully everyone’s Wi-Fi was good enough to withstand a long-term virtual collaboration. For the technically-minded reader, all details can be found in our paper, recently published in IEEE Transactions on Antennas and Propagation.
 
Image: ShutterOK / Shutterstock
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/london-first,"
londonfirst.co.uk/
"
https://www.turing.ac.uk/news/publications/data-justice-practice-guide-developers,"
The Advancing Data Justice Research and Practice project aims to broaden understanding of the social, historical, cultural, political, and economic forces that contribute to discrimination and inequity in contemporary ecologies of data collection, governance, and use. This is the consultation draft of a guide for developers and organisations, which are producing, procuring, or using data-intensive technologies. It provides actionable information for those who wish to implement the principles and priorities of data justice in their data practices and within their data innovation ecosystems. 
Leslie, David, Katell, Michael, Aitken, Mhairi, Singh, Jatinder, Briggs, Morgan, Powell, Rosamund, Rincón, Cami, Perini, Antonella, Jayadeva, Smera, & Burr, Christopher. (2022). Data Justice in Practice: A Guide for Developers. The Alan Turing Institute in collaboration with The Global Partnership on AI. https://doi.org/10.5281/zenodo.6428185
This report was commissioned by the International Centre of Expertise in Montréal in collaboration with GPAI's Data Governance Working Group, and produced by the Alan Turing Institute. The research was supported, in part, by a grant from ESRC (ES/T007354/1), Towards Turing 2.0 under the EPSRC Grant EP/W037211/1, and from the public funds that make the Turing's Public Policy Programme possible.
"
https://www.turing.ac.uk/people/external-researchers/chris-harbron,"
"
https://www.turing.ac.uk/news/publications/ethics-machine-learning-childrens-social-care,"
Across the press, academia, and the worlds of policy and practice, concerns abound about the possible impacts of the growing use of machine learning (ML) in children's social care (CSC) on individuals, families, and communities. Many express legitimate worries about how the depersonalising and de-socialising effects of trends toward the automation of CSC are harming the care environment and negatively altering the way frontline workers are able to engage with families and children. Others raise concerns about how these data-driven ML systems are merely reinforcing, if not amplifying, historical patterns of systemic bias and discrimination. Others, still, highlight how the mixed results of existing ML innovations are signalling widespread conditions of poor data quality and questionable data collection and recording practices.
It is against this backdrop that What Works for Children’s Social Care (WWCSC) commissioned The Alan Turing Institute and the Rees Centre, University of Oxford to write this report on the research question “Is it ethical to use machine learning approaches in children’s social care systems and if so, how and under what circumstances?”. The findings we present here take some preliminary steps to providing an answer. We offer a three-tiered framework for thinking about the ethics of ML in CSC, encompassing ethical values, principles, and professional virtues and apply them to the specific circumstances of children's social care in England and technical details of ML.
This research is informed by a range of methods – a literature review, an integrative examination of existing ethical frameworks in social care and ML, a stakeholder roundtable with 31 participants, and a workshop with 10 family members who have lived experience of children’s social care.
David Leslie, Lisa Holmes, Christina Hitrova, Ellie Ott (2020, January). Ethics of machine learning in children's social care [Report]. What Works for Children's Social Care.
733 KB
1.4 MB
"
https://www.turing.ac.uk/about-us/impact/twinning-ways-how-digital-twins-will-change-our-world-better,"
From bridges and buildings to trucks and turbines, every new piece of physical infrastructure has to be analysed to make sure that it is safe, efficient and functional. But the mathematics and measurements that we’ve traditionally relied upon can only tell us so much. Imagine if, instead, each real-world object had a virtual counterpart – a ‘digital twin’ – that could be endlessly prodded and probed to improve the object’s functioning, perhaps before it was even built.
This is the ultimate vision of the burgeoning field of digital twins. At The Alan Turing Institute, researchers with the AI for science and government (ASG) programme are building digital twins of bridges, farms, wind turbines, trucks, ships and more. Far from being mere 3D visualisations on a screen, these advanced models are allowing engineers to monitor the stresses that a railway bridge withstands when a train passes over it, and helping farmers to optimise growing conditions for their crops. Digital twins are offering up data and insights that can change the way their real-world versions operate for the better.
“A lot of different sectors have identified that digital twinning is a potentially disruptive technology,” says Andrew Duncan, Group Leader in the Turing’s data-centric engineering (DCE) programme. But, he adds, the technology isn’t that accessible right now: creating a virtual sidekick requires a lot of know-how, and it’s expensive. The Turing’s approach is to lower the barriers to access through strong foundational research in digital twins, which is now translating into impact across multiple sectors.
So what is a digital twin? At the simplest level, it can be a virtual sibling for a single structure. Engineers routinely use computational models to design bridges based on assumptions about load, strain and so on. But turning a physics-based model of a bridge into a digital twin requires something else: data from the structure itself. A digital twin that Turing researchers are creating for the world’s first 3D printed steel bridge in Amsterdam incorporates sensors that stream live data on load and strain, as well as tilt, vibration and other variables, providing information about how the material is performing in the real world. Gijs van der Velden, CEO of the Dutch company MX3D, which printed the 12-metre-long footbridge layer by layer, says the Turing’s work “will allow us to build better, lightweight bridges in the near future, without compromising on safety”.
The project, which is also supported by the Turing’s partner in DCE, the Lloyd’s Register Foundation, is the result of efforts to formally marry together data- and physics-based approaches in engineering. As the Turing’s Chief Scientist (and former Programme Director for DCE) Mark Girolami explains, it all started with trying to help engineers predict the risks posed by powerful ocean waves called solitons to offshore structures such as wind turbines and natural gas facilities. In engineering, a mathematical modelling approach called the Finite Element Method (FEM) can be used to predict how structures such as these will respond to their external environment. However, the engineers were increasingly getting information about the ocean, the weather and the status of their offshore structures from sensors: information that the FEM approach wasn’t exploiting. So, drawing on his own statistical expertise and teaming up with FEM specialist Fehmi Cirak from the University of Cambridge, Girolami set about upgrading the method to make use of these new data sources. “We basically reinvented it so that it could incorporate data from the real thing,” he says.
The new statistical FEM (‘statFEM’) that they developed is a generalisable framework envisaged as a mathematical basis for digital twins, and Girolami and Cirak quickly put it into practice by developing a digital twin for a railway bridge in Staffordshire, UK. As a train passes over, the bridge’s network of more than 100 sensors provides the digital twin with accurate measurements of the strain felt by the bridge. Meanwhile, the twin’s physics-based modelling fills in gaps in the data by providing predictions for strain in locations where there are no sensors. The twin is already helping Network Rail keep tabs on the structural health of its bridge in real time. Now Cirak and Research Associate Sharana Shivanand, both ASG-funded, plan to adopt the statFEM framework for the Amsterdam bridge. As Cirak notes, it was a “large step” translating the initial foundational work to a bridge. “But now to a 3D printed bridge, it’s a smaller step,” he says, “and we are making significant progress.”
A digital twin can also be a virtual partner for an entire system. In Clapham, London, for instance, growers at an unusual, underground farm are using a digital twin built at the Turing to help them manage conditions as they produce salad greens for local restaurants and shops. The hydroponic farm, run by Zero Carbon Farms as the ‘Growing Underground’ project, occupies over 1,000m2 of tunnels originally designed for use as a World War II air raid shelter.
The farm’s digital twin (dubbed ‘CROP’) stems from before the statFEM work, but it too merges real data with physics. Sensors dotted around the farm bring in information about temperature, humidity, water quality and other variables to automatically calibrate CROP’s physics-based model of conditions in the tunnels. The system, initially developed by ASG-funded researchers Ruchi Choudhary and Rebecca Ward, along with PhD student Melanie Jans-Singh, all from the University of Cambridge, evolved into a fully-fledged digital twin when software engineers within the Turing’s Research Engineering Group (REG) got involved. “When I joined the Turing and I understood the existence of the REG, I saw a huge opportunity not only to make the whole system more efficient but also to create a software architecture that we will continue to be able to adapt,” Choudhary says, noting that she originally explored using commercially available platforms, but they couldn’t incorporate the physics-based model as REG’s bespoke solution does.
The CROP platform works as a remote dashboard, enabling farm managers to assess and optimise conditions on the farm (by adjusting the ventilation or lighting) and track how different conditions affect the volumes of salad they produce. With the aid of CROP’s physics-based model, which predicts conditions three days ahead, growers can also, for example, take proactive steps to stop their salads wilting in a heatwave. “This collaboration has provided us with arguably the most advanced tool possible to manage and improve our operation in terms of yields and efficient resource use,” says Chief Information Officer for Zero Carbon Farms, Jakob Thomas.
The Turing team is thinking bigger than just the one farm, though. Choudhary hopes to turn the open-source code developed by the REG into a software package for growers in other indoor farming spaces. Indoor farms could provide one way to grow more crops on a planet where the food supply is increasingly vulnerable to the effects of environmental degradation and climate change.
What about if we want to better understand a system made up of lots of similar, interacting objects? Digital twins can help there, too. Duncan, Girolami and ASG-funded Research Associate Lawrence Bull are developing ways to create digital twins of whole fleets. Their approach relies on sharing data between machines – this might be valuable, for example, in predicting when a certain machine might fail if it has never failed before, but other machines in the network have. In partnership with vehicle manufacturer Scania, Bull is testing multi-level models for predicting more accurately when components on Scania’s heavy-duty trucks will fail. Groups of similar trucks have their own models at one level, whilst also sharing data with a ‘global’ truck model at another level. “You have all the models depend on this shared global model,” says Bull. “In that way, if the data associated with one of the models is particularly sparse, that model will revert back to the average global population estimate.”
Proof-of-concept work suggests that this approach can more accurately predict when trucks will need to come in for maintenance. The eventual aim is to provide Scania with a means to plan their truck maintenance schedules so they can avoid downtime wherever possible. As Scania Data Scientist Olof Steinert points out, demands on reliability are increasing, especially as autonomous trucks take to the roads. Understanding vehicle health status, he says, “is an area where data-driven methods and machine learning have strong industrial potential.”
Whilst the promise of digital twins is undeniable, practical questions remain. For instance, how do we democratise these complex models so that they’re accessible not just to specialists but to the many different types of organisations that might not have the in-house expertise to develop them.
This is the question that Quaisr, a spin-out from work at the Turing and Imperial College London, is trying to answer. As Co-Founder Indranil Pan explains, Quaisr takes the mathematical equations and data comprising a digital twin and turns them into software that a business can pick up and run with. “We are providing a platform that can take in any of these models, scale them up and operationalise them for multinationals,” he says. According to Duncan, Quaisr “drastically lowers” the barriers to accessing digital twins across different industries. It isn’t limited to models of structures or machines; in fact, the concept emerged from ongoing work with Procter & Gamble on digitising production processes for its bottle-filled goods, to increase efficiency and reduce waste.
Quaisr also collaborated with health tech company Multiwave Technologies on a digital tool to help them develop new materials for the positron emission tomography (PET) scanners used in cancer diagnosis, with the aim of improving image quality. The tool reduces the time and computing power it takes to design materials by automating the complex simulations involved in understanding material behaviour.
Another big question is: how do we trust a virtual sidekick to provide the right information, especially in industries where safety is critical? In the maritime industry, for instance, a digital twin could be a way to remotely monitor a ship’s electrical components, meaning all the data from, say, its radar and navigation systems would be continuously accessible to onshore engineers. This would reduce the need for on-board maintenance and provide access to up-to-the-minute data in the event of an incident at sea. However, for the sake of the crew’s safety, the reliability of such a system would have to be guaranteed.
Maritime communications company Furuno Hellas has already built an on-board voyage data recorder (VDR) – like a black box, but for a ship – capable of supplying this type of data in real time. In an industry-first, Duncan, Research Associate Domenic DiFrancesco and Data-Centric Engineering Group Leader Pranay Seshadri worked with Lloyd’s Register Foundation to independently verify and certify the VDR’s digital twin (marketed as ‘HermAce’) before it is used in practice, conducting an exhaustive list of software and hardware tests on the technology. DiFrancesco notes that this is akin to the verification process any physical engineering system would go through. “I think it’s important to have similar workflows for digital systems as well,” he says.
According to R&D Manager at Furuno Hellas, Nick Stavrou, the company is “so proud” of the outcome of the assessment, which Stavrou says will enable service inspections for vessels to be carried out remotely. Duncan adds that it could lay the groundwork for assuring digital twins for other vehicles, such as self-driving cars.
So, aided by the Turing’s wide-ranging work within the ASG programme, we can start to envision a future where our world’s physical infrastructure benefits from insights supplied by its virtual counterparts. The next phase of the Turing’s work in digital twins will see increased emphasis on translating its research into industrial applications and commercialising the tools we need to use these technologies, all whilst establishing the standards to do so safely. In March 2023, the Turing launches a new research hub – the Turing Research and Innovation Cluster in Digital Twins (TRIC-DT) – with a core aim of democratising digital twin technologies. The TRIC-DT will focus on solving significant societal challenges and generating tangible benefits across environment and sustainability, infrastructure, and health.
 
Header illustration: Jonny Lighthands
"
https://www.turing.ac.uk/events/nature-reviews-physics-machine-learning-condensed-matter-and-materials-physics,"
Machine learning methods are now used in the simulation of the building blocks of matter: from the electronic- to the molecular-level structure. These tools have boosted well-known computational methods such as density functional theory or molecular dynamics simulation, and are expected to lead to new physical insights which, in turn, can enable the engineering of exotic new materials.
Chair - Gábor Csányi 

In this event, we will hear from Professor Eun-Ah Kim and Assistant Professor Michele Ceriotti. 
Professor Eun-Ah Kim - Machine leaning quantum emergence 
Decades of efforts in improving computing power and experimental instrumentation were driven by our desire to better understand the complex problem of quantum emergence. The resulting ""data revolution"" presents new challenges. I will discuss how these challenges can be embraced and turned into opportunities through machine learning. The scientific questions in the field of electronic quantum matter require fundamentally new approaches to data science for two reasons. Firstly, quantum mechanics restricts our access to information and secondly, inference from data should be subject to fundamental laws of physics. Hence machine learning quantum emergence requires collective wisdom of data science and condensed matter physics. I will review rapidly developing efforts by the community in using machine learning to solve problems and gain new insight. I will then present my group’s results on the machine-learning-based analysis of complex experimental data on quantum matter.
Assistant Professor Michele Ceriotti - Atomic-scale simulations of matter with machine learning 
Machine learning has become an indispensable tool in the simulation of matter at the atomic scale, replacing or supporting electronic structure calculations that are in turn increasingly more accurate and predictive. I will present a brief overview of the key concepts that have emerged as the guiding principles in the effective application of ML to this class of problems. Additionally, I will provide several examples of applications that have been made possible by the synergies between data-driven and physics-based modelling. I will summarise what I see as the most pressing challenges and promising research directions in the field.
 


"
https://www.turing.ac.uk/people/researchers/krish-nirantharakumar,"
Krish is the theme lead for health informatics and Professor in Health Data Science and Public Health at the Institute of Applied Health Research, University of Birmingham. Krish’s research interest spans across methodology development in epidemiology to clinical research in diabetes, endocrinology and multimorbidity.
Krish’s unique innovative methodological programme of work, known as “Automated Clinical Epidemiology Studies (ACES)”, has led to the University of Birmingham gaining an international reputation for being one of the few universities globally to have successfully researched and knowledge-engineered epidemiological study designs into computer executable formats. University of Birmingham was the first to implement such a platform in the United Kingdom and publish the methods in a peer-reviewed epidemiology journal. 
The software within the ACES platform, named DExtER (Data Extraction for Epidemiological Research) has resulted in more than 60 peer-reviewed original research publications and has raised over 15 million pounds worth of research funding in the last 3 years. The DExtER tool is now being trialled as a data driven clinical trials platform (RADIANT trial with CEGEDIM funded by NIHR). Krish’s team also emulate clinical trials rapidly, with one such study informing the largest data driven clinical trial to be funded by the National Institute for Health Research (DaRe2THINK), in which Krish is a co-investigator and the team lead for data co-ordination and management. Krish’s team have demonstrated use of the ACES platform to answer clinical questions that arise during patient consultations within hours (Informatics Consult). Following this piece of work Krish (Joint Principal Investigator) and colleagues have obtained a grant from NIHR to develop an AI driven advanced patient similarity tool (OPTIMAL) to improve care for people with multiple long term conditions. 
Building on the success of the ACES platform Krish has established six clinical specialty-specific “Real World Evidence” research groups, focussing on diabetes and endocrinology (led by Krish), gastroenterology, ophthalmology, nephrology, women’s health (led by Krish) and mental health. These research groups have contributed to over 25 publications, many in high impact general medical and specialty journals. The ACES platform will also support the growth of small and medium industries by rapidly generating preliminary health intelligence as part of the recently funded DEMAND hub. As a result of Krish’s expertise in clinical informatics, he was invited to be part of the prestigious methodology panel of the Medical Research Council (MRC).
Krish’s clinical research interest is focussed on diabetes, endocrinology and multimorbidity. He leads the clinical epidemiology group at the centre for endocrinology, diabetes and metabolism (CEDAM). He has published his research in high impact general medical journals such as BMJ and PLOS Medicine and high ranking specialist journals such as Diabetes Care (DC) for diabetes and Journal of Clinical Endocrinology and Metabolism (JCEM) for endocrinology.
He has been an expert panel member to the National Institute for Health and Care Excellence (NICE) guideline committees focussing on diabetes and have been invited through a competitive process to be part of the clinical advisory group for acute care at Diabetes UK in recognition of his outstanding track record on in-patient diabetes research. He is currently leading the health needs assessment for people with diabetes in the West Midlands on behalf of the West Midlands Clinical Research Network. In addition to his interest in diabetes and endocrinology research Krish also has an interest in multimorbidity research. Krish leads two multimorbidity consortiums one focussing on multimorbidity in pregnancy (MuMPreDiCT: Principal Investigator) and another focussing on using artificial intelligence to improve care for people with complex multimorbidity (OPTIMAL: Joint Principal Investigator). He is also a co-investigator of the BIRM-CAM grant, developing innovative clustering methods to research multimorbidity and MurMur, a project to conceptualise multimorbidity and to establish multimorbidity cohorts.
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/special-issue-multi-agent-systems-research-uk,"

The new Special Issue on Multi-Agent Systems Research in the United Kingdom has now been published in the AI Communications journal. This volume contains 14 contributed articles from UK-based labs, detailing their research in multi-agent systems, developed methods, and open problems.Read the guest editorial, and access the full edition here.
 
Multi-agent systems are a core area of research of modern artificial intelligence. A multi-agent system consists of multiple decision-making agents – such as software-based systems, physically-embodied robots, and humans - which interact in a shared environment to achieve certain goals. Multi-agent systems (MAS) research spans a range of technical problems, such as how to design planning and learning algorithms which enable agents to achieve specified goals in MAS; how to design MAS to incentivise certain behaviours in agents; how information is communicated and propagated among agents; and how norms, conventions and roles may emerge in MAS. A vast array of applications can be addressed using multi-agent methodologies, including autonomous driving, multi-robot factories, automated trading, commercial games, automated tutoring, and robotic rescue teams. 
The purpose of this special issue, to be published in AI Communications, is to showcase current multi-agent systems research led by university and industry groups which are based in the UK. We invite all research groups or institutes in the UK that have significant activity in multi-agent systems research to submit an article describing:
There is no fixed article structure as long as the above questions are addressed in sufficient detail.
This special issue is being organised as part of the work of The Alan Turing Institute, the UK's the national institute for data science and artificial intelligence.
Your research group should be located in the UK and have significant activity in multi-agent systems research, e.g., as evidenced by a sustained publication track record in multi-agent systems research in leading conferences/journals. We define ‘group’ broadly, to include groups led by a single PI and institutes, which involve multiple PIs working in multi-agent systems research. Groups may be located at universities or at companies with significant research activity.
Articles should be formatted using the AI Communications LaTeX style. The article title should reflect the research focus of the group. There are no strict page limits, but we suggest no more than 10 pages. Articles can be submitted to the special issue via the submission portal below. At the 'select article type' step, please select 'Special issue: Multi-agent systems' from the drop down menu. Again at the 'section/category' step please also select 'Special issue on Multi-agent systems' from the drop down menu.
Submit article
Submissions must be received no later than 30 April 2022. Each submission will be screened by the editors for eligibility, formatting, and whether (1-3) above are covered. Accepted articles will be published together in a dedicated issue of the journal, and made available online. Authors may also upload their preprint to a repository such as arXiv.
For any questions regarding the special issue or eligibility, please contact the special issue editors.
Stefano V. Albrecht
University of Edinburgh
Michael Wooldridge
University of Oxford
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/neural-networks,"
"
https://www.turing.ac.uk/news/publications/tapas-toolbox-adversarial-privacy-auditing-synthetic-data,"
https://arxiv.org/abs/2211.06550
462.6 KB
"
https://www.turing.ac.uk/people/former-enrichment-students/cian-scannell,"
Cian is a second year PhD student in the biomedical engineering department at King’s College London. He is a member of the Central for Doctoral Training in Medical Imaging and works closely in collaboration with Philips Healthcare. Previously, he studied mathematics at University College Cork with a year abroad at the University of California, Santa Barbara. During his undergraduate degree, he also worked as a research intern at the Tyndall National Institute on the numerical analysis and mathematical modelling of photonic devices. He is currently working on image analysis techniques for cardiac MRI data.
Cardiac perfusion MRI can be used to detect blockages in patient’s coronary arteries. The examination is however not currently in widespread use as the images can be difficult to interpret and hence require extensive training to do so. Cian’s research is focused on developing techniques to automatically analyse these images. 
He is working on Bayesian inference techniques to fit kinetic models to the image data in order to estimate parameters such as blood flow. He is also working on image processing techniques using deep learning including the segmentation of anatomical regions in the images. A future goal of his is the combination of the information from the imaging examination with other patient details and risk factors to accurately predict treatment plans for the patient.
"
https://www.turing.ac.uk/research/research-areas/algorithms/numerical-algorithms,"
"
https://www.turing.ac.uk/people/doctoral-students/victor-alexandru-darvariu,"
Victor-Alexandru Darvariu was a doctoral student at University College London and The Alan Turing Institute. Before starting his graduate studies, he gained his MSci Computer Science with an Industrial Year degree at the University of Birmingham, a year of which he spent working as a Software Engineer in the information security division of Morgan Stanley. Victor previously held Research Assistant posts at both UCL and Birmingham, working on several projects in the data science realm, and interned as a Research Scientist with Spotify in 2021, where he worked on large-scale graph problems. 
 
Many systems can be conceptually thought of as networks: from the complex web of social interactions humans engage in every day to much of the infrastructure that powers our society. Victor aims to develop a framework for reasoning about these systems and to examine applications in areas such as communication, transportation, and social networks. He draws on techniques from the fields of network science, machine learning (with a particular focus on reinforcement learning), and artificial intelligence more broadly. He works under the guidance of Professor Mirco Musolesi and Professor Steve Hailes.
 
"
https://www.turing.ac.uk/research/research-areas/social-data-science/linguistics,"
"
https://www.turing.ac.uk/news/publications/secureabc-secure-antibody-certificates-covid-19,"
COVID-19 has resulted in unprecedented social distancing policies being enforced worldwide. As governments urgently seek to reopen society, there is a demand for technologies that may alleviate the requirement for social distancing whilst also protecting healthcare services and maintaining human and civil rights. In this work we explore the controversial technique of so-called immunity passports and present SecureABC: a decentralised, privacy-preserving system for issuing and verifying antibody certificates. We consider the implications of immunity passport systems, develop a set of general principles and security requirements for their deployment and show that these may be satisfied in practice.
Please note an updated version of this paper is currently under review, so the downloadable PDF has been removed temporarily until this is approved. Please follow 'Link to paper' to see the latest version under review.
Chris Hicks*, David Butler*, Carsten Maple, Jon Crowcroft. SecureABC: Secure AntiBody Certificates for COVID-19. CoRR, abs/2005.11833. 2020.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/safe-and-ethical,"
"
https://www.turing.ac.uk/people/external-researchers/nandini-chitale,"
Nandini Chitale is a Data Scientist at Genentech working on applications of artificial intelligence and natural language processing in clinical development. Her recent work includes the development of various machine learning pipelines for text and tabular data, with previous experience in computer vision and digital health. She is broadly interested in the intersection of healthcare and computing, and holds a Master's degree in Computer Science and a Bachelor's degree in Computational Biology from UCLA.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/pattern-formation,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/national-grid-eso,"
www.nationalgrideso.com
"
https://www.turing.ac.uk/people/researchers/daniel-wilson,"
Daniel Wilson is a historian of science and technology working on the politics and provenance of data and machines in the nineteenth, twentieth and twenty-first centuries. His work combines traditional close-reading and archival study with computational techniques.
Daniel works on the Living with Machines project: a radical collaboration between historians, curators, data scientists and computational linguists exploring the industrial revolution in Britain during the long nineteenth century using digitised sources.
Prior to joining the Turing, Daniel worked on the 'Technology & Democracy' project at CRASSH in Cambridge – an inquiry into the politics of the digital – alongside David Runciman and John Naughton. He continues to teach occasionally in the Department of History and Philosophy of Science at Cambridge.
Daniel's work is in the tradition of STS (Science & Technology Studies) and includes an interest in data provenance, digital archives, new approaches and pipelines for large collections of 19th-century maps and texts, including language models and computer vision. He collaborates daily with research software engineers, curators and librarians.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/evolution-adaptation,"
"
https://www.turing.ac.uk/news/artificial-intelligence-help-predict-arctic-sea-ice-loss,"
A new AI (artificial intelligence) tool is set to enable scientists to more accurately forecast Arctic sea ice conditions months into the future. The improved predictions could underpin new early-warning systems that protect Arctic wildlife and coastal communities from the impacts of sea ice loss. 
Published this week (Thursday 26 August) in the journal Nature Communications, an international team of researchers led by British Antarctic Survey (BAS) and The Alan Turing Institute describe how the AI system, IceNet, addresses the challenge of producing accurate Arctic sea ice forecasts for the season ahead – something that has eluded scientists for decades. 
Sea ice, a vast layer of frozen sea water that appears at the North and South poles, is notoriously difficult to forecast because of its complex relationship with the atmosphere above and ocean below. The sensitivity of sea ice to increasing temperatures has caused the summer Arctic sea ice area to halve over the past four decades, equivalent to the loss of an area around 25 times the size of Great Britain. These accelerating changes have dramatic consequences for our climate, for Arctic ecosystems, and Indigenous and local communities whose livelihoods are tied to the seasonal sea ice cycle.  
IceNet, the AI predictive tool, is almost 95% accurate in predicting whether sea ice will be present two months ahead - better than the leading physics-based model. 
Lead author Tom Andersson, Data Scientist at the BAS AI Lab and funded by The Alan Turing Institute, explains:
“The Arctic is a region on the frontline of climate change and has seen substantial warming over the last 40 years. IceNet has the potential to fill an urgent gap in forecasting sea ice for Arctic sustainability efforts and runs thousands of times faster than traditional methods.”
Dr Scott Hosking, Principal Investigator, Co-leader of the BAS AI Lab and Senior Research Fellow at The Alan Turing Institute, says:
“I’m excited to see how AI is making us rethink how we undertake environmental research. Our new sea ice forecasting framework fuses data from satellite sensors with the output of climate models in ways traditional systems simply couldn't achieve.” 
Unlike conventional forecasting systems that attempt to model the laws of physics directly, the authors designed IceNet based on a concept called deep learning. Through this approach, the model ‘learns’ how sea ice changes from thousands of years of climate simulation data, along with decades of observational data to predict the extent of Arctic sea ice months into the future.
Tom Andersson concludes:
“Now we’ve demonstrated that AI can accurately forecast sea ice, our next goal is to develop a daily version of the model and have it running publicly in real-time, just like weather forecasts. This could operate as an early warning system for risks associated with rapid sea ice loss.”

Video with lead author Tom Andersson discussing the research - all multimedia should be credited to British Antarctic Survey.
Seasonal Arctic sea ice forecasting with probabilistic deep learning by Tom Andersson, Scott Hosking, Maria Pérez-Ortiz, Brooks Paige, Andrew Elliott, Chris Russell, Stephen Law, Dan Jones, Jeremy Wilkinson, Tony Phillips, James Byrne, Steffen Tietsche, Beena Sarojini, Eduardo Blanchard-Wrigglesworth, Yevgeny Aksenov, Rod Downie, and Emily Shuckburgh was published in the journal Nature Communications on Thursday 26 August. Read the paper here: https://www.nature.com/articles/s41467-021-25257-4
About the British Antarctic Survey:
The Arctic is undergoing fundamental change. Long-term temperature records have revealed that the Arctic has warmed faster than other regions of our planet. This ‘Arctic amplification’ of global warming has led to major and quantifiable changes across the region from changing atmospheric circulation patterns and the melting of glaciers and ice sheets, through to the thawing of permafrost and the changing of the physical environment in, on, and above the Arctic Ocean.
The BAS AI Lab is a cross-disciplinary group of scientists and engineers leading in the application of AI and Data Science methods to tackle our greatest polar research challenges. The Lab has two overarching objectives: developing AI algorithms for automating our polar research vessels, our fleet of underwater vehicles, and Antarctic research bases; and the development of machine learning methods for understanding and predicting environmental change.
British Antarctic Survey (BAS), an institute of the Natural Environment Research Council (NERC) and part of UKRI, delivers and enables world-leading interdisciplinary research in the Polar Regions. Its skilled science and support staff based in Cambridge, Antarctica and the Arctic, work together to deliver research that uses the Polar Regions to advance our understanding of Earth as a sustainable planet. Through its extensive logistic capability and know how BAS facilitates access for the British and international science community to the UK polar research operation. Numerous national and international collaborations, combined with an excellent infrastructure help sustain a world leading position for the UK in Antarctic affairs.

"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-omics,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.

This event will be on the theme of omics. Omics is a broad field including things like genomics, proteomics, transcriptomics, metabolomics and more, which allows large scale analysis of biological molecules.

We'll be hearing from Dr Marius Rene Garmhausen, Principal Data Scientist in Personalised Healthcare at Roche and Dr Andrew Holding, Lecturer in Biomedical Science, University of York and former Turing fellow.
Marius will be discussing a recent publication on a large scale computational analysis of over 40,000 cancer patients – identifying 458 mutations that predict survival on targeted therapies. Andrew will be discussing the VULCAN algorithm- which utilises network analysis of genome-wide DNA binding data to predict regulatory interactions of transcription factor and provides new insights into estrogen receptor activation in breast cancer.
The presentations from the speakers and the Q&A session will finish at 16:00 and we will then host an optional networking session 16:00-16:30 for anyone who would like to stay on.
We also hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace.
You can watch a recording of this event.



"
https://www.turing.ac.uk/people/research-fellows/yue-wu,"
Yue works in the Mathematical Institute at the University of Oxford. She obtained her PhD in Stochastic Analysis from Loughborough University after which she took up a lectureship in financial mathematics at Xi'an Jiaotong-Liverpool University. Yue went on to accept postdoctoral positions, first at the Technische Universität Berlin and then at Edinburgh University; her areas of specialism have been stochastic analysis, rough path theory, numerical analysis and uncertainty quantification.
In her Turing role, Yue's focus is on rough path theory and its applications in machine learning and understanding mental health conditions. And more recently, Yue has applied her knowledge to the analysis of Covid-19 data.
"
https://www.turing.ac.uk/research-funding-call,"

"
https://www.turing.ac.uk/people/guest-speakers/stuart-russell,"
Stuart Russell is a Professor of Computer Science at the University of California at Berkeley, holder of the Smith-Zadeh Chair in Engineering, and Director of the Center for Human-Compatible AI.
He is a recipient of the IJCAI Computers and Thought Award and held the Chaire Blaise Pascal in Paris. In 2021 he received the OBE from Her Majesty Queen Elizabeth. He is an Honorary Fellow of Wadham College, Oxford, an Andrew Carnegie Fellow, and a Fellow of the American Association for Artificial Intelligence, the Association for Computing Machinery, and the American Association for the Advancement of Science.
His book ""Artificial Intelligence: A Modern Approach"" (with Peter Norvig) is the standard text in AI, used in 1500 universities in 135 countries. His research covers a wide range of topics in artificial intelligence, with a current emphasis on the long-term future of artificial intelligence and its relation to humanity. He has developed a new global seismic monitoring system for the nuclear-test-ban treaty and is currently working to ban lethal autonomous weapons.
"
https://www.turing.ac.uk/research/interest-groups/neuro-symbolic-ai,"
AI has vast potential, some of which has been realised by developments in deep learning methods. However, it has become clear that these approaches have reached an impasse and that such “sub-symbolic” or “neuro-inspired” techniques only work well for certain classes of problem and are generally opaque to both analysis and understanding. On the other hand, “symbolic” AI techniques, based on rules, logic and reasoning, while not as efficient as “sub-symbolic” approaches, have much better behaviour in terms of transparency, explainability, verifiability and, indeed, trustworthiness. A new direction described as “neuro-symbolic” AI has been suggested, combining the efficiency of “sub-symbolic” AI with the transparency of “symbolic” AI. This combination can potentially provide a new wave of AI tools and systems that are both interpretable and elaboration tolerant and can integrate reasoning and learning in a very general way. 
This approach provides a bridge between low-level, data intensive, perception and high-level, logical, reasoning and promises a future generation of AI tools that are not only efficient but transparent, reliable and trustworthy. This will provide an opportunity to take a step beyond the current orthodoxy of “data-driven” machine learning and to develop a hybrid approach that is much more acceptable to the public (since transparency and explainability are straightforward), to regulators (since verifiability and assurance are both viable within “symbolic” components) and industry (since this approach can help move practical AI and autonomous systems out of their “dead end” towards broader and more sophisticated applicability). Without a step change in the way AI systems are devised, then not only AI tools, but “driverless” cars, domestic robotics, and a range of robots deployed in distant environments will continue to under-deliver. Overall, this step-change will remain unlikely.
Click here to request sign-up and join
Vaishak Belle - Turing Spotlight
[email protected]
"
https://www.turing.ac.uk/people/researchers/shannon-vallor,"
Shannon Vallor is the Baillie Gifford Professor in the Ethics of Data and Artificial Intelligence at the University of Edinburgh, where she also directs the Centre for Technomoral Futures in the Edinburgh Futures Institute.
Professor Vallor's research explores how emerging algorithmic technologies reshape human moral and intellectual character. Her work includes advising academia, government and industry on the ethical design and use of AI, and she is a former Visiting Researcher and AI Ethicist at Google. She is co-Director of the UKRI BRAID (Bridging Responsible AI Divides) programme, with Prof. Ewa Luger. She is the author of Technology and the Virtues: A Philosophical Guide to a Future Worth Wanting (Oxford University Press, 2016) and the forthcoming The AI Mirror (Oxford University Press, 2024). She is the recipient of multiple awards for teaching, scholarship and public engagement, including the 2015 World Technology Award in Ethics.
"
https://www.turing.ac.uk/research/projects/probabilistic-numerics/,"
"
https://www.turing.ac.uk/research/research-programmes/finance-and-economics/sustainable-finance,"
Tackling global environmental challenges requires capital to flow into economic activity that supports sustainability and away from activity that does not. At the same time, environmental change (such as physical climate impacts) and societal responses to these changes (such as regulation and technological change) are affecting the value of assets across sectors of the global economy. The environment is now a material consideration in the performance of investments across sectors, geographies, and asset classes.
Aligning the global financial system with sustainability requires environmental factors to be properly measured and for capital to be efficiently deployed into solutions to environmental challenges.
Enabling technologies, such as distributed ledgers and smart contracts, can enable financial transactions and behaviours that support the transition to sustainability. Data science and AI can help the financial system secure much more accurate, consistent, and timely data to inform decision-making, risk pricing, and capital allocation. These innovations will lead to ultra-transparency that can upend the current imbalances in information that exist between companies and their investors, and between financial institutions and their regulators. This has huge potential to help align the financial system with environmental sustainability. 
This programme challenge will bring together researchers from across several fields of study, together with practitioners, to tackle the following challenges: 
Join the Sustainable finance mailing list.
This regular seminar series is an opportunity for researchers and practitioners in this emerging interdisciplinary field to come together and explore new methods, datasets, and research questions. It is also an opportunity to share updates with colleagues and network with your peers. To find out more, join our mailing list above.
"
https://www.turing.ac.uk/engageturing-students,"

The [email protected] student cohort consists of the 2020 Enrichment scheme offer holders. Following the cancellation of the scheme for 2020/21, a remote engagement package ([email protected]) was organised in order to offer the students a chance to access excellent training in data science and AI, and to form meaningful connections with fellow students, and the wider Turing research network.
As part of a Doctoral Showcase that took place on 19 February 2021, the [email protected] students produced one-slide posters and video presentations, which are available here. 
The Showcase themes included Natural Sciences, Theoretical ML, Health Care & Life Sciences, Fairness & Explainability, Applied ML and Privacy & Security.
 
Adam Ó Conghaile, University of Cambridge

[email protected]
 
 
 
 
 
 
 
 
Alys McAlpine, London School of Hygiene and Tropical Medicine

[email protected]
My research interests sit at the intersection of gender-based violence, migration, and public health using computational social science methods. My PhD explores low-wage labour migration pathways between Myanmar and Thailand to inform safer migration interventions aiming to prevent human trafficking and labour exploitation. This research incorporates complex systems and social network theory using egocentric network analysis, interactive visualisations, and multi-scale agent-based modelling. I am interested in collaborating with others on developing new methods to calibrate and validate ABMs informed by empirical mixed-methods data in data scarce research areas.
Andrea Luppi, University of Cambridge

[email protected]
Coming from a background in philosophy and cognitive science, Andrea’s PhD at the University of Cambridge aims to characterise how the capacity for cognition and consciousness arises from the complex interactions between brain systems. To this end, Andrea’s work combines tools from information theory, network science and whole-brain computational modelling to study information-sharing in the brain across pharmacological and pathological states of consciousness, with the ultimate goal of promoting recovery of consciousness in brain-injured patients.
Andreea Avramescu, University of Manchester

 
[email protected]
Andreea's research interests lie in the fields of personalised medicine, optimisation, and data science, and how all these can be used together to improve the availability of targeted treatments at a global scale. Andreea has an MSc in Data Science and has previously worked on various research problems from within the fields of social sciences, law, computer science, and operations research.
 
Andrew Mitchell, UCL

[email protected]
Andrew Mitchell is a doctoral researcher in urban soundscapes at University College London (UCL). His research interests include soundscape analysis, machine learning, and human perception of complex sounds. His current work focusses on creating computational models for predicting soundscape assessment in urban public spaces, making use of cutting edge acoustics analysis and machine learning methods and applying them to modern smart city design.
Anthony Lindley, University of Southampton and National Oceanography Centre

[email protected]
 
 
 
 
 
Arnaud Dyevre, London School of Economics

[email protected]
Arnaud is a first-year PhD student in economics, with interests in macroeconomics and public economics. He is interested in the formation of production networks—the complex webs of trading relationships between firms—and how their evolution relates to trends in inequality, innovation and market power. To address these questions, he uses large-scale administrative and patent data, as well as models inspired by graph theory. During his time at the Turing, he is interested to collaborate with, and learn from, anyone interested in (1) non-conventional economic data, (2) network formation and (3) partial differential equations or functional analysis.
Bryn Elesedy, University of Oxford

[email protected]
I did my undergrad at Cambridge, focussing on maths and theoretical physics, after which I spent some time working in quantitative finance. I am now doing a DPhil in machine learning at Oxford, supervised by Varun Kanade and Yee Whye Teh. I am broadly interested in theoretical aspects of statistical machine learning, deep learning and reinforcement learning. In the past I have worked on the lottery ticket hypothesis and the implications of symmetry (invariance/equivariance) for learning and generalisation. During the pandemic I have also worked on COVID related projects as part of the Royal Society's DELVE initiative.
 
Carlos Vladimiro Gonzalez Zelaya, Newcastle University

[email protected]
PhD student, part of the CDT in Cloud Computing for Big Data. Did a Bachelors in Maths, an MSc in Combinatorial Game Theory and an MRes in Cloud Computing. His main interests are algorithmic fairness and transparency. Specifically, he's interested in understanding and making use of data preprocessing techniques to detect and correct discriminatory behaviours in classification tasks. So far, he has developed two fairness-correcting methods and he is now working on a third paper on preprocessing pipelines optimisation.
Chaoyi Lu, University College Dublin

[email protected]
Chaoyi is a PhD student from University College Dublin and main focus are Approximate Bayesian Computation, MCMC and relevant area. Current interests are network models including LPM , SBM, etc.
 
 
 
 
 
 
Ekaterina Zossimova, University of Exeter

[email protected]
Ekaterina is pursuing a PhD in Physics with a focus on advanced computational electromagnetism for nanophotonics and biosensing. She uses a combination of classical and ab initio methods to predict how single molecules interact with plasmonic biosensors. She is interested in developing new methodologies based on machine learning to predict the optical response of these systems and is keen to connect with researchers from the Machine Learning for Molecules and Materials group at the Turing institute.
Fergus Imrie, University of Oxford

[email protected]
Fergus is a DPhil (PhD) student at the Department of Statistics, University of Oxford. He is a member of the Oxford Protein Informatics Group (OPIG) and works closely with his industrial partner, Exscientia. His work is focused on the development of novel deep learning methods for applications to drug discovery. His interests span predictive modelling, generative modelling, and understanding human decision-making.
 
Fu Xiang Quah, University of Cambridge

[email protected]
 
 
 
 
 
 
 
 
Hannah Nicholls, Queen Mary University of London

[email protected]
Hannah is researching machine learning applications to better understand the genetics of cardiovascular disease - specifically investigating the prioritisation of cardiovascular disease genes from genome-wide association studies. Her background is in biomedical science with previous experience in experimental electrocardiography research. Hannah’s research interests that she is looking to collaborate on are multi-omic data integration, machine learning for personalised medicine, bias in machine learning for biological applications, and data science approaches for improving experimental data interpretation.
Heather Jackson, Imperial College London

[email protected]
My PhD thesis focuses on using 'omic datasets to understand and diagnose paediatric infectious diseases. I am interested in using bioinformatic approaches to identify novel biomarkers of infection and to explore the heterogeneity in the host response to infection at the genetic, transcriptional and translational level. I use multi-omic integration methods to harness the full potential that these datasets provide. I am interested in all elements of infectious diseases and 'omic data analysis, and would be interested in collaborating with people working in these fields at the Turing.
Hector McKimm, University of Warwick

[email protected]
I am a third year PhD student at the University of Warwick, supervised by Gareth Roberts and Murray Pollock. I am on the Oxford-Warwick Statistics Programme (OxWaSP), a Centre for Doctoral Training (CDT) run by the Universities of Oxford and Warwick. My research is on Monte Carlo methods. I am interested in collaborating on novel algorithms for privacy-preserving statistical inference.
 
 
Ilaria Manco, Queen Mary University of London

[email protected]
Ilaria Manco is a PhD student at the UKRI CDT in Artificial Intelligence and Music. Her primary research area is multimodal deep learning for music informatics, with a focus on developing audio-linguistic models to help machines understand and reason about music. During her time at the Turing, she is interested in collaborating on grounded language models and creative applications of AI. Prior to her PhD, she worked as a data scientist and obtained an MSci in physics from Imperial College London.
Jason Gray, Royal Holloway University

[email protected]
Jason is a PhD cybersecurity student with the Systems & Software Security Lab at the Information Security Group (ISG) at Royal Holloway, University of London and the Systems Security Research Lab at King's College London where his research explores Malware Attribution from a binary perspective. He is supervised by Dr Jorge Blasco, Dr Daniele Sgandurra and Prof. Lorenzo Cavallaro. Jason’s research lies between computation forensics of binaries and linguistic forensics, in particularly the application of machine learning to those fields.
 
James Wilsenach, DPhil Student

[email protected]
James is a neuroinformatics and bioinformatics researcher interested in consciousness and applications of machine learning to understand the mind, improve the lives of others and combat disease. James has particular interest in Brain-Computer Interaction (BCI) with an emphasis on helping those with decreased mobility and disorders of consciousness (DOCs). James' research focuses on eliminating bias caused by long range dependencies in neurobiological data, including neuroimaging, proteomics and microscopy modalities. James is most interested in furthering his understanding of unsupervised, semi-supervised and reinforcement learning methods while at the Turing, as a means to extract signal from noisy spatio-temporal and networks data in the neuroimaging and neuromicroscopy space (where high quality labelled data is rare). James is also interested in learning about AI-applications to BCI and has a background in practical application of neural networks and analysis of complex systems (signals processing and modelling).
James Fulton, University of Edinburgh

[email protected]
I'm interested in the pragmatic application of machine learning to climate physics. Weather and climate poses a lot of interesting problems. And some of the solutions, but certainly not all, could be ML. Previously, I studied unsupervised ML for isolating the natural variability of our climate. More recently, I've been using cycleGANs to make our climate simulations match observations better. I'm also interested in how ML models can be used to replace the nastier parts of weather models.
 
 
Josh Nevin, University of Cambridge

[email protected]
Josh is a PhD student at Cambridge predominantly working on Bayesian approaches to physical layer problems in optical communications networks. Coming from a physics background, Josh is particularly interested in how prior knowledge obtained from physical models can be used to enhance the performance and explainability of machine learning approaches. He is looking for collaborations in application areas in which approximate physical models exist and domains which require a more explainable, probabilistic approach to machine learning.
Joshua Tan, University of Oxford

[email protected]
I am currently a doctoral student in computer science at Oxford studying under Samson Abramsky and Bob Coecke. For my thesis, I’ve been exploring different ways of applying category theory and sheaf theory to machine learning (especially deep generative methods). I also help run a research group called the Metagovernance Project (metagov.org) that does a lot of work in the modeling and specification of complex social systems, especially those that exist online.
 
Junwei Yang, University of Cambridge

[email protected]
He is a second-year PhD student with experience in processing and analysis of biomedical images. His research interests mainly lie in the application of deep learning algorithms to extract and understand the pattern behind biomedical images, with specific focuses on the design of reconstruction and segmentation algorithms to enable fast image acquisition and computer-assisted diagnoses. He looks forward to collaboration on topics related to the extraction of latent information across different modalities of biomedical images.
Kate Highnam, Imperial College London

[email protected]
Kate Highnam harnesses her professional experience in machine learning and cyber security to motivate her current research into domain adaption for intrusion detection with real world applications. Her working projects include a novel concept drift detector to later enhance the robustness of a black box machine learning model in a production environment and a novel cloud honeypot which gathers kernel-level logs. She welcomes further experience in control engineering/learning, optimal transport, Bayesian nonparametric modelling, and all things cybersecurity.
Katharina Zuhlsdorff, University of Cambridge

[email protected]
Katharina is a second year PhD student at the Department of Psychology and DAMTP at the University of Cambridge. She researches the neural basis of reinforcement learning and relates this to disorders such as depression and addiction. She is interested in the translating findings from rodent models to humans, as well as applying deep learning algorithms, such as Graph Neural Networks, to large-scale datasets and relating behaviour with neuroimaging data. If you are interested in collaborating on the application of neural networks to big neuroscience data, feel free to get in touch.
Kimberly Mai, UCL

[email protected]
Kimberly Mai is a PhD candidate at UCL researching representation learning for anomaly detection. Prior to commencing her studies, Kimberly worked in financial crime at a global investment bank and was funded by DeepMind to read a master’s degree in Data Science and Machine Learning. Beyond the scope of her PhD, she is interested in machine learning for security, explainable artificial intelligence, and learning with limited amounts of labelled data.
 
 
Linus Too, University of Southampton

[email protected]
Linus Too is a Theoretical Physics PhD student working on String Theory. His research focuses on obtaining Quantum Information quantity using geometric techniques by the virtue of the Holographic principle. There is a quantity called entanglement entropy which can be represented by the area of a minimal surface and is characterized by the topology of such surface. This led him to become interested in applying topological data analysis on tensor network that models some key features of entanglement in spacetime.
Lorenzo Zanisi, University of Southampton

[email protected]
Astrophysicist on a quest to understand how galaxies evolve in our Universe, with also a keen interest in medical applications of data science. Winner of a STEM for Britain 2019 Silver Award, he has publications in data-driven Monte Carlo modelling of populations, from galaxies to patients undergoing hypertension treatment. He developed an Out of Distribution detection framework to spot unrealistic galaxies in simulations, and he is interested in applications of generative models and graph neural networks in the healthcare sector.
 
Marios Kalomenopoulos, University of Edinburgh

[email protected]
Marios is a third year PhD at the University of Edinburgh, working at the interface between cosmology and gravitational waves. He finished his physics undergraduate degree at the University of Athens, followed by a Masters in Theoretical Physics in Edinburgh. Now he studies how matter in the universe affects the propagation of gravitational waves and how the latter can be utilised to calculate cosmological parameters. For this he exploits data from large, numerical simulations. During his time in Turing he wants to learn more about working with big data and discover about new statistical techniques to analyse gravitational wave signals.
Matthew Nixon, University of Cambridge

[email protected]
Matthew is a PhD student at Cambridge University working on exoplanet atmospheres. He has expertise in several areas of physics, including fluid mechanics and radiative transfer. He has substantial programming experience (Python/C++/Matlab) and has used statistical inference tools (e.g. MCMC) and supervised ML techniques (e.g. Random Forest) in his research. Matthew would love to collaborate with experts in other areas of ML, such as neural networks to develop surrogate atmospheric models, and Gaussian processes to analyse telescope data.
Melike Dila Karatas, University of Exeter

[email protected]
Melike Dila is a doctoral student in Computer Science at the University of Exeter. Her PhD project combines machine learning with nature-inspired algorithms to optimise highly-parametrised predictive models. While the methods concern areas including engineering and applied optimisation problems, her primary application area is gene regulatory networks from computational biology domain. Her research interests span Evolutionary Computation & Optimisation, Machine Learning, Uncertainty Quantification and Data Visualisation. As her research is multi-disciplined, she is interested to collaborate with computer scientists, mathematicians and system biologists at the Turing.
Michael Smith, University of Hertfordshire

[email protected]
Mike is interested in applying deep learning methods (particularly self-supervised, unsupervised, and generative learning methods) to cross-disciplinary problems in astrophysics, earth observation (EO), medical diagnosis and imagery, and other fields. His PhD work concentrates on building deep learning models that can efficiently mine and process very large scale (~petabyte!) astronomical datasets. He is also currently using deep learning to extract useful information from EO imagery data -- there is a surprising amount of overlap between EO and astronomy.
Michelle Seng Ah Lee, University of Cambridge

[email protected]
Michelle is a PhD student at Cambridge Computer Lab in the Compliant and Accountable Systems group, supervised by Jat Singh and Jon Crowcroft. Her research focuses on end-to-end governance of unfair bias in machine learning. Her past publications include: - landscape and gaps of open source fairness toolkits, - ethical trade-offs in mortgage lending, - contrasting “fairness” definitions in computer science/philosophy/welfare economics. She’s looking for collaborators in ML fairness, especially researchers in Bayesian inference or causality.
Mohammad Noorbakhsh, University of Warwick

[email protected]
Mohammad is a third-year PhD student in the MathSys CDT at the University of Warwick. His doctoral research focuses on developing forecasting models to improve long-lead prediction of drought. His research interests are Machine/Deep Learning, Reinforcement learning, Time Series, and Causality applied in real-world problems such as climate and finance. Before starting his PhD, he worked in financial services for several years. He earned three MSc degrees in finance (Cranfield), data science (Royal Holloway), mathematics (Warwick) and a BSc degree in computer science.
Nitin Agrawal, University of Oxford

[email protected]
Nitin is a doctoral student in Computer Science at the University of Oxford supervised by Prof Sir Nigel Shadbolt. He started at Oxford as a Masters student on Commonwealth scholarship after graduating from the B.Tech program in Information Technology & Mathematical Innovations based at Cluster Innovation Centre, University of Delhi. His area of research involves privacy aware machine learning. He works on designing privacy-preserving protocols for training, inference and integrity attestation over Deep Neural Network architectures, primarily employing Secure Multi-Party Computation protocols, Federated learning and Homomorphic Encryption. He also focusses on ethics in AI and human factors associated with adoption, explanations and barriers to privacy preserving computation.
Ondrej Bohdal, University of Edinburgh

[email protected]
Ondrej is currently a second-year PhD student in Data Science at the University of Edinburgh. He works on efficient meta-learning, with applications to problems such as dataset distillation, domain generalization and cross-domain few-shot learning. He is broadly interested in various aspects of machine learning, including the underlying mathematical theory and applications of ML to domains such as computer vision or NLP. He is especially keen to collaborate on projects where meta-learning could unlock new opportunities.
Paul Röttger, University of Oxford

[email protected]
Paul Röttger is a DPhil candidate in Social Data Science at the Oxford Internet Institute. His research focuses on adapting natural language processing methods to address linguistic challenges in online hate speech detection and other applications. Currently, he is most interested in how pre-trained language models could be modified to better account for context dependence and language change.
Prem Gill, University of Cambridge

[email protected]
Prem Gill is a PhD candidate leading the ""Seals from Space: the study of Antarctic pack-ice seals by remote sensing"" priority project with the Scott Polar Research Institute (SPRI), British Antarctic Survey (BAS) and World Wildlife Fund (WWF). For this, Prem explores the use of very high-resolution satellite imagery to study seals and their sea ice habitats. Prem is also interested in increasing opportunities for underrepresented groups to enter non-typical fields (e.g. polar / conservation science) at leading institutes.
Quentin Paletta, University of Cambridge

[email protected]
 
 
 
 
 
 
Rafiah Badat, City, University of London

[email protected]
Rafiah is an NHS Speech and Language Therapist supporting children with language and learning needs. In 2019, Rafiah began an NIHR Clinical Doctoral Research Fellowship; her PhD involves developing a user co-designed digital therapy tool. During the pandemic, Rafiah has supported clinicians with remote working. As part of this, she is the lead author on the Royal College of Speech and Language Therapist’s COVID-19 Telehealth Guidance. She is currently on a part-time secondment at NHSX where she is supporting the digital transformation of health and social care.
Rebecca Green, King's College London

[email protected]
Becki is a PhD student at King’s College London (Institute of Psychology, Psychiatry and Neuroscience), where she is supervised by Dr Petroula Proitsi and Professor Marcus Richards (UCL). Her current work looks at biological markers and risk factors of early dementia, and she is particularly interested in omics data, machine learning, epidemiology, and causal inference. She is keen to collaborate with others at the Turing, and is looking forward to learning from those applying similar methods in different fields. Twitter: @becki_e_green
Sabrina  Li, University of Oxford

[email protected]
Sabrina Li is a DPhil candidate in the School of Geography and the Environment at the University of Oxford. She is interested in investigating the contributions of human-environment interactions on viral infectious disease spread using a combination of GIS, spatial epidemiology, and machine learning modelling techniques. At the Turing, She plans to explore how AI and data science can be integrated to understand complex human population behaviour for predicting the spread of emerging and re-emerging infectious diseases, and how it can be used towards developing a data-driven public health policy framework.
Samuel Bell, University of Cambridge

[email protected]
Sam is a PhD student in Computer Science at the University of Cambridge. His research sits at the intersection of machine learning and the psychology of human learning, and he is broadly interested in whether, and in what circumstances, neural networks learn like humans. He aims to shed light on the complex learning dynamics, and surprisingly effective learning behaviour, of modern neural networks, using methods from psychology and cognitive science. Previously, Sam obtained his master’s in deep learning and natural language processing, also at Cambridge, and his bachelor’s in Computer Science from the University of Manchester. In between, he’s simulated financial crises in market risk at Goldman Sachs, built new retail banks at Thought Machine, and developed next generation credit scores at Credit Kudos. At the Turing, Sam is interested in applying his research to the domains of machine learning fairness and interpretability.
Sophie Parslow, Loughborough University

[email protected]
Sophie’s research uses the method of conversation analysis, to examine how primary care receptionists communicate with patients, and how they make decisions about the services offered. As part of her project, Sophie focuses on the tacit categorisation of callers to healthcare services, asking: “Can AI be trained to identify, analyse and respond to implicit vocal cues in order to progress requests for primary care services?”.
Syu-Ning Johnn, University of Edinburgh

[email protected]
Syu-Ning (Shunee) is currently a second-year PhD student from the Optimization and Operational Research research group at the University of Edinburgh. She is interested in mathematically modelling and formulating real-life supply chain problems using Combinatorial Optimisation techniques. Her research currently involves vehicle routing, facility location and other general network design problems in food logistics, taking customer uncertainty, driver workload balance and reliability issues into account. She looks forward to bringing Machine Learning into her model to deal with model deterioration, improve computational speed and analyse customer behaviours.
Teddy Cunningham, University of Warwick

[email protected]
Teddy is a third-year PhD researcher at the University of Warwick. His primary research is focused on incorporating real-world knowledge into differentially private data release and synthesis, with a particular focus on the location domain. His other research involves quantifying and improving access to key urban services (e.g. schools, job centres, vaccination centres) via the public transport network. He is interested in collaborating with any Turing academic or student that has an interest in extending the work in these two important research areas.
Thomas Statham, University of Bristol

[email protected]
 
 
Tiffany Vlaar, University of Edinburgh

[email protected]
Tiffany has a Physics BSc (Leiden University), Geophysics MSc (joint from TU Delft, ETH Zurich, RWTH Aachen), and a Theoretical Physics MSc (Perimeter Institute). For her Applied Mathematics PhD at the University of Edinburgh (joint with Heriot-Watt) she uses techniques from molecular dynamics and stochastic differential equations to develop new optimisation schemes for the training of deep neural networks. She's eager to talk about how to improve ML research in general and data applications that are not CIFAR/ImageNet.
Umang Bhatt, University of Cambridge

[email protected]
Umang Bhatt is a PhD candidate in the Machine Learning Group at the University of Cambridge. His research interests lie within the transparency, fairness, and robustness of machine learning systems. He aims to build symbiotic human-AI teams, wherein an ML system provides domain experts with transparency into the system's reasoning. He is a Fellow at the Mozilla Foundation and was a Research Fellow at the Partnership on AI. Before moving to the UK, he received his BS and MS from Carnegie Mellon University.
William Finnegan, University of Oxford

 
[email protected]
Bill is a social scientist whose doctoral research spans human geography and environmental education. He is exploring how secondary schools in the UK are responding to the climate crisis, the social practices of energy consumption in schools, and youth perspectives on climate futures. Bill is interested in how data feedback and visualisation of energy consumption and climate impacts can enable more sustainable schools and improve climate literacy in the classroom.
 
"
https://www.turing.ac.uk/people/guest-speakers/kevin-wu,"
Kevin Wu is a PhD student at Stanford University advised by James Zou. His primary research areas include clinical trials and regulation for AI, as well as methods to improve the fair and safe deployment of AI across hospitals. Prior to his time at Stanford, he worked for a startup focused on AI for medical imaging, where he helped develop and deploy algorithms for clinics.
"
https://www.turing.ac.uk/events/leveraging-ai-quantify-biodiversity-and-ecosystem-change,"
Earth’s biosphere is undergoing rapid and severe change across biomes, taxa and ecosystems. Beyond it’s inherent value, this biodiversity underpins the functioning of the Earth System, and in turn the conditions required for human existence. Therefore, quantifying and understanding this change represents a major research challenge, that the ecological community urgently needs to meet. Traditional ecological approaches have been limited in terms of spatial, temporal and taxonomic scales. These limitations have led to deep uncertainties about how complex ecological communities are responding to global change. However, with the advent of new technologies, large synthetic datasets, and novel data driven AI approaches, our ability to quantify ecological change across scales is being transformed. Importantly, successfully leveraging these new technologies and AI approaches will increasingly require forging new cross-discipline collaborations (e.g. among ecologists, computer scientists, data scientists and technologists).
In this workshop, we will bring together researchers from a range of backgrounds (ecology, computer science, data science etc) to share perspectives on how AI approaches can be leveraged to monitor biodiversity and ecosystem change.  By gathering cross disciplinary researchers from across the UK, we hope to contribute towards develop a new collaborative network to better integrate AI approaches to important questions related to biodiversity and ecosystem change.
The workshop will consist of a mix of keynote talks from leading experts, short form lightening talks from early career researchers (including PhD students and postdoctoral researchers) and ecology researchers that hope to explore a ecological problem by leveraging AI and data science but may not currently have the skills or collaborative relationships to do so  
Register now
"
https://www.turing.ac.uk/blog/technical-roadmap-uks-contract-tracing-app-functionality,"
For a contact tracing system (automated or manual) to be trusted and effective, it will have to identify those who are actually at risk of contracting the virus and inform them of the need to self-isolate. If the contact tracing system is unable to accurately identify those who need to self-isolate, for example by notifying people who are not at risk (known as a ‘false positive’) or by not identifying people at increased risk (known as a ‘false negative’), the consequences may be severe.
Self-isolation may negatively impact people’s lives, by compromising their ability to go to work and feed their family, or affecting people’s mental health when they’re told that they’re at risk of having the virus. On the other hand, not notifying individuals who carry the virus can increase the reproduction number, resulting in potential outbreaks, unnecessary deaths and, as a result, restrictive policy decisions (such as further lockdown measures) being enforced on large parts of society, further impacting the economy and societal fabric.
It is therefore important to have an app that is able to appropriately characterise risk; that is, individuals who are actually at risk of contracting and spreading the virus are notified to self-isolate.
The app determines whether a specific individual is at risk of contracting the virus by assessing its interaction with other, potentially infectious individuals. A risk-scoring approach is used to decide the level of risk by calculating the distance between the devices, the duration of the contact, and the infectiousness of the individuals encountered.[1] Whilst the time and date of the encounter is accurately captured by the app (provided that the phone’s clock is appropriately synchronised), the distance and duration of the encounter is estimated from the phones’ Bluetooth connectivity and signal strength.
The reliance on Bluetooth is an issue, as it was never designed to estimate distance in the first place. The Bluetooth signal strength is subject to a significant amount of interference, such as the Bluetooth signal bouncing off objects in the phones’ environment (also known as ‘multi-path’) or suffering from attenuation (e.g. by a phone being placed in a pocket behind a wallet).[2] Similarly, assessing the duration of contacts made based on Bluetooth signal strength is subject to errors in cases where the signal strength is low or the sampling rate is low. For instance, if we receive two Bluetooth signals between devices, 5 minutes apart, we do not know if two people have been sitting close to each other for that length of time, or just happened to be passing by at both points when the connections occurred.
This is a problem faced by all contact tracing apps being developed around the world regardless of the software architecture being used, using Bluetooth as a proxy for distance and duration. We cannot change the factors associated with Bluetooth signals, but we can statistically model the impact of these errors by allocating a degree of uncertainty associated with the signal strength and use this knowledge to inform the risk-score attached to individuals’ encounters.
Experts from The Alan Turing Institute, the Department for Health and Social Care and the National Cyber Security Centre are undertaking work to characterise and improve the performance of the app.
Using version 1.4 of the Google / Apple Exposure Notification (GAEN) Application Programming Interface (API), we have experimental and modelling evidence to suggest that the current app has an area under the curve (AUC) of 0.68, a detection performance >99% (i.e. the phones almost always detect each other when within reasonable Bluetooth range), and at the risk threshold selected for England's public trial of the contact tracing app starting this week, a true positive rate (TPR) of 69% and a false positive rate (FPR) of 45%. By way of a simple illustration, during the recent Leicester outbreak, the app would have generated ~50 false positives a day in a population of 330,000. These false positives would be individuals who had been in contact with an infected individual for greater than 15 mins between 2m and 4m apart.[3]
Critically, the TPR and FPR values correspond to the problem of determining classification performance at 2m for 15 mins – for an interaction with an individual that has tested positive for Covid-19. In other words, if you have really been sufficiently close to an individual with Covid-19 for sufficiently long, and so are at risk, you will be notified. Conversely, if you’ve not been near to an app user that has tested positive, then you will not be notified. The false positive rate is primarily associated with deciding whether a potentially affected user is within, or just over, the 2m threshold in terms of their interaction with a positive app user.[4] However, in these ambiguous cases, we can do better.
In order to achieve an improvement, we have developed four new ideas:[5]
Modelling the impact of these ideas, in particular the introduction of item 3, has demonstrated that we can achieve an AUC of >0.8 (generally considered “excellent” by, say, the international machine learning research community). We will continue to work with Apple and Google to obtain the best possible app using some of the ideas above.
Finally, as with all scientific endeavours, we need to continue to test our hypotheses, revise our assumptions in light of new evidence, listen to, and continue to collaborate with colleagues from around the world so that we can help to build a universally valuable digital contact tracing technical approach. This is a global challenge that will be solved through collaboration and open science, so we rise to the challenges of a virus that knows no borders.
[1] For a detailed description of the risk-scoring approach, including other factors not listed here, see the (un-reviewed) paper on Risk scoring calculation for the current NHSx contact tracing app.
[2] For a detailed description of the issues surrounding Bluetooth detection, see Coronavirus Contact Tracing: Evaluating The Potential Of Using Bluetooth Received Signal Strength For Proximity Detection.
[3] Figures are for illustration purposes only and rely upon a set of simplifying assumptions for modelling, which are not detailed here.
[4] There is also a link between false positives and low-quality duration estimation, which is not detailed here for the sake of brevity.
[5] The novelty is in the combination of ideas that have been around for many years - all the way back to the moon landing in 1969!
[6] The beginning of this work can be found in the (un-reviewed) paper on Risk scoring calculation for the current NHSx contact tracing app.
[7] MIT dataset used is available here: https://github.com/mitll.
[8] For further details on Unscented Kalman Smoothing algorithm, see the paper on Smoothing algorithms for state–space models and a write-up of our most recent work.
[9] For further details on the use of existence probabilities, see Bayesian visual tracking with existence process.
"
https://www.turing.ac.uk/blog/project-explain-enters-its-next-phase,"
‘Explaining decisions made with AI’, a guidance on AI explainability co-produced by The Alan Turing Institute and the Information Commissioner’s Office (ICO), was published on the ICO website in late May 2020. This publication marked the culmination of nearly two years of collaborative effort, which included extensive desk-based research, two citizens’ juries, and several rounds of multi-stakeholder consultation. The result of all this was the most comprehensive practical guide on AI explanation produced anywhere to date. The guidance has been cited many dozens of times in academic papers and by international law firms and AI policy-related blogs, newsletters and websites.
Since the release of ‘Explaining decisions made with AI’, the Turing and the ICO have been working together to undertake further public engagement to help shed light on how organisations are using it in practice, and on how the guidance can be made even more user-friendly and accessible. Phase two of Project ExplAIn has consisted so far of the creation of workbooks to help accompany the guidance, a series of workshops, and an informational video produced by Fable Studios (below) to help communicate the guidance’s key points to a wider audience.

At the beginning of 2021, our project team assembled two workbooks to help support the uptake of the guidance. The goal of the workbooks is to summarise the main themes from ‘Explaining decisions made with AI’ in a non-technical way. Additionally, each workbook has served as the basis of a workshop exercise built around one of two use cases, created to help organisations and individuals gain a flavour of how to put the guidance into practice.
The workbooks have been written to support our second phase of Project ExplAIn, centred on stakeholder outreach and practice-based evaluation. This has included a series of engagement activities held in January 2021 to assess the usability, accessibility and clarity of the guidance, as well as the readiness levels of organisations to put explainable AI principles into practice. In partnership with Manchester Metropolitan University (MMU) and the ICO, two workshops – one with SMEs from advertising, AI development, finance, recruitment, health, education, fraud protection, media and insurance sectors, and a second with public sector organisations – were held virtually. The workshops engaged participants from a variety of different backgrounds, levels of seniority, and roles across the public and private sectors. We are extremely grateful to them for their energy, enthusiasm and tremendous insight.
The workshops were held via Zoom and employed an e-board software solution, vWall, that allowed participants to write comments, ask questions, and engage with the material. The workshops were structured around two hypothetical case studies: one involving an AI-assisted recruitment tool and one based on a machine learning application in children’s social care. These were written to allow participants to apply aspects of the guidance to a specific scenario and to provide real-world examples of how the guidance could be applied. Before each session, the workbooks were sent out to participants with a series of reflection questions to help prepare them for the workshop.
We hope that our workbooks will allow for more widespread use and dissemination of the guidance. The workbooks begin with a truncated form of the ‘Explaining decisions made with AI’ guidance, presenting the four principles of AI explainability, the basics of an explanation-aware approach to AI innovation, and the practical tasks needed for the explanation-aware design, development and use of AI systems. They then provide some reflection questions, which are intended to be a launching pad for group discussion. The appendices of the workbooks are primarily focused on both the workshop setting and the case studies. Appendix A provides a structure for how to use the workbook in a workshop setting, including details on necessary resources, personnel, and recommended timelines. These recommendations are based on the workshops co-hosted with ICO and MMU in January 2021. Appendix B contains the case study, followed by appendix C which consists of a checklist for one or more of the explanation types to be used in tandem with the case study.
Case studies found in the workbooks:
These workbooks would simply not exist without the commitment and keenness of all our collaborators and workshop participants, and we would like to thank them again for their involvement.
Next in this phase of Project ExplAIn, we plan to build more engagements with SMEs and public sector organisations, and focus on the use of the guidance in practice as well as updating it in keeping with the insights drawn from these valuable engagements.
View the full guidance
We are grateful to EPSRC grants EP/T001569/1 and EP/W006022/1 for making this research possible.
 
Top image: OlegRi / Shutterstock
"
https://www.turing.ac.uk/people/enrichment-students/berker-banar,"
Berker is a PhD student at the UKRI Centre for Doctoral Training in AI and Music at Queen Mary University of London, supervised by Simon Colton. His PhD project is about composing contemporary classical music using generative deep learning, which focuses on developing and analysing deep learning techniques including transformers and self-supervised representation learning for symbolic and audio domain music generation. 
At the Turing, Berker is interested in conducting a quality and diversity-based analysis of deep generative music models encouraging out-of-distribution generation in conjunction with explainable AI techniques.
"
https://www.turing.ac.uk/people/researchers/ed-chalstrey,"
Ed is a Research Data Scientist in The Alan Turing Institute’s Research Engineering Group (REG), with a background in Bioinformatics. In the REG, he works on various academic projects providing support for collaborators via his skills in research software engineering and data science.
"
https://www.turing.ac.uk/research/research-projects/pilot-authority-financial-markets,"
This project is a partnership between The Alan Turing Institute and the Dutch Authority for the Financial Markets (AFM) and aims to improve the stability and integrity of financial markets using modern data science tools.
Generally, a simulation environment is needed to create agent-identifiable data which is lost in the flow of real-world orders. This project provides a unique opportunity to work with a real-world agent dataset, which in combination with modern technologies opens a range of new possible research questions.
Using this dataset, we will build models of individual-level trading activity, to form the basis of a market simulator. This simulator will then be used to understand the risks to market stability of changing trading patterns, to provide a testing environment for market participants, and to inform regulatory activity in financial markets.
From a research perspective, this is an exceptional opportunity to work with high-sensitivity data from financial markets, which allows us to go beyond the usual agent-based calibration patterns. There are challenges to be addressed in how to build models which preserve sufficient privacy of agents’ actions, which accurately reproduce market phenomena at both the individual trade and aggregate levels, and which can be simulated sufficiently quickly to be practically relevant for regulation.
The project aims to:

 
There are many potential research questions that such a framework could be applied to: What is the effect of short-sellers on a market? Can a machine-learning algorithm (unintentionally) learn to manipulate the markets? What happens in the market when a large market maker leaves? How does the market react to agents becoming more “risk-averse”? In what scenarios can we expect to see abnormal behaviour by market participants? This project connects closely with existing work at The Turing on time series modelling, simulation, anomaly detection and privacy preservation.
Monica Vakil-Dewar, [email protected]
"
https://www.turing.ac.uk/people/enrichment-students/elisa-rauseo,"
Elisa Rauseo is a PhD student in Artificial Intelligence and Cardiovascular Medicine at the William Harvey Research Institute, Queen Mary University of London, and an Honorary Clinical Fellow in Advanced Cardiovascular Imaging at Barts Heart Centre, Barts Health NHS Trust.
Coming from a clinical background and with skills in data science, she works at the interface of artificial intelligence and healthcare, bringing deep clinical knowledge and machine learning algorithms together to advance cardiovascular disease research.
Her doctoral project aims to use machine learning methods combining clinical data and established and novel cardiac imaging metrics to improve the characterisation and outcomes prediction of heterogeneous cardiovascular conditions. The ultimate goal of her research is to identify distinct risk profiles for disease progression to guide clinical decision-making, generate precision medicine solutions customised to individual characteristics, and improve clinical outcomes.
"
https://www.turing.ac.uk/people/spotlights/taha-yasseri,"

I am interested in understanding the social behaviour of humans. My approach is different to that used in more established social sciences that rely on self-reported data such as those produced by surveys or interviews.
In my work, we take advantage of the digital transformation of society which generates large-scale transactional data about almost every aspect of our digital lives today. This is the core material of my work. In other words, instead of studying what people say they have done, or they would do under certain conditions, I study what people 'are doing'. To cope with the complexity and noise in transactional data (in contrast to clean and 'tamed' surveys), I use data science techniques, even though I’m not actually a data scientist myself!
I have been lucky enough to establish collaborations with the public and private sector. One of the most exciting projects I’ve worked on was a collaboration with the British Museum (which also allowed me access to some rooms in the Museum that not everyone can go to!). In that project, I learnt so much through talking and working with the curators there - I’d also like to think that our work help to support their job and future projects, including how they are planning to re-open post lockdown.
I have spent many years studying edit wars and conflicts among Wikipedia editors and other crowd-sourcing platforms where people of differing opinions need to work together. Studying Wikipedia editors spend hours and days arguing over controversial and sometimes even not conventionally controversial topics was a great fun! But I was very surprised when in a recent project we found similar patterns of conflict among Wikipedia “bots” that are supposed to help human editors. We did not expect these bots, that are emotion-less pieces of Python code, to fight! They all have the same goal and have very little agency. However, we saw again that the emergent system behaviour can be very different from what you predict based on the behaviour of an isolated single entity, and even “good bots fight”, when they are controlled by humans!
Social Contagion, Online Dating, Collective Human-Machine Intelligence
I don’t know about everyone, but I would have been a different person if I hadn’t come across “Duncan Watts (2011). Everything is obvious: Why common sense is nonsense. Atlantic Books.”
You should have started by asking me about my hobbies (it’s always a good ice breaker!), to which I would have answered: Tango, Comedy, Football, and daydreaming about imaginary interviews and the questions that interviewers never ask!
"
https://www.turing.ac.uk/contact-us/our-brand,"

Our brand helps to define all aspects of The Alan Turing Institute and how we communicate: how we talk, who we talk to, how we operate, who we partner with, how we formulate policies and how we work together.
Individuals and organisations should not use the Institute’s intellectual property rights without its permission.
The Turing logo is for editorial use only and remains the copyright of The Alan Turing Institute. It should be used to indicate formal collaborations, sponsorship agreements, or established partnerships only.
Before using our logo, you must seek permission by emailing [email protected] with a description of how you would like to use it. Any permitted use must then be checked and signed off by our team.
The Alan Turing Institute brings great minds together to make leaps in data science research which will change the way we live for the better. Our brand – our logo, designs, words, tone of voice – showcases what we stand for and underpins our work.

Our full name is The Alan Turing Institute (with capital letter on the The) and should be used as the first reference to us in the copy. After that, depending on whether the tone is formal or informal, later references can be to 'the Institute' (formal) or 'the Turing' (more informal), or used alternately throughout if appropriate. Specifically, with use of 'the Institute', it should first be specified that the reference is to The Alan Turing Institute.
 
To receive an approved version of the Turing logo, please contact [email protected] to seek appropriate permissions and guidance on how to use the logo.
 


For any questions about our brand and for permission to use the Turing logo, please email [email protected].
Email us
"
https://www.turing.ac.uk/blog/what-dark-web-can-tell-us-about-future-social-networks,"
This week, Donald Trump leaves his post as US President to make way for Joe Biden. The inauguration comes just two weeks after the storming of the US Capitol, after which Twitter announced that it had permanently suspended Donald Trump “due to the risk of further incitement of violence”. Facebook, Instagram, Snapchat and Twitch similarly blocked Trump from posting new content.
Trump’s social media ban was followed by a rush in new users to Parler – a social networking site that markets itself as an unbiased, free speech alternative to mainstream platforms. However, accusations that Parler had been used to orchestrate the Capitol storming led to the platform being removed from the Apple and Google app stores, and from Amazon Web Services, forcing Parler offline.
Soon after, other alternative social media apps such as MeWe and CloudHub rapidly rose up the app store rankings. The big question now is whether users will gradually go back to mainstream platforms like Facebook and Twitter, or disperse into a myriad of smaller, less regulated services. I think that the ‘dark web’ can provide some useful insights.
The dark web is a hidden part of the Internet that’s only accessible through specialised, anonymising web browsers. Illegal activity is rife on the dark web, such as in dark ‘marketplaces’, where users trade goods, mainly using Bitcoin. Silk Road, regarded as the first dark marketplace, launched in 2011 and mostly sold drugs. Shut down by the FBI in 2013, it was followed by tens of dark marketplaces which also traded in weapons, fake IDs and stolen credit cards.
In research carried out with colleagues in the UK, US and Denmark, part-funded by The Alan Turing Institute’s ‘Economic data science’ programme, we looked at what happens after a dark marketplace is shut down by a police raid or an exit scam (where the moderators of a marketplace suddenly close the website and disappear with the users’ funds). We focused on ‘migrating’ users, who move their trading activity to a different marketplace after a closure.
We found that most users (on average, 66%) moved their activity to a single marketplace (typically the one with the highest amount of trading). User migration happened within a few days, possibly coordinated via a discussion forum such as Reddit or Dread, and the overall amount of trading across the marketplaces quickly recovered. So although individual marketplaces can be fragile, with participants regularly making large losses due to scams and shutdowns (just last week, a huge marketplace called DarkMarket was taken offline in an international operation involving the UK’s National Crime Agency), this coordinated user migration guarantees the marketplaces’ overall resilience, so that new ones continue to flourish.
This has striking parallels with what’s currently happening on social media.
Just as Silk Road was pivotal in sparking illicit online trade, Facebook and Twitter were the seeds of social networking. They enabled faraway individuals with shared interests to meet and start a conversation. Their centralised nature was a key feature. Users were able to discover communities that they did not know they belonged to: crowds of like-minded souls available to communicate with 24/7.
But we are now witnessing the evolution of a new, more fragmented social media ecosystem. When one platform – however big – experiences censorship or a shutdown, users are quick to coordinate their migration to another platform.
In the dark web, after a closure, users tend to migrate to the same place. The same is true of the social web, with user migration driven by like-mindedness on specific themes, chiefly politics. We are familiar with echo chambers developing within one platform, where users are presented with content and opinions that agree with and reinforce their own belief systems. Soon, we might see these echo chambers each developing their own, individual platform.
What does all this mean for social giants such as Facebook and Twitter? Our research has shown that dark web communities have a remarkable resilience, even when threatened with frequent shutdowns. If the same is true of the emerging constellation of alternative social networks, then we might see a shift in power away from the bigger platforms as they lose users permanently to the smaller ones. The increasing number of platforms, and the ability of users to coordinate mass migrations, will make it harder than ever for regulators to keep track of the content being posted online. What this will mean for the influence of controversial users such as Trump remains to be seen.
 
Reference: ElBahrawy, A., Alessandretti, L., Rusnac, L. et al. (2020). Collective dynamics of dark web marketplaces. Scientific Reports 10, 18827.
Top image: Palash Jain / Unsplash
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/mx3d,"
MX3D website
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/computing-networks,"
"
https://www.turing.ac.uk/events/data-study-group-september-2023,"
Toxin diagnosis by cellular morphology
Diagnosing toxin exposure is not a trivial task, unlike viruses and bacteria that both contain DNA and can be cultured both of which can be used to identify their presence after exposure, toxins do not exhibit either of these properties.
The Data Study Group will focus on a proof of concept challenge, utilising data from a published manuscript. We would like to see if it is possible to utilise the morphology of the cell to identify if it has been exposed to toxin. Cell exposure data collected by imaging flow cytometry over a 48 hour time course can be used in this challenge, with ground truth populations available for both toxin exposed vs unexposed cells. This data is an excellent resource for a proof of concept tool to diagnose toxin exposure. Participants will be split into two groups: one focusing on the analysis of images and one focusing on characteristics that can be extracted from the dataset. Participants in both groups can utilise any methodology they see fit to complete this task. 
These are intensive 'collaborative hackathons' hosted at the Turing, which bring together organisations from industry, government and the third sector, with talented multi-disciplinary researchers from academia.
Organisations act as Data Study Group 'Challenge Owners', providing real-world problems and datasets to be tackled by small groups of highly talented, carefully selected researchers.
Researchers brainstorm and engineer data science solutions, presenting their work at the end of the week.
Read reports from previous Data Study Groups to see challenges and outcomes.
Applications are being accepted through Flexi-Grant. Please note, this Data Study Group has limited places - apply now to avoid disappointment.
The application deadline is noon on Monday 10 July 2023.
Apply now
What if I am already part of the Turing community?
If you are employed at one of the universities in the Institute’s Turing University Network (TUN), please contact your Turing Liaison – list available here - to make them aware of your application. They can provide support, answer questions and involve you as part of the Turing community at your university from now on.
More FAQs for Data Study Group applicants.
University of Exeter - Streatham Campus
"
https://www.turing.ac.uk/people/enrichment-students/mauricio-figueroa,"
Mauricio is a Ph.D. Candidate at Newcastle Law School, where he is a member of the research group ""Law and Futures"". He holds an LL.M. (Master of Laws) on Law and Technology from Tel Aviv University.
Mauricio's doctoral thesis explores ""Ghostbots"", digital innovations that allow people to interact with the dead through Artificial Intelligence (AI). Under the supervision of Prof. Lilian Edwards, he studies how the existing legal categories of privacy, personhood and property fail or succeed to regulate this emerging and complex phenomenon. He also attempts to place this legal analysis within the wider intersection of Ethics, Technology and Society.
"
https://www.turing.ac.uk/people/researchers/smera-jayadeva,"
Smera Jayadeva is a Research Assistant in Data Justice and Global Ethical Futures under the public policy programme.  
Prior to joining The Alan Turing Institute, Smera has worked in a collaborative placement with the Austrian Institute for International Affairs wherein the European and Indian approaches to medical AI were comparatively evaluated. She has experience in conducting research in the Synergia Foundation on themes ranging from geopolitics and policymaking to disruptive technology. Additionally, Smera has worked as an independent researcher in policy evaluation and governance in public and non-profit organisations.  
 
Smera holds an International Master in Security, Intelligence and Strategic Studies with distinction jointly awarded by the University of Glasgow, Dublin City University, and Karlova University. Her graduate dissertation titled “Systems in the subcontinent: Data, power, and the ethics of medical machine learning in India” evaluated the scope, challenges, and mediatory role of AI in Indian healthcare systems. She also holds a BA with a triple major in History, Economic, and Political Science from Christ University (Bengaluru).
"
https://www.turing.ac.uk/research/research-projects/inverse-problems,"
Many data-centric engineering problems are ‘inverse’ in nature, i.e. they involve working out unknown parameters and causes from observations of a system of interest, rather than modelling a system from known parameters. For example constructing an image of someone’s internal organs from an x-ray. This project involves the improvement of the statistical modelling of these unknown parameters in a range of important contexts.
An inverse problem in science is the process of calculating from a set of observations the causal factors that produced them: for example, calculating an image in X-ray computed tomography, source reconstruction in acoustics, or modelling groundwater flow from geophysical measurements of the earth’s subsurface.
They are called inverse problems because they start with the results and then calculate the causes. This is the inverse of a forward problem, which starts with the causes and then calculates the results.
When faced with an inverse problem, the important question is how to make it computationally viable. Often the quantities you want to make inferences about are infinitely dimensional, in the form of a ‘function surface’ rather than just numbers. It’s therefore necessary to use statistical modelling to reduce the dimensions of parameter spaces and reduce the number of times you need to run computations.
The field requires mathematicians to describe the physics of a system, statisticians to deal with infinite dimensional nature of the unknown parameters, and computational people to run the calculations.
This project is aiming to provide efficient and accurate statistical solutions of inverse problems that are physics-constrained. This is being done by combining computational methods and related ‘probabilistic numerical methods‘ that aim to more accurately model the difference between a real, physical system and a mathematical approximation of that system.
Together, these tools enable the accurate quantification of uncertainty in inverse problems, in contexts such as providing statistical confidence regions for tissues in medical scans, or safely separating fluid mixtures with industrial hydro-cyclone equipment.
[email protected]
"
https://www.turing.ac.uk/people/former-enrichment-students/gianluca-detommaso,"
Gianluca is an Italian PhD student at the University of Bath. He obtained a two-year MSc in Mathematics at the University of Turin (Italy), in parallel with a honour program at the Collegio Carlo Alberto rewarding a MA in Statistics and Applied Mathematics. He started an integrated PhD program at the University of Bath, obtaining a MSc in Statistical Applied Mathematics from the first year, then moving into research. He is currently in the third year of his PhD program. Very active person, he enjoys keeping himself busy and pursuing activities with passion. He loves sports, travelling, food and sticking around with smiling people.
Gianluca's research aims to bring together techniques from statistics, numerical analysis and applied mathematics to accelerate Bayesian sampling. In particular, he deals with computationally expensive high-dimensional problems, trying to beat down the cost per iteration and performing algorithms that scale well in high-dimension. Gianluca's interests are very broad: he loves interactions among different research fields, bringing together knowledge and experimenting new ideas. He is always keen on trying out new potential sampling accelerations, or to apply his machinery to other topics. Some keywords of his current research are multilevel methods, MCMC algorithms, transport maps and Bayesian inverse problems.
"
https://www.turing.ac.uk/news/publications/bayesian-probabilistic-numerical-methods-time-dependent-state-estimation,"
The use of high-power industrial equipment, such as large-scale mixing equipment or a hydrocyclone for separation of particles in liquid suspension, demands careful monitoring to ensure correct operation. The fundamental task of state-estimation for the liquid suspension can be posed as a time-evolving inverse problem and solved with Bayesian statistical methods. In this paper, we extend Bayesian methods to incorporate statistical models for the error that is incurred in the numerical solution of the physical governing equations. This enables full uncertainty quantification within a principled computation-precision trade-off, in contrast to the over-confident inferences that are obtained when all sources of numerical error are ignored. The method is cast within a sequential Monte Carlo framework and an optimised implementation is provided in Python.
Oates CJ, Cockayne J, Aykroyd RG, Girolami M. Bayesian Probabilistic Numerical Methods in Time-Dependent State Estimation for Industrial Hydrocyclone Equipment. Journal of the American Statistical Association, to appear in 2019.
"
https://www.turing.ac.uk/people/external-researchers/samaneh-abbasi,"
Samaneh Abbasi is a senior data scientist and data science network co-lead at the PHC Data analytics and Imaging (DAI) team at Roche. More specifically, she is working on making prognostic models based on digital pathology slides, which helps in patient stratification in the clinical trials. Before joining Roche, Samaneh did her PhD and post-doc in medical image analysis in the Netherlands.
"
https://www.turing.ac.uk/events/turing-lectures-ai-drug-discovery,"
Noor Shaker is a serial biotech entrepreneur and the CEO at GlamorousAI, a biotech company that pushes the boundaries to what is possible with AI to cure debilitating diseases. She is a recognised healthcare leader, MIT innovator under 35 and in BBC 100 women.
Over the past few years we have witnessed the rise of AI and the enormous efficiency it can bring to drug discovery and development. While we are starting to see fruitful applications and preliminary success, the field is still in its infancy. This talk will introduce AI in drug discovery and its exciting potential. We will also touch on AI limitations and propose approaches to scale AI to address diseases with unmet needs.

"
https://www.turing.ac.uk/events/unlocking-cultural-heritage-and-creative-industries-through-data-science-and-ai,"
An interdisciplinary event jointly organized by the Universities of Reading, Durham and Warwick aimed at academics, industry and organisations interested in the application of data science and AI (especially machine learning) across a range of humanities-based sectors. 
The event will include presentations and discussions across three main themes:
Confirmed speakers include the BBC, AI companies Metaphysic.ai and DeepZen, Natural History Museum, Royal Botanic Kew Gardens, and the Universities of Durham, Reading and Warwick.
Register now
"
https://www.turing.ac.uk/events/ai-and-data-science-age-covid-19,"
In its role as national centre for data science and AI, The Alan Turing Institute is convening a public conference to assess the response of the UK's data science and AI community to COVID-19.
 
This one-day conference will explore the lessons learned during the COVID-19 pandemic. The event will be for researchers and policy makers to share best practice and work together to understand how the UK AI and data science research community can be better positioned to respond to major crises in the future.
This is an opportunity to hear from some of the country's leading voices in the UK's response to COVID-19 and ask the questions at the forefront of everyone's minds. The conference will include keynote talks suitable for a general audience and some more technical talks suitable for an academic audience. 
Following this event, a series of workshops will take place exploring the role of AI and data science during the pandemic and priorities for future research. The findings will be published in a report in early 2021.
This work is supported by researchers of the Wave 1 UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the “Health” theme within that grant and The Alan Turing Institute.
 
Michael Wooldridge (Programme Co-Director for AI, and Turing Fellow at The Alan Turing Institute)
Inken von Borzyskowski (Assistant Professor in Global Policy & International Relations at UCL))
Anjali Mazumder (AI and Justice and Human Rights Theme Lead at The Alan Turing Institute)
Bilal Mateen (Clinical Data Science Fellow at The Alan Turing Institute)
 


 
87.3 KB
"
https://www.turing.ac.uk/people/researchers/miguel-rodrigues,"
Miguel Rodrigues is a Professor of Information Theory and Processing at University College London. He also leads the Information, Inference and Machine Learning Lab that focuses on the foundations and applications of information theory, information processing, and machine learning systems.
He held various appointments with various institutions worldwide including Cambridge University, Princeton University, Duke University, and the University of Porto, Portugal. He obtained the undergraduate degree in Electrical and Computer Engineering from the Faculty of Engineering of the University of Porto, Portugal and the PhD degree in Electronic and Electrical Engineering from University College London.
Dr Rodrigues serves or has served as Editor of IEEE BITS, Editor of the IEEE Transactions on Information Theory, and Lead Guest Editor of the Special Issue on “Information-Theoretic Methods in Data Acquisition, Analysis, and Processing” of the IEEE Journal on Selected Topics in Signal Processing.
Miguel's research lies in the general area of mathematics of information, information theory, and information processing.
He will work on a variety of topics at the Turing including: (1) information-theoretic foundations of privacy (2) information-theoretic foundations of security and (3) information-theoretic foundations of deep learning algorithms.
"
https://www.turing.ac.uk/people/guest-speakers/christina-pagel,"
Christina Pagel is Professor of Operational Research (a branch of applied mathematics) at University College London (UCL) and Director of the UCL Clinical Operational Research Unit. Her main research area is using mathematical tools to support delivery of health services. This includes combining mathematical models and analysis of routinely collected national and local datasets to support service delivery and design both locally and nationally. She is co-director of the new UCL CHIMERA hub where researchers will examine anonymised data from 40,000 patients at University College London Hospital (UCLH) and Great Ormond Street Hospital (GOSH), to develop a better understanding using mathematical modelling of how people’s physiology changes during ill health and recovery. Since May 2020, she has been a member of Independent SAGE, a group of scientists who are working together to provide independent scientific advice to the UK government and public on how to minimise deaths and support Britain’s recovery from the COVID-19 crisis.
"
https://www.turing.ac.uk/people/external-researchers/katherine-kent,"
"
https://www.turing.ac.uk/events/turing-lecture-regulating-unreality,"

In collaboration with the Barbican. Please note that bookings for this lecture are through the Barbican.

“Deepfakes” or the use of AI to convincingly simulate or synthesise content, voice, images or video for malicious purposes (Nesta, 2018) , has become prominent recently, perhaps most obviously as a means to create realistic but fake pornography involving celebrities or particular victims .
As such,  there has already been some discussion regarding whether these “unreal” products constitute criminal material such as revenge porn ( better referred to as nonconsensual sharing of private or sexual images) or images of child sexual abuse ( “child pornography”) (Chesney and  Citron,2018) .
Its implications are however far greater. Techniques to generate deepfakes are evolving in response to a parallel arms war of detection techniques, and may eventually result in a world where the problems currently being experienced with “fake news” expand to everything we see, hear and experience , not just news we read. Obvious areas where this may impact on the law include the law of evidence: the law of intellectual property, primarily copyright: and fraudulent misrepresentation and anti-consumer scams: but these will only be the start of a deluge when our world of reality becomes inscrutably a constructed and manipulated text.
We suggest that pro-actively a number of governance approaches should be examined to try to forestall the impact of deepfakes on reality. These include;
Finally we consider in the light of work on transparency as to whether robots/ bots are persons  (EPSRC Principles of Robotics; California Senate Bill No 1001,  2018) if  a duty to certificate or separate reality from unreality or to filter unreality out, to the standard of comprehension of a reasonably perceptive person, should be imposed on service providers and platforms. Should the right to know what is real be a new human right?
 
Agenda
18:00 - 18:30  Registration
18:30 - 18:35  Introduction
18:35 - 19:35  Regulating unreality, Professor Lilian Edwards (Newcastle Law School, UK)
19:35 - 19:55  Q&A
 
 
Barbican Centre (Frobisher Auditorium)
Silk Street
London
EC2Y 8DS
"
https://www.turing.ac.uk/research/research-projects/role-synthetic-data-financial-systems,"
Like all financial institutions, HSBC uses a large amount of data to support its primary goals of providing trusted services to customers. Data that is personal can play a significant role in protecting the bank, and its customers, from financial loss including bad lending decisions and/or fraud. Therefore, this project investigates the role of synthetic data to undertake various analysis and modelling projects without having to directly rely on personal data. For example, better analysing customers propensity to acquire further products (mortgages, loans etc) or fraud. This project is also interested to understand the striking and appropriate balance between privacy and utility and to support the understanding of of synthetic data as a service, by investigating commercially available synthetic data techniques and tools.
There is a natural tension, in modern data science between wanting to apply or develop novel AI techniques and wanting to preserve the privacy of individuals or other sensitive information. One way to address this, is to use algorithms to construct synthetic datasets which will be close enough to the real data to perform the necessary development and testing, but far enough away not to impact the privacy of individuals or reveal sensitive information.
The project aims to understand the appropriate balance between privacy, fidelity and utility of synthetic data, and to develop methods around said generation in the context of finance, such as analysing banking customers’ propensity to acquire further products such as mortgages and loans.​​
As above this work is being applied with our HSBC partners, but the ideas have broad applicability, and we are working with other Turing teams working on similar areas.
Monica Vakil-Dewar[email protected]
"
https://www.turing.ac.uk/people/external-researchers/xiaowei-huang,"
Professor Huang is the co-organiser of the Neuro-symbolic AI interest group.
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-data-and-software-engineering,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of data and software engineering. This domain is becoming increasingly critical in all research where there is a need for high-quality tools, practices, methodologies, platforms and systems.
We'll be hearing from Daniel Sabanes Bove, Senior Principal Data Scientist and Associate Director, Statistical Engineering at Roche and Martin O' Reilly, Director of Research Engineering at The Alan Turing Institute. Both will discuss how software engineering teams are set up and operate respectively at Roche and the Turing, how they are working in a collaborative manner and examples of their work in the health sphere.
The event will conclude with a Q&A, we also hope to continue discussions pre and post the event via our Turing-Roche Slack Workspace which you can join.
You can watch a recording of this event here. 

"
https://www.turing.ac.uk/public-sector/,"



In-person tickets are now sold out. You can still secure your place at AI UK through the live stream and watch the showcase, which will host over 150 expert speakers from multi-disciplinary backgrounds, across three stages: conversation, research and impact.
Listen along to dynamic sessions outlining the latest research and innovation in data science and AI, including lightning talks, or panel sessions on exploring the latest research and technology being applied to everyday societal challenges. Live streaming AI UK means the flexibility to design your agenda around existing commitments.
Artificial intelligence (AI) can help make decisions and processes in your organisation both faster and simpler in a surprising number of efficient ways. Machine learning and data science are being used at every level to give organisations, charities, councils and other public and third sector bodies a cutting-edge advantage; but ethical use is paramount.
At AI UK 2023, you’ll be surrounded by the best and brightest in today’s AI ecosystem, who’ll be demonstrating the latest ethical data science innovations, giving talks on their cutting-edge inclusive research, running workshops, and forging partnerships.
It is a truly exciting time for the UK’s AI landscape and 2023 will be the very first time the event has been held in person. That means that not only do you have access to our packed agenda of innovative sessions and lightning talks led by trailblazing minds and innovators, but you can also get to know them – as well as catch up with your own sector peers – at the showcase’s networking events. What’s more, you’ll be able to dip into a great variety of live, interactive applied AI technologies in action through our demonstrations; hear the development and implementation stories from the minds behind and tech, and learn from and be inspired by their stories.
With previous speakers including Sir Patrick Vallance and Chris Philp, AI UK will provide a unique platform where prominent figures from government and local councils will meet world-leading academics. On stage,
in the exhibition arena and, more intimately, at the various roundtable sessions throughout the showcase, there will be curated opportunities to discuss how pioneering research can be implemented in regions across the UK and beyond.
AI UK is the place to learn about the latest developments in machine learning, data science and AI. The two-day showcase features a programme of industry-leading speakers, cutting-edge updates, stimulating discussions and debates, and bite-sized lightning talks, as well interactive demonstrations of state-of-the-art tech innovation in action.
At AI UK, you’ll find myriad ways to answer all your AI questions – as well as to pose those big questions yourself. How is AI changing the way the public sector thinks about and plans for the future? How can collaborations between researchers and charities lead to practicable, real-world solutions? What should councils and local government do to help narrow the skills gap faced by the UK’s AI ecosystem?
Dedicated networking spaces and sessions at AI UK mean that you can meet colleagues, peers and others interested in what AI can achieve for their organisations and constituents – and discuss ways in which to collaborate and make that happen. Connect over coffee or lunch or meet at a lightning talk; there will be plenty of opportunities to engage with leading minds to take your innovative thinking to the next level.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.


"
https://www.turing.ac.uk/news/publications/quantitative-historical-linguistics,"
This book is an innovative guide to quantitative, corpus-based research in historical and diachronic linguistics. Gard B. Jenset and Barbara McGillivray argue that, although historical linguistics has been successful in using the comparative method, the field lags behind other branches of linguistics with respect to adopting quantitative methods. Here they provide a theoretically agnostic description of a new framework for quantitatively assessing models and hypotheses in historical linguistics, based on corpus data and using case studies to illustrate how this framework can answer research questions in historical linguistics. The authors offer an in-depth explanation and discussion of the benefits of working with quantitative methods, corpus data, and corpus annotation, and the advantages of open and reproducible research. The book will be a valuable resource for graduate students and researchers in historical linguistics, as well as for all those working with linguistic corpora.
Jenset, G. B. and McGillivray, B. (2017). Quantitative Historical Linguistics. A Corpus Framework, Oxford: Oxford University Press.
"
https://www.turing.ac.uk/people/guest-speakers/ben-adcock,"
Ben Adcock is a professor of mathematics at Simon Fraser University.  He studied mathematics at the University of Cambridge, receiving his BA in 2005, MMath in 2006, and PhD in 2011. His PhD was in the Numerical Analysis group, under the supervision of Arieh Iserles.  From 2010 to 2012 he held NSERC and PIMS (the Pacific Institute for the Mathematical Sciences) postdoctoral fellowships at Simon Fraser University, working with Nilima Nigam.  He was an assistant professor in the Department of Mathematics at Purdue University from 2012 to 2014, before returning to Canada in August of that year as an assistant professor. He was promoted to associate professor in September 2018 and full professor in September 2021.
He received the CAIMS/PIMS Early Career Award in 2017, an Alfred P. Sloan Research Fellowship in 2015 and a Leslie Fox Prize in Numerical Analysis in 2011.
He has published over 50 peer-reviewed journal articles, 15 conference proceedings, and two book chapters. His work has also been featured on the cover of SIAM News. Further information about his research can be found on his Research and Publications pages.
He is currently PIMS-SFU Site Director and a member of the CAIMS (Canadian Applied and Industrial Mathematics Society) board. He is also a member of the Editorial Board for the SIAM Journal on Scientific Computing, and the SIAM Computational Science and Engineering book series. From 2015-2017 he was secretary of the SIAM Pacific Northwest Section.
He is the primary organizer for the 2020 Foundations of Computational Mathematics conference.
"
https://www.turing.ac.uk/people/external-researchers/chloe-colliver,"
"
https://www.turing.ac.uk/people/researchers/florian-ostmann,"
Florian Ostmann is Head of AI Governance and Regulatory Innovation within the Turing's public policy programme. His work focuses on regulation and non-regulatory governance mechanisms related to data and AI as well as on the use of data science and AI to facilitate the work of regulatory bodies. His broader research interests include the ethical implications of emerging technologies, the use of data science and AI to address societal challenges, health policy, and corporate responsibility (with a focus on business and human rights). Recent work led by Florian includes a report examining questions of responsible AI innovation in financial services (commissioned by the Financial Conduct Authority) and a project on the use of data to address modern slavery risks in the investment context. Florian serves on the Royal Statistical Society’s Data Science Section Committee and contributed to the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems as a member of the Initiative’s Law Committee. In previous roles, he worked at the Harvard Kennedy School's Shorenstein Center on Media, Politics and Public Policy, as a consultant on autonomous vehicle policy, social impact measurement, and responsible investing, and for the Pan American Health Organization. Florian holds a Master in Public Policy from the Harvard Kennedy School and a PhD in Political Philosophy and MA in Legal and Political Theory from University College London.
"
https://www.turing.ac.uk/people/researchers/mona-faraji-niri,"
Mona Faraji Niri is a Senior Research Fellow at Warwick Manufacturing Group, University of Warwick. She has a PhD in modelling and control engineering from Iran University of Science and Technology, Tehran. Mona’s main interests include modelling, control and optimisation of engineering and manufacturing systems. She has been endorsed by The Royal Academy of Engineering for her work in this filed as an exceptional promise. Her work has been published in Journal of Cleaner Production and IEEE Transactions of Intelligent Transportation Systems, among other journals.
Mona's research focuses on how modelling can support the optimised energy systems. She is particularly doing research in the field of renewable energy and energy storage systems and is expert with Li-ion batteries. She has developed models and control algorithms in order to optimise the performance of energy storage systems for electric transportation systems. She is working towards the optimisation of energy management, thermal management, and performance monitoring systems. She is also contributing to the advancement of battery manufacturing processes by introducing new AI and dynamic models as well as digital twins to increase the manufacturing transparency and predictability. Her research is inline with the Smart Manufacturing of energy storage systems for a clean and net-zero carbon future.
"
https://www.turing.ac.uk/news/publications/diorisis-ancient-greek-corpus,"
This corpus was created in the context of the project ""Computational models of meaning change in Ancient Greek""
 
Related data set “Diorisis Ancient Greek Corpus” with DOI https://www.doi.org/10.6084/m9.figshare.6187256 in repository Figshare. The Diorisis Ancient Greek Corpus is a digital collection of ancient Greek texts (from Homer to the early fifth century AD) compiled for linguistic analyses, and specifically with the purpose of developing a computational model of semantic change in Ancient Greek. The corpus consists of 820 texts sourced from open access digital libraries. The texts have been automatically enriched with morphological information for each word. The automatic assignment of words to the correct dictionary entry (lemmatization) has been disambiguated with the implementation of a part-of-speech tagger (a computer programme that may select the part of speech to which an ambiguous word belongs).
Vatri, A., & McGillivray, B. (2018). The Diorisis Ancient Greek Corpus, Research Data Journal for the Humanities and Social Sciences, 3(1), 55-65. doi: https://doi.org/10.1163/24523666-01000013
"
https://www.turing.ac.uk/people/researchers/aad-van-moorsel,"
"
https://www.turing.ac.uk/news/publications/stein-thinning,"
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-ai-and-health-equity,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of AI and Health Equity- exploring how we ensure emerging technologies improve healthcare for all and don't exacerbate existing inequalities in the system. 
 
We will be hearing from Honghan Wu, a Turing Fellow and Associate Professor at the Institute of Health Informatics, UCL; Fanny Sie, Head of AI and Emerging Technology External Collaborations and Joshua Hoang, AI and Digital Healthcare Partner both at Roche.
Honghan's talk will be on 'Fair’ in the context of AI enabled clinical decision making': we know bias exists in health data and AI in medicine might channel and exacerbate health inequality without effective mitigations. However, what is fair exactly in the context of health equity? There seems no consensus yet. In this talk Honghan will stimulate a discussion on this point. In particular, he will give one definition, present a quantification framework based on it and show the results of a set of experiments on several decision making scenarios on two real-world intensive care unit datasets.
Fanny and Joshua's talk 'AI with Roche Contributing to Health Equity through Open Science': AI with Roche (AIR) is a Canadian-based centre of excellence aimed at delivering better health outcomes to people through the discovery and application of AI research. Underpinned by an open and collaborative exchange uniquely supported by Roche expertise, AIR is developing and promoting the integration of AI for health care by fostering partnerships within Canada and beyond. AIR's collaborators include the Pan-Canadian AI institutes (Amii, MILA and the Vector Institute) as well as multidisciplinary innovators from across the health ecosystem (startups, multinationals, patient advocacy groups, clinicians). By creating mission and mandate-driven engagements, AIR democratizes access to data, tools and expertise in the pursuit of accelerating scientific discovery and health equity within a sustainable model for society. Experiences Fanny will share in this talk include the Roche Data Science Coalition and EndALS open science challenges which can be found on the AIR website.
You can watch a recording of this event here.

"
https://www.turing.ac.uk/news/publications/information-commissioners-office-consultation-its-age-appropriate-design-code,"
The Information Commissioner made a call for evidence and views on the Age Appropriate Design Code on 27 June 2018. The Code is a requirement of the Data Protection Act 2018. The Act supports and supplements the implementation of the EU General Data Protection Regulation (the GDPR).
The Code will provide guidance on the design standards that the Commissioner will expect providers of online ‘Information Society Services’ (ISS), which process personal data and are likely to be accessed by children, to meet. Once it has been published, the Commissioner will be required to take account of any provisions of the Code she considers to be relevant when exercising her regulatory functions. The courts and tribunals will also be required to take account of any provisions they consider to be relevant in proceedings brought before them. The Code may be submitted as evidence in court proceedings. The Commissioner seeks evidence and views on the development stages of childhood and age-appropriate design standards for ISS.
This document provides the response of The Alan Turing Institute to the Information Commissioner’s Office (ICO) consultation on its draft code of practice for online services likely to be accessed by children. The Institute’s response combines the perspectives of various Turing researchers and those in its wider university network.
This response is split into two sections. The first makes comments on the overall code including why it is necessary and what we believe is missing from it, while the second addresses specific areas of the code. 
"
https://www.turing.ac.uk/research/research-projects/safe-ai-surgical-assistance,"
As AI based decision-making methods make their way from internet applications to more safety-critical physical systems, questions about the robustness of the models and policies become increasingly more important. This project is developing methods to address this through novel methods for learning specifications from human experts and synthesising policies that are correct by construction. These developments are grounded in the domain of surgical assistance with autonomous robots in the operating room.
The work addresses a number of different scientific questions.
The project is developing techniques for interactive learning, going from rich cross-modal data obtained as a human expert performs various tasks of interest, to the underlying specifications that implicitly define the task and associated models of activity. This work is extending the familiar paradigm of learning from demonstrations, which has been used successfully in robotics before, to one of 'programming by discussion', emphasising the interactive nature of the learning, as well as having engineering design specifications focus on the target of learning specifically.
New techniques are being developed for interrogating the properties of the models being learned. By working with hybrid systems representations of planning and control tasks, wherein (probabilistic) programs are defined over underlying dynamical systems models, a variety of methods are adopted for analysis of behavioural properties, ranging from formal program analysis to statistical model checking. In particular, 'closed-loop' analysis methods are being developed that explore the input space to identify adversarial examples and subsequently repair the models to protect against these. The work also envisions the possibility of human inspection and knowledge injection, to facilitate incorporation of, for instance, 'codes of practice' that are already semi-formally available in many application domains. 
The project is also developing methods for synthesising policies for action, through well understood paradigms. These include model predictive control and dynamic programming or reinforcement learning methods, which are correct by construction - in the sense that they are shown to satisfy the specifications learned through the above mentioned approaches. 
This project will have two kinds of outcomes. 
Firstly, a suite of software tools aimed at interactively learning specifications and activity models, which will in turn be used for synthesising policies for action with guaranteed behavioural properties, such as for safety. This will include methods for interrogating the learned models regarding these same properties, in order to verify the behaviour under new inputs and to ensure robustness with respect to adversarial inputs. 
Secondly, this suite of tools will be developed in the context of autonomous robots being deployed as surgical assistants in operating rooms. From the outset, the project researchers are working closely with medical practitioners to understand the various use cases for such technology, and also to obtain the data needed to train the new models and conduct experiments. 
In this project, the development of methods and software tools is tightly coupled with their continual application in use cases drawn from the application domain of surgical assistance. 
Experimental work is carried out in a newly established living lab within the Bayes Centre at the University of Edinburgh, which provides a representative environment capturing many aspects of the eventual use case of the typical operating theatre in a hospital. Working closely with surgeons (including Co-I, Dr Brennan), the project is defining a collection of representative problems to be tackled, ranging from joint manipulation of tissue to automation of suction, tool handling and many other tasks that increase the cognitive load on the assisting surgical or nursing staff. 
It's envisioned that the experiments being carried out in this project as a first step towards larger scale future experiments in situ, to demonstrate the usefulness of these methods in practice. 
Equally, it's envisioned that the suite of tools and software will be useful more broadly in a variety of safety-critical applications of robots and autonomous systems.
There is an acknowledged sensitivity around the introduction of automated technologies in a medical domain. Some will be worried about the implications of transferring some forms of human expertise to a machine, even though this project's focus is on skill augmentation rather than replacement. Likewise, there will be some who are worried about potential job losses due to automation. Here again, the researchers stress that the focus is on making AI capable as an assistance, so emphasising more the notion of augmenting intelligence rather than merely automating it away.
[email protected]
S.D.S. Marín, D. Gomez-Vargas, N. Céspedes, M. Múnera, F. Roberti, P. Barria, S. Ramamoorthy, M. Becker, R. Carelli, C. A. Cifuentes, Expectations and perceptions of healthcare professionals for robot deployment in hospital environments during the COVID-19 pandemic, Research topic: Robotics, Autonomous Systems and AI for Nonurgent/Nonemergent Healthcare Delivery During and After the COVID-19 Pandemic, Frontiers in Robotics and AI - Biomedical Robotics, 2021.
C. Innes, S. Ramamoorthy, ProbRobScene: A probabilistic specification language for 3D robotic manipulation environments, In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2021. Code repository
M. Burke, K. Subr, S. Ramamoorthy, Action sequencing using visual permutations, IEEE Robotics and Automation Letters, 2021. Video clip
Y. Hristov, S. Ramamoorthy, Learning from demonstration with weakly supervised disentanglement, In Proc. International Conference on Learning Representations (ICLR), 2021. Video clip
E. Kahembwe, S. Ramamoorthy, Lower dimensional kernels for video discriminators, Neural Networks Journal, Special Issue on Deep Neural Network Representation and Generative Adversarial Learning, 2020. 
D. Angelov, Y. Hristov, S. Ramamoorthy, From demonstrations to task-space specifications. Using causal analysis to extract rule parameterization from demonstrations, Autonomous Agents and Multi-Agent Systems, Vol. 34(45), 2020. 
D. Angelov, Y. Hristov, M. Burke, S. Ramamoorthy, Composing diverse policies for temporally extended tasks, IEEE Robotics and Automation Letters, Vol 5(2): 2658-2665, 2020.  Video clip 
M. Asenov, M. Burke, D. Angelov, T. Davchev, K. Subr, S. Ramamoorthy, Vid2Param: Modelling of dynamics parameters from video, IEEE Robotics and Automation Letters, Vol 5(2): 414-421, 2020. Video clip
C. Innes, S. Ramamoorthy, Elaborating on learned demonstrations with temporal logic specifications, Robotics: Science and Systems (R:SS), 2020. Video clip
A. Straizys, M. Burke, S. Ramamoorthy, Surfing on an uncertain edge: Precision cutting of soft tissue using torque-based medium classification, In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2020. Video clip
M. Asenov, N. Zotev, S. Ramamoorthy, A. Kirrander, Inversion of ultrafast X-ray scattering with dynamics constraints, In Proc. NeurIPS Workshop on Machine Learning and the Physical Sciences, 2020. Supplementary information
M. Burke, Y. Hristov, S. Ramamoorthy, Hybrid system identification using switching density networks, Conference on Robot Learning (CoRL), 2019. 

Y. Hristov, D. Angelov, A.Lascarides, M. Burke, S. Ramamoorthy, Disentangled Relational Representations for Explaining and Learning from Demonstration, Conference on Robot Learning (CoRL), 2019. Supplementary information
M. Burke, S. Penkov, S. Ramamoorthy, From explanation to synthesis: Compositional program induction for learning from demonstration, Robotics: Science and Systems (R:SS), 2019. Video clip 
D. Angelov, Y. Hristov, S. Ramamoorthy, Using causal analysis to learn specifications from task demonstrations, In Proc. International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2019. 
D. Angelov, Y. Hristov, S. Ramamoorthy, DynoPlan: Combining Motion Planning and Deep Neural Network based Controllers for Safe HRL, In Proc. The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM), 2019. 
P. Ardón, M. Cabrera, È. Pairet, R. Petrick, S. Ramamoorthy, K. Lohan, M. Cakmak, Affordance-aware handovers with human arm mobility constraints, IEEE Robotics and Automation Letters, 2021. Video clip
"
https://www.turing.ac.uk/research/research-areas/social-data-science/management-science,"
"
https://www.turing.ac.uk/events/data-study-group-may-2022,"
Due to COVID-19 the Data Study Group will run online and in-person at the University of Birmingham and will be divided into two stages.
Please apply through Flexi-Grant. Participants are expected to attend the full duration of the event. This is a full time engagement and engaging part time impacts other team members and your learning potential. The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected] to discuss any reasonable adjustments.
The challenges are:
Please visit the University of Birmingham website for further challenge details
 
Applications are being accepted through Flexi-Grant – apply now to avoid disappointment.
Apply now
The Turing's Data Study Groups are popular and productive collaborative events and a fantastic opportunity to rapidly develop and test your data science skills with real-world data. The event also offers participants the chance to forge new networks for future research projects, and build links with The Alan Turing Institute – the UK’s national institute for data science and artificial intelligence.
It’s hard work, a crucible for innovation and a space to develop new ways of thinking.
Read reports from previous Data Study Groups to see challenges and outcomes.
Read our FAQs for Data Study Group applicants.
 
Learn more about being a DSG participant including FAQs
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
"
https://www.turing.ac.uk/research/research-projects/applied-research-centre-defence-and-security,"
The Applied Research Centre for Defence and Security (ARC) is a group of data scientists and data science software engineers who focus on the application of cutting-edge technology to problems within the defence and security sector. The centre is staffed by individuals who are full-time employed researchers at the Turing. It enables the UK’s defence and security community to draw on the very best of academia to achieve high impact solutions to the most pressing challenges in the field. The ARC was established in 2019 and has since completed over 20 projects for its funding partners.
The Applied Research Centre bridges the gap between academia and the defence and security sectors, bringing cutting-edge technology to bear on the unique and challenging problems faced in defence and security. It works on applied problems, primarily in response to operational use cases or policy needs, with a rapid turn-around time.
The emphasis is on delivering usable outputs – such as software code and demonstrators – aligned to practical and targeted research objectives. The ARC runs a flexible operating model, allowing for lightweight tasking and agile retasking as requirements change. They maintain a high level of interactivity with partners through regular meetings, as well as sharing access to GitHub repos and Kanban boards.
The ARC allows new technologies and methods to be de-risked for specific applications within defence and security. They produce proof-of-concept demonstrators and provide support on their application to partner data, as well as providing impartial advice on current research and potential applications. This informs partners’ decisions on policy, investment into data science technologies and future-proofing of data storage and processing.
The ARC provides a lightweight and flexible model for scoping and de-risking the application of data science to defence and security problems.
The team works on short-term applied research projects across the domain of deep learning. Projects are scoped collaboratively with funding partners and these stakeholders are a crucial part of the project’s development, through regular check-ins or direct collaboration. Projects typically deliver a technical report to stakeholders, alongside code, containing experimental results as well as commentary and recommendations. 
 Model Similarity
With the widespread adoption of deep neural networks across a range of application domains, defending against adversarial attacks is a clear challenge. This project investigated transfer attacks  and considered whether we could identify model features or measures of similarity between surrogate and target models that can help identify whether transfer attacks are more likely to succeed. Transfer attacks were simulated between pairs of models to examine the correlation of attack success with model features and model-pair similarity metrics, such as the cosine similarity between loss derivatives.  
Voice Liveness Detection
This project sought to demonstrate the applicability of transformer-based models for the defence against audio replay attacks on speaker verification systems. Researchers examined a range of pretrained audio models, with varying architecture and pretraining data, for adaptation to this task and provided experimental results on benchmark datasets. 
"
https://www.turing.ac.uk/people/former-enrichment-students/joe-shaw,"
"
https://www.turing.ac.uk/people/researchers/yulan-he,"
Yulan He is a Professor in Natural Language Processing at the Department of Informatics in King's College London. She is currently holding a prestigious UKRI Turing AI Fellowship (2021-2025). Yulan’s research interests lie in the integration of machine learning and natural language processing for text analytics. She has published over 200 papers. Topics include natural language understanding, sentiment analysis and opinion mining, question-answering, topic and event extraction, biomedical text mining, social media analytics and fake news detection.
She has received several prizes and awards, including a SWSA Ten-Year Award, a CIKM 2020 Test-of-Time Award, and AI 2020 Most Influential Scholar Honourable Mention by AMiner. She has served as a Program Co-Chair for EMNLP 2020, the General Chair for AACL-IJCNLP 2022, an Action Editor for Transactions of the ACL and an Associate Editor for the Royal Society Open Science journal. Yulan obtained her PhD degree in Spoken Language Understanding from the University of Cambridge.
"
https://www.turing.ac.uk/events/how-do-people-feel-about-ai,"
In late 2022 the Ada Lovelace Institute and The Alan Turing Institute conducted a nationally representative survey of over 4,000 members of the British public, to understand their awareness, experience and attitudes towards different uses of artificial intelligence (AI), including views and expectations on how these technologies should be regulated and governed. Following on from the publication of the report, this event will explore the key findings of the research.
Artificial intelligence (AI) technology and its widespread use in many aspects of daily life is developing at a rapid pace. It is therefore crucial to understand how people currently experience the many applications of AI, including people’s general awareness of these technologies, how they perceive the benefits and concerns, and whether their attitudes differ across key demographic factors. To effectively inform the design of policy responses, it is also important to understand people’s views on how these technologies should be governed and regulated.
To answer these questions, The Alan Turing Institute and the Ada Lovelace Institute partnered to conduct a new, nationally representative survey of the British public’s attitudes towards, and experiences of, AI. While previous surveys have tackled related questions, there remain several gaps in our understanding of public attitudes to AI.
Join research leads from The Alan Turing Institute and the Ada Lovelace Institute as they delve into some of the key findings, explain the methodology and answer questions.
14:00 - Welcome and introduction - Andrew Strait
14:05 - Key findings - Florence Enock and Roshni Modhvadia
14:15 - Initial responses from panellists - Helen Margetts, Shannon Vallor, Ben Lyons and Gina Neff
14:30 - Moderated Q&A - Helen Margetts, Shannon Vallor, Ben Lyons and Gina Neff. Chaired by Andrew Strait
15:00 - Closing remarks - Andrew Strait
 
This event is organised in collaboration with the Ada Lovelace Institute

"
https://www.turing.ac.uk/terms-and-conditions,"

These terms tell you the rules for using any of our websites, apps, and other digital platforms, including but not limited to https://www.turing.ac.uk/ and our Slack site (our “sites”).
Please read these terms carefully before using any of our sites.
Read the terms in full here.
 
"
https://www.turing.ac.uk/news/prams-parliament-what-machine-anyway,"
The Living with Machines project is rethinking the impact of technology on ordinary lives during the Industrial Revolution. Bringing together data scientists, curators, historians, geographers and computational linguists, the goal is to devise new methods in data science and artificial intelligence which can be applied to historical resources and produce tools and software to analyse digitised collections at scale for the first time.
Scientists from The Alan Turing Institute are collaborating with curators from the British Library to build new software which will analyse data from millions of pages of out-of-copyright newspaper collections from within the archive in the British Library’s National Newspaper Building, and other digitised historical collections, to track societal and cultural change in new ways. 
The project has hit a major milestone in its five-year journey with a newly launched crowdsourcing task which the public is being invited to take part in. These crowdsourcing tasks are a unique opportunity for the public to help researchers understand how people living through the Industrial Revolution talked about 'machines' in 19th century newspaper articles. Some of what they described as 'machines' may surprise you – from prams to Parliament – and algorithms can't yet understand what 'machine' meant in different contexts.
The tasks are simple and will create a lexicon (a ‘dictionary’ of words) about machines that will aid computational research at scale. The innovative and novel tasks have been designed to unearth new stories, looking beyond what is currently known to reveal a richer picture of our past. 
“The Living with Machines project is at the forefront of collaboration between libraries and data scientists. The tasks are a great example of this – they showcase our human ability to understand the meaning of words in context and the ability of computers to work at scale.”
Ruth Ahnert, Turing Fellow, Professor of Literary History & Digital Humanities, Queen Mary University of London and Principal Investigator on Living with Machines
“Most crowdsourcing projects ask people to 'type what they see', transcribing text or classifying images shown on the screen,” says Dr Mia Ridge, Digital Curator, British Library and Co-Investigator, Living with Machines. “Our tasks ask people to look closely at the content of an article to assign meaning to the 'machine' mentioned.
 
“This project uses the skill of reading closely to help develop computational language models to analyse data at scale. The tasks will get smarter over time as we analyse and learn from iterative results.”
This ground-breaking partnership and bold research project which places ‘ordinary people’ at its heart is now inviting the public to join them in the following two tasks:
Describe it: transcribe a word or phrase from pre-selected newspaper articles to build a list of words used in primary sources. 
Classify it: apply a pre-set definition to help us understand how dictionary definitions match actual usage in newspapers.
Click here to take part
Visit the Living with Machines website
Sign up to the newsletter
Follow @LivingWMachines on Twitter
"
https://www.turing.ac.uk/people/researchers/jon-crowcroft,"
Jon Crowcroft has been the Marconi Professor of Communications Systems in the Computer Laboratory since October 2001. He has worked in the area of Internet support for multimedia communications for over 30 years. Three main topics of interest have been scalable multicast routing, practical approaches to traffic management, and the design of deployable end-to-end protocols. Current active research areas are Opportunistic Communications, Social Networks, and techniques and algorithms to scale infrastructure-free systems. He leans towards a “build and learn” paradigm for research.
He graduated in Physics from Trinity College, University of Cambridge in 1979, gained an MSc in Computing in 1981 and PhD in 1993, both from UCL. He is a Fellow the Royal Society, a Fellow of the ACM, a Fellow of the British Computer Society, a Fellow of the IET and the Royal Academy of Engineering and a Fellow of the IEEE.
He likes teaching, and has published a few books based on learning materials.
Jon spent the last two years as chair of the Institute's Programme Committee, where a significant ongoing task has been the mapping of strategy for the Turing. As the Turing continues to grow, in his new role as 'Researcher at Large', Jon will continue in this task. With a remit to range over the wider set of activities at the Turing, he will be finding what the Institute does well and uncovering gaps where it needs to do more. The role will also involve helping with the Strategic Priorities Fund cross-cutting theme on tools, practices and systems.
Computing Systems at scale are the basis for much of the excitement over Data Science, but there are many challenges to continue to address ever larger amounts of data, but also to provide tools and techniques implemented in robust software, that are usable by statisticians and machine learning experts without themselves having to become experts in cloud computing. This vision of distributed computing only really works for “embarrassingly parallel” scenarios. The challenge for the research community is to build systems to support more complex models and algorithms that do not so easily partition into independent chunks; and to give answers in near real-time on a given size data centre efficiently.
Users want to integrate different tools (for example, R on Spark); don’t want to have to program for fault tolerance, yet as their tasks & data grow, will have to manage this; meanwhile, data science workloads don’t resemble traditional computer science batch or single-user interactive models. These systems put novel requirements on data centre networking operating systems, storage systems, databases, and programming languages and runtimes. As a communications systems researcher for 30 years, I am also interested in specific areas that involve networks, whether as technologies (the Internet, Transportation etc), or as observed phenomena (Social Media), or in abstract (graphs).
"
https://www.turing.ac.uk/news/publications/sponge-generalized-eigenproblem-clustering-signed-networks,"
We introduce a principled and theoretically sound spectral method for k-way clustering in signed graphs, where the affinity measure between nodes takes either positive or negative values. Our approach is motivated by social balance theory, where the task of clustering aims to decompose the network into disjoint groups such that individuals within the same group are connected by as many positive edges as possible, while individuals from different groups are connected by as many negative edges as possible. Our algorithm relies on a generalized eigenproblem formulation inspired by recent work on constrained clustering. We provide theoretical guarantees for our approach in the setting of a signed stochastic block model, by leveraging tools from matrix perturbation theory and random matrix theory. An extensive set of numerical experiments on both synthetic and real data shows that our approach compares favorably with state-of-the-art methods for signed clustering, especially for large number of clusters and sparse measurement graphs.
Mihai Cucuringu, Peter Davies, Aldo Glielmo, Hemant Tyagi; SPONGE: A generalized eigenproblem for clustering signed networks; Proceedings of Machine Learning Research, PMLR 89:1088-1098, 2019.
5.6 MB
"
https://www.turing.ac.uk/events/data-study-group-may-2023,"
Introduction
Stage 1: The Precursor Stage (part-time)
Event Stage (full-time)
Applicants should be able to commit to the duration of the event. The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected] to discuss any reasonable adjustments.
Challenges
The challenges are:
NBN Trust
Spatiotemporal analysis of species in the UK using the largest publicly accessible source of biodiversity data
This challenge presents an exciting opportunity for researchers to work on the UK’s largest publicly accessible source of biodiversity data to help ensure that this remarkable national resource has the greatest possible value and impact for nature.
The datasets on the NBN Atlas consist of records of species observations within the UK at a given location and time. While the datasets are rich and valuable, they are incredibly heterogenous in space, time and taxonomic coverage. This challenge seeks to examine the comprehensiveness of the biodiversity inventory and what the impact of records from individual data providers is on the coverage and completeness of the inventory. We ask, where are the gaps in data and how can we measure our progress towards a goal of a comprehensive biodiversity inventory?
Peak District National Park Authority
Using multi-spectral remotely sensed imagery to improve land cover classification
National Parks in the UK have been shaped by the longstanding interaction of natural and cultural forces, with no element of the landscape untouched by past or present human activity. This has created a rich diversity of habitats such as broad open moorlands, grasslands, enclosed farmlands and wooded valleys that exhibit variation in both their spectral and spatial properties. However there is currently no contemporary overview of land cover of National Parks in the UK.
This DSG’s objective is to develop techniques that are able to leverage the information of multi-spectral satellite images to improve automated land cover classification. The PDNP recently acquired detailed aerial and satellite imagery of the National Park, and has started to classify land cover based on visible imagery. However, other spectral bands outside the visible spectrum (such as infrared etc.) have the potential to drastically improve classification performance of particular classes.
If successful, this analysis would help quantify the effects of human activities and climate change on the National Park’s landscape, biodiversity and land use from the 1980s until today. Furthermore, an automated land cover classification protocol would enable National Parks in the UK to continue to monitor landscape changes in the future, and help target our resources to protect and enhance the landscape.
Environmental Investigation Agency
Identifying tiger stripe patterns
The Environmental Investigation Agency (EIA) has an existing image database of 158 unique individual tiger skins following encounters with illegal wildlife traders in physical markets and online. Each skin is linked to one or more traders. They also collect images of seized tiger skins and carcasses. New skins are periodically added to the database and manually cross-referenced to determine if they are duplicate images, or duplicate skins but in different images; either of which can reveal dynamics of interest to law enforcement.
This project seeks to develop a user-friendly tool for identifying individual tigers from their stripe patterns, to inform enforcement efforts and counter trade in tiger skins, carcasses, and individuals.
Dstl
Topic Modelling to find topics and trends in academic papers.
The Discovery Project at Dstl uses topic modelling to find emerging topics and trends in academic papers for futures analysts. We hypothesise that studying a time series of topic model runs would allow important dynamics to be uncovered that would otherwise remain unseen in a single run. By tracking how topic models change over time, we expect emerging technologies can be identified at a faster pace than manual scanning, therefore giving the UK a competitive edge in assessing these technologies' applicability to Defence and Security.
To help identify emerging technologies, the Discovery project aims to develop an algorithm that can flag significant differences between a time series of multiple topic model runs. Can machine learning be applied to locate these differences and find emerging, merging/splitting and converging/diverging topics? Further, can the method be extended such that future trends can be predicted?
Johnson Matthey
Meeting the Challenges of Sustainable Chemical Plant Operations: A Machine Learning Approach for Optimizing Renewable Energy Use and Transient Dynamics.
Chemical processes are inherently complex, with multiple process states and paths possible for a given setting. As the industry moves towards for sustainable operations, the use of renewable energy and feedstocks is a key priority. As a consequence of this, chemical plant operations will depart from conventional steady-state operation (slow dynamics) to a more transient basis (fast dynamics) to maximise the use of the resource (e.g. wind power). Such a change introduces process control challenges and the use of large amounts of time-series data to ensure safe and reliable operation.
This challenge is to develop a machine learning-based modelling framework that can accurately predict and optimise plant response in subsequent time steps based on these time-series data.
Our early work in this area, as shown through a feasibility study on a Methanol simulator, has demonstrated that flexible Bayesian regression techniques can effectively optimise nonlinear processes at steady-state.
What are Data Study Groups?
These are intensive 'collaborative hackathons' hosted at the Turing, which bring together organisations from industry, government and the third sector, with talented multi-disciplinary researchers from academia.
Organisations act as Data Study Group 'Challenge Owners', providing real-world problems and datasets to be tackled by small groups of highly talented, carefully selected researchers.
Researchers brainstorm and engineer data science solutions, presenting their work at the end of the week.
Read reports from previous Data Study Groups to see challenges and outcomes.
FAQs
What if I am already part of the Turing community?
If you are employed by one of the Institute’s 13 university partners, please contact your University Liaison Manager – list available here – to make them aware of your application. They can provide support, answer questions and involve you as part of the Turing community at your university from now on.
If you are employed at a university that received a Turing Network Development Award, please contact your Award lead – list available here (scroll to the bottom of the page) – to make them aware of your application.
More FAQs for Data Study Group applicants.
Find out more
Learn more about being a DSG participant including FAQs
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
 
"
https://www.turing.ac.uk/people/external-researchers/jonathan-c-roberts,"
Jonathan C. Roberts is a professor in the School of Computer Science, Bangor University, UK., He was a lecturer (Kent 1999), Senior Lecturer (Kent 2004, Bangor 2007) and became a professor in 2015. His research is focused on data visualization, visual analytics, human computer interaction and computing pedagogy. He is interested in how to analyse, visualise and interact with data, and how to sketch designs and make appropriate designs. He is the creator of the Five Design-Sheet methodology, and author of the book Five Design-Sheets: Creative Design and Sketching for Computing and Visualisation, Springer Nature, June 2017. He focuses on heritage and archaeology, oceanography, pedagogy, and social networking domains, Prof Roberts’ hypothesis is that by displaying the data in several different ways, and allowing the user to highly interact with it, then the user gains a better understanding of their data.
He is the School Director for Impact and Engagement and in July 2019 started a weekly Research Engagement meetups (trebling the quantity of School news articles) and lead on Project Rainbow (continuity of learning activities, see http://csee.bangor.ac.uk/). Leads the Visualization, Data-Modelling and Graphics research group (VDMG). Is a member of the UK CPHC (Council of Professors and Heads of Computing), a member of Senate, sits on the CoESE Ethics and Governance Committee (2018-) and on the Research Degrees Committee (2018-20).
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/transunion,"
www.transunion.co.uk
"
https://www.turing.ac.uk/people/former-enrichment-students/abhinav-mehrotra,"
Abhinav is a third year computer science PhD student at the University of Birmingham. He works under the supervision of Mirco Musolesi and Robert Hendley. His main areas of interest include social computing, context-aware computing, digital health and human behaviour modelling.
Abhinav’s current research focuses on understanding and predicting human behaviour for interacting with mobile devices by using contextual information obtained from the embedded sensors. He also work on the projects related to monitoring and predicting the cognitive context (such as mood) as well as the health of people through the analysis of mobile phone data.
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/causality,"
"
https://www.turing.ac.uk/news/data-scientists-and-marine-experts-join-forces-tackle-environmental-issues,"
The Alan Turing Institute has collaborated with The Centre for Environment, Fisheries and Aquaculture Science (Cefas) to create innovative solutions to global challenges affecting the marine environment.
The organisations have worked together on two Data Study Groups (DSG) challenges in 2021 and 2022 which used the Turing’s data science expertise to support Cefas’ aim of securing a sustainable future for marine ecosystems.
The first DSG challenge aimed to develop machine learning methods to automatically classify plankton species (using information on their shape, features and size) using a manually labelled dataset of 40,000 images.
The software was in use within six months of the DSG, following further development and testing by Plankton Analytics. The open-source software can be adapted to classify images of other marine objects and species, helping to transform the way scientists study the oceans.
The second DSG challenge aimed to detect and classify Sea Pens. The DSG team developed a prototype machine learning algorithm that Cefas estimates can detect and classify sea pens with up to 95% accuracy compared to manual identification.
Building on the existing collaborations, researchers from Cefas have visited the Turing’s offices to explore future opportunities to bring data insights to marine research.
The Environment and Sustainability is a Grand Challenge in The Alan Turing Institute’s recently launched strategy. The challenge aims to address the climate and biodiversity crisis and the need for greater sustainability.
A recent white paper from the Turing’s AI for Science and Government (ASG) programme found that AI and data science are an underused asset in the fight against climate change. The report highlights previous research in a case study between the Turing and Cefas. More can be found out about this project in The Turing Podcast, Series 3, Episode 4: Turing deployment at sea: identifying plankton in real time.
The Institute’s Director of Innovation, Simon Reeve, said: “This collaboration is an excellent example of innovative, interdisciplinary research between the Turing and environmental organisations delivering new solutions. Protecting the health of oceans is crucial in the climate emergency. We look forward to working with Cefas in the future to continue to explore and develop innovative solutions in response to this global challenge.”
Commenting on partnering with the Turing, Cefas CEO, Neil Hornby, said: “Our rivers, seas and the ocean are facing significant global threats. Communities around the world are facing the devastating consequences of climate change, biodiversity loss, energy and food insecurity and pollution. At Cefas, we are committed to developing science solutions that respond to these pressing problems. By working in collaboration, together we can harness the power of data and innovative technology to support UK and International governments to fulfil their commitment to restoring nature.”
Header image credit: 'Crown copyright'
"
https://www.turing.ac.uk/research/research-projects/ways-what-arent-you-seeing,"
“As you can see in Figure 1…” may well be the most frequently made claim in science. Unlike claims involving data, statistics, models and algorithms, those relating to visualisations are rarely evaluated or verified. So how do data scientists understand visualisations’ effectiveness and expressiveness when they are developing these graphs, maps and networks? 
This project is using three user research methods to understand how people visualise data in terms of workflows comprised of ‘decision-making’, ‘making queries’ and ‘coding and making’ the visualisation. These methods operate at different scales and depths, and will produce a variety of views into the visualisation work of data scientists which will direct the design and development of visualisation solutions. 

Designing effective visualisations goes far beyond selecting a graph, its scales and a ‘pretty’ style. Effective visualisations must negotiate sensitivities and interactions between visual elements (e.g. encodings, coordinate systems, guides, annotations), data (e.g. characteristics, transformations, partitions), and the discriminator function, which in this case is the perceptual and cognitive systems of humans. Despite their criticality, these methodological and design considerations are rarely surfaced, limiting the value extracted from visualisations. What does 'Figure 1' actually visualise?
The project's end goal is to develop tools which enhance people’s capacity to visualise data, by letting them see what can and can’t be seen in the visualisation. The outcomes will be made available in R and Python which are used widely and intensively in data science. The goal is to further develop the substantial graphics functionality of these programming languages, into a greater capacity to visualise data and understand the properties of those visualisations.
Visualisation, formerly known as plotting data or creating maps, is an everyday activity in data science, having a pivotal role in exploratory data analyses, making sense of data and communicating information. Yet visualisation may be something we have received far less training in. As such, it is unlikely that one has scoured the visualisation literature to understand the impact of our design choices. By closing the epistemological and methodological distance between crucial visualisation concepts and the project team's work, the WAYS projects aims to help any data scientist.
How do Data Scientists Create Data Visualisations? Workflows, Experiences and Perceptions when visualising data with R
This report presents the first results of the User Research conducted between May-August 2020 as part of the WAYS project, with a primary focus on the processes that data scientists go through when creating their data visualisations. Our goal is to characterise the visualisation workflows that people have developed, spanning the initial decision to plot their data, to the point where they consider a data visualisation to be finished. The results presented in this report gather the different perspectives the participants have over data visualisation processes, aiming to identify characteristics, trends, pain points, needs and emotions related to their data visualisation workflows.
 
"
https://www.turing.ac.uk/research/research-areas/privacy-trust/cryptography,"
"
https://www.turing.ac.uk/people/researchers/rob-ewing,"
Rob Ewing is Associate Professor in Proteomics and Systems Biology at the University of Southampton. He received his DPhil from the University of Oxford in molecular genetics, and has held both industry and academic posts in bioinformatics and computational biology. Research interests are in understanding molecular networks and how they are altered in human disease as well as data-mining large-scale molecular omics data to better understand tumor development. As program lead for Big Data in Biology, Rob is also interested in how best to teach data-science to biological and biomedical scientists.
"
https://www.turing.ac.uk/people/guest-speakers/phil-blunsom,"
Phil Blunsom is the Chief Scientist of Cohere, a Professor of Computer Science at the University of Oxford, and a Senior Research Fellow of St Hugh's College. At Cohere he leads the Foundation Models team which builds the infrastructure for training large commercial language models. From 2014 to 2021 he founded and led the Natural Language Research group at DeepMind London.
"
https://www.turing.ac.uk/people/researchers/ioannis-kosmidis,"
Ioannis Kosmidis is a Professor of Statistics at the Department of Statistics, University of Warwick, Theory and Methods Challenge Fortnights Lead, and Turing Fellow at The Alan Turing Institute, the UK's national institute for data science and artificial intelligence.
He obtained his BSc in Statistics from the Athens University of Economics and Business in 2004. He was awarded a PhD in Statistics in 2007 at the Department of Statistics, University of Warwick with a thesis titled ""Bias reduction in exponential family nonlinear models"". He then held an appointment as a CRiSM Research Fellow until 2010. In September 2010, he joined the Department of Statistical Science at University College London as a Lecturer (equivalent to Assistant Professor), and he got promoted to Senior Lecturer (equivalent to Associate Professor) in 2015. He moved back to University of Warwick as a Reader in Data Science in January 2018, and got promoted to Professor of Statistics  in August 2021.

Ioannis' theoretical and methodological research focuses on optimal estimation and inference from complex statistical models, penalized and pseudo-likelihood methods and clustering. A particular focus of his work is the development of efficient, in terms of computational complexity and implementation, algorithms for applying the methods he develops to prominent data-analytic scenarios. He is doing extensive work in producing corresponding, well-documented, open-source software that delivers the methodological advances to the data science community and beyond (see his software page for information). Ioannis also actively engages in a range of cross-disciplinary applications (e.g. applications in earthquake engineering, finance, sport and health analytics, and neuroscience), particularly in settings where statistical modelling and the associated algorithms can impact policy-making. 
He is a founding member of the Data science for sports, activity, and well-being Turing Interest Group, before which he led and run the Statistics in Sports and Health research group at UCL between 2014 and 2017.
He is an associate editor for Biometrika and the Journal of Statistical Software, and a member of the Research Section of the Royal Statistical Society. Detailed, up-to-date information on his research, teaching, enabling and engaging activities can be found on his website and his CV.
"
https://www.turing.ac.uk/people/external-researchers/jess-wade,"
Jess is an excitable scientist with an enthusiasm for equality. She has been involved in several projects to improve gender inclusion in science, as well as encouraging more young people to study science and engineering. Jess won the Institute of Physics (IOP) Early Career Communicator Prize (2015), “I’m a Scientist, Get Me Out of Here!” (2015), the IOP Jocelyn Bell Burnell Award (2016), the Institution of Materials, Mineral and Mining's 'Robert Perrin Award' (2017), the Imperial College Dame Julia Higgins Certificate (2017) and the IOP Daphne Jackson Medal and Prize (2018). Jess sits on the committees of the IOP’s Women in Physics Group, Physics Communicators Group and London & South East Branch. She is on the Council of the Women’s Engineering Society (WES) and Women in Science & Engineering (WISE) Young Women’s Board. In 2017 Jess was the UK representative on a US State Department International Visitor Leadership Program, travelling across America for a month looking at initiatives to recruit and retain women in ‘STEM’. Jess co-led the UK Team at the 2017 International Conference for Women in Physics. She is a keen Wikipedian, and is helping to upload the biographies of women, LGBTQ+ and POC scientists - creating one every day in 2018.
Jess works on organic light emitting diodes that emit circularly polarised light. To achieve this, she creates chiral nanostructures out of carbon-based materials. Jess believes that when it comes to nanoscale molecular engineering; nature is the expert and we humans are only just catching up. Our world and our bodies are full of “chiral” systems – non-superimposable mirror images, like your left and right hand, DNA, or the stacks of fibrous chitin in the shell of a beetle. Understanding how to create and control left and right-handed systems will transform drug discovery, cryptography, the diagnosis of diseases and even our televisions.
"
https://www.turing.ac.uk/news/learn-explore-and-participate,"

[email protected]
"
https://www.turing.ac.uk/research/research-projects/skill-interaction-and-complexity,"
An understanding of the ways skills interact with each other to influence career trajectories of individuals, and the productivity of occupations is key to developing labour market policies (e.g. upskilling programs). Leveraging rich microdata covering multiple countries and novel techniques developed within the computational social science group, this project explores how workplace skills interact with each other to impact outcomes at the micro- and macro-level, as well as their evolution through the labour market.
"
https://www.turing.ac.uk/people/researchers/alexander-harris,"
Within the defence and security programme, Alex works across a research portfolio exploring the application of data science and artificial intelligence to a range of national security and defence challenges and works closely with national and international partners. 
Alex also produces research for the Turing’s Centre for Emerging Technology and Security (CETaS) and has recently co-authored the following research reports: The Future of Open Source Intelligence for UK National Security, Privacy and Intelligence, and Behavioural Analytics and UK National Security.
"
https://www.turing.ac.uk/people/external-researchers/jeremy-rowe,"
"
https://www.turing.ac.uk/people/researchers/filipe-bento,"
Filipe has a PhD in Information and Communication Technologies in Digital Platforms at the University of Aveiro / University of Porto, a Masters in Electronic Information Management from the University of Sheffield, and an Engineering degree in Electronics and Telecommunications from the University of Aveiro.
Before working at the British Library, he was an independent consultant in the areas of digital strategies and initiatives, library technologies, information management, digital policies, Software as a Service (SaaS) and Open Source Software (OSS). Previous to that, he worked at EBSCO Information Services in several roles, firstly as the Discovery Service Engineering and Support Team Manager (Europe and Latin America) and then for three years as the Software Services, Application Programming Interfaces (API) and Applications (Apps) manager. Filipe started his career as a lecturer and then as a computer specialist at the University of Aveiro’s Library, coordinating the development of its information systems for Library’s many branches for over fifteen years.
He is the former chairman of the board of the Portuguese Ex Libris Systems’ Users Association, and a previous member of the DigiMedia Research Center - Digital Media and Interaction at the University of Aveiro.
Filipe joined The Alan Turing Institute for the Living with Machines project in 2022.
 
Filipe is passionate about digital initiatives in libraries having a specific interest in Open Knowledge and Scholarly Communication and the dissemination of scientific information. He is an advocate of (Linked) Open Data and all the related innovative services that support them, not only within academia but also within industry and society in general.
"
https://www.turing.ac.uk/people/researchers/lukasz-szpruch,"
Lukasz is a Professor at the School of Mathematics, the University of Edinburgh, and the Programme Director for Finance and Economics at the The Alan Turing Institute, the National Institute for Data Science and AI. At Turing, he is providing academic leadership for partnerships with the National Office for Statistics, Accenture, Bill and Melinda Gates Foundation and HSBC. He is the Principle Investigator of the research programme FAIR on responsible adoption of AI in the financial services industry. Lukasz is also a co-Investigator of the UK Centre for Greening Finance & Investment (CGFI). He is also an affiliated member of the Oxford-Man Institute for Quantitative Finance.
Before joining Edinburgh, he was a Nomura Junior Research Fellow at the Institute of Mathematics, University of Oxford.
Lukasz's broad research interests span probability theory, stochastic analysis and theoretical machine learning. He is currently researching on the mathematical foundation of deep learning, mean-field models, (inverse) reinforcement learning, game theory and multiagent sysmtems, sampling and optimisation algorithms, computational optimal transport, and the theory of gradient flows. He is also interested in applications of these areas in finance and economics.
For updates on his research, see his Google Scholar page. You may also check his profile on the Research Gate or the MathSciNet
"
https://www.turing.ac.uk/research/research-programmes/health-and-medical-sciences,"
AI and data science will improve the detection, diagnosis, and treatment of illness. They will optimise the provision of services, and support health service providers to anticipate demand and deliver improved patient care.
The Turing’s health programme is delivering a schedule of major research into the theory and methods of AI, statistics, and data analytics underpinning biomedical and health applications that are enabling scientists to do better science, without compromising respect for privacy and patient trust.
Using data-driven innovation in AI and statistical science, our aim is to accelerate the scientific understanding of human disease and improve human health.
We are now working on several large collaborations including with the pharmaceutical company Roche on missingness in data and predictive modelling, with Alzheimer's Research UK on early detection of neurogenerative diseases and with National Institute for Health and Care Research on the AI for multiple long-term conditions Research Support Facility. 
The Health programme is leading the development of one of Turing’s next Grand challenges - Transformation of healthcare - and we are actively seeking to establish new partnerships and programmes to underpin work in our four priority areas: Cell and Molecular Medicine, Disease prediction and early diagnosis, Clinical trials and clinical evidence evaluation, and Population Health.
To prepare for the next phase of research at the Turing, we are bringing together stakeholders from different sectors to scope the future of AI in health – a report from our recent activities in two priority areas – Cell and Molecular Medicine and Population Health can be found here: Scoping the landscape for the future of AI and machine learning in health .
The Turing-Roche strategic partnership was established in the summer of 2021 and is a five-year partnership focused on advanced analytics that is working to better understand patient and disease heterogeneity and its relevance to clinical outcomes to improve clinical care. 
Our partnership will cover multiple activities to investigate how and why diseases affect people differently and why responses to treatment can vary. We will continue to build upon the connections and collaboration opportunities between The Alan Turing Institute, Roche, and the wider community.
We will be launching further research themes this year as well as building our community through activities such as our Slack Workspace, knowledge share series and newsletter.
Read more about the Turing-Roche strategic partnership
The Research Support Facility (RSF), based at the Turing in conjunction with Swansea University and University of Edinburgh, offers AI and advanced data science support to the research teams funded by AIM and helps foster collaboration.
The facility, led by Dr Kirstie Whitaker and Professor Chris Holmes, works to embed best practices in data security and standards, reproducibility, and public and patient engagement across the research collaborations funded by the programme, ensuring effective knowledge sharing and reinforcing the Turing’s role as a national convenor and capacity builder in data science and artificial intelligence.
Read more about AI for multiple long-term conditions: Research Support Facility | The Alan Turing Institute
The Turing Health Programme is working with the UK’s leading dementia research charity Alzheimer’s Research UK on a global initiative to revolutionise the early detection of neurodegenerative diseases like Alzheimer’s. The project will harness and analyse a wealth of digital data to develop signatures of disease – or “fingerprints” – that can be then detected using wearable technologies, such as smart watches.
EDoN is a collaboration between The Alan Turing Institute, the National Physical Laboratory, and teams from University College London, Newcastle University, Exeter University, University of Oxford and the University of Cambridge, working with funders and clinical delivery partners across the world.
Find out more here: Dementia charity spearheads global initiative to use wearables to revolutionise disease detection | The Alan Turing Institute
 

To learn more or get involved please contact the team:[email protected]
"
https://www.turing.ac.uk/people/former-enrichment-students/adriano-koshiyama,"
Adriano Koshiyama is a Research Fellow in Computer Science at University College London. He is the Co-founder of Holistic AI, a start-up focused on providing assurance of AI systems. He was part of The Alan Turing Institute as an Enrichment Scheme Student during 2018-2019. He has held many roles as a Data Scientist in Retail, Finance, Recruitment, and R&D companies over the last 7 years. Academically, he has published more than 30 papers in international conferences and journals. His main research topics are related to Machine Learning, Finance, Trustworthy AI. He also holds BSc in Economics and an MSc in Electrical Engineering. 
During his PhD, Adriano is focusing on the application of computational statistics and machine learning as building blocks for profitable and reliable investment decisions. Specifically his thesis considers three complementary topics: a) devising a new trading recommendation system that supports decision-making processes of a trader looking for derivatives-based strategies; b) putting forward a complete framework for assessment of algorithmic trading strategies; and c) establishing a protocol to certificate a predictive model performance before deployed in a real setting.
However, due to his unorthodox academic background, Adriano has been always involved in different topics of research over the years. During his Bachelor's, he has been very active in understanding the economic, health & safety and welfare factors that impact beekeepers in Brazil. While completing his Master's Degree, he was involved in projects for the power supply and oil companies, involving new methods in neural networks, evolutionary computing, and fuzzy logic. During a spell in the industry and a fertile interaction with some collaborators, Adriano expanded his research interest in concept drift detection, entrepreneurship and teams assessment, and multi-modal optimisation. Overall, he is very passionate about the applications of data science methods (stats, machine learning, heuristics, etc.) to solve real-world problems.
"
https://www.turing.ac.uk/events/exploring-foundation-models,"
Photo by Alina Grubnyak on Unsplash
This event is now sold out. Please email Katy at [email protected] to request a place on the waiting list.
Foundation models are an important emerging class of artificial intelligence (AI) systems, characterised by the use of very large machine learning models, trained with extremely large and broad data sets, requiring considerable compute resources during training. Large language models (LLMs) such as Open AI’s GPT-3 and Google’s LaMDA are the best-known examples of foundation models and have attracted considerable attention for their ability to generate realistic natural language text and engage in sustained coherent natural language dialogues.
They have also demonstrated limited capabilities in other classic AI domains, such as common-sense reasoning and problem-solving. A key bet with foundation models is that they acquire competence in a broad range of tasks, which can then be specialised with further training for specific applications. Foundation models are already finding innovative applications, such as GitHub’s CoPilot system, which can generate computer code from natural language descriptions (“a Python function to find all the prime numbers in a list”).
The Alan Turing Institute will host a one-day symposium to explore the state-of-the-art in foundation models – how they work, what they are and will be capable of, how they are being and will be used, and how to address the many challenges – both technical and ethical – that they raise.
During the event you will hear from key researchers in foundation models. Topics of interest include but are not restricted to:
A key outcome for this event will be to meet researchers and academics in foundation models and to build a network. There will be networking opportunities throughout the day, including a networking reception at the end of the event. We will be creating a shared mailing with the hope that these will facilitate future collaborations.
A full agenda is available online.
 
We welcome participation from all backgrounds, but this event will be of particular importance to researchers, practitioners, and policy makers with an interest in foundation models. Within the Turing, we aspire to establish a substantial body of work around foundation models in the years ahead: if you have an interest in participating in this work, then we urge you to attend.
Register now
Amend your registration
IET London: Savoy Place
2 Savoy Place
London
WC2R 0BL
"
https://www.turing.ac.uk/people/researchers/daphne-ezer,"
Daphne earned a double BSc in Computer Science and Biology at Duke University as an Angier B. Duke Scholar, and then went on to earn a PhD modelling stochastic gene expression in the University of Cambridge as a Marshall Scholar. She has also conducted biological network inference and bioinformatics research in the Sainsbury Laboratory in the botanic gardens of the University of Cambridge. Daphne is currently Lecturer in Computational Biology at University of York.
Daphne wants to use data science to develop sustainable agriculture in the light of climate change. In particular, she is interested in network inference algorithms to detangle how plants integrate environmental signals to determine how much to grow. She is also interested in computer-aided experimental design—i.e. systematically identifying the most informative biology experiments to conduct next, given the cost and time constraints. Finally, she is helping organise a large crowdsourcing project that aims to identify how fluctuations in light and temperature affect spring onion growth. 
"
https://www.turing.ac.uk/people/research-associates/emma-karoune,"
Emma works as a Senior Community Manager for the Turing RSS Lab and the DECOVID project. She is a core contributor to The Turing Way (an open-source community-led guide to reproducible research), and a member of the Bookdash planning committee, helping to build resources and training for other researchers. 
Emma is a specialist in Archaeobotany with a PhD from the Institute of Archaeology, University College London, and as well as her role at The Alan Turing Institute, Emma is the Principal Investigator on the FAIR Phytolith Project with Historic England and Universitat Pompeu Fabra funded by EOSC Life. She is also involved in a project to develop the use of phytolith research in British archaeology by developing a comprehensive and open reference collection. Emma writes a blog called The Open Archaeobotanist and regularly gives talks about her open archaeological work.
Emma is a Software Sustainability Institute Fellow focusing on promoting a more accessible and inclusive research culture. She is also working with Elixir-UK as a FAIR Data Stewardship Training Fellow to develop training resources for FAIR data management.  Emma works closely with the Open Life Science Programme as a mentor and expert. 
All of her research and community building embraces an Open Scholarship approach in which she is striving to develop more open and sustainable research practices within her discipline and beyond.
"
https://www.turing.ac.uk/people/spotlights/mhairi-aitken,"

My background is in sociology. I have always been really fascinated by the role of public participation in processes around science, technology and innovation. My previous research has examined this in relation to renewable energy developments, health-related data science, and uses of machine learning in finance and banking.
I am an Ethics Fellow in the public policy programme. In the Ethics Theme, a lot of our work involves working closely with public sector organisations – both in the UK and internationally – to conduct research and develop practical guidance and tools to inform ethical approaches to AI.
Over the last few years, there’s been a huge amount of interest in the ethics of AI. However, what is really exciting about working in the Ethics Theme here at the Turing is that we are moving well beyond abstract principles and theories to develop really practical and meaningful ways of pursuing ethical AI. I find that incredibly exciting and rewarding.
Our work is shaping public sector approaches to AI and data science, equipping public sector organisations with the knowledge and tools to ensure that ethics is at the heart of innovation and policy processes.
Yes! This is a real passion of mine. I love finding creative ways of engaging the public in conversations around AI and data. I dabble in stand-up comedy and have performed in several new act comedy nights. For a number of years, I’ve been involved in the Cabaret of Dangerous Ideas (CoDI) at the Edinburgh Festival Fringe. CoDI is a series of shows compered by a professional comedian which pushes academics out of their comfort zones to discuss their research.
This year I will be speaking about the relationships people have with smart devices, and the decisions we make about whether to trust in these technologies. The really exciting thing about CoDI shows is that the majority of a show is reserved for discussions with the audience. Over the years, I’ve found it impossible to anticipate the questions and comments that come up. I love the unpredictability of the discussion and the chance to have really frank – and often hilarious – conversations about the topics I research.
I am especially excited about CoDI this year as for the first time the show will have both an in-person live audience and simultaneously be live-streamed – opening it up to a much bigger audience.
If I hadn’t gone into research I would most likely be a goat farmer.
I’m usually up a hill, in a forest or knee-deep in mud mucking out my ponies’ field!
"
https://www.turing.ac.uk/people/researchers/nilo-pedrazzini,"
Nilo is a Research Associate in corpus-based digital humanities within the Living with Machines project. He holds a BA in Russian and English from the University of Pavia (Italy) and an MSt in Slavonic Studies (Serbian and early Slavic) from the University of Oxford, where he is currently undertaking his DPhil in Linguistics. He specialises in corpus linguistics and the syntax-semantics interface, focussing on the computational modelling and statistical analysis of historical languages.
He has taught general linguistics at the University of Oxford and digital humanities at King’s College London, and he is currently a Fellow at RROx, the Oxford network of the UK Reproducibility Network (UKRN). 
At the Turing, Nilo mainly works on the development of methods for querying large historical corpora at scale and tracking changes in linguistic phenomena.
Nilo is particularly interested in the adaptation of research methods from outside the humanities to humanities research, and in exploring how historical data can help our understanding of real-world issues.
"
https://www.turing.ac.uk/people/researchers/scott-hosking,"
Scott Hosking leads a team of scientists and engineers across both the British Antarctic Survey and The Alan Turing Institute who are focused on the development of AI and digital twin technologies for understanding, monitoring and predicting environmental change.
His projects include the intelligent fusion of data from satellite and in-situ surface sensors to help understand our changing planet; probabilistic machine learning for localised climate impacts; AI for seasonal sea ice forecasting and improving climate models; and computer vision toolkits for tracking environmental change and wildlife monitoring.
Scott is also a Co-Director for the Centre for Doctoral Training (CDT) in the Application of AI to the study of Environmental Risks (AI4ER), a £6m UKRI funded programme to train over 50 top students to become future global leaders in environmental science.
"
https://www.turing.ac.uk/people/former-enrichment-students/david-watson,"
David Watson is doctoral candidate at the University of Oxford and an enrichment student at The Alan Turing Institute. He received his MSc from the Oxford Internet Institute in 2015, studying under the supervision of Professor Luciano Floridi. He went on to become a Data Scientist at Queen Mary University’s Centre for Translational Bioinformatics before returning to Oxford for his DPhil in 2017. He is passionate about promoting more interpretable techniques for analysing complex systems in the social and life sciences.
David is a founding member of the Digital Ethics Lab, where his research focuses on the epistemological foundations of machine learning. He develops new methods for explaining the outputs of black box algorithms, with the goal of better understanding causal relationships in high-dimensional systems. In addition to his academic work, David is a regular contributor to The Economist, where he writes articles and builds models for the Graphic Detail and Game Theory blogs.
"
https://www.turing.ac.uk/people/business-team/jess-spinner,"
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/get-involved-challenge-owner,"

A team of doctoral students, early career researchers, and post-docs from across a broad range of disciplines, whose quick, agile thinking will help tackle your challenge. A facilitator is also pre-selected by the Institute for each challenge to help the team fulfil their potential and ensure everybody is engaged.
On the last day of the event your Challenge Team will present their findings. Following the week, you will be sent a report detailing approaches explored and possible further avenues to investigate, along with any code generated. The reports and code (with embargoes or minor redactions if necessary to protect any commercial or other sensitivities) will later be published on the Institute’s website, so researchers can evidence their participation.
Many organisations who propose a challenge for a Data Study Group continue their engagement with the Institute and with the participants themselves. This could be through collaborating on an academic paper or conference submission, deeper investigation of results and recommendations, or even recruiting participants.
""The Data Study Group week was one of the best experiences I’ve had working with external suppliers of any kind in my eight years in the Ministry of Defence. A large team of very capable academics worked extremely hard on my problem and demonstrated multiple approaches to solve it. To have so many interesting approaches thoroughly explored in such a short time period is very impressive and incredibly useful to our organisation.""
'Matt', Dstl (Defence Science and Technology Laboratory)
 
Preparing for a successful Data Study Group requires a commitment and buy-in from you and your team. The process usually needs around five to six months of preparatory time; this can sometimes be accelerated for challenges which already have a high readiness level.
If your challenge is accepted for a Data Study Group, the Turing's team will work with your organisation to complete preparations, and our DSG Science Leads will help you shape your challenge so that your organisation gets the most out of the week.
We require a representative of each DSG Challenge Owner, with the necessary technical background (e.g. an analyst, engineer, scientist), to take part in the week embedded alongside the Challenge Team; this is an excellent opportunity for the representative to learn about new data science approaches and enhance their professional development. This person should not direct the team, but should be someone who can provide insight into the business problem, the dataset, and any approaches that have been tried before.
We also welcome challenges from academia. They will need to go through the same process as other organisations and should submit a challenge proposal form or an enquiry below, with an overview of the challenge to be presented.

 
If a Data Study Group is not right for your organisation or your particular challenge, there are other ways for you to engage with the Turing and with our researchers.
 
 
If you would like to enquire about being a DSG Challenge owner please fill in the enquiry form below.
Please take our survey
Data Study Group challenges can be a great launchpad for follow-on collaboration and research. Below is a selection of research that has in some way come out from a previous Data Study Group or where the Data Study Group has been part of a larger body of research work.
1.9 MB
"
https://www.turing.ac.uk/research/research-projects/optimising-analysis-network-graphs,"
Large-scale analysis of symbolic representations of networks, or graphs, is fundamental to research in a number of fields, from cyber security to biology. However, such analysis methods often require expensive supercomputing resources, that the methods are not always designed efficiently to utilise. Similarly, the resources themselves are often not optimised to run such analyses. This project aims to investigate how to optimise graph processing applications to achieve substantially improved performance on multi-core processing devices, such as those produced by Intel, to benefit UK industry and research.
 
In graph theory, a graph is a symbolic representation of a network and of its connectivity as a set of linked nodes. Analysing these graphs can provide insights into complex relationships present between different types and feeds of real-world data, that are otherwise difficult to obtain.
Large-scale graph processing is increasingly becoming a fundamental component of a variety of data science workloads, from diverse domains including cyber security, machine learning, and computational biology. It is therefore of paramount importance to industry, academia, and several of the Turing’s research projects.
Increases in the data volumes being produced, and pressure to reduce time-to-solution for end-users, mean that the processing of large graphs frequently requires expensive supercomputer resources. Current performance of these workloads on existing supercomputer systems is frequently unsatisfactory, and likely future development trends in the design of supercomputer architectures will represent further challenges. Improvements in performance are therefore likely to come from advances in implementing analyses which more efficiently exploit the parallel structure of current and future supercomputer systems.
This project will undertake research to determine whether novel hardware technologies can be utilised for improving the current state-of-the-art, and examine how software applications should be enhanced in order to best utilise these technologies. 
The project will take an end-to-end view of the entire analytics pipeline to obtain information on how best to improve processor and system design, and how existing and future graph applications should be implemented in order to optimally utilise current and future supercomputer architectures.
In particular, this project will collaborate with Intel to improve performance on their multi-core processing devices, such as the Intel Xeon Phi and the Intel Xeon architecture, as well as other emerging processing technologies. The utility of novel memory technologies, such as Intel 3D XPoint, will also be examined as a vehicle for further improving the capabilities of graph analytics.
The project will also assess the performance of the recently proposed GraphBLAS specification, which aims to define standard building blocks for graph algorithms. Additionally, the techniques for improving the programming models, and the performance of the underlying storage models, supporting these applications will also be examined.
Lastly, software technologies will also be developed in order to improve the productivity of researchers implementing graph processing applications.
All development work undertaken will be based on open standards, and outputs from the projects will, wherever possible, be released as open source. These outputs should help ensure that UK data science researchers remain at the forefront of the field, as well as enable UK industry to benefit from the resulting capability improvements which this research aims to deliver.
Members of the Research Engineering Group at the Turing are contributing their expertise to this project.
They have been working alongside EPCC and Intel on performance and scalability analysis of graph algorithms on existing CPU architectures, and to develop a model of the performance of these algorithms for emerging specialised processor architectures.
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai,"
Alan Turing first considered the question, ""Can machines think?"" in his seminal paper, Computing Machinery and Intelligence, published in 1950. Since that time, major advances in computer power, colossal increases in the volume of data being produced every day, and a global economy waking up to the value of data analytics have meant that artificially intelligent software and tools – ‘thinking machines’ – are everywhere: from speech recognition software and book and film recommendations, to facial recognition and medical diagnosis from images.
It is a technology which, like data science, has the potential to transform the world we live in. As innovation in AI grows, there is an opportunity – and responsibility – to ensure that artificially intelligent systems are built to contribute to the public good and to a well-functioning economy, with fairness, reliability, security, and appropriate transparency and privacy at its core.
Watch this video to learn about the Turing's AI programme, its background and purpose as well as its activities. If you would like to know anymore about the programme or discuss collaboration opportunities, please get in touch with the team [email protected]   

The AI programme’s goal is to advance world-class research into artificial intelligence, its applications and its implications for society, building on the wealth of expertise and knowledge across our academic network.
Our programme expertise draws on the cross-disciplinarity at the heart of the Turing, bringing together technical experts to achieve breakthroughs in AI research and its applications, with social scientists, ethicists, legal experts, industry, and policy makers who are considering the use of AI in real-world applications.
We will contribute to leadership around this area, drawing on our links with industry and government to help ensure that the UK remains at the forefront of scientific innovation while building an ethical and regulatory framework for the use of AI that prevents misuse and inappropriate discrimination.
The initial challenges focus on three areas: safe and ethical AI; robotics and autonomous systems (RAS); and multi-agent systems (MAS), which all contribute to the Institute’s mission to build a world-leading centre for AI research. 
A new report, led by Dr Marion Oswald, explores police perspectives on the algorithmic transparency standard. Read now.
To find out more about some of the work linked to the programme challenges, explore our safe and ethical AI and robotics pages.
Follow @turinginst and join our mailing lists for regular updates on our research activities and outreach.
[email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/science-and-technology-facilities-council,"
stfc.ukri.org
"
https://www.turing.ac.uk/people/business-team/sarah-nietopski,"
"
https://www.turing.ac.uk/news/publications/data-diff-interpretable-executable-summaries-changes-distributions-data-wrangling,"
Many analyses in data science are not one-off projects, but are repeated over multiple data samples, such as once per month, once per quarter, and so on. For example, if a data scientist performs an analysis in 2017 that saves a significant amount of money, then she will likely to be asked to perform the same analysis on data from 2018. But more data analyses means more effort spent in data wrangling. We introduce the data diff problem, which attempts to turn this problem into an opportunity. Comparing the repeated data samples against each other, inconsistencies may be indicative of underlying issues in data quality. By analogy to text diff, the data diff problem is to find a “patch”, that is, transformation in a specified domain-specific language, that transforms the data samples so that they are identically distributed. We present a prototype tool for data diff that formalizes the problem as a bipartite matching problem, calibrating its parameters using a bootstrap procedure. The tool is evaluated quantitatively and through a case study on an open government data set.
Sutton, C, Hobson, T, Geddes, J & Caruana, R 2018, Data Diff: Interpretable, Executable Summaries of Changes in Distributions for Data Wrangling. in Knowledge Discovery and Data Mining Conference 2018. Knowledge Discovery and Data Mining Conference 2018, London, United Kingdom, 19/08/18.
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/estimation-theory,"
"
https://www.turing.ac.uk/people/research-fellows/cecilia-mascolo,"
Cecilia Mascolo is Full Professor of Mobile Systems in the Computer Laboratory, University of Cambridge, UK. Prior joining Cambridge in 2008, she has been a faculty member in the Department of Computer Science at University College London. Her research interests are in human mobility modelling, mobile and sensor systems and networking and spatio-temporal data analysis.  
At The Alan Turing Institute, Professor Mascolo hopes to research on aspects related related to interpretation and inference of mobile and wearable sensor data efficiently on devices, on how to make sense of this kind of data in ways which respect its fine grained spatial and temporal granularity. She hopes then to look at how to use the interpretation to improve systems and interventions for users. Her research is highly interdisciplinary and applications of this research will span various other disciplines like, to name a few, urban data science, mobile health and organisation analytics.
"
https://www.turing.ac.uk/news/publications/diachronic-word-embeddings-19th-century-british-newspapers,"
Diachronic word embeddings trained on a 4.2-billion-word corpus of 19th-century British newspapers, using Word2Vec and the following parameters:
sg = True
min_count = 1
window = 3
vector_size = 200
epochs = 5
The embeddings are divided into time slices of ten years each, with the vectors from each decade aligned to the ones from the most recent decade (1910s) using Orthogonal Procrustes.
Source code used to train the embeddings and tools to carry out time-series analysis (e.g. change point detection): https://github.com/Living-with-machines/DiachronicEmb-BigHistData
Pedrazzini, Nilo & Barbara McGillivray. (2022). Diachronic word embeddings from 19th-century British newspapers [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7181682
131 KB
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/lloyds-register-foundation,"
Lloyd’s Register Foundation is a charity that helps to protect life and property and supports education, engineering-related research and public engagement.
It has partnered with the Turing on an ambitious programme on data-centric engineering. The programme brings together world-leading academic institutions and major industrial partners from across the engineering sector, driving new advances in research, innovation and education in data-centric engineering.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-southampton,"
"
https://www.turing.ac.uk/people/external-researchers/katrin-solveig-lohan,"

"
https://www.turing.ac.uk/people/guest-speakers/maria-fox,"
Maria Fox is a Professor of Artificial Intelligence and an expert in Automated Planning. She joined BAS in 2021 after 5 years in industry, working in plan-based automation at Schlumberger Cambridge Research. Before that she spent more than 20 years in the UK university sector, taking an active part in the international research field of AI Planning. She has worked on a large number of projects concerned with intelligent automation of robotic and cyber-systems. Her work at BAS has been focused on automating passage and mission planning for the fleet of underwater vehicles and, most recently, the RRS Sir David Attenborough.
"
https://www.turing.ac.uk/research/research-programmes/finance-and-economics,"
The finance and economics programme was founded in 2016. Initially the programme played a key role in fostering economic data science research, an emerging discipline at the intersection of economics and machine learning. Over the last five years the programme has grown exponentially and is today a nationally and internationally recognised research and innovation hub with the vision to enable a more resilient, secure, inclusive and productive economy through digital innovation.
Our work includes developing models to better understand economic networks through transaction data and economic nowcasting, developing privacy-enhancing technologies to improve data flow across government departments and working with regulators, and building monitoring systems for detecting market collusion and systemic risks.

The programme seeks to foster innovation and leverage our expertise through strategic, boundary-breaking collaborations with industry, academia, third sector and beyond.
We have a proven ability to set up and lead impactful national and international partnerships and collaborations, which range from project partnerships to multi-year, multi-party strategic partnership programmes. Key examples include our partnerships with HSBC, the Office for National Statistics (ONS), Accenture and the Bill and Melinda Gates Foundation.  
We believe that by bringing industry and academia together we can combine our unique skill sets to tackle the challenges towards a more inclusive and productive digital economy.
The finance and economics programme has three main research challenges.
1. AI and data science for socio-economic systems
Improved resilience and economic security through:
 
 2. Safe and trusted AI systems for business and industry  
Improved productivity, and more inclusive, fair and robust services through:
 
 
3. Decentralisation and democratization of technology
Increased inclusion, efficiency, resilience, and security through:
​​​​​​You can get in touch by email at [email protected].

"
https://www.turing.ac.uk/research/research-projects/artificial-intelligence-data-analytics-aida/why-practical-data-analytics-still-so-painfuland-how-we-can-help,"
This article is based on a presentation that was prepared for DataTech20 in Glasgow and The Turing Present: AI UK in London, both of which were due to be held in March 2020.
We hear so much about artificial intelligence (AI) and deep learning these days that it is tempting to believe that such approaches to extracting insight and predictions from data can now solve a wide variety of problems. There are indeed notable successes, such as predicting the objects present in an image, but images are a particular kind of rather clean data. However, many areas give rise to messy data, and the analyst often needs to spend up to 80% of their time cleaning and preparing such data (a process known as data wrangling). Examples of messy data include electronic healthcare records, published government tables, and household survey data.
The 'Artificial intelligence for data analytics (AIDA)' project based at The Alan Turing Institute is an initiative to address these problems. In this post we'll take a closer look at the challenge of messy data and share a few tools and insights that we've developed to make the lives of data scientists and data engineers easier.
In the AIDA project we have identified the following data engineering challenges:
First, it is important to realise that predictive models (such as deep neural networks) are just one part of the data analytics process. This process starts with a problem specification, asks what data are available to address this question, then prepares the data before carrying out predictive modelling. After this the model needs to be evaluated and then deployed. This is not a linear process, there can be many feedback loops that require revisiting earlier steps.
Second, whilst research such as that being done on self-driving vehicles ultimately aims to remove the human aspect, we find that for the process of data cleaning human insights can be crucial. Thus we do not aim to fully automate data engineering but instead focus on 'semi-automated' tools that keep the human in the loop, and let the analyst guide the overall process and provide key insights.
AIDA combines multidisciplinary work from machine learning, semantic technologies, and programming languages to:
Of course we are not the only people to work on problems in data engineering, this is an interdisciplenary topic that benefits from insight from the databases, data mining, and programming languages communities. Below we give several examples of tools and insights generated by the AIDA project.
(Other work done in the AIDA project includes the filling in of missing data in tables, see 'Handling Incomplete Heterogeneous Data using VAEs', and identifying and repairing outliers in tabular data, see 'Robust Variational Autoencoders for Outlier Detection in Mixed-Type Data'.)
Often an analyst is given a number of datasets from successive time periods (let's say different years). If each set has the same structure and format it is easy to collate them together. But usually this isn't the case: column names might have changed, the columns have been re-ordered, new columns have been added, etc. Datadiff is an AI assistant that seeks to match the columns between dataset A and dataset B, based on the idea that there should be a simple transformation that makes the distribution of the data in the matched columns the same. This might involve swapping, inserting or deleting columns, a linear transformation (e.g. change of units) etc.
For each pair of columns in A and B, we first identify the best single transformation between them and compute an associated weight. We then find the best weighted bipartite matching to identify how the columns of A and B should match up. For example we have used datadiff to match data about UK broadband performance over various years. 
See the related publication here and code is available via GitHub.

 
To analyse data properly, we need to know its datatype, e.g. integer, date, string. But missing and anomalous data can mess up the automatic identification of data types, as shown in the graphic below. If we are not able to identify ""Error"" as an anomalous entry, and and ""Null"" as an indicator of missing data, these columns will be identified as being of string type, which is incorrect and cause problems downstream if not addressed.
Our solution is a method for probabilistic type inference. This seeks to explain the data in each column in terms of a model that can generate appropriate entries, or anomalies or missing data indicators. We have built probabilistic finite state machines (PFSMs) for each datatype, and then run probabilistic inference to infer the column type given the data. Experiments show that ptype performs better than its competitors in identifying column types, and is particularly strong on identifying anomalous or missing type cells.
See the related publication here and code is available via GitHub.

 
You might think that a CSV file would be comma separated, right? But unfortunately real life is not that simple. In a CSV file one needs to specify the column delimiter, quote character and escape character to specify the ""dialect"". CleverCSV seeks to automatically identify the dialect, based on an objective function that maximises both the uniformity of rows by pattern, and the uniformity of the identified columns by type. The method achieves 97% overall accuracy on large corpus of CSV files, and improves accuracy on messy CSV files by 22%. Together with ptype, this paves the way to automatically converting datasets from CSV format to properly typed data frames ready for analysis.
See the related publication here, code available via GitHub, additional motivation provided here, and a relevant discussion here.
 
Suppose we have a column of data with entries which include ""Mute swan"", ""Eider duck"" and ""Wandering albatross"". A human would likely suspect that the column describes types of bird, or perhaps a more general class like ""species"". Identifying such semantic column types can be useful for data quality checking, and potentially for integrating other information known about the semantic type. 
How can an AI assistant predict the semantic type from the data itself? ColNet works by first retrieving column cells’ corresponding entries in a relevant knowledge base, and returning the classes of the matched entities as a set of candidate classes for annotation. It then scores each candidate class using a customised neural network classifier. ColNet was shown to outperform state-of-the-art approaches.
See the related publication here and code is available via GitHub.
 
The standard notebook architecture for data science (e.g. Jupyter) involves the notebook interacting with a server. However, this has limitations regarding reproducibility, lacks the ability to rollback state, and is limited to a single programming language. Wrattler is an architecture that overcomes these issues, by allowing for reproducibility and versioning, polyglot programming, and providing a platform for AIDA's AI assistants.
See the related publication here and code is available via GitHub. A demo is also available at wrattler.org.

 
One of the main problems with research on data wrangling and data cleaning is that few organisations are willing or able to share both the original messy data and the final clean data. Datasets shared on Kaggle or other platforms usually come pre-cleaned and ready to use, even though it is incredibly valuable for data scientists and researchers to see the process of data cleaning and understand all the steps that needed to be performed. 
To further understanding of the process of data wrangling, the AIDA team has curated four datasets and documented the data wrangling process. These datasets are from different domains and include the Tundra Traits dataset of plant measurements, the Household Electricity Survey, a dataset of Electronic Health Records, and the aforementioned Ofcom Broadband survey. For each dataset the analysis and data wrangling process is available online.
 
A common view is that up to 80% of a data mining project is spent on data understanding and data preparation. The AIDA team seeks to produce AI assistants and a software framework to reduce the time and effort needed for data engineering tasks. The work highlighted above illustrates a diverse set of issues covering data organization and quality. So far we have focused on tabular data, but data engineering issues also apply to time series, images, graph data etc. There is still lots to do!
The AIDA project has received funding from The Alan Turing Institute, the UK Government’s Defence and Security Programme in support of The Alan Turing Institute, and starter funding from the Lloyds Register Foundation.
"
https://www.turing.ac.uk/news/collaboration-boosts-research-intersection-climate-science-and-ai,"
The collaboration will drive new, cross-disciplinary research at the interface of data science, AI, climate science and weather forecasting. This important new alliance demonstrates the Institute’s continued commitment to tackling climate change and its impacts through data science and AI research.
The Met Office is the UK's national weather service, providing world-leading climate science and critical weather services to the government, industry, emergency responders and the public. They are pushing the boundaries of scientific, technological and operational expertise with one of the most powerful supercomputers of its kind in the world and have ambitions to address grand challenges of the future through research in data science, machine learning and AI.
Together, researchers at the Turing and Met Office will draw on their combined expertise to address grand challenges in weather and climate science, data sciences and AI. They will help deliver research advances in areas key areas of common interest, including: the interfaces between climate and the loss of biodiversity; high-impact weather events and society; research at the interface between climate science and AI; predictive data analytics for air quality; and physics-driven machine learning in weather forecasts.
A key element of the collaboration will be delivered through the recently established Joint Centre for Excellence in Environmental Intelligence: a partnership between the Met Office and the University of Exeter. Kick-starting the collaboration will be two joint projects, launching early next year. These will focus on environmental models, bringing different spatial scales together, from surface sensors to satellite sensors, and the impact of climate change on agriculture. This work is funded via the Turing's AI for science and government programme, which aims to deploy AI and data science in priority areas to support the UK economy and society.
Jon Rowe, Programme Director for Data Science for Science at The Alan Turing Institute, said “This significant new collaboration will help to address a range of critical challenges in climate science. Being able to build a digital picture of our natural environment will allow for better monitoring of the impacts of climate change in agriculture, biodiversity, oceans, land, water, and the cryosphere.” He goes on say “The unique combination of expertise from the Turing and the Met Office will provide opportunity for innovative, and impactful, research outcomes.”
“This significant new collaboration will help to address a range of critical challenges in climate science. Being able to build a digital picture of our natural environment will allow for better monitoring of the impacts of climate change in agriculture, biodiversity, oceans, land, water, and the cryosphere.""
Kirstine Dale, Met Office Principal Fellow and Co-Director of the Joint Centre for Environmental Intelligence said: “Data Science is a rapidly developing area, with countless applications across science and technology. I’m delighted the Met Office has this opportunity to work closely with The Alan Turing Institute as the UK’s national institute for data science and artificial intelligence. We will be exploring how data science can help transform our understanding of a changing environment at an online Environmental Intelligence conference this week , featuring a keynote address from Professor Sir Adrian Smith FRS, Institute Director and Chief Executive of The Alan Turing Institute.”
"
https://www.turing.ac.uk/people/former-enrichment-students/bertrand-nortier,"
Bertrand is a PhD enrichment scheme student. He is currently reading for a PhD at the School of Mathematics, University of Bristol in the area of computational statistics under the supervision of Simon N. Wood. His research interests include regression, regularisation and optimisation for statistics.
Quantile regression became an important tool in statistics that allows to obtain a better picture of the relationship between an observed variable and regressors. Quantile additive models (QAMs) are non-linear multivariate extensions of linear quantile regression. One of Bertrand's current goals is to develop efficient methods to estimate QAMs together with a publicly available software (R package).
"
https://www.turing.ac.uk/sessions/creating-economic-and-societal-impact/,"


"
https://www.turing.ac.uk/news/turing-work-university-sydney-data-science-research,"
The Alan Turing Institute and the University of Sydney's Centre for Translational Data Science have signed a memorandum of understanding to collaborate on joint research projects of strategic importance to the Australian economy, including  criminology, air quality, and geosciences. The collaboration will be centred around The Alan Turing Institute’s data-centric engineering programme, a major research programme funded by the Lloyd's Register Foundation.
The Centre for Translational Data Science was established in the University of Sydney in 2016 and is one of only a few centres worldwide that has a research focus of modelling complex phenomena and the translation of this research into practical outcomes which benefit society.
""Three areas of collaboration in data science already underway are space-time models for environmental air quality monitoring and improvement, probabilistic modelling for Australia’s natural resources, and Bayesian optimisation for criminology. All of these areas of collaboration will make a real difference in the world,"" said Professor Sally Cripps, Co-Director of the Centre for Translational Data Science.
The first collaboration focuses on an air quality and improvement project. The data-centric engineering programme is already working with the London Mayor’s Office on a joint project to understand and improve air quality over London by developing advanced spatio-temporal statistical and machine learning methods for estimating and forecasting air pollution levels at a hyper-local scale. These are further linked to critical monitoring stations and policy interventions.
The Centre for Translational Data Science is working with the Nature Conservation Fund to develop statistical machine learning models to assess the impact that ‘greening’ cities has on air quality, and how this improvement in air quality affects health outcomes, vital for sustainable cities of the future.
The second project has the two centres working with statisticians, machine learners and earth scientists from the Universities of Sydney and Western Australia to use the latest advances in data science to transform the process by which decisions are made in the management of natural resources.
Discussions are underway with IAG, McKinsey, Rio Tinto, Lloyd's Registry, and government agencies to form a multidisciplinary partnership to drive transformational advances in the earth sciences to build scale and human capacity in the resources and environment industries.
“The world’s economic, societal and environmental future depends upon the balance we place on competing outcomes when making decisions and policy surrounding the use of our natural resources.  Despite the importance of this issue, many decisions and policies are based on limited amounts of data and rudimentary data analysis, with little appreciation of the critical role that understanding and quantifying uncertainty plays in the process. 
The collaboration will allow data science researchers in Australia to tap into the wealth of experience that exists within the Turing to revolutionise the way decisions are made in this important industry sector,” said Professor Cripps.
“This collaboration opens up huge potential for exciting new projects. We look forward to exchanging researchers with the University of Sydney to strengthen the existing Memorandum of Understanding and jointly-funded collaborations we have,” says Professor Mark Girolami, Programme Director for Data-Centric Engineering, The Alan Turing Institute
“This partnership represents an unrivalled opportunity to transform many domestic and international industries. The combined skill set of the international collaboration between the two institutions will result in outstanding outcomes that neither could achieve alone,” concluded Professor Girolami.
The Centre for Translational Data Science criminology research is led by Dr Roman Marchant, who works closely with Turing Fellows, Associate Professor Theo Damoulas and PhD student Louis Ellam.
“Statistical models can help us to understand criminal behaviour and determine the key drivers and dynamics of crime. In particular, to achieve a future reduction in crime arising from data-driven and informed policy decisions,” says Dr Merchant.
Their collaboration is developing new Bayesian optimisation algorithms to uncover hidden patterns in criminal activity.
“Modelling the dependence between different types of crime leads to greater understanding of the dynamics criminal behaviour. The Turing is looking forward to continuing this research with colleagues at the Centre for Translational Data Science,” said Associate Professor Damoulas.
"
https://www.turing.ac.uk/news/uk-japanese-projects-explore-effects-ai-society-and-economy,"
UKRI recently announced (on 22 January) six innovative projects set to uncover the multiple, uncertain and wide-ranging impacts Artificial Intelligence (AI) could have on our society, culture and economy.
The projects will boost our understanding of how AI technologies affect people’s lives, from its use in healthcare to its potential to transform housework, and the ethics of using AI to make legal decisions.
The projects cover a wide range of topics including its effects on our happiness and wellbeing, its economic implications for skills, work and education, to the transparency, responsibility, governance and ethics of using AI.
This includes one between The Alan Turing Institute and RIKEN—Japan's largest comprehensive research institution— is PATH-AI: Mapping an Intercultural Path to Privacy, Agency, and Trust in Human-AI Ecosystems. It will jointly study the values of privacy, agency, and trust from a comparative and intercultural perspective looking at both Japan and the UK.
Ultimately, the goal of the research is to initiate a truly inclusive and international conversation on the difficult issues surrounding the global AI ethics and governance landscape. UK-based and Japan-based researchers will work together very closely, providing the basis for a strong and far-reaching UK-Japan research collaboration.
Commenting on the announcement, David Leslie, one of the project’s Principle Investigators and the Turing’s Ethics Theme Lead and Ethics Fellow in the public policy programme said:
“As the UK’s national institute for data science and AI, the Turing is committed to tackling society’s biggest challenges by advancing world-class research and applying it to real-world problems. Part of doing this responsibly involves acknowledging that these problems often don’t have borders, and neither do the innovations in AI and data science that we develop to address them.
“We are extremely excited to take a step toward steering the direction of innovation and technology policy on a global stage in collaboration with our Japanese colleagues from RIKEN. Our project will include a comparative investigation of key concepts and empirical problems in Japanese and British contexts, a commissioned art exchange based on this research and public engagement, and the unique co-design of an interculturally-informed governance framework for responsible AI innovation.
“We are very grateful to UK Research and Innovation as well as to the Economic and Social Research Council and the Arts and Humanities Research Council for making this important and exciting research possible.”
The projects have been funded through UK Research and Innovation’s (UKRI) Fund for International Collaboration (FIC) in a joint UK-Japan initiative. The Economic and Social Research Council (ESRC) and the Arts and Humanities Research Council (AHRC), both part of UKRI, contributed £2.4m via FIC, while the Japanese Science and Technology Agency (JST) contributed ¥180m. 
The projects will each run for three years beginning in January 2020.
Contact
For media enquiries, contact [email protected] or Tamera Jones, 0734 202 5443, [email protected]
Notes
 
"
https://www.turing.ac.uk/people/guest-speakers/aleksander-madry,"
Aleksander Madry is the Cadence Design Systems Professor of Computing at MIT, leads the MIT Center for Deployable Machine Learning as well as is a faculty co-lead for the MIT AI Policy Forum. His research interests span algorithms, continuous optimization, and understanding machine learning from a robustness and deployability perspectives. 
Aleksander's work has been recognized with a number of awards, including an NSF CAREER Award, an Alfred P. Sloan Research Fellowship, an ACM Doctoral Dissertation Award Honorable Mention, and Presburger Award. He received his PhD from MIT in 2011 and, prior to joining the MIT faculty, he spent time at Microsoft Research New England and on the faculty of EPFL.
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/people,"

"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/oxquant,"
oxquant.com
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/nats,"
www.nats.aero
"
https://www.turing.ac.uk/people/alan-lowe,"
Alan R. Lowe is Associate Professor of biophysics at University College London (UCL). He has a Ph.D. in biophysics from the University of Cambridge and conducted his postdoctoral research in the physics department at the University of California, Berkeley. His research is focused on integrating tools from computer vision, machine learning and data science, with problems in experimental biology.  
 
 
Alan’s interests include convolutional neural networks, probabilistic models, multi-object tracking and the use of unsupervised learning to build meaningful representations of systems from data. 
 
 
"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/agent-based-modelling,"
"
https://www.turing.ac.uk/news/publications/riemannian-stein-kernel-method,"
This paper presents a theoretical analysis of numerical integration based on interpolation with a Stein kernel. In particular, the case of integrals with respect to a posterior distribution supported on a general Riemannian manifold is considered and the asymptotic convergence of the estimator in this context is established. Our results are considerably stronger than those previously reported, in that the optimal rate of convergence is established under a basic Sobolev-type assumption on the integrand. The theoretical results are empirically verified on S^2.
Barp A, Oates CJ, Porcu E, Girolami M. A Riemannian-Stein Kernel Method. 2018.
"
https://www.turing.ac.uk/people/researchers/max-little,"
Dr Max Little is a world-leading expert in signal processing and machine learning. His research in machine learning for digital health is highly influential and is the basis of advances in basic and applied research into quantifying neurological disorders such as Parkinson's disease. He has published over 60 articles in the scientific literature on the topic, two patents, and a textbook. He is an advisor to government and leading international corporations in topics such as machine learning for health.
"
https://www.turing.ac.uk/people/external-researchers/hayley-bennett,"
"
https://www.turing.ac.uk/people/researchers/andrea-baronchelli,"
Andrea Baronchelli is Professor of Complexity Science at City University of London, Token Economy theme lead at The Alan Turing Institute, and a aresearch associate at the UCL Centre for Blockchain Technologies.
Watch: Shaping the world through research with Dr Andrea Baronchelli
Andrea investigates the dynamics of decentralised socio-technical systems using concepts and tools from the physics of complex systems, network science and data science.
Topics he researches include the emergence and dynamics of social norms and shared category systems, tipping points in collective behaviour, polarisation in social networks, blockchain & crypto ecosystems, and human mobility.
Andrea's work has been supported by public and private institutions (UKRI, ESRC, InnovateUK, UK Govt., PayPal, etc). In 2019, Andrea received the Young Scientist Award for Socio and Econophysics from the German Physical Society.

 
"
https://www.turing.ac.uk/people/researchers/weisi-guo,"
Dr. Weisi Guo is a Professor of Human Machine Intelligence at Cranfield University, and an Honorary Professor at University of Warwick. He graduated from the University of Cambridge with degrees in General Engineering and Computer Science.
Dr Guo's research expertise is in information and networks. In terms of applications, he is particularly interested in interdependent critical infrastructures and urban analytics for defence and security. His pioneering research in molecular signalling and complex networks has won him a number of awards & nominations. At the Turing, he focuses on using complexity and data science techniques to improve engineering safety and reduce conflict likelihood [1].
[1] ""Retool AI to Forecast and Limit Wars,"" Nature, 562, 331-333 (2018)
IET: Innovation Award 2015
Bell Labs Prize: Finalist 2014, Semi-Finalist 2016
IEEE: Best Paper and Exemplary Reviewer
"
https://www.turing.ac.uk/people/business-team/alden-conner,"
Dr Alden Conner is a Senior Researcher for Research Applications at The Alan Turing Institute. Previously, she worked as a product manager for optical microscopy systems and as a consultant with the Chan Zuckerberg Initiative to facilitate open science practices in the Neurodegeneration Challenge Network. She holds a PhD in Molecular and Cell Biology from the University of California, Berkeley.
"
https://www.turing.ac.uk/research/research-programmes/data-centric-engineering/journal,"

Now open for submissions, the Cambridge University Press Data-Centric Engineering journal is a cutting edge, cross-disciplinary and open access journal focusing on research at the intersection of data science and a broad range of engineering subjects. It covers the use of data science methods to model systems downstream from the lab in order to build prototypes and engineering solutions that are safer, more resilient and fitter for purpose.
Mark Girolami, Chief Scientist at The Alan Turing Institute, is Data-Centric Engineering's Editor-in-Chief.
The journal will publish the following types of peer-reviewed paper:
Data-Centric Engineering welcomes contributions from researchers in academia and industry that explore:
Further information about the journal and instructions for authors can be found on the Cambridge University Press website.
Contact: [email protected]
Twitter: @dce_journal
 
Published by:

Supported by:

"
https://www.turing.ac.uk/news/chief-scientist-receives-honour-royal-statistical-society,"
Professor Mark Girolami, Chief Scientist at The Alan Turing Institute has been awarded the Guy Medal in Silver by the Royal Statistical Society today (Wednesday 29 March). 
Professor Girolami has been awarded the medal for his contributions to computational statistics and machine learning - in particular, his work on differential geometric approaches to stochastic simulation for statistical inference, published in the paper “Riemann manifold Langevin and Hamiltonian Monte Carlo methods” (with co-author Calderhead), which was read to the Society in 2011.
Every year the Royal Statistical Society celebrates outstanding contributions to the development of statistics through their honours. This year, the winners will be presented with their awards at a ceremony during their annual conference in Harrogate in September. 
Dr Andrew Garrett, RSS President and Chair of the Honours Committee, said: ‘On behalf of the Royal Statistical Society, I offer my congratulations to this year’s recipients. Their work has not only strengthened the discipline but led to greater understanding of many important issues.’
Professor Girolami was appointed Chief Scientist at the Alan Turing Institute following his successful leadership of the Data Centric Engineering Programme. He said: “I am truly honoured to receive the Guy Medal in Silver from the Royal Statistical Society. It is an even greater honour to be placed in such distinguished ranks as those of the past honourees, all of whom have made important and innovative contributions to the theory and application of Statistical Science.”
The citations in full for each of this year’s recipients can be seen on the Royal Statistical Society’s dedicated honours page. 
The Guy Medal in Silver is awarded alongside the Guy Medal in Bronze, the David Cox Research Price, The Barnett Award and the Honorary Fellowship.
The Guy medals are named after the British medical statistician, William Augustus Guy. The Bronze and Silver medals are awarded annually, with a Gold Medal awarded every two years. The Research Prize is awarded to those near the beginning of their research career for an outstanding published contribution to statistical theory or application. In 2021 the Prize was renamed in honour of former RSS president Professor Sir David Cox. The Barnett Award is awarded for contributions to environmental statistics, established in memory of Vic Barnett. The Honorary Fellowship is awarded to significant individuals working in fields related to statistics who are not necessarily or primarily members of the statistical profession. 
"
https://www.turing.ac.uk/research/interest-groups,"

What is an interest group?
An interest group enables researchers to gather around shared areas of interest in data science and AI.What are the aims of an interest group?
To find out more, please contact [email protected]. You are also welcome to message interest group organisers directly.
 
"
https://www.turing.ac.uk/people/guest-speakers/richard-walters,"
Dr Richard Walters is Assistant Professor in the Department of Earth Sciences at Durham University. He is a geophysicist, specialising in earthquakes and the tectonic processes that cause them, and has more than 10 years' experience using satellite data to investigate tectonic deformation of the Earth’s crust, from the scale of individual earthquakes to entire mountain ranges and tectonic plates.
"
https://www.turing.ac.uk/events/london-robotics-ai-summer-mini-school,"
Join us at the Mile End Campus of the Queen Mary University of London for the first pilot of the London Robotics & AI Summer School, jointly organised by the Queen Mary University of London (QMUL) and the University College London (UCL). This first edition will be a one-day event (i.e. a Mini-School), with presentations by academics of QMUL, UCL and Imperial College, followed by a short robotic hackathon in which you will be challenged to update the ROS code of a small robotic arm to pick as many objects as you can in the shortest possible time: can you beat our robot baseline? The event is free! But places are limited (first comes first served). 
Check the updated info here, including registration:
https://www.robotics.qmul.ac.uk/events/4648/london-robotics-ai-summer-mini-school
Detailed program (note that this is a ""Bring Your Own Laptop"" event, if you want to code!)
Register now
"
https://www.turing.ac.uk/opportunities-turing/jobs,"

View and apply online for current vacancies via our careers site (opens a new window)
We are committed to making sure our recruitment process is accessible and inclusive. This includes making reasonable adjustments for candidates who have a disability or long-term conditions. Please contact us at [email protected] to find out how we can assist you.
Find out more about accessibility and facilities at the Turing.
For more information on the Institute’s commitment to equality, diversity and inclusion please visit our equality, diversity and inclusion page.
View our privacy notice
"
https://www.turing.ac.uk/people/guest-speakers/abeba-birhane,"
Dr Abeba Birhane is a cognitive scientist researching human behaviour, social systems, and responsible and ethical Artificial Intelligence (AI). Her interdisciplinary research explores various broad themes in cognitive science, AI, complexity science, and theories of decoloniality. More specifically, Birhane examines the challenges and pitfalls of computational models of human behaviour.
"
https://www.turing.ac.uk/news/publications/mbo-scheme-clustering-and-semi-supervised-clustering-signed-networks,"
We introduce a principled method for the signed clustering problem, where the goal is to partition a graph whose edge weights take both positive and negative values, such that edges within the same cluster are mostly positive, while edges spanning across clusters are mostly negative. Our method relies on a graph-based diffuse interface model formulation utilizing the Ginzburg-Landau functional, based on an adaptation of the classic numerical Merriman-Bence-Osher (MBO) scheme for minimizing such graph-based functionals. The proposed objective function aims to minimize the total weight of inter-cluster positively-weighted edges, while maximizing the total weight of the inter-cluster negatively-weighted edges. Our method scales to large sparse networks, and can be easily adjusted to incorporate labelled data information, as is often the case in the context of semi-supervised learning. We tested our method on a number of both synthetic stochastic block models and real-world data sets (including financial correlation matrices), and obtained promising results that compare favourably against a number of state-of-the-art approaches from the recent literature.
M. Cucuringu, A. Pizzoferrato, Y. van Gennip, An MBO scheme for clustering and semi-supervised clustering of signed networks, (arXiv) (2019)
5.5 MB
"
https://www.turing.ac.uk/news/alan-turing-institute-brings-data-expertise-nationwide-walking-and-cycling-schemes,"
The Alan Turing Institute has been commissioned by Active Travel England to create new software and data science techniques to support local authorities in offering walking, wheeling and cycling schemes, as announced by the Department for Transport. 
The collaboration will run for two years and provide evidence for Active Travel to develop schemes that will offer the most benefits for residents. It will help councils to map out proposed schemes and see the impact they could have locally.
Researchers on the project will enable the development of new tools which will be paired with existing data sources such as OpenStreetMap, to create innovative solutions that will help build the evidence needed to meet national government’s objectives on active travel, including for 50% of short trips in urban areas to be made by walking, wheeling and cycling by 2030. 
To launch this new collaboration, Active Travel Minister Jesse Norman recently attended an event at The Alan Turing Institute where he tested the technology that Active Travel England’s head of data Dr Robin Lovelace and Alan Turing Institute developer Dustin Carlino have been working on.
Active Travel Minister, Jesse Norman, said: “I hugely welcome this new partnership, which will enable local councils to draw on the latest technology and maximise the environmental, economic and health benefits of active travel.”
Active Travel England’s CEO, Danny Williams, said: “This exciting new collaboration will help to accelerate progress towards our vision of making walking, wheeling and cycling the natural choice for short trips nationwide.”
“The Active Travel Infrastructure Platform is going to save councils’ time, improve local decision-making and enable evidence-based decisions to be made that will have maximum impact.”
The Alan Turing Institute’s Chief Scientist, Professor Mark Girolami, said: “We are excited to be partnering with Active Travel England to develop new data science tools and software that will support the shared mission to make active travel more accessible and enjoyable for everyone.""
"
https://www.turing.ac.uk/people/external-researchers/adam-knott,"
Adam is a Principal Clinical Scientist in Pharma Development Clinical Science Oncology serving as the clinical science lead for the HER2 franchise and sitting on the Global Development Team.

Adam has been at Roche for 13 years which has included being the lead clinical scientist for the pertuzumab pivotal clinical trials CLEOPATRA and APHINITY, being a Senior Strategic Leader within the Personalised HealthCare Oncology group, acting as Welwyn Site Head for both Clinical Science Oncology and Personalised HealthCare, being a member of the Welwyn Leadership Team and sitting on the Portfolio Enabling Committee. 
 
"
https://www.turing.ac.uk/people/researchers/sunil-manghani,"
Sunil Manghani is Professor of Theory, Practice and Critique at Winchester School of Art, University of Southampton (UK), where he has established the School’s Data Image Lab. He is co-editor of Journal of Visual Art Practice and managing editor of Theory, Culture & Society. His work brings together various aspects of critical theory, visual arts and image studies. His books include Image Studies (2013), Zero Degree Seeing (2019); India’s Biennale Effect (2016) and Farewell to Visual Studies (2015). He curated Barthes/Burgin at the John Hansard Gallery (2016), as well as Building an Art Biennale (2018) and Itinerant Objects (2019) at Tate Exchange, Tate Modern.
Sunil Manghani’s research as Turing Fellow traces aspects of the history of structuralism to consider its significance for our current handling and advancement of AI, data and natural language processing (see Sunil's project website). The work includes a re-reading of the influential anthropologist Claude Lévi-Strauss, along with the noted cultural theorist Roland Barthes. Both thinkers, when turning to questions of culture, developed persuasive accounts for identifying specific ‘units of analysis’, which included sophisticated approaches to classification models, systems of thinking and cultural meaning. Currently, when we consider what data is being tracked and how it is being used it is often not particularly rewarding and is rarely accessible and meaningful to the individual’s associated with the data. There remains a space for more careful consideration and cross-disciplinary dialogue. Starting points for this research appear in Manghani’s analysis of the data artist Paolo Cirio (‘The Art of Paolo Cirio: Exposing New Myths of Big Data Structures’, Theory, Culture & Society, 2017, Vol. 34, No. 7-8, pp.197-214) and in a book chapter ‘Idiorrhythmy: An (Unsustainable) Aesthetics of Ethics’, which appears in Manghani’s co-edited volume Rhythm and Critique: Technics, Modalities, Practices (Edinburgh University Press, 2020). 
"
https://www.turing.ac.uk/people/former-enrichment-students/bogdan-toader,"
Bogdan is a final year PhD student in mathematics at the University of Oxford. There, he has been trained as an applied mathematician as part of the Centre for Doctoral Training in Industrially Focused Mathematical Modelling (InFoMM) and the Numerical Analysis Group. 
His research is on grid-free compressed sensing/super-resolution under the supervision of Professor Jared Tanner and Dr Andrew Thompson and his project is in collaboration with the National Physical Laboratory.
Previously, he received a BSc in Computer Science and Mathematics from the University of Manchester and he worked as a software developer in the financial sector in London for three years.
Bogdan is interested in techniques for infering useful information from high-dimentional data. Specifically, he focuses on the mathematical approach to super-resolution, or grid-free compressed sensing, a field which was started when mathematicians took an interest in how we can improve the resolution of a microscope beyond the limit imposed by the laws of physics.
The work done by Bogdan so far has been on the stability properties of the super-resolution problem. This gives us an insight into how well we can recover a sparse signal when the measurements are corrupted by noise. 
For the rest of his PhD, and during his time at the Turing, Bogdan will work on the analysis of super-resolution algorithms. His PhD will conclude with the application of such algorithms to relevant datasets, for example from single-molecule fluorescence microscopy or localisation of ships in a shipping lane.

"
https://www.turing.ac.uk/events/turing-lecture-education-ai-ready,"
94 KB
Rose Luckin is Professor of Learner Centred Design at the UCL Knowledge Lab in London. Her research involves the design and evaluation of educational technology using theories from the learning sciences and techniques from artificial intelligence (AI).
She has a particular interest in using AI to open up the 'black box' of learning to show teachers and students the detail of their progress intellectually, emotionally and socially. Rose is also Director of EDUCATE, a London hub for educational technology startups, researchers and educators to work together on the development of evidence-informed educational technology.
Rose was named on the Seldon List 2017 as one of the 20 most influential people in Education. She is a UFI charity trustee, a governor and trustee of St Paul's school in London and a governor of the Self-Managed Learning College in Brighton. She has taught in the state secondary, Further Education and Higher Education sectors, and she was previously Pro-Vice Chancellor for Teaching and Learning at the University of Sussex.
(Full bio in speaker profile)
COVID-19 has precipitated a major experiment for the UK’s education system that may change the way we teach and learn forever, but what role can and should AI play in this transformation? Professor Luckin will discuss the current stage of AI's application in education and the ways in which AI has supported teachers and learners during the pandemic.
Professor Luckin will also look towards the future and consider how AI could be used to support a COVID-compliant transformation for our education system - a transformation that seeks to enable all learners to achieve their full potential. Throughout the lecture examples from different AI systems will be presented to illustrate what is happening in the present and what could happen in the future.
We will also hear recommendations as to how our education system can become 'AI ready.'
5.6 MB
"
https://www.turing.ac.uk/events/ai-advance-wellbeing-and-society,"
In this talk, we propose a novel hybrid deep learning model that combines Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) together to forecast air quality at high resolution. Our model can utilize the spatial correlation characteristic of our air pollutant datasets to achieve higher forecasting accuracy than existing deep learning models of air pollution forecast.
The University of Hong Kong, under the HKU-Cambridge AI to Advance Well-being and Society Research Platform, led by Prof. Victor OK Li and Dr. Jacqueline CK Lam, has been performing research on AI technologies and its application to pressing societal problems, especially those problems related to environment and health. 
Poor air quality has become an increasingly critical challenge for many metropolitan cities, and has catastrophic physical and mental consequences on human health and quality of life. However, accurately monitoring and forecasting air quality remains a highly challenging endeavour.vLimited by geographically sparse data, traditional statistical models and newly emerging data-driven methods of air quality forecasting mainly focused on the temporal correlation between the historical temporal datasets of air pollutants. However, in reality, both distribution and dispersion of air pollutants are highly location-dependent.

"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/public-health-england,"
www.gov.uk/government/organisations/public-health-england
"
https://www.turing.ac.uk/people/researchers/emma-mccoy,"
Professor of Statistics at Imperial College London and Strategic Leader for the 'Monitoring of complex systems' Grand Challenge of the Turing's programme in Data-centric Engineering.
Emma’s research interests include time series, wavelets and causal inference, with a particular interest in transport studies.
"
https://www.turing.ac.uk/people/former-enrichment-students/alessandro-barp,"
Alessandro is a PhD student that studied math and physics in Warwick (focus on continuum and fluid mechanics) and theoretical physics and geometry in Cambridge. He is now working in the math department of Imperial (supervised by Prof Girolami) on information geometry and statistics.
Alessandro works on sampling methods (mainly Hamiltonian Monte Carlo methods) as well as Information geometry. More generally he is interested in all applications of geometry (including functional analysis) in science.
"
https://www.turing.ac.uk/people/external-researchers/sofia-poni,"
"
https://www.turing.ac.uk/people/guest-speakers/adrian-bevan,"
Adrian Bevan is Head of the Department of Physics and Astronomy at Queen Mary University of London. He uses a variety of data science techniques for his day-to day research in order to learn about the sub-atomic nature of particles. His research interests include understanding the matter-antimatter asymmetry in the Universe and developing a deeper understanding of the Higgs boson. He also works on the development of semiconductor sensor devices for operation in novel experiments and the Large Hadron Collider at CERN.
Adrian currently participates in Big Data-based experiments at international laboratories and specialises in parameter estimation through complex modelling and simulation of data. Adrian has almost two decades of experience in this area. He has worked on experiments at international laboratories in Switzerland (CERN and PSI), the US (SLAC National Accelerator Laboratory) and CERN (Switzerland). Many of the research topics of interest have significant amounts of background ""noise"" obscuring the signal and as a result Adrian has had to use machine learning techniques in order to identify signal for subsequent analysis. He has also written a book on Statistical Data Analysis and edited a book on The Physics of the B Factories.
"
https://www.turing.ac.uk/people/guest-speakers/benjamin-joachimi,"
Benjamin Joachimi is a Professor of Astrophysics in the Department of Physics and Astronomy at University College London and a core member of the Cosmoparticle Initiative. His research addresses some of the major open questions in fundamental physics: How did structure in the Universe form and evolve and what role does dark matter play in this? What is the cause of the recent accelerated expansion of the Universe? Is Einstein's theory of gravity correct on the largest cosmological scales? 
He plays leading roles in developing, running and analysing some of the largest and deepest galaxy surveys ever undertaken, such as the ESO Kilo-Degree Survey, the ESA Euclid Mission, the DESI Survey, the Legacy Survey of Space and Time and the PAU Survey.
Benjamin received a Royal Astronomical Society Winton Award and an STFC Ernest Rutherford Fellowship. He leads the departmental equality, diversity and inclusion work and serves on the management team of UCL's doctoral training centre for data-intensive science and the Centre for Data-Intensive Science and Industry.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/connected-places-catapult,"
cp.catapult.org.uk
"
https://www.turing.ac.uk/people/business-team/ian-carter,"
"
https://www.turing.ac.uk/research/asg,"

The Alan Turing Institute was awarded £38.8 million over five years in 2018 through UKRI’s Strategic Priorities Fund. AI for science and government (ASG) is delivered in partnership with the Engineering and Physical Sciences Research Council (EPSRC) and in collaboration with a number of other research councils.
The biggest challenges that our societies face – from the COVID-19 pandemic to social and economic instability, overpopulation, and the climate emergency – involve complex interconnections between environmental, healthcare, social, economic, political and engineering systems. Addressing these grand challenges therefore requires both deep disciplinary expertise and ways to convene and combine expertise from different disciplines, strongly connected to problem owners.
Traditionally, this has been a substantial challenge. However, by providing the tools and methods to handle, combine and model large, disparate datasets, modern data science and AI tools have the potential to transform this process, with myriad benefits to society. The vision of ASG is to demonstrate – via a diverse range of activities organised and presented as an integrated whole – how AI and data science can be used to effectively address significant societal challenges and transform health, science, engineering, and government.
Themes, projects, events and associated programmes can be explored below.


Here you will find a collection of one page briefing notes that provide an overview of the impact achieved to date on projects funded by the AI for Science and Government (ASG) programme.
A digital twin for London’s underground farm (PDF)
Accessible training for diverse data users (PDF)
Colouring Cities: a worldwide network to inform buildings research and policy (PDF)
EnergyFlex: using simulation to make our homes more energy-efficient (PDF)
IceNet: faster, more accurate sea ice forecasting with a new AI-based tool (PDF)
Raphtory: deeper network analysis (PDF)
scivision: making computer vision tools more accessible (PDF)
SPARRA: using AI to predict emergency hospital admissions in Scotland (PDF)
The National Health Index: developing a reliable measure of the nation’s health (PDF)
The Turing Way for responsible research (PDF)
Working with Cefas to create cutting-edge software for surveying sea life (PDF)
In consultation with the programme’s External Advisory Board, the Management Board agreed in July 2020 to fund several new and strategic research initiatives to build on research outcomes from the first two years of the ASG programme and address important issues for the UK government and public sector made more urgent since the onset of the COVID-19 pandemic. Below provides a description of the new research initiatives and will be further detailed as the projects associated with them progress. 
Measuring the impact of policy interventions related to COVID-19 and building resilience against future shocks.
The COVID-19 crisis has highlighted how vulnerable societies and governments are to shocks. This sensitivity is largely due to the propensity to design policy for narrow siloes relating to sectors and government departments, without adequate consideration of the interdependencies between them or proper appreciation of the interconnected nature of local and global societies. It has become evident that resilience in one policy area (e.g. health) can come at the cost of resilience in another (e.g. the economy). The overall aim of this research is to develop a better understanding of resilience in interconnected health, social, and economic systems and to use this understanding to identify robust policy measures.
Find out more about shocks and resilience
Digital twins are now well established in a number of domains and are increasingly being linked into ‘ecosystems of digital twins’ (EDTs), but foundational challenges remain. We seek to develop new methods, tools and underpinning foundations to build well-defined EDTs which are spread across spatial and temporal scales, addressing specific use cases in engineering, health, commerce, economics, urban infrastructure and community modelling, as identified by relevant commercial and government stakeholders.
Find about more about ecosystems of digital twins
Environmental models: Bridging the spatial scales, from surface sensors to satellite sensors.
Satellite sensors can now provide an amazing level of detail of the Earth surface, yet with sparse and imperfect ground-truth sensors to validate them, and due to their relatively short record (a few decades) their usefulness when used on their own is somewhat limited. To make new leaps in understanding environmental change and to improve prediction we must find intelligent ways to combine satellite data with surface sensors and the output from physics-based environmental simulators (e.g., climate models). To bridge these spatial scales and various modalities we are creating a team of scientists and engineers to build and deploy toolkits driven by real-world case studies. This effort will create the foundations for building UK research capacity in developing Digital Twins of the natural environment.
The impact of climate change on agriculture.
Agriculture is highly vulnerable to climate change, with important socio-economic implications for food sustainability and greenhouse gas emissions. We can begin to model this by bringing together data from plant science, hydrology, soil science, insect population dynamics, economics, consumer behaviour and climate models. In this way we will contribute to the sustained development of a national crop modelling platform which will continue to develop at scale beyond the duration of this particular project.
Find out more about environment and sustainability
This theme is aligned with the urban analytics programme.
This theme is aligned with the data-centric engineering programme.
This theme is aligned with the health and medical sciences programme.
This theme is aligned with the public policy programme.
This theme is aligned with the data science for science and humanities programme.
Researchers at The Alan Turing Institute, Science Technology Facilities Council and their collaborators, have access to the PEARL computing service for research falling under this theme. Click here for more information.
This theme is aligned with the tools, practices and systems programme, and involves collaboration with the Research Engineering Group.
The ASG programme is governed by its Management Board which includes its Theme Leads as members as well as representatives from EPSRC, Delivery Partner of the investment, and independent members from the Turing’s University Partner Network. In April 2021, two new leadership roles within ASG were appointed on an initial 12 month interim basis: Professor Jonathan Rowe as Programme Chair, and Professor Ben MacArthur as Programme Director.
The ASG programme also has an External Advisory Board which offers strategic advice to the Management Board. Members were invited based on their independence from the ASG programme and their range of scientific, industry and government expertise. The Chair of the External Advisory Board is Dame Wendy Hall and Deputy Chair is Professor Tom Rodden.
"
https://www.turing.ac.uk/people/researchers/mark-gilthorpe,"
Mark is a Professor of Statistical Epidemiology in the Obesity Institute, Leeds Beckett University. He is also a Fellow of the Alan Turing Institute. 
Trained as a mathematical physicist, Mark's driving interest centres on improving our understanding of the observable world through modelling. After his PhD, Mark worked as a Consultant Data Analyst before entering academia and has since fashioned a programme of interdisciplinary research that spans the gap between theoretical and applied data analytics. He focuses on modelling complexity and highlighting and solving common analytical problems in observational research. 
Mark's research and teaching interests have converged around the insights and utility of causal inference methods in observational research, especially how causal methods might be integrated with machine learning and AI to better understand and model complex systems.
Mark is seeking to understand complex relationships between individuals within their natural environment through the development and application of observational methods, specifically through the integration of causal inference modelling and agent-based modelling. An example domain of this challenge is modelling patterns, causes and consequences of obesity within our society – these approaches were also applied to Covid-19 pandemic.
"
https://www.turing.ac.uk/people/enrichment-students/chang-luo,"
With the dual backgrounds in computer science and financial engineering, Chang is currently perusing his PhD degree at the University of Edinburgh aiming to model the densely, dynamically, interactively, yet implicitly, connected relationships among the corporations in the open market. To achieve that, Chang’s work utilises theories and techniques from network science, geometric learning, social computing and natural language understanding.

"
https://www.turing.ac.uk/people/business-team/joanna-stacey,"
"
https://www.turing.ac.uk/news/publications/understanding-vulnerability-online-misinformation,"
Health-related misinformation risks exacerbating the COVID-19 public health crisis if it leads the public to refuse treatment when needed, not follow official guidance, such as policies on social distancing and mask-wearing, or even to use harmful ‘miracle’ cures. If left unchecked, misinformation could seriously undermine the vaccine rollout and heighten people’s anxiety and mistrust during this time of national stress.
Several large-scale research projects have started during the crisis with the aim of understanding the nature, prevalence and spread of health-related misinformation online. However, relatively little is known about who is vulnerable to believing false information and why. This is crucial for developing more targeted and effective interventions which tackle the root causes of misinformation rather than just its symptoms. To address this gap, researchers from The Alan Turing Institute’s public policy programme have conducted original research using a survey and assessments to understand (1) which individuals are most vulnerable to believing health-related falsities and (2) the role played by the content that individuals are exposed to.
1.9 MB
"
https://www.turing.ac.uk/people/former-researchers/florentin-goyens,"
Florentin Goyens started his doctoral studies at The Alan Turing Institute in October 2017. He is registered at the University of Oxford. Prior to this Florentin was at the Université Catholique de Louvain. After previous work with Prof. Pierre-Antoine Absil and Raphaël Jungers in Belgium, he is now completing his PhD under the supervision of Coralia Cartis (Oxford) and Armin Efthekari (Edinburgh). His research interests include nonlinear optimisation problems defined on Riemannian manifolds.
Florentin's MSc dissertation analysed variant of quasi-Newton method for nonsmooth optimisation problems. During his PhD, he will look at various optimisation methods to solve problems defined on Riemannian manifolds. The field of Riemannian optimisation is in great expansion as it offers an alternative to nonlinear problems which are often hard to solve, and linear problems that can be simplistic models. Numerous questions in data processing and machine learning are naturally expressed as optimisation over manifolds.
"
https://www.turing.ac.uk/people/external-researchers/jasmina-bogojeska,"
Jasmina Bogojeska is a Senior Principal Data Scientist at Roche Diagnostics working on digital health solutions for improving patients care. Before joining Roche she was a machine learning scientist at IBM Research – Zurich working in the areas of machine learning, deep learning and natural language processing (NLP). She received her M.S. and Ph.D. degrees both in Computer Science from the Max-Planck Institute for Informatics/Saarland University, Germany in 2007 and 2011, respectively. The main focus of her work is application-inspired machine learning where she is developing and applying machine learning solutions to various challenging real-world problems from the areas of healthcare, services research and document analysis. Jasmina has published her research in renowned international journals and presented her work at national and international conferences. She was a recipient of the IMPRS-CS fellowship awarded by the Max-Planck Society in 2005, IBM’s Corporate Research Award in 2017 and several IBM research accomplishment awards.
"
https://www.turing.ac.uk/news/publications/understanding-bias-facial-recognition-technologies,"
Over the past couple of years, the growing debate around automated facial recognition has reached a boiling point. As developers have continued to swiftly expand the scope of these kinds of technologies into an almost unbounded range of applications, an increasingly strident chorus of critical voices has sounded concerns about the injurious effects of the proliferation of such systems on impacted individuals and communities.
Opponents argue that the irresponsible design and use of facial detection and recognition technologies (FDRTs) threatens to violate civil liberties, infringe on basic human rights and further entrench structural racism and systemic marginalisation. They also caution that the gradual creep of face surveillance infrastructures into every domain of lived experience may eventually eradicate the modern democratic forms of life that have long provided cherished means to individual flourishing, social solidarity and human self-creation. Defenders, by contrast, emphasise the gains in public safety, security and efficiency that digitally streamlined capacities for facial identification, identity verification and trait characterisation may bring.
In this explainer, I focus on one central aspect of this debate: the role that dynamics of bias and discrimination play in the development and deployment of FDRTs. I examine how historical patterns of discrimination have made inroads into the design and implementation of FDRTs from their very earliest moments. And, I explain the ways in which the use of biased FDRTs can lead to distributional and recognitional injustices. I also describe how certain complacent attitudes of innovators and users toward redressing these harms raise serious concerns about expanding future adoption.
The explainer concludes with an exploration of broader ethical questions around the potential proliferation of pervasive face-based surveillance infrastructures and makes some recommendations for cultivating more responsible approaches to the development and governance of these technologies.   
Leslie, D. (2020). Understanding bias in facial recognition technologies: an explainer. The Alan Turing Institute.
https://doi.org/10.5281/zenodo.4050457
6.8 MB
"
https://www.turing.ac.uk/research/research-programmes/finance-and-economics/token-economy,"
Blockchains and smart contracts allow everyone to create tokens that can be used as media of exchange, represent an asset or be employed to reinforce and build desirable behaviours. However, how to design and apply tokens is still unclear due to the ever-complicated relationship between software engineering, human behaviour and the complex socio-technical systems we form and inhabit.
This programme challenge, or special interest group, aims to put together researchers and industrial partners interested in understanding the socio-economic implications of tokens. The goal is to investigate the current usage of tokens in order to improve the design of future tokens, minimise potential risks for users and maximise potential societal benefits.
Andrea Baronchelli took part in the NFTs & the future of digital art session at CogX 2021. The session was co-curated by CogX and The Alan Turing Institute.

[email protected]
 
Shruti Appiah, Independent Researcher
Tomaso Aste, Professor and Director of the Centre for Blockchain Technologies (UCL)
Astrid Ayel, Blockchain & Fintech, Knowledge Transfer Network (InnovateUK)
Keith Bear, Cambridge Centre for Alternative Finance (Cambridge)
Jessica Flack, Head of the Collective Computation Group at Santa Fe Institute
Angela Gallo, Lecturer in Finance (Cass)
Philip Gradwell, Chief Economist at Chainalysis
Gianvito Lanzolla, Head of Faculty of Management (Cass)
Gabriele Musella, CEO & Founder at Coinrule
Simone Righi, Lecturer in Financial Computing (UCL)
Tom Robinson, Chief Scientist at Elliptic
Lex Sokolin, Global Fintech Co-Head at ConsenSys 
Alex Teytelboym, Associate Professor in Economics (Oxford)
 
"
https://www.turing.ac.uk/people/enrichment-students/jennifer-sykes,"
Jen Sykes is an artist, designer and lecturer based in Glasgow, Scotland. Having graduated from the MFA in Computational Arts at Goldsmiths University of London in 2012, her work spans creative computing, sculpture and electronic hardware designs often connecting the physical analog and digital worlds. Her PhD research at the Creative Coding Institute, UAL, London focuses on communicating errors and mistakes to creative practitioners exploring computer programming as a material for creative expression. 
Her research has led her to develop her own circuits and hardware as well as frequently fabricating sensors, interfaces and interactive installations for other artists and designers. Alongside her own practice she is often commissioned to fabricate work for other artists and designers. Recent commissions include the BBC, Glasgow International Visual Art Festival, Sonica Festival and Cryptic. 
Alongside Jen’s research she is a lecturer in Physical Computing and Creative Coding, teaching internationally at the Glasgow School of Art, The Danish Royal Academy of Art and Design and Umeå Institute of Design, Sweden. 
Creative Coding has become a popular material used within the Studio-based model of education in Art and Design Schools. This exploratory material approach to the use of technology provides important potential for creative expression and pathways within creative industries for experiential approaches. However, there still remains a friction in how a materials, originating from didactic modes of delivery, can be translated to creative practices where there is an emphasis on exploration, reflection and process.
Often, we approach introductory methods of understanding programming by following step by step instructions or tutorials. Whilst these methods are beneficial in their early principles there remains a steep learning curve when transitioning to individually driven projects. It is through the experience of making mistakes and problem solving that provides us with self sufficiency and creative agency. When transitioning from a predominantly screen-based exploration of computer code to electronic physical components, beginners are faced with three areas to troubleshoot; the software, the hardware, and the bridging of the two together. The research seeks to question how the bridge between software and hardware can be communicated more effectively for students in an Art School environment?
For the past year Jen Sykes has continued her practice-based research focusing on how we can explore methods of teaching mistakes in Creative Coding through working with Functional Errors. A Functional Error is defined as as a piece of code that compiles without error in software yet it results in a different kinetic or graphical output than conceptually desired. A Functional Error is also the result of a gap in understanding between established and new creative disciplines and can often be difficult to identify through the use of different descriptive language around each subject area.
This research aims to produce tangible tools focusing on three key areas; feedback, exploration and augmentation. An objective of the physical, educational prototypes is to apply machine learning programming methods in capturing complex, data-driven user input from existing hardware. In doing so, the tools will provide students with alternative, Artificially Intelligent modes of engaging with creative coding programming environments. Each practical prototype seeks to highlight and educate users as to how to identify common Functional Errors and in turn aims to provide more immediate feedback, legibility and independence to students. 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/hsbc,"
Since 2016, HSBC has partnered with The Alan Turing Institute to develop an advanced programme of research in finance and economics. Our collaboration continues to develop new methods to protect and gain insight from financial data, helping to connect people to new ideas and opportunities across the world.
In 2022 we begun the next phase of our partnership with the goal of developing safe and secure data sharing techniques, using leading-edge Privacy-Enhancing Technologies (PETs). These include the use of synthetic data, homomorphic encryption and multi-party computation. Our partnership is also creating opportunities for a diverse range of students to learn and contribute through a number of Turing Intern positions hosted at HSBC.
"
https://www.turing.ac.uk/people/enrichment-students/jiawen-huang,"
Jiawen is a PhD student at the UKRI CDT in AI and Music (AIM CDT) at Centre for Digital Music, Queen Mary University of London. Her PhD topic is lyrics alignment and transcription, which aims at recognising and aligning the lyrics of music. Her research interest lies in music information retrieval topics, especially singing voice analysis, performance assessment and mashup generation. 
Given the lyrics to a song, audio-to-lyrics alignment aims at identifying for each lyric the corresponding position in a recording of the song. Providing time-aligned lyrics makes it easier to navigate through vocal music and enhance the listening experience. One of the open questions for lyrics alignment is that most of the existing systems cannot operate in real-time, because their alignment algorithms need the whole recording to process. Real-time application of lyrics alignment would allow the audience to enjoy live music performance. During the scheme, she is interested in developing a real-time lyrics alignment system.
"
https://www.turing.ac.uk/work-turing/research-and-funding-calls,"

"
https://www.turing.ac.uk/research/research-projects/causal-inference-improved-cybersecurity-threat-detection,"
Understanding how a cyberattack unfolds is key to defending against it. At present it is difficult to ascertain that an attack is taking place and what an optimal defence might look like. One potential solution to these problems lies in the space of causal inference where we study the causal effects of measurable actions and observational data, to build models capable of detecting threats but also automatically acting on them.
A causal approach will be explored to determine the root cause of an anomaly, i.e. whether a detected anomaly has been caused by a cyber-attack, or a non-malicious action, in a network defence system. In current cyber security systems, combining information is typically achieved by correlating indicators; however, this may yield misleading insights as correlated events are not necessarily causally related. An approach to addressing this issue is to use causal models – which can capture expert knowledge – that describe the relationships between indicators of anomalous behaviour and the likelihood they have a certain root cause.
The project is led by Principal Investigator Neil Dhir who is looking to develop new mathematical methods and related software tools which could be utilised in machine learning environments. Where the system would automatically learn the nature of an attack and consequently recommend actions to defend against the attack. Another bigger part is the interaction with complex, real world data, to be able to easily tackle questions where there is a variety of different data to consume. Cybersecurity data is notoriously difficult to work with and this project aims to make analysis of the latter a bit easier.
This project aims to use novel techniques in the area of causal inference to:
1. Understand which indicators from the framework could lead to others, and what is the optimal sequence of both attack and defence actions.
2. How to best combine heterogenous cyber datasets and leverage these for causal queries.
3. Add a causal dimension to Threat Detection in networks.
A better understanding of these areas will enable the NCSC to better advise the UK Government, and private companies, of cyber risks inherent in working online, and how to better keep their systems safe and secure.
[email protected]
"
https://www.turing.ac.uk/research/research-areas/algorithms/compression-algorithms,"
"
https://www.turing.ac.uk/sessions/did-we-do-better-in-data-science/,"


"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/un-development-programme,"
undp.org
"
https://www.turing.ac.uk/events/rsf-seminar-series-using-routinely-collected-electronic-healthcare-record-data-study,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility (RSF) project
Event title: Using routinely collected electronic healthcare record data to study respiratory disease
Speaker: Professor Jennifer Quint, Professor of Respiratory Epidemiology, Imperial College London
Routinely collected electronic healthcare record data is becoming increasingly commonplace. Its importance is also increasing in clinical practice and for understanding disease trends, informing policy and in the planning and allocation of resources. However, any data set is only as good as what was entered. Professor Jennifer Quint has used routinely collected electronic healthcare record data to study several respiratory diseases, including most recently COVID-19, as well as working to maximising the quality, linkage and usage of these data for clinical and research purposes. 
In her talk, Professor Quint will give examples of the value and importance of these data. She will also explore how we use this data in the UK to make national-level decisions.  
 
Everyone is welcome at the session. However, the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn more about developing and publishing code within secure research environments and how this learning can be applied to multiple long-term conditions research.
By registering for the event, you are agreeing to the events code of conduct: Events code of conduct The Alan Turing Institute.
After registering, you will receive a confirmation email containing information about joining the meeting.
 
Header image: Accuray, Unsplash 
"
https://www.turing.ac.uk/people/researchers,"

Our Researcher list is optimised to use JavaScript. Without Javascript enabled our filter and search function, which helps you find Turing researchers by speciality, title and institution, can’t function. Please consider allowing Javascript to run on this page or use the site wide search function. Contact [email protected] if you have any issues.
Find out more about Research Engineering at the Turing
"
https://www.turing.ac.uk/news/publications/wrattler-reproducible-live-and-polyglot-notebooks,"
Notebooks such as Jupyter became a popular environment for data science, because they support interactive data exploration and provide a convenient way of interleaving code, comments and visualizations. Alas, most notebook systems use an architecture that leads to a limited model of interaction and makes reproducibility and versioning difficult.
In this paper, we present Wrattler, a new notebook system built around provenance that addresses the above issues. Wrattler separates state management from script evaluation and controls the evaluation using a dependency graph maintained in the web browser. This allows richer forms of interactivity, an efficient evaluation through caching, guarantees reproducibility and makes it possible to support versioning.
Petricek, T, Geddes, J & Sutton, C 2018, Wrattler: Reproducible, live and polyglot notebooks. in 10th USENIX Workshop on Theory and Practice of Provenance.10th USENIX Theory and Practice of Provenance, London, United Kingdom, 9/07/18.
"
https://www.turing.ac.uk/people/business-team/kate-wicks,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/john-innes-centre,"
www.jic.ac.uk
"
https://www.turing.ac.uk/people/guest-speakers/professor-rose-luckin,"
Rosemary (Rose) Luckin is Professor of Learner Centred Design at UCL Knowledge Lab. Rose’s research involves the design and evaluation of educational technology using theories from the learning sciences and techniques from Artificial Intelligence. She has a particular interest in how AI techniques can be used to enable more effective, continuous, formative assessment processes and tools. Her 2018 book: Machine Learning and Human Intelligence: The Future of Education for the 21st Century describes how we can best benefit from using AI to support teaching and learning, and how the prevalence of AI in our future means that we need to revise what and how we teach and learn now. She has also published numerous academic articles, authored two monographs and edited two paper collections.
Rose is also Director of EDUCATE: a London hub for Educational Technology Start-ups, researchers and educators to work together on the development of evidence-informed Educational Technology; Specialist Adviser to the UK House of Commons Education Select Committee for their inquiry into the Fourth Industrial Revolution, Co-founder of the Institute for Ethical AI in Education; President-elect of the International Society for AI in Education; a member of the UK Office for Students Horizon Scanning panel, adviser to the AI and Robotics panel of the Topol review into the future of the NHS workforce; a member of the European AI Alliance, holder of an International Francqui Chair at KU Leuven. Rose was named as one of the 20 most influential people in Education on the Seldon List 2017.
Rose is a member of the computing college of the EPSRC and has advised research councils in various countries. The total value of the grants she has had funded to date is in excess of £15 million and further applications are in process. She has also acted as an advisor for various organisations including: DfE, BIS, Pearson, BBC, McGrawHill Education.
Selected 2019 Keynotes: Global Summit (AIAED), Beijing: AI and Adaptive Education
World Forum, London. How AI can Enable Teaching and Learning?
UNESCO, Paris. Policies to harness AI for SDG4
 
"
https://www.turing.ac.uk/research/research-projects/detecting-and-understanding-harmful-content-online,"
This project aims to systematise research in online harms (e.g. research on hate speech). It will do this by developing lists of datasets and benchmarks to compare different attempts to solve the problem (e.g. benchmarks to compare different hate speech classifiers), and developing guidelines for practitioners who wish to use the outputs of online harms research (e.g. government or policy experts who want to develop a quantitative understanding of hate speech in a certain context).
The project will produce three outcomes: 
In summary, this project aims to create 'meta tools' - tools and best practices for using or combining existing tools for detecting and understanding online harms.                
hatespeechdata.com is a website which collects together several known datasets related to hate speech online. This is expected to serve as a resource for researchers in the area.
"
https://www.turing.ac.uk/people/spotlights/morgan-briggs,"

I am fortunate to work on the Ethics Theme within the Public Policy Programme, where I research a variety of topics all related to the overarching theme of responsible research and innovation. The projects I’m currently working on range from advancing global data justice to AI explainability to promoting children’s rights in relation to AI.
I have always been interested in a diverse set of interdisciplinary subjects. When I was at university, I studied both Data Analytics and Religion (with a specific focus on ethics). I wanted to find a way to combine both these interests, and was fortunate to be able to complete an MSc in Social Data Science which approached data science and AI from a sociotechnical lens.
My current role draws on both aspects of my previous education. It also reflects my continued interest in technology – both in relation to its situated context and everyday impact.
My hope is to contribute to a future that creates AI technologies that are representative, inclusive, beneficial, and place the prioritisation of social justice at the forefront. I also hope we can make the concepts we work on accessible to the public and provide them with the tools necessary to make informed decisions about how and where their data is used.
Being a female in a STEM field has taught me the importance of standing up for my thoughts and opinions, in spaces where they are not always welcomed.
Find mentors who you can learn from and go to for advice. Additionally, surround yourself with people who have your best interests at heart but challenge you to go outside of your comfort zone and question your own status quo.
I am from the country music capital of the United States – Tennessee – but I can’t stand country music.
I enjoy working in nature on urban farms throughout London, playing the guitar, or grabbing a nice cup of coffee.
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/queen-mary-university-london,"
"
https://www.turing.ac.uk/about-us/impact/next-step-precision-agriculture,"
By 2050, the world’s population will be pushing 10 billion, according the United Nations. With enormous numbers of people already underfed, estimates for the increase in food production required by mid-century vary wildly: some call for a doubling of production, with other estimates suggesting increases as low as 25% may suffice.
One thing that few dispute is that if we’re going to squeeze ever more food out of the world’s limited farmland, it should be done sustainably or we ultimately risk a humanitarian calamity. It’s a looming global challenge. “Farming the world’s arable land to within an inch of its life is not the solution,” says Julie McCann, a Professor of Computer Systems at Imperial College London, and Strategic Leader for the 'resilient and robust infrastructure' Grand Challenge of the Turing's programme on Data-centric Engineering. “On one hand, we need to protect the soil to ensure its longevity. On the other, we need to get more from it.” That’s why, in 2017, the Turing funded McCann’s work to develop precision-agriculture technology.
“On one hand, we need to protect the soil to ensure its longevity. On the other, we need to get more from it.”
Professor Julie McCann, Data-Centric Engineering Strategic Leader, The Alan Turing Institute
Precision agriculture requires that farmers use sensor networks to become intimately acquainted with the state of their crops and soil at any given time and location in their fields. By understanding what’s happening in fine detail, inputs such as fertiliser and water can be tailored, and disease outbreaks quickly identified. In this way, inputs are optimised and crop yields maximised. And through more judicious use of pesticides harm to the soil and biodiversity are minimised.
But running wired infrastructure all over a farm is impractical and expensive, so sensors need to be low-powered and run on batteries or renewables such as solar. Modern low-powered wide-area networks work well for agricultural applications, but they are designed for one-way traffic – i.e. sending sensing data to a farmer’s computer for analysis. McCann and her team have designed a unique system able to transform a passive sensing network into a control network – a crucial innovation for precision farming.


Listen to Julie McCann speaking to BBC Radio 4's Farming Today about her team's work.
McCann already had a research relationship with the Ridgeview winery in the South Downs in Sussex. “Julie’s team had worked with us before. monitoring energy, temperature and water use within the winery building,” says Vineyard Manager Matthew Strugnell. “We were excited to see how remote sensors could help us to more closely see what is happening in our vineyard, providing a ‘higher resolution’ picture.”
The researchers placed sensors every 50 metres throughout the wine estate and within the winery building. This set-up makes it possible to measure how the air around the vineyard changes in terms of humidity and temperature. The ability to measure soil moisture content at both the micro and macro levels across the fields of vines will also soon be possible.
The initial goal is to model, in as close to real time as possible, the state of the fields. The research challenge is how to make sense of data trickling in from potentially hundreds of sensors, at different times. The trick is to process the data in such a way that it remains robust, even if parts of the network misbehave – as they always will “in the wild” – so an accurate prediction can be made of the current state of the field.
“We are able to understand the small changes in different areas of the vineyard. This can help us to identify potential disease hotspots.”
Matthew Strugnell, Winery Manager, Ridgeview
“We've got sensor data coming in. We can see if some plants are affected by certain localised conditions – that kind of thing,” says McCann. Strugnell agrees: “We are able to understand the small changes in different areas of the vineyard. This can help us to identify potential disease hotspots, for example.” The team is also exploring trends within microclimates in the hope of gaining valuable insights into optimal growing conditions.
The development of sensor networks to read the state of a farm at high resolution will facilitate sustainable practices, which is Ridgeview’s primary focus. For agriculture more broadly, the sensor technology is only half of the challenge. The other half is to take the information gathered by a sensor network, use it to decide what the crops in various parts of a farm need, then use LoRa – a low-power, long-range transmission technology – to control the farming processes, such as using actuators to add fertiliser or water in the appropriate amounts and locations. To create a farm that can not only feel itself, but can also feed itself.
“The protocols used for sensing, which are low-cost, slow, and long distance, were not designed for control,” McCann explains. “We've taken something that wasn't designed to be a control system, and we’ve built a control system out of it. As far as I’m aware, we are the first in the world to do it.”
Data scientists typically have plenty of code and processing power to play with; not so for McCann's sensor systems. “To make our systems smart, we have to take a great deal of computer intelligence and boil it down to super-lightweight code. We have to take the powerful resources you expect from a data centre, and somehow shrink that down onto tiny devices and allow them to evolve their own intelligence as a network – a bit like ant and bee swarms.”
This control part of McCann’s system has been tested in London but not at Ridgeview. “The winery is very happy for us to read the vineyard,” McCann explains, “but they are understandably less comfortable letting a bunch of academics with their bleeding-edge research equipment feed their vineyard, so we’re not doing anything disruptive to the vineyard.” That hasn’t prevented McCann from simulating their control protocol. “We've shown that our system can give the timing and reliability guarantees expected of such systems, for both this application and other applications. The next stage is to see how far we can go with practical actuation.”
Increasingly, farms are using sensor systems, imaging techniques, autonomous vehicles, robots and drones to make farming more efficient, profitable and productive. The ability of farms not only to sense their multi-kilometre fields but also to use that streaming data to regulate themselves in real time offers hope for a more efficient and sustainable future for farming. 
But the potential of the control protocols developed my McCann and her team have potential beyond farming. “It could be used for water networks, which are much more complicated,” says McCann. In the future, she predicts that water pipe networks could be largely self-regulating. That would require sensors to be distributed around existing networks, and more switching points (valves) added to the infrastructure. “Ultimately, the goal is to make water networks as resilient as the internet, in the sense that water can be routed around failures or leaks, and be better matched to demand. We've simulated such networks in the lab and shown that such a system is very viable.”
This research on sensing and control protocols could therefore help to meet the Turing's Data-Centric Engineering challenge of 'resilient and robust infrastructure', with respect to two fundamental resources that will become ever more important in a densely populated, warming world: food and water.

"
https://www.turing.ac.uk/people/governance/wendy-hall,"
Dame Wendy Hall, DBE, FRS, FREng is Regius Professor of Computer Science, Pro Vice-Chancellor (International Engagement) at the University of Southampton, and is Executive Director of the Web Science Institute. She became a Dame Commander of the British Empire in the 2009 UK New Year's Honours list, and is a Fellow of the Royal Society. She is a member of the World Economic Forum’s Global Futures Council on the Digital Economy, and was co-Chair of the UK government’s AI Review, which was published in October 2017.
"
https://www.turing.ac.uk/research/research-projects/high-performance-large-scale-regression,"
Regression analysis is a common set of statistical procedures for estimating the relationships between variables. Problems that involve very large amounts of data with a high number of variables can be computationally intensive. This project is investigating how scalable, distributed computer systems and associated algorithms/software perform on such large scale problems. This will inform current best practice in terms of algorithms, architectures, and implementations.
Regression analysis is a form of predictive modelling technique which investigates the relationship between a dependent (target or output) variable and independent (predictor or input) variables. This is used for forecasting, time series modelling, machine learning, and finding causal relationships between variables. For example, the relationship between rash driving and number of road accidents.
The overall goal of regression is to examine whether recorded data effectively predict some other outcome variable, and in which ways particular variables impact the outcome variable. The statistical and computational performance of regression analysis methods in practice depends on the model relating the variables, the data actually recorded and the algorithm used to produce estimators and associated quantities.
This project is looking at the effectiveness of running regression analysis for large datasets with a high number of variables, on distributed computing systems. Parallel distributed computing is a type of computation in which many calculations, or the execution of processes, are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. Related to this, a computer cluster is a set of connected computers that work together so that, in many respects, they can be viewed as a single system. Each node in the system is set to perform tasks that are controlled and scheduled by system software.
The ultimate goal of this project is to critically understand how well different, readily available, large-scale regression algorithms, software, and frameworks perform on distributed systems. This understanding will help isolate computational and statistical performance issues.
Challenging benchmark datasets be developed to add additional focus, and there is the potential for more sophisticated, but less readily-available algorithms to be analysed for comparative purposes.
This project aligns to the Institute’s strategic priorities in establishing leadership and providing guidance for common data analysis tasks at scale. It also feeds into the larger data science at scale programme looking at the performance and usability of modern hardware and algorithms.
In collaboration with Cray, the analysis in this project will be conducted on their Urika-GX agile analytics platform. The skills and software developed by the investigation will then be applied to large and challenging datasets.
Throughout the project, documentation will be written that will enable other data scientists to perform large scale regressions with greater ease, and understand the implications of using different architectures, frameworks, algorithms, and implementations.
Visit the github page for this project to read a blog summary of the work and view the code produced: turingintern2018.github.io.
"
https://www.turing.ac.uk/research/research-areas/privacy-trust/verification,"
"
https://www.turing.ac.uk/people/researchers/aleksandar-mijatovic,"
"
https://www.turing.ac.uk/people/researchers/helen-margetts,"
Helen Margetts is a Turing Fellow and Director of the Public Policy Programme at The Alan Turing Institute, and Professor of Society and the Internet at the University of Oxford and Professorial Fellow of Mansfield College.  From 2011 to 2018, she was Director of the Oxford Internet Institute, a multi-disciplinary department of the University of Oxford dedicated to understanding the relationship between the Internet and society, before which she was UCL's first professor of Political Science and Director of the School of Public Policy (1999-2004).  After an undergraduate degree in Mathematics, she worked as a computer programmer and systems analyst for Rank Xerox and Amoco before returning to study political science at LSE (MSc 1990, PhD 1996), where she also worked as a researcher.
Helen sits on the UK government’s Digital Economy Council, the Home Office Scientific Advisory Council, the WEF Global Agenda Council on Agile Government and the Ada Lovelace Institute for Data Ethics.  She was a member of the UK government’s Digital Advisory Board (2011-16).  She is the founding Editor of the journal Policy and Internet, published by Wiley. In 2018 she was awarded the Friedrich Schiedel Prize by the Technical University of Munich for research and research leadership in technology and politics. In the 2019 New Years Honours List she was awarded an OBE for services to social and political science. In 2019 she was elected as a Fellow of the British Academy and also took up a visiting appointment as the John F Kluge Senior Chair in Technology and Society at the Library of Congress.
She has researched and written extensively about the relationship between technology, politics, public policy and government including over 150 articles and policy reports and six books on the topic, the latest of which is Political Turbulence: How Social Media Shape Collective Action, which won the Political Studies Association’s W.J.Mackenzie prize for best politics book in 2017. She has worked hard to maximize the policy impact of her research, leading a series of policy studies for the UK National Audit Office in the 2000s, with Patrick Dunleavy (LSE) for which they received the  PSA’s Political Scientists Making a Difference Award. She has presented her work all over the world at forums from the Hay Literary Festival (2016, 2018) to Harvard University and MIT (2016) and the Royal Society (2011, 2018) to Davos (2014), as well as innumerable academic and policy-making events and media appearances.
"
https://www.turing.ac.uk/about-us/impact/optimising-worlds-first-underground-farm,"
This is an edited version of a piece first published by the University of Cambridge in March 2021.
Thirty-three metres below London’s Clapham High Street is the world’s first underground farm. It’s shaping the future of urban farming.
Stacked racks of fresh green leaves thrive under banks of LED lights – peashoots, basil, coriander, parsley, salad rocket, pink radish, mustard plants – the fragrance of the ‘microgreens’ filling a former World War Two air raid shelter under south London.
A post-war plan to join the tunnels to the London Underground system never happened and, in 2015, the deserted subterranean space sprouted new life when co-founders Richard Ballard and Steve Dring decided it was a perfect site to grow food while reducing the carbon footprint of transport and supply.
An unheated tunnel without natural light seems an unlikely place for a farm and yet plants grow extremely well here – yielding a harvest roughly every 10 days all year long. Either side of the 2.5m-high passage, the plants grow without soil (‘hydroponically’) on wool carpet cut-offs that were destined for landfill, and watered from tanks housed on a lower level.
“Growing Underground is a farm that feeds the city from within the city,” explains Ballard. “We sow, pack and grow on site, taking the harvest to New Covent Garden Market less than a mile away for distribution across the capital, reducing food miles, pollution and food waste.”
Zero-carbon food is at the core of the Growing Underground vision – which means paying close attention to what happens to the plants below ground as well as above ground.
Down in the tunnels, a team of engineers and data specialists has been helping the farmers to optimise crop performance and reduce energy use. They are led by Ruchi Choudhary, a Group Leader in the Turing’s data-centric engineering programme who is also Reader in Architectural Engineering at the University of Cambridge’s Department of Engineering.
Together, they’ve reduced the time it takes to grow some crops by 50% and all crops by an average of 7%, and increased yields by 24%. Meanwhile, the crops are grown using less space and water than conventional greenhouse growing, no pesticides and 100% renewable energy.
This can only happen if every element of the farming process is carefully measured and tweaked and measured again. The plants on this farm get exactly what they need at every moment of every day thanks to the power of data – and a digital twin looking out for its sibling from a laboratory in Cambridge.
The underground farm grows 12 times more per unit area than a traditional greenhouse grows in the UK, but farming crops without sunlight is a much more energy-intensive process.
From the outset, Ballard and Dring knew that for the enterprise to live up to their sustainable vision, they needed not just to use 100% renewable energy but also to find ways to cut energy use while maximising crop growth.
“Whenever we saw an opportunity to mash two potatoes with one fork, we did it. We want to be sustainable at every stage of the process. Optimising crop performance for the energy we used seemed an obvious step forward,” says Ballard.
“Smart farming was a new area for us and we were one of the first people to do it in this way – so having Cambridge and the Turing really pushing forward on the data side really sold us on the benefits of capturing and using information to solve some of the sustainability challenges we faced.”
Four years ago, Melanie Jans-Singh – a PhD student at Cambridge’s Department of Engineering – began installing sensors in the tunnels to capture everything you might ever want to know about how this garden grows. Back in Cambridge, she and colleagues set about building a digital twin.
“From day one, Richard and Steve bet on data to help them, and we’ve assisted them from the start of their data journey,” explains Jans-Singh, who was working with Ruchi Choudhary’s ‘Digital twins for the built environment’ group at the Turing. “We learned about their farm at the same time as they did.”
There are 25 sensors measuring 89 variables transmitting to eight Raspberry Pi loggers in the tunnels. The data is warehoused on a server in Cambridge and sent over Wi-Fi to an online horticultural data platform.
Nutrients, water, lights, heat, CO2, airflow, humidity. Even crop growth is tracked minutely by the farm operators. All of this is measured and compared with how the crops perform and then represented in a virtual 3D representation of what’s happening on the ground.
“This virtual representation is the digital twin,” she explains. “What the digital twin shows is better than being in the tunnel in person – it can monitor, learn, feed back and forecast information that will make the real-life twin work better.”
“Having Cambridge and the Turing pushing forward on the data side really sold us on the benefits of using information to solve the sustainability challenges we faced.”
Richard Ballard, co-founder of Growing Underground
At the same time, Turing and Cambridge research associate Rebecca Ward set about analysing the ‘physics of the farm’ – everything from the transfer of heat through the tunnel to the way plants use it to transpire, releasing water through their leaves by evaporation.
“A model based on data can only inform on what the data shows,” explains Ward. “But we wanted the digital twin to cope with unexpected conditions that had never been encountered before – like a very hot day outside. We’ve accomplished this by linking the data to a model based on the laws of physics.”
“We found that the tunnel environment was mainly affected by external weather through the extraction fans and the LED lights, which together consume 95% of electricity,” explains Jans-Singh. “We help the growers tweak conditions – such as tailoring light and heat use – so that the plants grow the best in terms of taste, nutrients and yield with least waste of resources.”
If there is any doubt about how the plants are growing at a certain spot, the researchers can look at their measurements of the different environmental variables to identify the precise conditions in that area. When the plants are growing better in one place than another, the instrumentation helps them to work out why.
The digital twin doesn’t just help with day-to-day decision-making. Its added value over a simple monitoring and visualisation system is the fact it can integrate forecasting models, suggest operational changes for the day ahead and then feed back on how this went.
“The farm managers tend to check the dashboard and sensor data at the beginning and end of the workday,” says Jans-Singh. “So we’ve set the system to provide a forecast at 4am ready for the first workers arriving at 6am to help with decision-making for the day, and at 4pm to warn about possible conditions that will happen overnight just before the workers leave. This might mean reducing ventilation if the farm is likely to be too cold, temporarily adding a heater in a specific location, or trialling different light settings.”
“All the time we’re trying to improve yield but while we’re doing this we can tweak different elements of the light spectrum to make plants grow in a different way to increase sugars and starches, and therefore their nutritional value,” adds Ballard.
The researchers work closely with Growing Underground’s Data Platform Lead Jakob Thomas, who explained that the digital twin has another very important use: it’s guiding the growers in how to build another farm – this time in a disused warehouse that will supply an even larger area of London.
“The digital twin allows us to simulate a hypothetical farm so well that in the engineering and planning phase we can pinpoint much more precisely what equipment we need to keep the environment stable and use the least amount of energy,” he explains.
“That’s pretty important because the systems that are required for heating, ventilation and cooling are quite expensive to build in the first place. We want to be sure we don’t overinvest in equipment that is overpowered and producing unnecessary carbon for the space, but we also want to prevent underinvesting and needing to retrofit.”
“Digital twins are increasingly being considered for optimising energy use in agriculture but our study is the first time a digital twin has been shown to mirror an urban farm through real-time streams of data,” says project leader Ruchi Choudhary, whose research group at the Turing is funded by both the Lloyd’s Register Foundation and UK Research and Innovation (via the Turing’s ‘AI for science and government’ (ASG) programme). “We’re using open-source software and economical platforms like Raspberry Pi so that our system will be transferrable to other bespoke environments.”
Choudhary is an expert at integrating data and models to support the energy efficiency of urban built environments: “As awareness of the need for sustainable food production has risen, so has the drive to incorporate green infrastructure within cities. Urban integrated farming offers an excellent mechanism to contribute to meeting the UK targets of net zero carbon emissions by 2050.”
The World Resources Institute estimates that feeding 10 billion people sustainably by 2050 will require closing a 56% food gap between crop calories produced in 2010 and those needed in 2050. Part of the answer must be to farm in non-traditional spaces, says Ballard.
“Over the next few years we’ll see an exponential growth in the tech behind LEDs, Internet of Things, cloud computing and wireless sensors. But the real game changer will be cheap renewable energy and battery storage. Then we’ll really start to see large vertical farms in and around urban areas growing staples like wheat and maize.
“We realise that growing microgreens won’t address world hunger but we feel that what we are doing is a step in the right direction of food security.”
Ward agrees: “It’s the ambition of the business that’s so fantastic: to grow crops in the heart of the city, get rid of all those food miles, and do it in an efficient way with minimum waste. Anything we can do to try and help them achieve their ambition is just great.”
The team has also valued the chance to see research and a real-world context working together, as Thomas explains: “Connecting the tremendous amount of academic work that goes into creating something like a digital twin to the practice of providing high-quality food locally and sustainably has been really rewarding for the whole team. We learn from each other.”
As the growers continue with their plans to create other farms, they are justifiably proud of what they have achieved so far. They also know there can’t be many farmers who can answer the nursery line ‘how does your garden grow?’ with the words ‘with Raspberry Pi, a digital twin and sensors all in a row.’
 
Further reading:
 
Top image: Betsy McCue / Zero Carbon Farms
"
https://www.turing.ac.uk/people/researchers/antonio-hurtado,"
Dr Hurtado received the PhD degree in Photonic Technology from the Universidad Politécnica de Madrid (UPM), Spain in December 2006. In 2007, he joined the Optoelectronics Research Group of the University of Essex (England, UK) first as a Post-Doctoral Fellow and then as a Marie Curie Fellow (IEF Programme, Project ISLAS, 2009-2011). In 2010 , Dr Hurtado was awarded by the European Commission with a second Marie Curie Fellowship (IOF Programme, Project NINFA, 2011-2014) to carry out a collaborative project between the Universities of New Mexico (USA) and Essex (UK). Following this award in 2011, he started working in the Optoelectronics Research Group at the Center for High Technology Materials (CHTM-UNM) in Albuquerque (NM, USA). In 2014 I was awarded a Chancellor Fellowship by the University of Strathclyde (Glasgow, Scotland, UK). As a result of this award I started working from October 2014 as a Lecturer at Strathclyde’s Institute of Photonics.
"
https://www.turing.ac.uk/people/former-doctoral-students/javad-hosseini,"
Javad started his PhD at University of Edinburgh in Autumn 2016. His research interests are in developing novel Machine Learning techniques to solve Natural Language Processing problems, especially in the area of semantics.
In particular, Javad’s interested in detecting the various statements that are made about the same identifiable entities of various types. He will use the datelines associated with the original stories to detect causal relations between event-types, as well as relations of paraphrase and entailment. The resulting semantics will be built into a semantic parser, which will be evaluated on standard datasets for question answering and text-entailment.
"
https://www.turing.ac.uk/people/enrichment-students/samuel-tonks,"
Samuel is a PhD candidate in the School of Computer Science at the University of Birmingham under the supervision of Iain Styles and Alexander Krull. His research interests lie in developing machine learning based, computer vision approaches to help scientists understand images from a wide range of imaging systems within drug discovery. He aims to improve our understanding of large quantities of data that are generated when imaging multiple samples over time while also combining information from across the modalities and time points to summarise the data in a form that can be qualitatively and quantitively interpreted by a human expert. In particular he is interested in developing advancements in generative modelling to learn to map information across modalities. 
The majority of his time at the Turing is spent working on the SciVision project within the Data Science for Science and Humanities program. The goal of the project is to facilitate interdisciplinary collaboration, through the unification of models and data sets from different scientific disciplines into a python-based computer vision framework. 
"
https://www.turing.ac.uk/research/research-projects/adaptive-machine-learning-changing-environments,"
Adaptive real-time machine learning requires efficient reinforcement learning (how an algorithm should continuously interact with its environment to maximise its reward), online learning (dealing with continuous sequences of real-time data), and adaptive learning from a small sample size. This project aims at developing a series of real-time meta-learning algorithms that can be utilised to achieve continuous learning, predicting and controlling when functioning in a changing environment. The algorithm should function in a wide range of real-world applications, such as urban sensing, industry, and precision agriculture.
Unlike supervised or unsupervised learning based on large amounts of unified and stationary datasets, reinforcement learning focuses on how an agent should continuously interact with its environment to maximise its reward. Under such a setting, massive trials are generally necessary before or during the learning process, and success still highly depends on manually crafted learning architectures and targets. This project aims at solving the sample inefficiency problem in existing approaches.
'Off-line' machine learning approaches typically learn from well-organised and stationary large-scale datasets through batch processing. In stream-oriented, sensor-based systems that are continuously delivering data, this would be unrealistic and inappropriate for many real-world infrastructure applications. Ideally, the learning of targeted hidden knowledge should be incremental, leveraging multi-source data streams. This project aims at developing online algorithms for continuous learning with guarantees of success.
Existing machine learning approaches generally train their models with massive amounts of data collected in stationary environments. However, such approaches are not applicable to the ever-changing environments typically found when sensor/actuator systems are deployed to make an infrastructure smart (e.g. smart cities, autonomous vehicles etc.). It's unfeasible to re-collect and re-organise the data and re-train the previously learnt models whenever the environment changes. This project aims at developing high level 'meta-learning' algorithms that can rapidly notice environmental changes based on a limited amount of sensing data samples, and continuously adjust the rest of the learning model accordingly.
The overarching aim is to deliver real-world application-driven research into real-time adaptive machine learning techniques, by developing the applicable theory, algorithms, architectures and applications. Theoretical foundations will drive the distributed online meta-learning algorithms that can run at the network edge. Potential solutions to specific real-world problems in fields like urban sensing, cyber-physical system (CPS) controlling and precision agriculture will also be explored.
 
The expected results in terms of effective reinforcement learning, online learning and small-sample-size adaptive learning will bridge the gap between the theoretical study of static batch machine learning and the wide range of real-world applications within changing environments.
This project is part of the Data-centric engineering programme's Grand Challenge of 'Resilient and robust infrastructure'.
The expected results can be applied to complicated systems which are difficult to explicitly describe, and where hidden knowledge about the system can be learnt continuously and adaptively. For example, promising applications include (but are not limited to) short-term rainfall prediction for urban infrastructure scheduling in smart cities, continuous control of CPS assets under changing environmental influences, and adaptive yield prediction based on real-time crop vigour analysis in precision farming.
Working with National Institute of Agricultural Botany's East Malling Research (NIAB EMR) station to deploy precision farming solution test.
Working with Housing and Development Board (HDB) Singapore to develop precise short-term rainfall prediction solutions for the city's water buffer scheduling and adaptive controlling.
Working with Ridgeview Winery to deploy precision farming solution test.
Dr. Cong Zhao - [email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/strategic-priorities-fund-lwm,"
Research funded by the Strategic Priorities Fund, EPSRC Grant P/T001569/1
Read more about the Strategic Priorities Fund here
"
https://www.turing.ac.uk/research/interest-groups/economic-data-science,"
The goal of this interest group is to provide a platform for economics and econometrics research at the Turing, and to foster collaborations and exchange between economics, statistics, and computer science. Those fields share a strong joint interest in artificial intelligence and machine learning methods, and in the application of those methods to topics like causal inference, policy evaluation, welfare analysis, social networks, language processing, spatial mobility, and high-dimensional inference, to name just a few. We will organize reading groups, informal workshops, seminars, and conferences on such topics at the intersection between economics and data science. 
We want to encourage economists to come to the Turing on a regular basis, and to interact and engage with the other fields that are already strongly represented here. There is a large potential for collaborations on joint research projects and funding applications that will be stimulated by the regular activities of the interest group.
Addressing causal inference problems in economics applications.
Policy evaluation and design with heterogeneous populations.
Applications of reinforcement learning and agent-based modelling.
Addressing normative questions of AI from the perspective of causal impact and social welfare.
Inference methods for social network data.
Production networks and supply chains.
Analysis of large-scale transaction data both on consumer and firm-to-firm data.
Analysis of high frequency national account data.
Natural language processing applications in economics.
Click here to join us and request sign-up
Mingli Chen[email protected]
Martin Weidner[email protected]
"
https://www.turing.ac.uk/events/automating-crowd,"
296 KB
The relationship between automation and human work is a longstanding area of concern and reflection, and raises questions around public policy and across disciplinary boundaries. The use of ‘crowdsourced’ efforts and the increasing diversity of contributors in human-machine interactions presents particular challenges. Together these raise complex questions that have technical, social and ethical implications.
Automating the crowd is a one day workshop which will bring together participants from neuroscience, digital humanities, sociology, health and other disciplines to shape the public debate around these challenges. 
#AutomatingTheCrowd
The collaborative workshop focuses on four topics as a frame for discussion:
Automating the crowd will create a space for dialogue and enquiry about the relationship between automation and human work, which we hope will act as an incubator for future collaborations across disciplines and professions. Participants will be encouraged to share their own ideas and will be invited to engage with and shape proposals from the organisers about new writing projects and networks.
The workshop aims to reflect on how human work can be hidden within apparently automated systems in a discussion of ‘Who is the real Mechanical Turk?’ It will also focus on the importance of human ‘microwork’ in the development of automated systems, for instance in considering the case of crowdsourced health research and its role in informing the development of machine learning systems.
During the workshop, participants will consider questions about how and why people come to be involved in the development and use of automated systems and how their involvement is shaped by social, political and economic factors associated with digital literacy and community engagement.
After the workshop, we will host a public event, Real people behind artificial intelligence. This panel-led debate will focus on opening up and addressing questions about whose work underpins automation, whether this is work at all, and if so what kind? It will be a lively and multidisciplinary conversation, with as much audience participation as possible.

"
https://www.turing.ac.uk/research/research-projects/turing-commons,"
Launched in 2020, the project sits under the public policy programme’s ethics and responsible innovation theme. The aim of the Turing Commons is to provide a range of resources for various stakeholders that are curious about the ethical and responsible use of data. The guidebooks and subsequent training offerings aim to host resources for everyone, whether you're a curious citizen looking to have greater control over local matters, or a data scientist looking to take more responsibility over the consequences of your research.
Visit the Turing Commons
 
The four guidebooks below are updated and added to regularly, representing a living body of knowledge and resources.
Since 2020, we have developed reusable illustrations collaboratively with Jonny Lighthands. These have been shared for public use through our GitHub repository and can be reused under CC-BY 4.0 license with the appropriate citation:
View more illustrations
The Responsible Research and Innovation (RRI), Public Engagement of Data Science and AI (PED) and AI Ethics and Governance (AEG) guidebooks are designed for specific training courses that have been run previously or are currently in the process of being organised in collaboration with the Skills Team.
Typically, the training courses are held over the course of a week, are free, and open to any participants that meet the application criteria – this varies across course. For reference, please see the RRI and PED courses held remotely via Zoom in 2021 and 2022 respectively. Self-paced, online iterations of these training offerings will also be made publicly available via The Alan Turing Institute’s Online Learning Platform, starting in Autumn 2022.
Following a successful launch of the Responsible Research and Innovation training, the Turing are now working with partner organisations, such as the University of Edinburgh and their Biomedical AI Centre for Doctoral Training, to revise and tailor the structure, content, and delivery of the course to specific disciplines or domains. This will allow for open-source training material and guidance to be used in both self-directed learning and by external facilitators.
Dr Christopher Burr[email protected]

"
https://www.turing.ac.uk/students/,"



In-person tickets are now sold out. You can still secure your place at AI UK through the live stream and watch the fantastic showcase, which will host over 150 expert speakers across three stages: conversation, research and impact. Participants can get involved in sessions by asking questions via Slido, taking part in polls and chatting online with other like-minded data science enthusiasts.
Live streaming AI UK means the flexibility to design your agenda around existing commitments. With an affordable ticket price and a plethora of talks, roundtables and panels, virtual attendance is ideal if you want to stay up to date with the cutting edge of data science and AI.
Whether you’ve discovered machine learning on your undergraduate journey and can’t get enough, you’re setting out towards destination data science on your MSc or your PhD studies are taking you deeper into artificial intelligence (AI), AI UK 23 is an essential stop on your research path.
Experience the latest developments in cutting-edge tech, delve deeper into foundational AI and emerging research areas, and discover new professional and academic opportunities.
AI UK is where you’ll be with your community. Listen and learn from the academics, researchers and innovators already making waves in a variety of sectors. You’ll have opportunities to take part in debates and discussions on invigorating topics and find inspiration through our programme of world-leading speakers.
Learn from some of the sector’s best and most informative speakers, and discover the incredible leaps being made in machine learning through myriad interactive displays, given by the experts.
At AI UK you’ll have access to the top minds in the UK’s burgeoning AI and data science landscape. With networking and community-growth workshops across both days, there’ll be plenty of opportunities for you to connect with the ecosystem’s leading thinkers.
And it’s not just a showcase for experienced professionals; AI UK is an event for everybody who has an interest and role to play in the UK’s machine learning landscape – including you! Come along to listen to student-led discussions and partake in debates while making diverse connections and expanding your network.
Whether you’re interested in interactive learning, inspiring interviews and debates or connecting with our diverse ecosystem, AI UK will allow you to delve deeper into new developments and current trends in data science, AI and machine learning.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.


"
https://www.turing.ac.uk/people/enrichment-students/andreea-avramescu,"
Andreea's research interests lie in the fields of personalised medicine, optimisation, and data science, and how all these can be used together to improve the availability of targeted treatments at a global scale. Andreea has an MSc in Data Science and has previously worked on various research problems from within the fields of social sciences, law, computer science, and operations research.
"
https://www.turing.ac.uk/news/publications/ptype-probabilistic-type-inference,"
Type inference refers to the task of inferring the data type of a given column of data. Current approaches often fail when data contains missing data and anomalies, which are found commonly in real-world data sets. In this paper, we propose ptype, a probabilistic robust type inference method that allows us to detect such entries, and infer data types. We further show that the proposed method outperforms existing methods.
Ceritli, T., Williams, C.K.I. & Geddes, J. (2020) ptype: probabilistic type inference. Data Mining and Knowledge Discovery, 34(3), pp. 870–904
"
https://www.turing.ac.uk/events/northern-data-debates-rage-against-technology-machine,"
Spirited conversations that bring world-leading experts together from a variety of backgrounds to discuss issues of critical interest where data science, artificial intelligence and the real world collide. Sponsored by The Alan Turing Institute, the National Innovation Centre for Data (NICD) and Newcastle Data.
From Amazon to Facebook and from algorithms to locational surveillance, technological advances over the past decade have profoundly changed society. Many of these changes have been positive, including connecting people in new ways, expanding new forms of entrepreneurship, helping government and cities run more efficiently, and improving health and medical care. Other impacts have proven more worrisome, however, especially where individual privacy and technological bias are concerned.
Our second Northern Data Debate brings together experts in technology, digital change, and society from a variety of perspectives to discuss why we’re so frustrated with the technology machine, how we can ensure technology is a force for good for people and society, and how we best tackle the challenges that new technologies throw our way.

"
https://www.turing.ac.uk/research/research-projects/living-machines,"
The Alan Turing Institute and the British Library, together with researchers from a range of universities, have been awarded £9.2 million from the UKRI's Strategic Priorities Fund for a major new project. Led by the Arts and Humanities Research Council (AHRC), ‘Living with Machines’ is one of the biggest and most ambitious humanities and science research initiatives ever to launch in the UK.
The project brings together historians, data scientists, geographers, computational linguists, and curators to devise new methods in data science and artificial intelligence that can be applied to historical resources, producing tools and software to analyse digitised collections at scale.
Living with Machines has a particular set of research interests: specifically to examine the ways in which technology altered the lives and culture of people in Britain in the long 19th century (c.1780-1918). The project is marshalling a whole range of sources that have already been digitised from maps, census returns, newspapers, books and journals. While our data is historical, the questions are perhaps never more pertinent as AI is changing the way we live in the 21st century. 
Its ten key aims are to:
The research methodologies and tools developed as a result of the project are transforming how researchers can access and understand digitised historic collections. We offer not only a host of new software and methods for wrangling this data, but examples of how this changes the history we can tell. Find out more about software and methods.
Furthermore, we have sought to lower the bar to others trying to do this work in future in our forthcoming book Collaborative Historical Research in the Age of Big Data (Cambridge University Press, November 2022). The book discusses not only how to set up an interdisciplinary collaborative team, but also takes readers through the steps of getting hold of cultural heritage data and the infrastructure required. While it suggests a set of pragmatic strategies that projects can employ it also highlights some key structural issues that can only be addressed by a rethink of national policy and funding priorities in the UK. 
 
Ahnert, Ruth, et al. Collaborative Historical Research in the Age of Big Data. Cambridge University Press, Available Open Access.
Ardanuy, Mariona Coll, et al. “Resolving Places, Past and Present: Toponym Resolution in Historical British Newspapers Using Multiple Resources.” Proceedings of the 13th Workshop on Geographic Information Retrieval, Association for Computing Machinery, 2019, pp. 1–6.
Arenas, Diego, et al. Design Choices for Productive, Secure, Data-Intensive Research at Scale in the Cloud. arXiv:1908.08737, arXiv, 15 Sept. 2019.
Beelen, Kaspar, Jon Lawrence, Daniel C. S. Wilson, and David Beavan. “Bias and Representativeness in Digitized Newspaper Collections: Introducing the Environmental Scan.” Digital Scholarship in the Humanities, July 2022, p. fqac037.
Beelen, Kaspar, Ruth Ahnert, David Beavan, Mariona Coll Ardanuy, Kasra Hosseini, et al. Contextualizing Victorian Newspapers.
Beelen, Kaspar, Ruth Ahnert, David Beavan, Mariona Coll Ardanuy, Emma Griffin, et al. Living with Machines: Exploring Bias in the British Newspaper Archive.
Beelen, Kaspar, Federico Nanni, Mariona Coll Ardanuy, Kasra Hosseini, et al. “When Time Makes Sense: A Historically-Aware Approach to Targeted Sense Disambiguation.” Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics, 2021, pp. 2751–61.
Boyd Davis, Stephen, et al. “Can I Believe What I See? Data Visualization and Trust in the Humanities.” Interdisciplinary Science Reviews, vol. 46, no. 4, Oct. 2021, pp. 522–46.
Coll Ardanuy, Mariona, Kasra Hosseini, et al. A Deep Learning Approach to Geographical Candidate Selection through Toponym Matching.
Coll Ardanuy, Mariona, Federico Nanni, et al. “Living Machines: A Study of Atypical Animacy.” Proceedings of the 28th International Conference on Computational Linguistics, International Committee on Computational Linguistics, 2020, pp. 4534–45.
Coll Ardanuy, Mariona, Kaspar Beelen, et al. “Station to Station: Linking and Enriching Historical British Railway Data.” Proceedings of the Conference on Computational Humanities Research 2021, Proceedings of the Conference on Computational Humanities Research 2021.
View all Living with Machines publications
Publication of the Living with Machines book on data, infrastructure, and collaboration
We are delighted to share with you Open Access our book ‘Collaborative Historical Research in the Age of Big Data: Lessons from an interdisciplinary project’, published by Cambridge University Press, as part of the series Elements. Co-authored by Ruth Ahnert, Emma Griffin, Mia Ridge, Giorgia Tolfo, and the LWM team, the book addresses the challenges of establishing and managing a truly multidisciplinary digital humanities project in the complex landscape of cultural data in the UK. In contrast to many previous digital humanities projects which have sought to create resources, the project was concerned to work with what was already there, by leveraging more than twenty-years' worth of digitisation projects in order to deepen our understanding of the impact of mechanisation on nineteenth-century Britain. Sharing our experience, we hope to provide tools and methods for future researchers seeking to undertake digital history projects.
The book launch will take place online on 7th March 2023, 5PM GMT, as part of AI UK Fringe Events. Register here.
 
Living with Machines exhibition 
The exhibition ‘Living with Machines’, co-curated by the British Library and Leeds City Museum, and inspired by the Living with Machines research project, opened on July 29, 2022 and concluded on January 8, 2023 at Leeds City Museum.
The Living with Machines exhibition revealed surprising parallels between the Industrial Revolution and today’s world of ‘big tech’, from the origins of football leagues and fast fashion to the 9-to-5 working day. The exhibition reached more than 42,000 visitors in total.
Find out more about the exhibition and related events
Email [email protected]
Visit the Living with Machines website
Sign up to the Living with Machines newsletter
Follow @LivingWMachines on Twitter
The research engineering groups from the Turing and British Library are developing new cutting-edge research methods blending data science and digital humanities, in collaboration with linguists and historians. These range from developing methods to query terabytes of unstructured data, to designing deep learning techniques able to identify individuals and extract their life story from multiple sources of data, spanning decades.
While underpinning research through infrastructure, the Research Engineering Group is also inspiring new questions to be asked at a hitherto unprecedented scale in the humanities.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-strathclyde,"
www.strath.ac.uk
"
https://www.turing.ac.uk/people/guest-speakers/neil-ferguson,"
Neil Ferguson is founding director of the MRC Centre for Global Infectious Disease Analysis at Imperial College London. He also heads the NIHR Health Protection Research Unit for Modelling and Health Economics and the Jameel Institute for Disease and Emergency Analytics (J-IDEA). He is an international member of the US National Academy of Sciences and a fellow of the UK Academy of Medical Sciences.
Neil Ferguson uses mathematical and statistical models to investigate the processes shaping infectious disease pathogenesis, evolution and transmission. In addition to basic theoretical work, Professor Ferguson has applied models to study the transmission and control of COVID-19, Ebola, influenza, SARS, BSE/vCJD, HIV, dengue, foot-and-mouth disease and bioterrorist threats.
He was educated at Oxford University where he also undertook postdoctoral research, before he moved to Imperial College. He is best known for the real-time modelling of emerging infectious disease outbreaks, having worked on the transmission dynamics and control of SARS, avian (H5N1) and pandemic (H1N1) influenza, MERS-coronavirus, Ebola, Zika and now COVID-19. His other major research interest is in vector-borne diseases – he has worked on dengue throughout his career and more recently has expanded his research to include Yellow fever, Zika and malaria.
Neil Ferguson has always worked at the interface of science and policy, and advises governments, the World Health Organisation and multiple other bodies on emerging infections and infectious disease epidemiology and modelling.
"
https://www.turing.ac.uk/people/researchers/griffith-rees,"
"
https://www.turing.ac.uk/sessions/how-to-make-an-impact-public-participation-stories-2/,"


"
https://www.turing.ac.uk/people/guest-speakers/sundeep-bhandari,"
Sundeep works at the National Physical Laboratory (NPL) the UK’s National Measurement Institute, responsible for measurement strategy and delivery in the UK. NPL (the home of atomic time keeping, packet switching and once Alan Turing himself) develops and maintains the national primary measurement standards and is a Public Corporation owned by HMG's Department of Business, Energy and Industrial Strategy (BEIS).
Sundeep’s work focuses on shaping the evolution of physical metrology and measurement into the ‘cyber-physical’ world, embedding measurement and traceability into processes using digital and data sciences to deliver confidence in the intelligent and effective use of data. NPL’s more recent work address’ research and development in measurement standards that provide traceability and confidence in end-end digital process’, as well as develop the assets required in the future to underpin and enable economies and societies to have confidence in ‘data driven decision making at the speed of AI’. Sundeep sits on multiple cross-government and departmental steering boards and coordination groups that cover digital technologies and digital technology standards. He has led NPL’s involvement in the partnership tasked with establishing the UK Governments International AI Standards Hub Pilot.  
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/visualisation-computer-systems-architectures,"
"
https://www.turing.ac.uk/people/guest-speakers/james-halverson,"
Jim Halverson is an Associate Professor of Physics at Northeastern University in Boston, Massachusetts.
His research focuses on the interface between string theory, particle physics, cosmology, mathematics and deep learning. He is particularly interested in the string landscape and its implications for particle physics and cosmology beyond their standard models. These implications often follow from the structure of extra-dimensional geometries, of which there are many possibilities. Halverson’s research, therefore, requires importing techniques from mathematics and computer science.
Recently, Halverson’s interest in the interface of physics and deep learning has continued to grow. To that end, he is a co-PI and serves on the institute board of the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI) and co-organises Physics ∩ ML.
"
https://www.turing.ac.uk/people/guest-speakers/emily-jesper-mir,"
Emily Jesper-Mir is a Partnerships and Engagement Manager within Data for Science and Health, where she shares the role with Rebecca Asher. She joined Wellcome in 2020 as a Strategy and Engagement Manager for Understanding Patient Data, a program that has since moved to NHS Confederation as of April 2023.
Emily's work focuses on the potential benefits of data science and digital technologies for improving people's health, while also acknowledging the risks and unintended harms that may arise. She supports, devises, and funds work that takes into account the needs, views, and aspirations of the communities involved in developing, using, and affected by new tools and ways of using data.
Emily is dedicated to ensuring that patients' voices and the public interest are at the heart of data science research, policy, and practice. She advocates for engagement that involves people from diverse backgrounds and creative, inclusive ways of having conversations as part of building trusted relationships.
Before joining Wellcome in 2020, Emily worked as a Partnership Director at Sense about Science, a charity that stands up for the public interest in sound science and evidence. Alongside Rebecca Asher, she led SaS' public engagement program, which helped researchers to communicate difficult evidence debates or health datasets simply and accurately through co-design and co-creation, such as the co-design of the INSIGHT HDRUK hub website with patients and the public. She was also involved in facilitating the co-design of a public resource: Data Science: a guide for society.. Emily joined Sense about Science following a successful career in scientific publishing.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/game-theory,"
"
https://www.turing.ac.uk/people/guest-speakers/yonina-eldar,"
Yonina C. Eldar is a Professor in the Department of Math and Computer Science at the Weizmann Institute of Science, Rehovot, Israel, where she heads the center for Biomedical Engineering and Signal Processing. She is also a Visiting Professor at MIT and at the Broad Institute and an Adjunct Professor at Duke University, and was a Visiting Professor at Stanford University. She is a member of the Israel Academy of Sciences and Humanities, an IEEE Fellow and a EURASIP Fellow.
She has received many awards for excellence in research and teaching, including the IEEE Signal Processing Society Technical Achievement Award, the IEEE/AESS Fred Nathanson Memorial Radar Award, the IEEE Kiyo Tomiyasu Award, the Michael Bruno Memorial Award from the Rothschild Foundation, the Weizmann Prize for Exact Sciences, and the Wolf Foundation Krill Prize for Excellence in Scientific Research.
She is the Editor in Chief of Foundations and Trends in Signal Processing, and serves the IEEE on several technical and award committees. She heads the Committee for Promoting Gender Fairness in Higher Education Institutions in Israel.
"
https://www.turing.ac.uk/news/publications/feasibility-driven-approach-control-limited-ddp,"
Differential dynamic programming (DDP) is a direct single shooting method for trajectory optimization. Its efficiency derives from the exploitation of temporal structure (inherent to optimal control problems) and explicit roll-out/integration of the system dynamics. However, it suffers from numerical instability and, when compared to direct multiple shooting methods, it has limited initialization options (allows initialization of controls, but not of states) and lacks proper handling of control constraints. In this work, we tackle these issues with a feasibility-driven approach that regulates the dynamic feasibility during the numerical optimization and ensures control limits. Our feasibility search emulates the numerical resolution of a direct multiple shooting problem with only dynamics constraints. We show that our approach (named Box-FDDP) has better numerical convergence than Box-DDP+ (a single shooting method), and that its convergence rate and runtime performance are competitive with state-of-the-art direct transcription formulations solved using the interior point and active set algorithms available in Knitro. We further show that Box-FDDP decreases the dynamic feasibility error monotonically—as in state-of the- art nonlinear programming algorithms. We demonstrate the benefits of our approach by generating complex and athletic motions for quadruped and humanoid robots. Finally, we highlight that Box-FDDP is suitable for model predictive control in legged robots.
2.3 MB
"
https://www.turing.ac.uk/events/rsf-seminar-series-developing-and-publishing-code-trusted-research-environments,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility (RSF) project.
Event title: Developing and publishing code for trusted research environments 
Sub-title: Best practices and ways of working - lessons from the Wales Multimorbidity Machine Learning Project
Speaker: Ed Chalstrey, Research Data Scientist, The Alan Turing Institute
Trusted Research Environments (TREs) are becoming commonly used for the analysis of data from a range of sources, particularly electronic health records. Data within TREs are kept secure and are only accessible following appropriate approvals and access being granted, to comply with the legal requirements of data providers (e.g., the NHS), allowing research to be carried out safely.
But how can researchers ensure that analyses carried out with TREs are as reproducible as possible, given the constraints of working with sensitive data in a largely closed environment? What are the considerations when it comes to publishing results and crucially methods code, when that code has been developed to run on sensitive data that cannot itself be published?
In this talk, Ed Chalstrey will discuss some of the ideas around beginning to answer these questions that came out of the Wales Multimorbidity Machine Learning (WMML) project. This is in collaboration with Swansea University, The University of Manchester and The Alan Turing Institute. The talk will begin with an introduction to TREs, with some examples of what these platforms look like. Ed will then move on to discuss the report he has authored offering practical advice that researchers using TREs can follow.
 
Everyone is welcome at the session. However, the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn more about developing and publishing code within secure research environments and how this learning can be applied to multiple long-term conditions research.
By registering for the event, you are agreeing to the events code of conduct.
Register now
After registering, you will receive a confirmation email containing information about joining the meeting.
"
https://www.turing.ac.uk/events/building-future-urban-analytics-blueprint,"
56.4 KB
The world is increasingly urban, complex, data-rich, and machine-learned. Urban analytics uses data on our everyday experiences in cities to improve the efficiency of urban systems, raise the quality of life for inhabitants, and to increase our understanding of how people interact with the built environment. This research is critical as cities and their inhabitants face intensifying, system-wide environmental, social, and economic challenges.
Research areas in Urban Analytics include transit, commuting, and spatial interaction; segregation and the study of urban society; architecture and design; environmental and sustainability science; Internet of Things (IoT), digital twins, and smart cities; social network analysis; crowd behaviour; and economic development, among others. Given the central position of the city in society, The Alan Turing Institute’s Urban Analytics Programme seeks to address these challenges through data-intensive science. 
This collaborative workshop will take place in Bristol, UK.
This workshop is a 2+1 day event that brings together experts from a range of backgrounds. The first two days, we will revise and finalise our shared vision for the Urban Analytics Programme. On the third optional day, we invite all attendees to sessions & our public consultation at the Festival of the Future City on Friday, engaging with the broader public. 
We invite participants from a variety of institutional backgrounds and career stages, in particular early career & postgraduates, to attend.  
Day 1 and 2 (16 and 17 October) form the core of the “Building a Future” event. In these two days, we expect attendees from a variety of institutions and career stages to come together to review, revise, and improve our Urban Analytics programme vision statement & strategic plan. This will mainly be oriented around short research talks, breakout sessions to discuss and critique the vision statement, collaborative writing to improve the statement, and networking events to foster introduce and connect workshop participants.  
The optional Day 3  (18 October) of the workshop fosters broader public engagement at the Festival of the Future City. We invite researchers in all stages of their career to attend workshops and discussion at the Festival to represent the Urban Analytics Programme in discussions at this international on the future city. In addition, we invite these researchers to our public consultation with organisations at the Festival who are interested in getting started with data science and cooperating with the Turing and the Urban Analytics programme. 
Apply to attend is now closed. 
"
https://www.turing.ac.uk/people/researchers/christopher-yau,"
Christopher is Professor of Artificial Intelligence at the University of Oxford in a joint position between Nuffield Department for Women's and Reproductive Health and the Nuffield Department of Population Health. His group is based at the Big Data Institute. He is also Director of the Health Data Research-Alan Turing Institute Wellcome PhD programme in Health Data Science and a recipient of the UKRI Turing Artificial Intelligence Fellowship.
Christopher was previously Professor of Artificial Intelligence at the University of Manchester, a Turing AI Fellow and Director of the Health Data Research UK-Turing Wellcome PhD Programme in Health Data Science. He received his undergraduate degree in Engineering at the University of Cambridge in 2004 and a D.Phil in Statistics from the University of Oxford in 2009. He was also previously a Lecturer in Statistics at Imperial College London, Associate Professor in Genomic Medicine at the University of Oxford, and Professor of Artificial Intelligence at the University of Birmingham.
His research is focused on issues related to the interpretation of high-dimensional data arising from modern molecular technologies and health systems and how such data can be used to give insights into the molecular basis of human disease particularly cancer. His efforts in this area span a spectrum of areas from core statistical and machine learning methodological research to wet lab-based experimental investigations to translational clinical research. He currently co-leads the Machine Learning Clinical Interpretation Partnership as part of the Genomics England 100,000 Genomes Project. He is a recipient of a UKRI Turing AI Acceleration Fellowship.
"
https://www.turing.ac.uk/people/former-doctoral-students/zhenzheng-helen-hu,"
Helen started her doctoral studies at The Alan Turing Institute in October 2017. She is registered in the Statistics Department at UCL and is supervised by Dr. Ioanna Manolopoulou and Dr. Ioannis Kosmidis. Her main research interests are Statistical Learning, Statistical Inference, and Bayesian Modeling. Helen has recently graduated from University of Edinburgh with a masters in Statistics and Data Science and holds a BA from University of California, Berkeley with a double degree in Mathematics and Statistics.
Helen will be working on addressing a common challenge in binary studies: observations which are potentially mislabelled or may be missing, but whose mislabelling or missing mechanisms are related to the underlying process itself. The goal is to develop a formal framework of statistical modeling in the presence of inherent missingness and sampling biases. This topic relates to many applied research in Turing.
"
https://www.turing.ac.uk/people/researchers/david-de-roure,"
David De Roure is Professor of e-Research at University of Oxford, where he is jointly based in the Department of Engineering Science and the Humanities Division. He was previously Director of the Oxford e-Research Centre 2012-17.
David's research is distinctively interdisciplinary: his major recent grants are Fusing Audio and Semantic Technologies, The Theory and Practice of Social Machines, and Cyber Security of the Internet of Things. He is best known for his work on semantic web, scientific workflow systems, and digital musicology, and is a frequent speaker on the future of digital scholarship.
From 2009-13 David was the UK National Strategic Director for Digital Social Research for the Economic and Social Research Council, and subsequently a Strategic Advisor. He was closely involved in the UK e-Science programme and a founder of the UK Software Sustainability Institute.
Prior to moving to Oxford in 2010 David was Professor of Computer Science at University of Southampton, UK. He holds a PhD in Computer Science from Southampton. He is a Fellow of the British Computer Society and a Fellow of the Institute of Mathematics and its Applications. 
 
David's interests are in Digital Humanities and collaborations with libraries, archives, and creative industries. He is part of the Data Science and Digital Humanities Interest Group.
His research activities are primarily focused on data science and music, including the use of AI in composition.
He is also interested in social data science, focusing on new and emerging forms of data and especially Internet of Things.
David has a longstanding interest in research infrastructure, including computational methods, automation, and software sustainability, and is interested in developing further work in computational archival science.
"
https://www.turing.ac.uk/research/research-areas/algorithms,"
"
https://www.turing.ac.uk/people/researchers/ali-shahin-shmasabadi,"
I am a Research Associate within the artificial intelligence (safe and ethical) programme under supervision of Adrian Weller at The Alan Turing Institute. Meanwhile, I am a visitor at CleverHans Lab where I was a Postdoctoral Fellow under the supervision of Nicolas Papernot at Vector Institute before joining the Turing.
I have received a PhD in Computer Science from the Queen Mary University of London. During my PhD, I did internships at Inria (with Aurélien Bellet and Emmanuel Vincent on the intersection of differential privacy and audio) and The Alan Turing Institue (with Adria Gascon and Matt Kusner on the intersection of multi-party computation and machine learning). I was also a PhD visitor at Imperial College London working with Hamed Haddadi.
 
I love to explore the intersection of machine learning, privacy, fairness and image/audio processing.
My research interests and recent works include:
"
https://www.turing.ac.uk/people/external-researchers/luc-de-witte,"
"
https://www.turing.ac.uk/research/research-areas/machine-learning/supervised-learning,"
"
https://www.turing.ac.uk/news/intel-turing-programme-helps-detect-cancer-cells-more-accurately-hospital-artificial,"
​
Cancer cells are to be detected and classified more efficiently and accurately, using ground-breaking artificial intelligence – thanks to a new collaboration between the University of Warwick, Intel Corporation, the Alan Turing Institute and University Hospitals Coventry & Warwickshire NHS Trust (UHCW). The collaboration is part of The Alan Turing Institute’s strategic partnership with Intel.
Scientists at the University of Warwick’s Tissue Image Analytics (TIA) Laboratory – led by Professor Nasir Rajpoot from the Department of Computer Science – are creating a large, digital repository of a variety of tumour and immune cells found in thousands of human tissue samples, and are developing algorithms to recognize these cells automatically.

Microscopic landscape of various types of cells – including tumour cells (in red).
Professor Rajpoot commented:
“The collaboration will enable us to benefit from world-class computer science expertise at Intel with the aim of optimising our digital pathology image analysis software pipeline and deploying some of the latest cutting-edge technologies developed in our lab for computer-assisted diagnosis and grading of cancer.”
The digital pathology imaging solution aims to enable pathologists to increase their accuracy and reliability in analysing cancerous tissue specimens over what can be achieved with existing methods.
“We have long known that important aspects of cellular pathology can be done faster with computers than by humans,” said Professor David Snead, clinical lead for cellular pathology and director of the UHCW Centre of Excellence.
“With this collaboration, we finally see a pathway toward bringing this science into practice. The successful adoption of these tools will stimulate better organisation of services, gains in efficiency, and above all, better care for patients, especially those with cancer.”
The initial work focuses on lung cancer. The University of Warwick and Intel are collaborating to improve a model for computers to recognize cellular distinctions associated with various grades and types of lung cancer by using artificial intelligence frameworks such as TensorFlow.
UHCW is annotating the digital pathology images to help inform the model. The aim is to create a model that will eventually be useful in many types of cancer – creating more objective results, lowering the risk of human errors, and aiding oncologists and patients in their selection of treatments.
The TIA lab at Warwick and the Pathology Department at the UHCW have established the UHCW Centre of Excellence for Digital Pathology and begun digitising their histopathology service.
This digital pathology imaging solution will be the next step in revolutionising traditional healthcare with computerised systems and could be placed in any pathology department, in any hospital.
“This project is an excellent example of data science’s potential to underpin critical improvements in health and well-being, an area of great importance to the Alan Turing Institute,” said Dr. Anthony Lee, Strategic Programme Director for the Intel-Turing partnership.
Rick Cnossen, general manager of HIT-Imaging Analytics in Intel’s Data Center Group, commented,
“This project has massive potential benefit for cellular pathology, and Intel technologies are the foundation for enabling this transformation. We’ve seen what has happened over recent years with the digitisation of X-rays (PACS). The opportunity to transform the way pathology images are handled and analysed, building on experience with PACS and combining data with other sources, could be truly ground-breaking. This collaboration could not only improve service efficiency, but also open up new and exciting analytical techniques for more personalised precision care.”
-ENDS-
For more information on the Intel-Turing partnership, see turing.ac.uk/intel-turing-programme.
For media enquiries, please contact:
Luke Walton, International Press Officer
+44 (0) 7824 540 863
+44 (0) 2476 150 868
[email protected]
​
"
https://www.turing.ac.uk/people/researchers/kenneth-heafield,"
"
https://www.turing.ac.uk/events/data-study-group-september-2021,"
Applications are now closed. 
Due to COVID-19 the Data Study Group will run remotely over three weeks and will be divided into two stages.
Stage 1: The Precursor Stage (part-time)
Stage 2: The Event Stage (full-time)
Applicants should be able to commit to the duration of the event. The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected] to discuss any reasonable adjustments.
The challenges are:
Please see below for further details on each challenge.
The skills that we think are particularly relevant to the challenges for this Data Study Group are listed under each challenge description below. Please note, the lists are not exhaustive and we are open to creative interpretation of the challenges listed. Diversity of disciplines is encouraged, and we warmly invite applications from a range of academic backgrounds and specialisms.
The following challenges and data sets are related to dementia research and have been provided by the UK Dementia Research Institute (UK DRI) and DEMON Network for researchers to work on during the event.
Amyloid plaques are protein aggregates in the brain of patients with Alzheimer's Disease (AD) and a main hallmark of the disease. Their role in AD is however not yet fully understood. The Dementia Research Institute and Flemish Institute for Bioscience have used a novel omics technique to study the role of amyloid plaques: Spatial Transcriptomics (ST). This technique allows the measurement of all genes in a tissue, retaining spatial information with hundred-micron resolution and allowing in parallel the capture of histological information.
Current research recognizes a number of different plaque morphologies, but the following questions are yet to be answered: Do different plaque shapes affect the cellular and molecular reaction around these plaques? Do they have a different role in the pathology? Are there more shapes than the one we currently distinguish? A better understanding of the relationship between protein aggregate presence and the plaque morphology will hopefully help in better understanding the role of amyloid plaques, and may help in gaining a new perspective on understanding, preventing, and treating AD.
The goal of this project is to predict Amyloid plaque presence and morphology from the histological and gene expression landscape. The methodology generated by this project would also advance the interpretation of actively growing ST data that is of huge analytical demand. In particular, our aims are:
1.    Determine the presence of amyloid plaques based on gene expression and plaque morphology;
2.    Identify potential new sub-types of amyloid plaques using unsupervised clustering methodologies;
3.    Characterise the ST landscape around the plaques for (biologically) relevant features extraction;
4.    Generate machine learning models to learn the relationship between plaque morphology and molecular variation; and
5.    Introduce explainable models to gain (biological) understanding of the model's predictions. 
Useful skills: Primarily image analysis and machine learning methods.
Image analysis, classification and clustering, CNN/VAE, techniques to validate model robustness – e.g. boot-strapping, data visualisation. No experience in working with biological data is required.
 
Cellular development of healthy organisms, as well as their predisposition and response to disease are regulated by molecular programs which ultimately control gene expression. Existing deep learning models are able to predict where DNA is likely to bind regulatory proteins, and what is the probable epigenetic state. From these models we can get predictions about which DNA bases are most important for regulatory activity. However, most models are local in their predictions and only sensitive up to a certain genomic distance. They also have been trained on data from a reference genome assembly: the training/validation data comprise of different regions from the same genome, without explicit mapping across different genomes.
 
In this project we will consider experimental data (specifically histone mark QTLs, linking sequence variability to changes in epigenetic signal) to validate whether state-of-the-art models can pay attention to the right variants, and how this depends on distance to the affected sites. We will also encourage the creation of custom solutions, particularly to enable the use of existing models on new cell-types. Finally, we will aim to obtain predictions for the effects of Alzheimers-associated genetic variants on cell-type specific histone modifications.
Essential skills: Machine learning, predictive modelling, feature selection.
Useful skills: Sequence models, deep learning (CNN, RNN, transformers, feature attribution, transfer learning), understanding of gene regulation and omics data, multimodal data integration, causal inference.
 
People living with dementia (PLWD) have been known to suffer from sleep disorders, for example having difficulties falling asleep, waking up irregularly during the night, and waking up too early in the morning. 
Our challenge explores whether data-driven learning tools can complement and personalise the generic advice to help PLWD sleep better.
This dataset contains more than 18,000 nights from multiple dementia patients who were monitored for two years with multiple sensors, like Withings Sleep Mat. This challenge will explore building a personalised recommendation engine that will allow domain experts to suggest possible interventions to promote better sleep techniques.
In the challenge, we will explore the effect on sleep metrics like total time spent in bed when different conditions are changed such as room temperature. More ambitiously, can the model suggest a set of gradual changes in environmental and sleep factors that could potentially allow people to regain a healthy sleep pattern? Personalized therapeutics depends on answers to these questions. Further, our setting is general enough to encourage the development of models that apply to beyond the dataset at hand.
Useful skills: Generative modelling (VAE, GAN), Time series prediction, machine learning, recommender system.

The following are further health research challenges provided by the University of Birmingham for participants to work on during the event:
The microcirculation is critical to life as it is where oxygen is transferred from blood to the tissues. The state of the microcirculation is disrupted in severe illnesses such as trauma or septic shock (serious infection). Current methods of assessing resuscitation are based on evaluating the patient’s global, or macro circulation using advanced cardiac output monitoring. However, there is evidence from septic patients to suggest that where a patient exhibits improvement in their macro-circulation, their microcirculation may remain impaired, meaning the patient is not being adequately fluid resuscitated.
The microcirculation can be imaged directly using so-called dark-field microscopy to image the microscopic blood vessels under the tongue. These images, which take the form of short video sequences of blood flow in the micro vascular system of arterioles and capillaries under the tongue, can be used to assess the perfusion (blood content) of the tissue. This process is currently performed by a human expert but needs specialist equipment, is very labour intensive (45 minutes per video) and has high levels of variability between assessors. In this challenge, the goal is to automate this analysis and to predict a single measure of perfusion from a video with the aim of dramatically reducing analysis time so that assessments may be made in real time.
Useful skills: Computer vision and machine learning, image processing.
 
Understanding the function of proteins in the body often requires a detailed understanding of their structure. Modern tomographic imaging techniques using x-rays and electrons can enable protein structures (or more precisely, their electron density) to be visualised directly at nanometre-scale resolution in 3D. This is usually done on samples that contain many molecules and a major challenge is to identify repeated structures in the data which can, when averaged, improve the resolution of the resulting volumetric reconstruction.
This process, known as sub-tomogram averaging, is commonly used when there is a single known (or easily identifiable) repeating structure. The core of this challenge is to develop techniques that enable all of the repeating structures in the data to be identified. This would enable sub-tomogram averaging to be applied across multiple repeating structures which would lead to further resolution improvements. Ideally, this should be done without manual intervention.
Pattern identification is common in image analysis. However, the 3D images here are very large: 4000 x 4000 x 4000 pixels; the repeated patterns can be in any orientation; and manual identification of the patterns is exceptionally difficult. Brute force searching is therefore unfeasible. Recent advances in computer vision and machine learning hold promise here and it is these that we wish to explore.
Useful skills: Computer vision and machine learning with the following specific techniques of particular interest: self-supervised learning; representation learning; group-equivariant convolutional networks; pattern recognition.
The Turing Data Study Groups are popular and productive collaborative events and a fantastic opportunity to rapidly develop and test your data science skills with real-world data. The event also offers participants the chance to forge new networks for future research projects, and build links with The Alan Turing Institute – the UK’s national institute for data science and artificial intelligence.
It’s hard work, a crucible for innovation and a space to develop new ways of thinking.
Reports from previous Data Study Groups are available here.
Read our FAQs for Data Study Group applicants.
 
Learn more about being a DSG participant including FAQs
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
"
https://www.turing.ac.uk/research/research-projects/visualising-data-profiles-and-analysis-pipelines,"
Analysts and researchers face major hurdles understanding the quality of their data and the knock-on consequences of the choices they make during one stage of data processing on those that follow. Data visualisation offers many benefits that could help analysts and researchers to overcome those hurdles. This project will identify how visualisation techniques should be exploited, and develop a novel visualisation tool for key aspects of data profiling and pipeline design.
This project has three aims. First, to characterise the way in which analysts and researchers profile data and design data processing pipelines. This is important in order to understand the limitations of current profiling and pipeline design methods, the barriers that analysts and researchers face, and the ways in which visualisation techniques could be transformative.
Second, the project will engage with public and private sector analysts and researchers to identify quick wins, share best practice and develop a research agenda for the adoption of visualisation techniques in data profiling and pipeline design. The primary measure of success will be organisations beginning to adopt the techniques that are proposed, to make their profiling and pipeline design more rigorous and efficient. This is a catalyst for more scalable and higher quality data science.
Third, the project will develop a novel visualisation tool for key aspects of data profiling and pipeline design. Success will be defined by uptake of the tool by analysts and researchers during field evaluation, and the benefits that the tool brings to their work. This will be a launch pad for further research, which acknowledges visualisation as an equal partner to computational modelling in data science.
This project can benefit any application area, because data profiling and processing are universal. Existing partners are drawn from domains as widespread as health, urban analytics, retail and government.
[email protected]
"
https://www.turing.ac.uk/research/research-projects/solar-nowcasting-machine-vision,"
Precisely how much solar energy is being pumped into the UK’s electricity grid at any time is not well known, even by the National Grid. This project aims to fix that, using a combination of AI (machine vision), open data and short-term forecasting.
There are many uncoordinated sources of geodata about solar PV installations: government, crowdsourced, and machine vision derived. This project will establish a worldwide open data “clearing house” for solar PV geodata. The clearing house will reconcile data sources, and transform the data into clean datasets consumable directly by machine learning algorithms. The algorithms being used for PV out-turn prediction by regional and national network operators (National Grid) and commercial market participants. This “missing link” will be a force multiplier, reducing carbon missions by enabling new demand management and energy-trading innovations.
Generation forecasting has been identified as a high-leverage opportunity at global scale. This work's open data approach to solar PV installs will enable wider third-party uses, such as analysis to plan future PV installs, and bring broader economic benefits – and in fact was demanded in 2019 by the UK Energy Data Taskforce.
Specific applications of this work include:
"
https://www.turing.ac.uk/people/external-researchers/lilian-edwards,"
Lilian Edwards is a leading academic in the field of Internet law, since 2018 Chair in Law, Innovation and Society at Newcastle University. Prior to joining Newcastle University in 2018, Lilian worked at the University of Strathclyde from 1986 to 1988 and the University of Edinburgh from 1989 to 2006. Lilian then became Chair of Internet Law at the University of Southampton from 2006 to 2008, followed by Professor of Internet Law at the University of Sheffield, when she returned to Scotland to become Professor of E-Governance at the University of Strathclyde until 2018, when she left for Newcastle.
At the Turing, Lilian is a Group Leader on the AI Programme, providing expertise on legal aspects of AI, algorithms and the datified economy. ​Lilian’s research revolves generally around the regulation of new technologies but in particular the Internet, and the Internet of Things. In recent years Lilian has moved towards specialisation in privacy and data protection, especially as applied to AI and algorithms. Beyond privacy, she is interested in algorithmic transparency and justice; robots and autonomous/connected vehicles; and regulation of platforms, especially social media platforms such as Facebook and Twitter. Lilian is a recognised expert in European intermediary liability and the E-Commerce Directive. She is currently looking at regulation of emergent technologies to protect public safety in the pandemic, especially contact tracing apps, immunity passports and social distancing IoT technologies; workplace surveillance, algorithmic hiring and the Internet of Things; and the regulation of ""deepfakes"" or synthetic data and media. ​
​
"
https://www.turing.ac.uk/events/ghost-machine-impact-cyber-physical-interactions-data-centric-engineering,"
Synopsis: Networks of sensor devices are being embedded into the world around us, indeed we are already seeing the benefits that data from sensor-based systems bring to asset management, condition monitoring and other engineering applications. To ensure their continuous operation requires new network protocols, data analytics, but what is less understood is the impact and strong understanding of how the physical world impacts on this cyber world. This talk will mention some of the key challenges that such interactions bring and what technologies we have been developing to help the continuous assurance that sensed data is correct, timely and cost-effective. 
Prof Julie A. McCann is a Professor of Computer Systems in Imperial College London (IC), where she leads the Adaptive Embedded Systems Engineering Research Group, she is Director for the Imperial wide Centre for Smart Connected Futures, was Co-Director of the Intel Collaborative Research Institute for Sustainable Cities and she is CI for the NEC Smart Water Systems Lab and many other substantive projects with industry and academia with a focus on networking and sensing infrastructures to support environments such as smart cities, water and gas networks etc. She is CI on the EPSRC energy/water/food nexus WefWebs project where her focus is on precision farming and wine making.
In the past, her NERC FUSE project designed and deployed a now patented sensing infrastructure for floodplain monitoring in Oxfordshire which serendipitously lead to the patented Cognisense system that can be used for asset monitoring. Her research typically takes the angle of being highly decentralized and self-organizing frugal, and embedded. She is a Fellow of the British Computer Society and is the Associate Editor for ACM Transactions on Adaptive Autonomic Systems (TAAS), has been General and Technical chair for the IEEE International Conference on Self-Adaptive and Self-Organising systems (SASO) and IEEE SECON 2016, SMARTCOMP 2017 and 2019 and has been on the programme committee for IEEE INFOCOM, ACM UBICOMP and many more. Julie has presented her work in A* conferences and keynoted at the Indian Science Conclave Congregation of Nobel Prize Winners, for the encouragement of disadvantaged kids into science and computing in 2008 and in 2018 she received the Suffrage Science award in Maths and Computer Science for scientific achievements and ability to inspire others.

"
https://www.turing.ac.uk/research/research-projects/trustworthy-synthetic-data-practice,"
The project will build upon and further develop capabilities of earlier work on synthetic data and address the associated challenges. We will work directly with data controllers to understand the challenges in evaluating the privacy of client datasets and in communicating these privacy concerns with data owners. The tools, methods and metrics we develop in the project will contribute to an integrated framework that will be operationalised through the Toolbox for Adversarial Privacy Auditing of Synthetic Data (TAPAS), enabling data holders and others in the wider community to generate and evaluate the trustworthiness, scalability, and usability of synthetic data.
The project builds on previous work with synthetic datasets, which are constructed by algorithms to resemble real data enough for research purposes but mitigate concerns around privacy and availability. 
The project aims to consider the use of synthetic data in practice. To this end, we consider the use of synthetic data in the strategically important areas of net zero, digital twins, health, defence and security, and the digital economy. Our work with partners has helped shape this proposal, and we take a challenge-led approach to science and innovation. 
 Our goals are:
• Develop preprints on the development of a framework to assess the trustworthiness of synthetic data and the tools used to generate them.
• Integrate tools for time series, network, and relational data (for finance, health, and CPS for Digital Twins) into the SDG workflow.
• Prototype integration of federated learning techniques with synthetic data.
• Further develop the TAPAS Toolbox for SDG.
• Present a framework with associated tools to measure the trustworthiness of SDG.
• Conduct two workshops: to explore the challenges of trust in synthetic data, and one to measure the efficiency of our work.
In 2021 Gartner predicted that by 2024, 60% of the data used for the development of AI will be synthetically generated, however there remain outstanding questions regarding the efficiency of synthetic data, and a growing need for quality assurance. 
The project, through its workshops, and connection to the Turing Synthetic Data Interest Group, will build a community of researchers, practitioners, and end-users of synthetic data.  By convening this group, we will build trust and understanding around the use of synthetic data and deliver world-class research.  
The variety of our partners indicates the need for synthetic data in tackling diverse challenges including financial fraud, digital twins for manufacturing and transport, and health data analysis. 
[email protected]
"
https://www.turing.ac.uk/events/standards-based-approaches-distributed-energy-resources,"
Hosted by: Supergen Energy Networks Hub, The Alan Turing Institute, Carbon Co-op and OpenADR Alliance

 
 
 
Agenda:
14:30-14:35: Introduction, Myriam Neaimeh, Ben Aylott
 
Part 1: UK standards development and flexibility trials
14:35- 14:50: U.K. PAS 1878 Energy smart appliances, Dr. Nina Klein, BEIS
14:50- 15:05: PowerShaper: A case study in the use of open standards for demand response in the UK, Ben Aylott, Carbon Co-op
15:05-15:15 Q&As
15:15-15:30: ENA’s Open Networks project on Flexibility, Farina Farrier, Energy Networks Association
15:30-15:45: Flexible Power and FutureFlex, Ben Godfrey, WPD
15:45-16:00 Q&As
 
Part 2: OpenADR protocol standard and the trend toward standards-based utility integration for distributed energy resources. 
16:00-16:15: OpenADR Overview and Updates, Rolf Bienert, Managing Director, OpenADR Alliance
16:15-16:30: OpenADR and CTA-2045 Compatibility , Walt Johnson, U.S. Electric Power Research Institute (Ret.)
16:30-16:45: OpenADR and EV Managed Charging, Bob Andrews, CEO, Zevtron
16:45-17:00: Q&As
 
Part 3:
17:00-17:30: Interactive Discussion - Options and Challenges for flexibility using standards-based approaches.
"
https://www.turing.ac.uk/people/researchers/radka-jersakova,"
Prior to joining the Turing, Radka completed a PhD in Cognitive Psychology at the University of Leeds. During her PhD she researched human learning, memory and metacognition. She was also involved in a number of projects evaluating and developing behavioural research methods.
Radka is passionate about using data science tools in collaboration with researchers, research software engineers and external partners (industry, government, third sector) to help solve real world problems. 
"
https://www.turing.ac.uk/people/researchers/hao-ni,"
Dr Hao Ni is a senior lecturer in financial mathematics at UCL since September 2016. Prior to this she was a visiting postdoctoral researcher at ICERM and Department of Applied Mathematics at Brown University from 2012/09 to 2013/05 and continued her postdoctoral research at the Oxford-Man Institute of Quantitative Finance until 2016. She finished her D.Phil. in mathematics in 2012 under the supervision of Professor Terry Lyons at University of Oxford.
Her research interests include stochastic analysis, financial mathematics and machine learning. More specifically she is interested in non-parametric modelling effects of data streams through rough paths theory and statistical models. Rough paths theory is a non-linear extension of classical theory of control differential equations to model highly oscillatory systems, and the core concept in rough paths theory is the signature of a path, which can be used as useful features for learning to summarize sequential data in terms of its effect. Moreover, she is also interested in its applications, e.g. online Chinese handwritten character and financial data streams.
"
https://www.turing.ac.uk/cdn-cgi/l/email-protection,"The website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots. You must enable Javascript in your browser in order to decode the e-mail address.
If you have a website and are interested in protecting it in a similar way, you can sign up for Cloudflare.

Cloudflare Ray ID: 7d73cd7fcef600a7
•

      Your IP:
      Click to reveal
193.60.220.253
•

Performance & security by Cloudflare
"
https://www.turing.ac.uk/research/research-projects/diffuse-multiple-scattering-analysis,"
Diffuse multiple scattering (DMS) is a powerful new X-ray technique that can provide information about all the phases (crystal structures) present in a material sample at unprecedented resolution. However, highly complex data analysis requirements currently limit the application of this technique. This project is developing neural networks that can take a DMS pattern and automatically infer the crystal structures present as well as providing an estimate of the lattice parameters of the materials. The methods developed will be applied in the analysis of DMS patterns for important technological systems such as ferroelectrics and alloys.
Header image: Science and Technology Facilities Council
X-ray scattering happens when X-rays hit a material and alter their direction of travel. Diffuse multiple scattering (DMS) happens when a material has defects in the perfect crystal and it provides structural information not only about the region of the sample in the X-ray beam but in remote regions too. It also allows characterisation of the relative locations of atoms with far higher precision than regular X-ray scattering. 
Ferroelectrics are materials with in internal permanent electric field which can be switched in direction by an applied electric field. Ferroelectrics are currently widely applied as actuators, converting electrical to mechanical energy. These are important for example in sonar pulse systems of submarines. Ferroelectrics also have great potential for application in computer memory. Current champion ferroelectrics all contain lead and there is a drive to understand the role that lead plays in these materials in order to design new materials with comparable function, but no toxic elements. DMS has the potential to provide unprecedented levels of detail regarding the mechanisms underpinning the function of current lead-based ferroelectrics.  
This project aims to develop a workflow that can be applied for rapid, easy analysis of DMS patterns. The work will develop examples and pre-trained network architectures for extracting:
The project will also establish a database of simulated patterns and data labels to allow users to train different networks in the future. This data-analysis workflow will make DMS accessible to a much wider community of materials scientists than it is currently available to. DMS has the potential to offer unprecedented insights into complex phase transition phenomena which underpin the operation of ferroelectric materials (e.g. in sensors, actuators and antennae). The availability of an automated, easy to apply analysis pipeline is crucial to the success of the method.
The work will be applied by users of the Diamond Light Source facility at Rutherford Appleton Laboratory. In particular the work is of interested to the industrial community interested in ferroelectrics, an industry worth up to $480M worldwide. Moreover ferroelectrics are of intense research interest in a range of areas, such as next generation computing, photovoltaics and catalysis. The current champion materials are lead-based and the atomic scale understanding of the mechanisms of ferroelectricity and how multiple phases interact is critical to a rational design programme to make lead free ferroelectrics.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/keele-university,"
"
https://www.turing.ac.uk/events/atlas-ai,"
Kate Crawford is a leading scholar of the social and political implications of artificial intelligence. Her work focuses on understanding large-scale data systems in the wider contexts of history, politics, labor, and the environment. She is a Research Professor at USC Annenberg, a Senior Principal Researcher at MSR-NYC, and she currently holds the inaugural Visiting Chair for AI and Justice at the École Normale Supérieure in Paris. Her latest book, Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence, was published by Yale University Press in May 2021, and has been described as ""timely and urgent"" by Science Magazine.
Registrants for the event will receive a 30% discount code for Atlas of AI when ordering through the Yale UK website. 
 
In this special event, Turing Fellow and Women in Data Science and AI project lead, Judy Wajcman, will host a fireside chat and audience Q&A with Kate Crawford to uncover the damaging effects of the global networks underpinning AI technology, and how our technical systems are designed to serve and intensify existing systems of power, as detailed in the book.
This event is hosted by the Women in Data Science and AI project, part of The Alan Turing Institute's Public Policy Programme. Women in Data Science and AI conduct data science and social science research to inform policy measures aimed at increasing diversity and inclusion in the data science and AI fields, and mitigating AI biases. Their project Hub hosts resources, news and research in the area, including their new report, ‘Where are the women? Mapping the gender job gap in AI’, which reveals persistent structural inequalities in data and AI careers globally.
 

"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/scottish-enterprise,"
"
https://www.turing.ac.uk/taxonomy/term/1281,"
"
https://www.turing.ac.uk/people/researchers/yarin-gal,"
Yarin is an Associate Professor of Machine Learning at the Computer Science department at University of Oxford, Tutorial Fellow in Computer Science at Christ Church, Oxford and part-time fellow at the Alan Turing Institute. Prior to taking his position at Oxford he was a Research Fellow in Computer Science at St Catharine's College, Cambridge. He obtained his PhD from the Cambridge machine learning group, working with Zoubin Ghahramani and funded by the Google Europe Doctoral Fellowship. Prior to that he studied Computer Science at Oxford for a Master's degree under the supervision of Phil Blunsom.
Yarin's interests lie in the fields of linguistics, applied maths, and computer science. Most of his work is motivated by problems found at the intersections of these fields. In his current research he develops Bayesian techniques for deep learning, with applications to reinforcement learning. In the past he has worked on Bayesian modelling, approximate inference, and natural language processing. This work included Bayesian nonparametrics, Gaussian processes, inference algorithms for big data, and work on machine translation.
"
https://www.turing.ac.uk/people/researchers/alessandro-abate,"
Alessandro Abate is Professor of Verification and Control in the Department of Computer Science at the University of Oxford. Previously he has done research at SRI International and at Stanford University, and has been an Assistant Professor at the Delft Center for Systems and Control, TU Delft. He has received a Laurea degree from the University of Padova and a MS/PhD at UC Berkeley. 
For more details, please see his departmental page or his research group page.
Alessandro's research interests lie in the analysis, verification, and optimal control of heterogeneous and complex dynamical models -- in particular of stochastic hybrid systems -- and in their applications in cyber-physical systems (particularly involving energy networks) and in the life sciences (systems biology). He is interested in a principled integration of model-based mathematical techniques with data-driven learning algorithms.
"
https://www.turing.ac.uk/people/research-fellows/bernie-hogan,"
Bernie Hogan completed his BA(hons) at the Memorial University of Newfoundland in Canada, where he received the University Medal in Sociology. Since then he has been working on Internet use and social networks at the University of Toronto under social network analysis pioneer Barry Wellman. Bernie received his Masters of Arts at Toronto in 2003, and defended his PhD Dissertation in the Fall of 2008. His dissertation examines how the use of ICTs alters the way people maintain their relationships in everyday life. In 2005 he was an intern at Microsoft’s Community Technologies Lab, working with Danyel Fisher on new models for email management. 
Bernie Hogan’s research focuses on the creation, maintenance and analysis of personal social networks, with a particular focus on the relation between online and offline networks. Hogan’s work has demonstrated the utility of visualisation for network members, how the addition of new social media can complicate communication strategies, and how the uneven distribution of media globally can affect the ability of people to participate online. Currently, Hogan is working on techniques to simplify the deployment of personal network studies for newcomers as well as social-theoretical work on the relationship between naming conventions and identities.
"
https://www.turing.ac.uk/research/research-programmes/data-science-science,"
It is becoming easier and easier to collect large amounts of data across a broad range of research areas, and there is a growing need to understand how this can best be exploited to make new discoveries. The use of modern computational methods has already revolutionised research in physics and biology, and the stage is set for this approach to become a standard methodology in many different fields. However, we know it is not sufficient to just collect data, and hope that some generic algorithms will be able to help – the crucial step is in how to incorporate the deep knowledge that already exists about a system into the computational methods used.
For example, in particle physics, the search for the Higgs boson was guided by the well-developed theory of the Standard Model. In contrast, the search for particles of 'dark matter' has very little theory to guide it. In linguistics, there has been much progress in developing statistical models of language use, but it is not clear how to combine this with what is understood theoretically about how humans read, write and speak. In others cases, a computational model may be needed to make sense of data – for example, the need for an atomic-level model of a material when characterising its properties from high resolution images.
The main aim of the programme is to work with researchers from all disciplines across the Turing's university partner network, and with national research facilities, to make effective use of state of the art methods in artificial intelligence and data science.
The social sciences and arts provide particularly interesting challenges within the programme. Our understanding is often qualitative, and aligning this with what data sets are telling us can be difficult. There is also a considerable need to provide relevant training in these new methodologies, in a way that is acceptable and meaningful to researchers from non-numerate disciplines.
It is envisaged that, within a very few years, research across universities and national facilities will more and more come to be based on the computational data science and AI methods being developed at The Alan Turing Institute and its partners. It is therefore a key challenge that the programme's research remains at the vanguard of this movement nationally.

In April 2017, the Royal Society published the results of a major policy study on machine learning. This report considered the potential of machine learning in the next 5 – 10 years, and the actions required to build an environment of careful stewardship that can help realise its potential.  
Its publication set the direction for a wider programme of Royal Society policy and public engagement on artificial intelligence (AI), which seeks to create the conditions in which the benefits of these technologies can be brought into being safely and rapidly.
As part of this programme, in February 2019 the Society convened a workshop on the application of AI in science. By processing the large amounts of data now being generated in fields such as the life sciences, particle physics, astronomy, the social sciences, and more, machine learning could be a key enabler for a range of scientific fields, pushing forward the boundaries of science.  
Read the joint discussion paper by the Royal Society and The Alan Turing Institute, which summarises discussions at the workshop. It is not intended as a verbatim record and its contents do not necessarily represent the views of all participants at the event, or Fellows of the Royal Society or The Alan Turing Institute.
For more information about the programme, please contact [email protected]
"
https://www.turing.ac.uk/people/doctoral-students/augustinas-sukys,"
Augustinas is a first year Quantitative Biology PhD student at the University of Edinburgh. He graduated from the University of Edinburgh with an Integrated Master’s degree in Computational Physics. Over the course of his studies Augustinas undertook various research projects which involved electronic structure calculations, applying machine learning to astronomical data, and investigating the black-box behaviour of artificial neural networks. He developed a keen interest in neuroscience, systems biology, statistical physics and machine learning—fields he is excited to delve deeper into during his time at the Turing.
The firing of neurons subject to input stimuli is highly variable so that random fluctuations in the system must be considered when modelling biologically realistic neurons. Augustinas studies the properties of such stochastic neuron models, hoping to shed light upon neuronal dynamics and brain function.
By adapting and extending mathematical approximation techniques previously developed in the context of modelling biochemical cellular processes, he aims to further investigate both single neuron models and population models involving interactions between many neurons. He will also develop parameter estimation methods based on Bayesian inference in order to build a statistically rigorous quantitative description of the studied neural systems.
"
https://www.turing.ac.uk/events/nature-reviews-physics-machine-learning-astrophysics-and-cosmology,"
Machine learning is no longer restricted to data analysis, currently being used in theory, experiment and simulation ⁠- a sign that AI is becoming pervasive in all traditional aspects of research. But are theorists, experimentalists and computational scientists aware of each other’s problems, and the solutions developed to tackle them? Are researchers working in different areas of physics aware of developments in other areas?
Chair - Benjamin Joachimi 

With the bigger and better observatories and state-of-the-art large-scale simulations, researchers in astrophysics and cosmology need to handle and analyse ever-growing volumes of data. These communities have been pioneering new data analysis methods, planning infrastructure to support the huge data-pipelines and building a community approach to tackling their problems. What can the wider physics and AI communities learn from their experience and how can they contribute to the ongoing efforts?


"
https://www.turing.ac.uk/people/external-researchers/eva-grobbink,"
"
https://www.turing.ac.uk/people/doctoral-students/georgia-tomova,"
Georgia is a health data science doctoral student registered with the Leeds Institute for Data Analytics at the University of Leeds. Her main interests are within the fields of epidemiology and causal inference methods.
Her background includes a BSc in Nutrition from the University of Leeds, followed by an MSc in Health Data Analytics during which she was introduced to causal inference methods and developed a keen interest in this area. As part of this, she co-leads the Causal Inference Interest Group at the Institute and has significant experience in teaching causal inference methods to health and social scientists of all career levels.
Georgia's research interests are largely focused on the use of causal inference methods in health and medical research. More specifically, they include:
1. Understanding causal effects in nutritional epidemiology involving compositional data. This is the predominant focus of Georgia's doctoral research. Using directed acyclic graphs (DAGs) and simulations, she examines the challenges surrounding strategies for energy intake adjustment and substitution modelling through the lens of causal inference. Her research in this area has attracted considerable international interest.
2. Exploring the challenges in the analyses of composite variables and compositional data in health research. Georgia is interested in the problems that arise when estimating causal effects involving composite variables and compositional data. This includes considering how to depict such variables within DAGs, considering the corresponding causal estimands, and planning the appropriate causal analyses.
"
https://www.turing.ac.uk/people/guest-speakers/cliodhna-ni-ghuidhir,"
Clíodhna is leading delivery of the multi-agency advisory service project, which brings regulatory and health technology assessment partners together to provide joined-up guidance and advice to developers and adopters of these technologies, and has developed the AI & Digital Regulations Service. She has also served on funding award panels for AI technologies, and chaired the award panels for round 3 of the NHS AI Award last year (phases 3-4). Previously, Clíodhna worked on a broad range of national innovation programmes, including real-world evaluation and led the development of the NHS Innovation Service, a single front door for innovators to the NHS. Her health-innovation policy work is informed by her frontline experience, as she worked in mental health and acute services within the UK and in supply chain operations in South Africa.
"
https://www.turing.ac.uk/people/guest-speakers/david-spiegelhalter,"
David Spiegelhalter is Winton Professor for the Public Understanding of Risk at Cambridge University. He works to improve the way in which risk and statistical evidence are taught and discussed in society, and makes frequent media appearances.  In 2017-2018 he was President of the Royal Statistical Society, and in 2011 he came 7th in an episode of Winter Wipeout.   
"
https://www.turing.ac.uk/events/event-type/other,"
"
https://www.turing.ac.uk/people/researchers/charisma-choudhury,"
Charisma Choudhury is an Associate Professor of the University of Leeds where she is currently serving as the Deputy-Director of the Choice Modelling Centre. Prior to that, she has worked as an Assistant Professor at Bangladesh University of Engineering and Technology (BUET), as a Postdoctoral Research Associate at Massachusetts Institute of Technology (MIT) and as Analysts in RAND Europe, UK and Cambridge Systematics, USA. Charisma holds a PhD and MSc from Massachusetts Institute of Technology (MIT). In recognition of her doctoral research, she has received the Gordon Newell Best Dissertation Prize from the HKSTS and an Honourable Mention from the IATBR.
At the Turing, Charisma plans to work towards bridging choice modelling (CM) with data science and artificial intelligence Travel behaviour models have traditionally relied on manually collected survey data, which are expensive to obtain and thereby generally have limited sample sizes and lower update frequencies. On the other hand, over the last decade, passively collected data sources, commonly termed as Big Data or Ubiquitous Data, have emerged as a very promising source of activity and travel information. The applications of such data have however been primarily limited to visualizations and pattern identifications using machine-learning (ML) techniques.
However, there are criticism about the effectiveness of the ML algorithms due to the black-box approach and over-reliance on the training data, which questions their applicability in human behaviour prediction in the context of disruptive changes (e.g. radically new technologies, collapse of economy, etc.). Bridging ML with choice modelling holds the promise to make the best use of the data. CM techniques, which are based on principles of economic and psychology also have significant potential to augment AI. A particularly topical example is the set of AI models for Connected and Autonomous Vehicles (CAVs). Building on her previous research on driving behaviour modelling using CM techniques, Charisma is keen to augment AI models with behavioural underpinning.
2011 Faculty for the Future Fellowship, Schlumberger Foundation Awarded each year to 100 exceptionally talented female STEM researchers from developing and emerging economies for pursuing postdoctoral research or doctoral studies at leading universities.
2010 Gordon Newell Memorial Prize, Hong Kong Society for Transport Studies 1st prize of the annual dissertation competition launched in 2003 to laud the best doctoral dissertation in transportation conducted in any country by a student of Asian origin.
2007 Honourable Mention, Eric Pas Dissertation Award, International Association of Travel Behaviour Research (IATBR) 2nd prize of the annual dissertation competition launched in 1998 to recognise the outstanding doctoral dissertations in the area of travel behaviour research.
2004 Martin Family Society for Sustainability Fellowship, MIT Awarded annually to 20 outstanding doctoral students at MIT pursuing sustainability research.
"
https://www.turing.ac.uk/people/former-enrichment-students/fatima-batool,"
Fatima is a 3rd year PhD at the department of statistics at UCL. Prior to beginning her Phd she holds an MPhil and Msc (research) in statistics form Quaid - i - Azam University and Bsc in mathematics from the Punjab University Pakistan. She is also Lecturer at Department of Statistics at CIIT, Pakistan.
Fatima's Ph.D. research aims at developing efficient data clustering algorithms. There are many challenges in cluster analysis, for instance, the estimation of number of clusters, definitions of clusters, validations of clustering, consistency, stability, reproducibility and robustness of methods. The designed algorithms will be capable of estimating appropriate number of clusters and clustering data. The present partitioning clustering algorithms need to specify number of clusters in advance. The idea is to design such clustering algorithms that can estimate number of clusters and give data clustering based on this estimate.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/lancaster-university,"
"
https://www.turing.ac.uk/people/enrichment-students/charlie-pilgrim,"
Charlie is interested in better understanding social systems - with the eventual goal of helping to build a better society. At the Turing he will be looking at models of how people influence each other when communicating, and how that influence spreads through society. Charlie is attached to the University of Warwick’s Maths and Psychology departments, and he uses a multidisciplinary approach that includes opinion dynamics, Bayesian updating, linguistics, information theory, mathematical modelling, simulations and data science.
"
https://www.turing.ac.uk/people/honorary-fellows/james-hetherington,"
James is an experienced senior leader in the application of mathematically and computationally intensive methodologies for innovation and insight, working with stakeholders at the highest levels across government, academia and industry.
As Director of Digital Research Infrastructure at UK Research and Innovation, James led on strategy for the software, supercomputers, skills, data services and clouds that underpin computational science and digital scholarship in the UK.
As Director of Research Engineering at The Alan Turing Institute, he founded, grew and led a team of thirty research software engineers and data scientists contributing to a huge range of data- and compute-intensive research. The team continue to build and use tools to analyse and present large datasets, and create complex models running on state of the art supercomputers. In particular, he directed the ""Tools, Practices and Systems"" research programme within the UK's strategic priority research programme ""AI for Science, Engineering, Health and Government"".
James was founding head of UCL’s Research Software Engineering Group, the first such group in a UK university. Fields addressed included machine learning for intensive care, ancient Mesopotamian history, graph theoretical approaches to modelling chemical catalysis, computer vision for astronomy, trans-oceanic journalistic exchanges, data centric engineering, brain blood flow simulations and DNA crime scene analysis. This new model for applied computational research groups in universities, pioneered under his leadership, has been adopted by research intensive universities across the globe.
As senior scientist at AMEE UK Limited, a London startup funded by Amadeus Capital and Union Square Ventures, he developed systems to make it easier for organisations to understand and cost their environmental impact and resulting liabilities. He conceived, prototyped, and led development and release of AMEE Discover, winner of a Best of What’s New award in Popular Science Magazine.
As senior developer in the Model Management group at the MathWorks, creators of MATLAB, he designed and launched new capabilities for their technical computing and systems modelling software, applied to domains from computational finance to automotive engineering. In particular, these tools focused on searching for, linking, differencing and combining mathematical models.
At the UK Business Department’s flagship ""Beacon Project"" at the UCL Centre for Mathematics and Physics in the Life Sciences, he developed a framework for understanding disease by combining physiological models using different assumptions, formalisms, and computational platforms, later used by Merck Inc.
He is currently engaged as Chief Data Science Advisor to the Joint Biosecurity Centre in the Department of Health and Social Care, leading on the development of a robust ensemble of mathematical and statistical models constituting our understanding of the progress of the pandemic in the UK, in partnership with the Turing’s Health programme.
He is also currently a leading contributor to the UK’s National Digital Twin programme, focused on helping to define the protocols and standards that will enable an interoperable open, secure marketplace for digital twins in the built environment and beyond
Data Science and Artificial Intelligence have the potential to provide transformative benefits to productivity, capability and decision making in all aspects of our economy and society. These benefits can only be realised through deep and multi-directional connections between the academic and technical communities where innovations are developed and the professional, commercial and social contexts where they have their impact. As Director of Data Science in Practice, Dr James Hetherington helps to deliver on this vision through strategic leadership and individual contribution.
Continued engagement as a professional practitioner in applied Data Science and AI is central to effective strategic insight. As such, Dr Hetherington maintains a significant portfolio as an applied data scientist and research engineer. His particular focus is the robust delivery of novel mathematical and statistical approaches through well-engineered, scalable software and systems: the ‘AI plumbing’ that makes advanced approaches real.
Get in touch with James at [email protected] for availability.
"
https://www.turing.ac.uk/fringe-events/,"At a series of events between 6 – 31 March 2023, AI UK Fringe brings together leaders in academia from across the UK’s AI ecosystem to demonstrate, exhibit and update on their ground-breaking work.
Attendees at our AI UK Fringe events will be able to network with local colleagues and peers and take part in specialist debates and roundtables relevant to their regions. Browse this year’s events below and make sure to register your place.
Additional Fringe events will be added to this page as they’re confirmed; check back to see the latest.
Date: Wednesday 8 March 2023Time and location: 15:00 – 17:30 GMT, 5G/6G Innovation Centre, University Of Surrey, Guildford, GU2 7XHOrganised by: Surrey Institute of People-Centred AI, University Of Surrey
To celebrate International Women’s Day on 8th March, the Surrey Institute of People-Centred AI is holding a seminar on the theme of Women in AI.
The confirmed speaker line-up includes:
Additional speakers will be announced shortly. These sessions aim to be inspirational and practical, with the audience engaging with highlighted challenges, approaches and experiences, with a view to improving gender equality in AI and in STEM subjects more generally.
Register now.

Date: Wednesday 29 March 2023Time and location: 14:00 - 15:00, OnlineOrganised by: Edinburgh Futures Institute, The University of Edinburgh
Our love of fashion is driving an unsustainable system of manufacturing, distributing and disposing of our clothing that is impacting our planet. How can we change our behaviour and the system for the love of the planet? What does the data tell us, how can we harness new technology such as Artificial Intelligence, robotics and informatics, and creative tools such as Augmented and Virtual Reality. What is the role of 3D environments, games and NFTs in fashion? Register now.
Love Machine is a programme of events, created in collaboration with partners across the Edinburgh Futures Institute. The season explores the interconnections of AI and data with humanity through conversations, performances, and workshops. Explore the programme.

Date: Friday 31 March 2023Time and location: 14:00 - 17:00, HybridOrganised by: The Alan Turing Institute
We welcome you to join our first hybrid event to meet, connect and formally launch the Supra-Clinical AI Groups. These groups are focused on specialist interests in the particular clinical fields of Medical imaging and Computer Vision, Neurology and Mental Health and Anaesthetics and Intensive Care.
You will hear from the leads of the Supra-interest groups, about our plans for the Clinical AI Summer School as well as it being a welcome opportunity to meet the wider community.
Our keynote will be given by Alastair Denniston, Consultant Ophthalmologist (Uveitis and Medical Retina), University Hospitals Birmingham NHSFT & Honorary Professor, University of Birmingham, UK.
His talk will be about 'Ensuring AI in health is safe, effective and equitable’.
Register now.

Date: Friday 31 March 2023Time and location: 10:00 – 15:00 GMT, Online via Microsoft TeamsOrganised by: School of Electronics and Computer Science, University of Southampton
This workshop brings together researchers and practitioners working on and with AI in the Defence and Mental Health sectors to share experience gained and lessons learnt between these disciplines. We will discuss the key challenges AI faces today and attempt to signpost the current directions of travel within AI research to overcome these challenges in the short and medium term. Register now.

Date: Monday 27 March 2023Time and location: 10:00 – 16:30 GMT, The Alan Turing Institute, British Library, 96 Euston Rd, London, NW1 2DBOrganised by: Safe and Ethical AI, The Alan Turing Institute
Over the past few years, the use of AI has continued to increase in healthcare services. Many healthcare providers are deploying AI-driven applications across various business lines, services, and products. Although AI can bring a range of benefits to healthcare providers and patients, AI can also raise novel risks for patients and even healthcare providers. The UK government is working hard to regulate AI in health to support the use of responsible and trustworthy AI. By doing so, we can unleash the full potential of AI while safeguarding our fundamental values and keeping us safe and secure. Register now

Date: Thursday 30 March 2023Time and location: 14:00 - 17:00, OnlineOrganised by: Queen's University Belfast
After years of rapid progress in Artificial Intelligence and Natural Language Processing, ChatGPT has captured the public imagination in a way that is unprecedented for an AI system, reportedly acquiring over 100 million active users within two months of its launch. What are the scientific and societal implications of such models, and how will they impact the language sciences, language technology and society more generally? In this series of talks, experts in AI and the language sciences describe their recent research on the scientific and governance implications of large language models (LLMs). The event will also include a Q & A section, facilitating a more general discussion of the capabilities and impact of LLMs. Register now.

Date: Wednesday 29 March 2023Time and Location: 10:00 – 16:00 GMT, Teesside University, Campus Heart, Southfield Rd, Middlesbrough, TS1 3BXOrganised by: Teesside University
In this exciting one-day event, you will hear from leading researchers in the field of AI and cancer research, who will present the most recent applications of Data Science in healthcare. Through a series of inspiring talks from cancer modelling to machine learning techniques, you will learn about the different AI methodologies currently applied in cancer research. Register now

Date: Friday 24 March 2023Time and Location: 09:00 – 16:30 GMT, Hybrid (Online via Zoom and Enigma 2.0, The Alan Turing Institute, British Library, 96 Euston Rd, London, NW1 2DB)Organised by: Theory and Methods Challenge Fortnights, The Alan Turing Institute
This workshop will act as the follow-up workshop on ""Accelerating Generative Models and Nonconvex Optimization"" and will focus on theoretical and methodological challenges, solutions to some of those challenges, and exciting future directions on the foundations of generative models. Register in-person (deadline: Thursday 16 March 2023) or Register online.

Date: Tuesday 7 March 2023Time and location: 17:00 GMT, Online via ZoomOrganised by: Living with Machines, The Alan Turing Institute
Please join us for the launch of 'Collaborative Historical Research in the Age of Big Data: Lessons from an interdisciplinary project' in an online roundtable discussion, led by hosts Professor Jane Winters and Professor James Smithies, with the authors, Ruth Ahnert, Emma Griffin, Mia Ridge and Giorgia Tolfo. Register now.

Date: Wednesday 15 March 2023Time and location: 10:00 – 12:00 GMT, Online via ZoomOrganised by: Department of Public Health and Primary Care, University of Cambridge
An interactive, online workshop providing an introduction to the fundamentals of biomedical signal processing and learning for wearable signals of multiple modalities. The workshop will use pre-prepared teaching materials consisting of online Jupyter notebooks running Python code on the cloud so no installation is required on participants’ computers. Register now

Date: Friday 31 March 2023Time and location: 14:00 - 17:00, HybridOrganised by: The Alan Turing Institute
We welcome you to join our first hybrid event to meet, connect and formally launch the Supra-Clinical AI Groups. These groups are focused on specialist interests in the particular clinical fields of Medical imaging and Computer Vision, Neurology and Mental Health and Anaesthetics and Intensive Care.
You will hear from the leads of the Supra-interest groups, about our plans for the Clinical AI Summer School as well as it being a welcome opportunity to meet the wider community.
Our keynote will be given by Alastair Denniston, Consultant Ophthalmologist (Uveitis and Medical Retina), University Hospitals Birmingham NHSFT & Honorary Professor, University of Birmingham, UK.
His talk will be about 'Ensuring AI in health is safe, effective and equitable’.
Register now.
Date: Thursday 30 March 2023Time and location: 14:00 - 17:00, OnlineOrganised by: Queen's University Belfast
After years of rapid progress in Artificial Intelligence and Natural Language Processing, ChatGPT has captured the public imagination in a way that is unprecedented for an AI system, reportedly acquiring over 100 million active users within two months of its launch. What are the scientific and societal implications of such models, and how will they impact the language sciences, language technology and society more generally? In this series of talks, experts in AI and the language sciences describe their recent research on the scientific and governance implications of large language models (LLMs). The event will also include a Q & A section, facilitating a more general discussion of the capabilities and impact of LLMs. Register now.
Date: Wednesday 29 March 2023Time and location: 14:00 - 15:00, OnlineOrganised by: Edinburgh Futures Institute, The University of Edinburgh
Our love of fashion is driving an unsustainable system of manufacturing, distributing and disposing of our clothing that is impacting our planet. How can we change our behaviour and the system for the love of the planet? What does the data tell us, how can we harness new technology such as Artificial Intelligence, robotics and informatics, and creative tools such as Augmented and Virtual Reality. What is the role of 3D environments, games and NFTs in fashion? Register now.
Love Machine is a programme of events, created in collaboration with partners across the Edinburgh Futures Institute. The season explores the interconnections of AI and data with humanity through conversations, performances, and workshops. Explore the programme.
Date: Monday 27 March 2023Time and location: 10:00 – 16:30 GMT, The Alan Turing Institute, British Library, 96 Euston Rd, London, NW1 2DBOrganised by: Safe and Ethical AI, The Alan Turing Institute
Over the past few years, the use of AI has continued to increase in healthcare services. Many healthcare providers are deploying AI-driven applications across various business lines, services, and products. Although AI can bring a range of benefits to healthcare providers and patients, AI can also raise novel risks for patients and even healthcare providers. The UK government is working hard to regulate AI in health to support the use of responsible and trustworthy AI. By doing so, we can unleash the full potential of AI while safeguarding our fundamental values and keeping us safe and secure. Register now
Date: Wednesday 8 March 2023Time and location: 15:00 – 17:30 GMT, 5G/6G Innovation Centre, University Of Surrey, Guildford, GU2 7XHOrganised by: Surrey Institute of People-Centred AI, University Of Surrey
To celebrate International Women’s Day on 8th March, the Surrey Institute of People-Centred AI is holding a seminar on the theme of Women in AI.
The confirmed speaker line-up includes:
Additional speakers will be announced shortly. These sessions aim to be inspirational and practical, with the audience engaging with highlighted challenges, approaches and experiences, with a view to improving gender equality in AI and in STEM subjects more generally.
Register now.
Date: Tuesday 7 March 2023Time and location: 17:00 GMT, Online via ZoomOrganised by: Living with Machines, The Alan Turing Institute
Please join us for the launch of 'Collaborative Historical Research in the Age of Big Data: Lessons from an interdisciplinary project' in an online roundtable discussion, led by hosts Professor Jane Winters and Professor James Smithies, with the authors, Ruth Ahnert, Emma Griffin, Mia Ridge and Giorgia Tolfo. Register now.
Date: Friday 24 March 2023Time and Location: 09:00 – 16:30 GMT, Hybrid (Online via Zoom and Enigma 2.0, The Alan Turing Institute, British Library, 96 Euston Rd, London, NW1 2DB)Organised by: Theory and Methods Challenge Fortnights, The Alan Turing Institute
This workshop will act as the follow-up workshop on ""Accelerating Generative Models and Nonconvex Optimization"" and will focus on theoretical and methodological challenges, solutions to some of those challenges, and exciting future directions on the foundations of generative models. Register in-person (deadline: Thursday 16 March 2023) or Register online.
Date: Friday 31 March 2023Time and location: 10:00 – 15:00 GMT, Online via Microsoft TeamsOrganised by: School of Electronics and Computer Science, University of Southampton
This workshop brings together researchers and practitioners working on and with AI in the Defence and Mental Health sectors to share experience gained and lessons learnt between these disciplines. We will discuss the key challenges AI faces today and attempt to signpost the current directions of travel within AI research to overcome these challenges in the short and medium term. Register now.
Date: Wednesday 29 March 2023Time and Location: 10:00 – 16:00 GMT, Teesside University, Campus Heart, Southfield Rd, Middlesbrough, TS1 3BXOrganised by: Teesside University
In this exciting one-day event, you will hear from leading researchers in the field of AI and cancer research, who will present the most recent applications of Data Science in healthcare. Through a series of inspiring talks from cancer modelling to machine learning techniques, you will learn about the different AI methodologies currently applied in cancer research. Register now
Date: Wednesday 15 March 2023Time and location: 10:00 – 12:00 GMT, Online via ZoomOrganised by: Department of Public Health and Primary Care, University of Cambridge
An interactive, online workshop providing an introduction to the fundamentals of biomedical signal processing and learning for wearable signals of multiple modalities. The workshop will use pre-prepared teaching materials consisting of online Jupyter notebooks running Python code on the cloud so no installation is required on participants’ computers. Register now


"
https://www.turing.ac.uk/people/former-doctoral-students/lina-gerontogianni,"
Lina graduated first in the 4-year degree of Statistics at the Athens University of Economics and Business. She continued her studies in the same university and received her MSc degree in Statistics in September 2017. In October 2017 she joined the University of Cambridge as a first year PhD student in the Department of Medical Genetics. Her research interests focus on the interface between Bayesian inference and machine learning techniques with the aim to explore the trade-off between computational accuracy and speed in inferential problems involving high-dimensional structured data.
In her PhD research project, Variational Inference (VI) will be used in combination with Markov Chain Monte Carlo (MCMC) sampling algorithms to enable Bayesian inference to meet the demands of the new “big data” era in biology and biomedicine. In particular, off-the-shelf solutions provided by probabilistic programming languages will be compared with crafted learning algorithms that combine variational approximation and MCMC simulation. In summary, during her PhD, Lina aims to introduce hybrid computational strategies to improve the scalability of Bayesian models and to greatly extend their applicability to a wider range of computationally demanding problems arising in biology and biomedicine.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/robotics,"
"
https://www.turing.ac.uk/news/publications/contrastive-audio-language-learning-music,"
As one of the most intuitive interfaces known to humans, natural language has the potential to mediate many tasks that involve human-computer interaction, especially in application-focused fields like Music Information Retrieval. In this work, we explore cross-modal learning in an attempt to bridge audio and language in the music domain. To this end, we propose MusCALL, a framework for Music Contrastive Audio-Language Learning. Our approach consists of a dual-encoder architecture that learns the alignment between pairs of music audio and descriptive sentences, producing multimodal embeddings that can be used for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to this property, MusCALL can be transferred to virtually any task that can be cast as text-based retrieval. Our experiments show that our method performs significantly better than the baselines at retrieving audio that matches a textual description and, conversely, text that matches an audio query. We also demonstrate that the multimodal alignment capability of our model can be successfully extended to the zero-shot transfer scenario for genre classification and auto-tagging on two public datasets.
I. Manco, E. Benetos, G. Fazekas, and E. Quinton, ""Contrastive Audio-Language Learning for Music"", in 23rd International Society for Music Information Retrieval Conference (ISMIR), Dec. 2022.
2 MB
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/durham-university,"
www.dur.ac.uk
"
https://www.turing.ac.uk/people/researchers/anthony-cohn,"
Anthony (Tony) Cohn is Professor of Automated Reasoning at the University of Leeds and is currently partially seconded to the Alan Turing Institute to work on a project jointly led with  Professors Mike Wooldridge and Nigel Shadbolt which is evaluating Foundation Models, in particular Large Language Models such as (Chat)GPT and Bard.
He holds BSc and PhD degrees from the University of Essex where he studied under Pat Hayes. He spent 10 years at the University of Warwick before moving to Leeds in 1990 where he founded  a research group working on knowledge representation and reasoning with a particular focus on qualitative spatial/spatio-temporal reasoning, the best known being the well cited region connection calculus (RCC) – the KR-92 paper describing RCC won the 2020 KR Test-of-Time award.  He was awarded the 2021 Herbert A. Simon Prize for Advances in Cognitive Systems for his research on qualitative representation and reasoning about space and time, cognitive vision and robotics, and visually-grounded language processing.
He is Editor-in-Chief Spatial Cognition and Computation and has been Chairman/President of the UK AI Society SSAISB, the European Association for Artificial Intelligence (EurAI), KR inc, the IJCAI Board of Trustees and was the Editor-in-Chief for Artificial Intelligence 2007-2014 and of the AAAI Press 2004-14. He has been a Director of KR Inc. since 2000.
He is the recipient of the 2015 IJCAI Donald E Walker Distinguished Service Award which honours senior scientists in AI for contributions and service to the field during their careers, as well as the 2012 AAAI Distinguished Service Award for “extraordinary and sustained service to the artificial intelligence community”. He is a Fellow of the Royal Academy of Engineering, and is also a Fellow of AAAI, AISB, EurAI, AAIA, the BCS, and the IET; he is also a Chartered Engineer.  He was a member of the UK Research Excellence Framework (REF) 2014 Sub Panel 11 (Computer Science and Informatics) of Panel B.
His early interest in knowledge representation and reasoning methods, in particular on taxonomic knowledge and (qualitative) spatial knowledge remains a continuing focus. He has published extensively on methods for machine learning for activity recognition from video data, using qualitative spatial representations, and on grounding language to vision. He has developed a number of ontology-based and other decision support systems for the built environment. He has worked on aspects of robotics including robotic vision and manipulation in cluttered environments.
Aside from his work on Foundation Models at Turing, he is also currently collaborating with humanities scholars at Lancaster, Manchester, Bristol and Stanford universities in an ESRC/NSF project investigating the use of qualitative spatial techniques for analysing spatial information in texts. He also works on an OECD project on AI and the future of skills, which is developing a programme to assess the capabilities of AI and robotics, and their impact on education and work. He is also interested in analysing hippocampal single cell recording data, in collaboration with Yuri Dabaghian at the McGovern Medical School in Texas.
"
https://www.turing.ac.uk/research/research-programmes/data-centric-engineering,"
The next decade will see step changes in data-driven technology, impacting all aspects of engineering and industry. In preparation, the Lloyd’s Register Foundation and The Alan Turing Institute have partnered on a major initiative to address new challenges in data-centric engineering.
Based at The Alan Turing Institute, the programme brings together world-leading academic institutions and major industrial partners from across the engineering sector.
The goal of the programme is to make data science, mathematical and statistical techniques fundamental to engineering practice, making engineering safer and smarter and leading to a world in which all that is engineered – buildings, transport, energy systems and much more – is more intelligently designed, built and maintained, more energy efficient and ultimately safer to use and live in.
Read more about data-centric engineering at the Turing: the story so far.

The programme is structured around three over-arching ‘Grand Challenges’ designed to meet the data-centric engineering needs of society and industry:
These challenges have been selected to enable the programme to provide the greatest societal impact. The programme has initiated a range of projects focused on delivering the path to achieving the Grand Challenges.
Learn more about the programme’s projects
Using machine learning to improve the reliability of wireless communication systems
A collaboration involving the Turing has developed a new method for improving wireless technologies such as Wi-Fi.
Optimising the world’s first underground farm
In the heart of London there is a farm like no other. It’s subterranean, sustainable and energy smart. It also has a digital twin looking out for its every need.
­­­­World first as 3D printed steel mart bridge opens to pedestrians in Amsterdam
The world’s first 3D printed steel bridge opens to the public today: a pedestrian structure located in the city centre of Amsterdam. The bridge is a result of a multi-disciplinary team of experts collaborating on the future design of public spaces.
Read more about the real world impact of the data-centric engineering programme
The programme has partnered with Cambridge University Press on the launch of a new open access journal, Data-Centric Engineering.
The vision for the journal is to publish high quality research using data-intensive approaches in any of the engineering sciences so that emerging ideas can be accelerated in research and practice. The journal can be read, redistributed, and re-used without barriers. Importantly, this includes those with a stake in these developments outside of academia who usually do not have access to academic publications, such as those in industry and policy fields.
Read more about the Data-Centric Engineering journal
To have greatest impact, the programme has always recognised that data-centric engineering has to be adopted on a global basis. From the foundational community established at the Turing, the programme has worked to develop international connections to drive forward data-centric activity within engineering across the world.
Memoranda of understanding have been signed on three continents so far: with the Canadian Statistical Sciences Institute, the Oden Institute (The University of Texas at Austin), the Finnish Centre for Artificial Intelligence, and The University of Sydney. These agreements formally recognise shared ambitions around embracing data-centric methods in engineering and encourage the co-development of activities, working together on research, sharing knowledge, and hosting exchange visitors and events.
The programme has also developed collaborative international projects with partners based in Amsterdam, San Francisco and Singapore.

For more information, please contact Katy Henderson, Research Project Manager, [email protected]
"
https://www.turing.ac.uk/people/external-researchers/tamsin-edwards,"
Dr Tamsin Edwards is a climate scientist specialising in quantifying the uncertainties of climate model predictions, particularly for the Antarctic and Greenland ice sheet contributions to sea-level rise.
She is a Lead Author of the forthcoming Intergovernmental Panel on Climate Change (IPCC) Sixth Assessment Report (to be published in 2021) and a Contributing Author to the IPCC Special Report on the Ocean and Cryosphere in a Changing Climate (2019).
Tamsin regularly advises the UK Government on sea-level rise, climate science and science communication, and provides expert comment to international media and business. She is an award-winning communicator, including through Twitter (@flimsin), her blog for the Public Library of Science, PLoS (All Models Are Wrong) and articles for the Guardian.
She is also the director of the MSc Climate Change: Environment, Science and Policy.
Tamsin regularly appears on various news outlets as an expert commentator for climate related news stories, included BBC News at 10, ITV News, BBC R4 Today Programme.
Research
Previous talks include:
"
https://www.turing.ac.uk/events/modern-cryptography-context-electric-vehicles,"
Photo by israel palacio on Unsplash
Join the Microsoft Teams event here. This link will be live from 16:00 Wednesday 11 November. You do not need to register. In the meantime, please add the event to your calendar.
Electric vehicles (EV) exist in a context that sees cars exchange data more than ever before. Vehicle software updates, communication with nearby devices, and even managing and unlocking vehicles remotely are all now becoming part of the daily reality of EV consumer ownership. But how are all of these communications secured?
This talk will introduce how modern cryptography is used to secure transport layer communications and especially how lightweight secure channel protocols can be applied in the context of Electric Vehicles. Namely, the following questions will be discussed (and more):
"
https://www.turing.ac.uk/news/publications/augmented-ensemble-mcmc-sampling-factorial-hidden-markov-models,"
Bayesian inference for Factorial Hidden Markov Models is challenging due to the exponentially sized latent variable space. Standard Monte Carlo samplers can have difficulties effectively exploring the posterior landscape and are often restricted to exploration around localised regions that depend on initialisation. We introduce a general purpose ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable scheme to exchange information between the chains in an efficient way. The latter exploits a genetic algorithm within an augmented Gibbs sampler. We compare our technique with various existing samplers in a simulation study as well as in a cancer genomics application, demonstrating the improvements obtained by our augmented ensemble approach.
Märtens, K., Titsias, M., Yau, C. (2019) Proceedings of Machine Learning Research, PMLR 89:2359-2367.
"
https://www.turing.ac.uk/people/researchers/honghan-wu,"
Dr Honghan Wu is an associate professor at Institute of Health Informatics, UCL. He holds a PhD in Computing Science. His current research interest is in using text technologies and Knowledge Graph techniques to analyse health data. He leads a health informatics group (https://knowlab.github.io/) and co-leads Edinburgh Clinical NLP group (https://www.ed.ac.uk/usher/clinical-natural-language-processing).
Dr Wu plays technical leadership roles in several Health Data Research UK funded initiatives including National Text Analytics project. He works closely with National Health Service (NHS) organisations across the UK to use artificial intelligence technologies in supporting research and health service improvements. He has been applying abovementioned information technologies in tackling challenges in managing and treating mental disorders, diabetes, and cardiovascular diseases. Recently, he is studying clusters of diseases (multimorbidity), particularly in elderlies.
"
https://www.turing.ac.uk/industry/,"



In-person tickets are now sold out. You can still secure your place at AI UK through the live stream and watch the showcase. With over 150 expert speakers from multi-disciplinary backgrounds across three stages – conversation, research and impact – AI UK offers something for anyone interested in the data science industry or wanting to understand how the latest tools and technologies can be applied to other industries.
Live streaming AI UK means the flexibility to design your agenda around existing commitments. Participants can get involved in sessions by asking questions via Slido, taking part in polls and engaging online with other curious minds. With an affordable ticket price and a plethora of talks, roundtables and panels, virtual attendance is ideal if you want to stay up to date with the cutting edge of data science and AI.
AI UK 2023 will be held in person for the very first time, at Westminster’s Queen Elizabeth II Centre. Building on its impactful two-year history, not only will AI UK expand on its previous years’ inspiring content of talks, debates and discussions from sector-leading professionals, but QEII will come alive with a wealth of interactive demonstrations, bite-size lightning talks and unique networking opportunities.
In previous years, AI UK has welcomed industry leaders Priya Lakhani OBE and Dame Stephanie Shirley CH, among many others, to the stage. In 2023, we look forward to welcoming even more pioneering entrepreneurs, business innovators and top academic minds from around the UK and across the diverse spectrum of the data science, machine learning and artificial intelligence (AI) ecosystem.
The multi-discipline event will span a whole host of research areas across data science, machine learning and AI, including the very latest in design, development and industry innovation. With renowned figures in policy, research and AI entrepreneurialism in attendance, AI UK’s collaboration and community-growth opportunities will be second to none.
Whether you’re actively seeking state-of-the-art research that can inspire your team or you’re interested in learning about the latest AI policies and strategies, AI UK is an unmissable event. Take the chance to get to grips with the very latest updates, meet leading innovators and discuss the future of data science, machine learning and AI with the ecosystem’s leading figures
Not only will some of the country’s top minds in AI and data science be present and giving talks on their pioneering work, they’ll also be leading interactive workshops, participating in networking events, and demonstrating their latest research at exhibition stands, where you can get hands-on, learn and understand their stories. Bring the future closer by expanding your network and opening new doors for your team in this stimulating environment.
In its brilliant new location, AI UK will provide a fantastic platform to make new connections and partnerships across disciplines. Commercial trailblazers can connect over coffee or collaborate at a workshop or roundtable, exploring new learning opportunities and forging new partnerships.
As a hub for industry, academics, entrepreneurs, and public sector and national institution leaders alike, this unique showcase will feature unforgettable sessions and demonstrations on the latest cutting-edge research and innovation across data science, machine learning and AI, as well as myriad opportunities to meet the minds behind AI research and innovation from across the UK
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.


"
https://www.turing.ac.uk/people/researchers/timothy-hobson,"
Tim is a mathematician and developer of software for data analysis. He received his PhD from the University of Warwick for research in the field of probability and stochastic processes, after which he worked in the City of London as a consultant specialising in quantitative risk management.
In 2011 he relocated to Rio de Janeiro, his wife’s home town, and worked as a quantitative analyst at Fundação Getulio Vargas, a leading Brazilian university and think tank. In this role he led projects in diverse areas of data science including econometric modelling of international trade, methods for constructing composite indicators and state space time series analysis. He also taught courses on topics in applied economics and statistical computing.
After six years in Brazil, Tim returned to London to take up the role of Senior Research Software Engineer at The Alan Turing Institute. In addition to data analysis and visualisation his interests include object-oriented design, functional programming, cryptography and cryptocurrency.
"
https://www.turing.ac.uk/people/business-team/catherine-lawrence,"
Prior to joining the Turing, Catherine spent most of the previous decade in roles at the UK’s national academies of science and engineering, managing a variety of activities including the Royal Society's medals and awards and Summer Science Exhibition, and the Royal Academy of Engineering's Enterprise Fellowships and SME Leaders programme. Catherine joined the Turing in June 2017 as Programme Manager with her main role being to work with the Programme Director to establish and grow the Turing's Defence and Security Programme. During 2018, she also managed the Turing's Data Study Groups. At the end of 2018, Catherine moved over from managing the Defence and Security and Data Science at Scale programmes, to the Health and Medical Sciences programme as Senior Programme Manager. In February 2020, Catherine was appointed as Director of Programme Management.
"
https://www.turing.ac.uk/research/research-programmes/urban-analytics,"
Cities are now home to the majority of the world’s population, and are drivers of economic growth, wealth creation, social interaction and well-being. They also present huge inequalities in health, affluence, education and lifestyle with persistent challenges for management, administration and policy. For example, alongside an existing global market of £400 billion for smart city technology, the UN estimates that $350 trillion or five times global GDP needs to be spent on urban infrastructure to address urgent needs.
The urban analytics programme at the Turing is focused on the process, structure, interactions and evolution of agents, technology and infrastructure within and between cities across spatial and temporal scales. Data science and AI will be developed and exploited alongside spatial analysis, geostatistics and a wide variety of disciplinary perspectives.
Urban analytics draws from data which are captured by governments, businesses and other intermediaries. Conventional mechanisms such as censuses and surveys are complemented by devices which are increasingly ubiquitous. The programme supports assessments, projections and interventions which determine the economic and social welfare of people, businesses, governments and third sector agencies.
In addition to government and public planning, the programme is relevant to organisations across sectors including retail, financial services, mobility, health, policing, and utilities. The diversity in content and approaches gives rise to productive overlaps with many other Turing programmes and challenges.
Urban populations
Data science is supporting advances in the personalisation of individual circumstances, with crucial impacts for medical diagnosis, business and service delivery, traffic management and energy consumption. Microsimulation and mathematical models of citizen behaviour, mobilities, decision-making and choices will help to support healthier lifestyles, active travel, safety and cleaner cities.
The Turing is fostering ground-breaking innovations, for example in agent-based modelling, probabilistic programming, reinforcement learning and data assimilation to create more powerful representations, projections and scenario forecasts for city futures. Advanced statistical approaches are complemented by rigorous theory, for example using methods such as causal inference to create robust and scalable insights into population dynamics and behaviour.
Urban infrastructure
Urban infrastructure is becoming transformed by advances in AI and robotics. Current research will lead to roads, pipes and mains which can repair themselves, vehicles which can talk to one another, and self-aware bridges which know when to rest or regenerate. Learning organisations of the future will accelerate business performance through enhanced understanding of their customers and employees.
Cities are often the geographic nexus of interdependent critical infrastructures. Nevertheless, simulations are advancing to become ‘digital twins’ of the real world in which the impact of both natural events and human interventions can be anticipated with increasing confidence.
We advocate the recognition of data itself as infrastructure, seeking new ways to encourage the sharing of data between organisations for the public good.
Urban environments
The programme richly informs the study of future urban environments, underpinned by ubiquitous networks which can sense the status and rhythms of the city, from air quality to housing, urban design and business performance. The urban analytics programme will exploit enhanced monitoring of these environments – and the development of new methods for the integration of data into consistent models – to gauge the equity, sustainability and liveability of cities. Long-term projections of urban dynamics will be complemented by nowcasting the condition of the city at or close to real-time.
The programme aims to take a leading role in education and promotion of the benefits of responsible data research, but also supports critiques which challenge the boundaries of ethical practice, and controls on digital surveillance which balance the public good with individual privacy, consent and personal preference.
Urban policy
The deployment of data science technologies in the urban context allows businesses to become more efficient and productive, for example through better understanding of their customers, or more efficient distribution networks.
The urban analytics programme will embed models and simulations within scenario designs to enable more effective appraisal of interventions through high level government policy (e.g. sugar tax), local planning contexts (e.g. land-use zoning), specific investment decisions (e.g. construction of a shopping centre or suburban railway station) and allocation of resources (to hospitals, schools, libraries or emergency services).
Data science and AI can support living laboratories in which intelligence can be extracted through the natural variation between places, and by longitudinal assessment of incremental changes e.g. dynamic pricing of congestion zones. We seek to provide planners of the future with the skills and the systems to support effective interventions to the benefit of economy and society.
[email protected]
"
https://www.turing.ac.uk/news/publications/towards-ecosystems-connected-digital-twins-address-global-challenges,"
A digital twin is a virtual representation of a physical entity. By bringing together data, modelling and simulation, digital twins enable insights about the performance of structures, products and processes. They are powerful tools for understanding and optimising complex systems that are already beginning to demonstrate their potential across multiple sectors, from engineering and manufacturing to health and the environment.
In this white paper, we share learnings from The Alan Turing Institute’s AI for science and government (ASG) programme, which has been conducting foundational research and applied work in digital twins under the ‘Ecosystems of digital twins’ (EDT) theme. ASG researchers are expanding the scale and complexity, and deepening the application and deployment, of digital twins.
We organise digital twins into three groups according to their level of complexity, scaling up from a digital twin of a single asset or system, to a digital twin incorporating multiple assets or systems. At the third level, digital twins of differing levels of complexity are linked together in an ‘ecosystem of digital twins’. We provide case studies demonstrating the EDT theme’s multidisciplinary work on digital twins for each of these three levels.
Through digital twins, we can develop unprecedented capabilities for understanding, manipulating and managing complex systems. Within the EDT theme, we have found that for the full promise of the technology to be realised, we must consider the development and deployment of digital twins at the ecosystem-level. By connecting together digital twins within a virtual environment, we can enable sharing of information between ‘federated’ assets – assets under different jurisdictions and restrictions – unlocking more advanced insights and capabilities than are possible at the lower levels. Within an ecosystem, connected digital twins can also promote more varied applications, for example, by combining representations of, and insights from, both physical and social systems to support decision-making.
There are, however, certain technical, social and institutional issues that need to be negotiated in developing ecosystems of digital twins. We therefore provide the following three recommendations in support of the progress which needs to be made:
By following these recommendations, with support from the right investments, we can build robust ecosystems of digital twins capable of helping us tackle some of the most important global challenges, from pandemics to adapting to climate change.
DOI: 10.5281/zenodo.7840266
11.5 MB
"
https://www.turing.ac.uk/people/external-researchers/ray-jones,"
"
https://www.turing.ac.uk/events/how-regulate-foundation-models-can-we-do-better-eu-ai-act,"
Join us for a hybrid presentation by Professor Lilian Edwards followed by a panel session, asking “How to regulate foundation models: can we do better than the EU AI Act?” Lilian will discuss how large or “foundation” models could be governed, identifying their unique features and the legal issues in their deployment as well as the EU AI Act and its risk based approach. 
Lilian’s presentation will be followed by these speakers
This event will be chaired by Adrian Weller from Turing Safe and Ethical AI.
You must register in order to attend the event; the default attendance is online. Once registered you will receive a confirmation email containing information on how to add the meeting into your calendar. After registering, please email [email protected] if you would like to attend in person.
Limited spaces are available.
 
You must register to attend this event.
"
https://www.turing.ac.uk/blog,"

Sign up to our mailing lists to get the latest Turing updates
"
https://www.turing.ac.uk/people/external-researchers/albert-merono-penuela,"
Dr Meroño Peñuela is the co-organiser of the Neuro-symbolic AI interest group.

"
https://www.turing.ac.uk/research/research-projects/chronotopic-cartographies-literature,"
Visualisations of fictional places can be generated through natural language processing of, for example, a novel's text and transformed into a range of maps, from static geographic maps to full 3D worlds. These visualisations (of otherwise abstract spaces and places) will enable innovative ways for new audiences to engage with, and explore spatial meaning in, literature.

This academic project, 'Chronotopic Cartographies', a collaboration of institutions, led by the University of Lancaster, is based upon Russian theorist Mikhail Bakhtin's idea of the 'chronotope'; how configurations of time and space are represented in language and literature.
For example; in Robert Louis Stevenson's 'Treasure Island', the novel is mapped in terms of three distinct chronotopes: England (home); the ship (transition), and the island (the 'other' space of conflict and death). The first of these maps onto the 'real-world' geography of Devon, the second reduces space to the extent of the ship in motion; the third is set in an entirely imaginary place. These chronotopes exist sequentially within the narrative but also overlap and affect each other (e.g. the boy narrator, Jim, projects an imagined version of the island forward from home that differs from the actual island).
Ordinary everyday life at 'home' at The Admiral Benbow inn is interrupted by the intersection with the world of the pirates, and Jim is sucked out of one kind of chronotope (safe, secluded, the space of childhood) into another far more exciting, but also threatening, one. 
In the case of 'Treasure Island', the book also includes a map of the island drawn by the author, so the island map is both inside and outside the narrative, functioning as an object of power calling out to be claimed and reclaimed and shaping the narrative around it. Full visualisation of different chronotopes allows for this deep response to the rich complexity of such a text.
The primary digital objective of the project is to solve a major conceptual problem for 'literary mapping': the creation of a base map for a fictional world out of the text itself (particularly when the fictional location has no real-world correspondence). 
The project will develop an entirely original method for generating literary maps semantically, using place names and descriptions given within the text to create GIS (geographic information system) maps and 3D visualisations. Creating maps and 3D worlds directly from the language used in a text creates a far more integrated model of literary mapping than has previously been possible. This should prove attractive to scholars in the field of literary studies and beyond, enabling more traditional practices of critical analysis and interpretation to connect with spatial visualisation techniques.
The project is interested in creating maps that combine time and space and allow for qualitative elements to be mapped, such as the reliability of the narrator, changes in mood, and the effect of subjectivity upon perceptions of space.
The project is strongly focused upon the usefulness of its outputs to others within academia and beyond it. In academic terms it should be of value to those in literary studies since it offers an entirely new way of 'reading' literary space, both visually and verbally. It is also relevant to interdisciplinary researchers working across geography and cartography, literary studies, and history. Finally, it is directly relevant to those in digital humanities, spatial humanities, and literary mapping.

Educational beneficiaries, such as schools, will be working closely with the project, developing new teaching materials to use alongside 3D visualisations and gaming platforms. Museums and cultural heritage institutions will enjoy new interactive interfaces and tools that engage people with literature who might not otherwise be interested. The general public will have direct access to the digital tools that are being developed, allowing for others to re-use and re-mix them to inspire new ways to explore literature.
Previous work on the project has produced an educational resource for schools called Litcraft which uses the popular Minecraft platform to create an immersive experience for literature. This has been extremely successful in creating multiple partnerships for the later maps and visualisations of the full project to build on. The first two worlds created are for Robert Louis Stevenson's 'Treasure Island' and Michael Morpurgo's 'Kenzuke's Kingdom'. These are now being rolled out to schools and across library regions. MinecraftEdu is also collaborating with the work and collaborations are being developed with commercial world-builders.
Project updates will also be posted via the blog and Twitter
July 2018: Read the Guardian's article about the project's work: 'How Minecraft is helping kids fall in love with books'
February 2018: Watch Reuter's video: 'Bringing children's books to life in the virtual world'
Video credit: Reuters
Members of the Research Engineering Group at the Turing are contributing their expertise to this project.
They are shaping the representation of the novels in electronic form, using and expanding an XML schema to mark-up time and place, making explicit the implicit or hidden. They are also leading the development of new methods to perform text analysis of the content, aiming to identify salient linguistic data, which will then power the 3D visualisations of these fantastic worlds.
"
https://www.turing.ac.uk/events/hate-and-harassment-can-technology-solve-online-abuse,"
Governments around the world are increasingly concerned by the prevalence, spread and impact of harmful online content, such as harassment, bullying and hate speech. Online abuse poses myriad concerns: it can inflict harm on targeted victims, pollute civic discourse, make online environments unsafe, create and exacerbate social divisions, and erode trust in the host platforms.
Many hope that increasingly sophisticated and powerful algorithms will ‘solve’ the problem of online abuse by making this content easier to detect and take down. However, abusive content detection has proven to be a wicked challenge. Not only is it a very difficult engineering task; it is also imbued with complex legal, social and political challenges. Researchers are increasingly drawing attention to the biases in some widely used tools and datasets, raising concerns that they might perpetuate the injustices they are designed to overcome.
Currently, Facebook’s ‘Supreme Court’ of content moderation is gearing up to pass judgements; the UK Government is reviewing its wide-ranging Online Harms White Paper; social media platforms across the world are tightening up their community guidelines and investing in more tech to counter online abuse. In this pertinent moment, our experts discuss a fundamental question for society: Can technology solve online abuse?
This is the third event of the 'Driving Data Futures' lecture series in the Public Policy Programme, where we invite audiences to learn and critically engage with new research at the intersection of new technologies, public policy, and ethics. At this event, there will be presentations delivered by academia, industry, and government, in addition to the Turing’s Hate Speech project team presenting their latest research in the field. This will be followed by a detailed Q&A, chaired by Dr Bertie Vidgen
Additional speakers will be announced soon
For more details on the Hate Speech project, including our most recent paper “Challenges and frontiers in abusive content detection”, please see – https://www.turing.ac.uk/research/research-projects/hate-speech-measures-and-counter-measures
 

"
https://www.turing.ac.uk/people/researchers/alberto-vecchio,"
Alberto Vecchio is an astrophysicist, and founding director of the Institute for Gravitational Wave Astronomy. His interests span the astrophysics of compact objects - black holes, neutron stars and white dwarfs - and gravitational-wave observations across a wide frequency spectrum, e.g. using ground-based detectors such as LIGO, pulsar timing arrays and the future Laser Interferometer Space Antenna (LISA). He is a member of the LIGO Team that detected gravitational waves in September 2015 and discovered the first binary black hole merger. He is a Royal Society Wolfson Fellow and co-recipient of the 2016 Special Breakthrough Prize in Fundamental Physics. 
 
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-personalised-medicine-face-multi-scale-heterogeneity,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the topic of personalised medicine in the face of multi-scale heterogeneity.
Complex pathologies often display heterogeneity at multiple scales. At the population level, different individuals have diverse exposures to disease modifying risk factors (e.g. cigarette smoking, body mass index, etc.), which drive inter-individual heterogeneity in pathological expression. While at the individual level, different tissues can have various degrees of disease involvement (e.g. due to local inflammation), contributing to intra-individual heterogeneity. Lastly, within disease-affected tissues different cells can exhibit a wide distribution of molecular hallmarks for disease involvement, resulting in intra-tissue heterogeneity. 
Understanding this multi-scale heterogeneity is essential to design and evaluate therapies in an increasingly personalised manner, with profound implications for drug discovery, biomarker identification and outcome assessment.
 
We will be hearing from our Turing-Roche Senior Postdoctoral Research Associate Christopher Banerji. Chris will use the example of facioscapulohumeral muscular dystrophy (FSHD), a complex rare-disease, as a prototype for muti-scale heterogeneity in pathology. He will show that integrating datasets at diverse scales, coupled with appropriate mathematical, clinical and cell biological analysis leads to important and novel insights into therapeutic development, biomarker discovery and lifestyle risk-factor identification.
Register now.
"
https://www.turing.ac.uk/people/former-enrichment-students/andrea-santoro,"
Andrea is a third year PhD student in Mathematics at Queen Mary University of London and an enrichment student at The Alan Turing Institute. Before joining Queen Mary, Andrea studied applied mathematics at the University of Catania, Italy (both BSc and MSc). He was also a student of the Scuola Superiore di Catania, a public institution that provides university students with advanced additional courses. During his Masters, he did two internships in the Complex Systems and Networks Research Group at Queen Mary. He also worked on evolutionary algorithms for solar cell design and synthetic biology problems.
Andrea's current research interests cover several aspects of complex networks, with a special focus on multi-layer networks and their applications to large-scale systems. His PhD project aims to develop new measures, based on optimisation principles and information theory techniques, in order to extract relevant features from large-scale multi-layer networks.
"
https://www.turing.ac.uk/news/filed-under/edi,"
"
https://www.turing.ac.uk/people/research-fellows/george-danezis,"
George Danezis is a Professor of Security and Privacy Engineering at the Department of Computer Science of University College London, and Head of the Information Security Research Group. He has been working on anonymous communications, privacy enhancing technologies (PET), and traffic analysis since 2000. He has previously been a researcher for Microsoft Research, Cambridge; a visiting fellow at K.U.Leuven (Belgium); and a research associate at the University of Cambridge (UK), where he also completed his doctoral dissertation under the supervision of Prof. R.J. Anderson.
His theoretical contributions to the Privacy Technologies field include the established information theoretic and other probabilistic metrics for anonymity and pioneering the study of statistical attacks against anonymity systems. On the practical side he is one of the lead designers of the anonymous mail system Mixminion, as well as Minx, Sphinx, Drac and Hornet; he has worked on the traffic analysis of deployed protocols such as Tor.
His current research interests focus around secure communications, high-integirty systems to support privacy, smart grid privacy, peer-to-peer and social network security, as well as the application of machine learning techniques to security problems. He has published over 70 peer-reviewed scientific papers on these topics in international conferences and journals. He was the co-program chair of ACM Computer and Communications Security Conference in 2011 and 2012, IFCA Financial Cryptography and Data Security in 2011, the Privacy Enhancing Technologies Workshop in 2005 and 2006. He sits on the PET Symposium board and ACM CCS Steering committee and he regularly serves in program committees of leading conferences in the field of privacy and security. He is a fellow of the British Computing Society since 2014.
George's research at the Turing revolves around two key themes, or privacy and distributed ledgers. First, he research how the degree of privacy protection may be quantified and experimentally calculated for different proposed Privacy Enhancing Technologies. His approach is influenced by Differential Privacy definitions, however it is adapted to the settings and driven by experimental, rather than purely analytical evaluations. Second, he researches distributed ledgers, which are transparent and accountable distributed computational platforms. Those form the core of `blockchain’ technologies, and are challenging to scale while retaining beneficial security and governance properties
"
https://www.turing.ac.uk/people/guest-speakers/alicia-boyd,"
Dr. Alicia Boyd (she/her) is a Data Scientist Researcher. She received her Ph.D. in Computer Science from DePaul University in Chicago, Illinois, in 2021. Her research involves interrogating methodologies and practices used in data-driven technologies from design to implementation. She created Quantitative Intersectional Data (QUINTA), a methodological framework that critiques the data science process and its underlying structural and societal injustices and oppressions. She has earned both her Masters in Mathematics and Higher Education from University Missouri-Saint Louis. Dr. Boyd has worked in the medical, power industry, and post-secondary education fields.
"
https://www.turing.ac.uk/people/researchers/mariarosaria-taddeo,"
Mariarosaria Taddeo is Associate Professor and Senior Research Fellow at the Oxford Internet Institute, University of Oxford, where she is the Deputy Director of the Digital Ethics Lab, and is Faculty Fellow and Defence Science and Technology Fellow at the Alan Turing Institute. Her recent work focuses mainly on the ethical analysis of Artificial Intelligence, cyber security , cyber conflicts, and ethics of digital innovation. Her area of expertise is Philosophy and Ethics of Information, although she has worked on issues concerning Epistemology, Logic, and Philosophy of AI. She has received multiple award for her work, among which the 2010 Simon Award for Outstanding Research in Computing and Philosophy; the 2016 World Technology Award for Ethics. In 2018, InspiringFifty named her among the most inspiring 50 Italian women working in technology. In the same year, ORBIT listed her among the top 100 women working on Ethics of AI in the world. She has been named one of the twelve 2020 ""Outstanding Rising Talents"" by the Womens' Forum for Economy and Society.
Since 2016, Taddeo serves as editor-in-chief of Minds & Machines (SpringerNature) and of Philosophical Studies Series (SpringerNature). She is also a member of the Exploratory Team on Operational Ethics, established under the auspices of the Human Factors and Medicine (HFM) panel of the NATO Science and Technology Organization. Her research has been published in major journals like Nature, Nature Machine Intelligence, Science, and Science Robotics.
Her area of expertise is Information and Computer Ethics, although she has worked on issues concerning Philosophy of Information, Epistemology, and Philosophy of AI. She published several papers focusing on online trust, cyber security and cyber warfare and guest-edited a number of special issues of peer-reviewed international journals: Ethics and Information Technology, Knowledge, Technology and Policy, Philosophy & Technology. She also edited (with L. Floridi) a volume on ‘The Ethics of Information Warfare’ (Springer, 2014) and is currently writing a book on ‘The Ethics of Cyber Conflicts’ under contract for Routledge.
"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/dynamicstatic-mathematical-modelling,"
"
https://www.turing.ac.uk/people/external-researchers/jorn-kohlhammer,"
"
https://www.turing.ac.uk/people/enrichment-students/fatma-elsafoury,"
Fatma (she/her) is a 3rd year PhD student at the University of West of Scotland. She works on bias and fairness in hate speech detection models. Fatma is interested in AI for social good and in particular hate speech detection, and investigating bias and fairness in Natural Language Processing (NLP) models. She understands from my personal and research experiences how bias in NLP and ML applications has a direct impact on the lives of the under-represented groups of people and how hate speech towards minorities has severe implications on society like hate crimes. She believes that these topics are crucial to study now more than ever for building models and communication platforms that are safe, and accessible to all people regardless of their gender, ethnicity, or sexual orientation.
For the Turing research project, her goal is to understand the influence of social bias on the task of hate speech detection. Training a machine learning (ML) model for the task of text classification, e.g. hate speech, involves encoding the textual content in datasets into a numerical representation (numerical vectors). This process is very important as a good representation would capture the semantic relationship between words and sentences. These numerical representations are called word embeddings. For these word embeddings to be effective they are trained on a large corpus of text like Wikipedia or news articles. Recently, many research papers have shown that these word embeddings are socially biased.
Her research goal can be achieved by answering the following question: What is the effect of social biases in word embeddings on the task of hate speech and abuse detection? To answer this research question, She proposes to use causal inference methods to learn how the social bias in the word embeddings causes hate speech detection models to treat different groups of people. Understanding how social bias influences the task of hate speech detection models, could help us to develop effective methods to remove the bias.
"
https://www.turing.ac.uk/blog/what-alan-turing-means-us,"
“We can only see a short distance ahead, but we can see plenty there that needs to be done.”
This is what Alan Turing, the pioneering mathematician, computer scientist, philosopher, code-breaker and icon of the 20th Century, said of the challenge of developing ‘machines that can think’ in his seminal paper ‘Computing Machinery and Intelligence’ in 1950. Seven decades later, and computing, machine learning and artificial intelligence (AI) have not only blossomed, but exploded.
Today, the horizon has transformed, but Turing’s words still hold true. This Institute, named in his honour, is committed to tackling the plenty that needs to be done to ensure data science and AI changes the world for the better.
To find out more, read the article below and watch the video where we asked the Institute's community what Alan Turing means to them:

Throughout his life, Alan Turing’s fearless approach to daunting problems helped him break new conceptual ground. From his time at Cambridge, when he published papers now recognised as the foundation of computer science, through his vital work at Bletchley Park cracking German codes – shortening the Second World War by years – to his exploration of the notion of artificial intelligence and his fascination with the application of mathematics to the biological world.
At the Institute we aim to adopt a similarly ground-breaking, multi-faceted approach to our research.
Despite being a singular genius, Turing was also a great collaborator, both with the hundreds of women and men at Bletchley Park, and throughout his career working with other mathematicians, engineers and scientists.
The biography Alan Turing: The Enigma by Andrew Hodges includes the following quote from Turing, which sums up the spirit in which the Institute operates: “The isolated man does not develop any intellectual power. It is necessary for him to be immersed in an environment of other[s]…The search for new techniques must be regarded as carried out by the human community as a whole, rather than by individuals.”
""The search for new techniques must be regarded as carried out by the human community as a whole""
Alan Turing
Turing’s life was tragically affected by the societal norms of his time: despite his pivotal part in ensuring the safety of the nation and saving countless lives, his sexuality resulted in him being defined as a security risk, and he was harassed by police surveillance up until his untimely death in 1954.
Though we now live in a more progressive and open society, at the Institute we recognise the importance of actively ensuring anyone in ‘the human community’ can contribute effectively to changing the world through data science. We do this through our commitment to equality, diversity and inclusion, demonstrated by events such as ‘Gamechangers for diversity in STEM’.
On Turing’s influence on the modern world of data science Vinton Cerf, Chief Internet Evangelist for Google, says: “His practical realisations of computing engines shed bright light on the feasibility of purposeful computing and lit the way towards the computing rich environment we find in the 21st Century.” Our programme in Data Science at Scale continues this legacy, identifying the ways in which computers and algorithms can be better designed to fulfil a huge range of purposes and tasks. And our Research Engineering team, which likes to think of itself as an echo of the Bletchley Park ‘Hut 8’ group led by Turing, helps the Institute develop practical data science tools.
The mathematical foundations strand of our Data-centric Engineering programme also recognises that delivering reliable and robust data science solutions requires rigorous theoretical research and practices. It’s a notion which aligns well with the ‘from first principles’ approach Turing often adopted in his work.
Turing’s revolutionary ideas in cryptography were developed in service of public safety and security, and the Institute’s programme in Defence and Security is continuing this purpose. For example, we have multiple projects looking at ways to store sensitive data, such as health records, in the cloud, in a way that not only allows the data to remain encrypted, but also makes them accessible to publicly beneficial research, without compromising anyone’s privacy.
In a talk broadcast by the BBC in 1951, Turing said: “I believe that the attempt to make a thinking machine will help us greatly in finding out how we think ourselves.” Today, ‘thinking machines’ are ubiquitous, from speech-recognition software to algorithms used in criminal justice systems. Serious consideration needs to therefore be given to how human mindsets – our social norms and subconscious biases – affect the development and implementation of these systems.
Our programmes in Artificial Intelligence and Public Policy, and our Data Ethics Group, all explore how we can ensure that algorithmic systems are built to contribute reliably and fairly to the public good, and how they affect the way we interact with our world.
In his later career, Turing became fascinated with the challenge of understanding biological pattern formation in nature. He proposed an explanation in terms of chemical interactions and developed equations for them. Decades later, scientists confirmed his theory. Ian Stewart, Emeritus Professor of Mathematics at the University of Warwick explained in a BBC documentary: “In an area where mathematics had never been used before, suddenly the door was opened.”
This value of applying mathematics and computer science to other fields can be seen throughout the Institute’s research, particularly in the Data Science for Science programme and projects such as ‘Living with machines’ in collaboration with the British Library, which is bringing together data scientists with historians, computational linguists, and archivists to think critically about the interdependence of humans and machines in modern society.
Alan Turing’s legacy and influence is felt the world over, and we make no claim that our Institute is sole heir to his mantle. For example, the Turing Trust, a charity run by Alan Turing's family, does fantastic work supporting education by reusing old ICT equipment that anyone in the UK can donate, providing the opportunity for students around the world to use digital technology for the first time.
What we are is a proud torch-bearer for the man often dubbed the ‘father of computer science’ and we believe that our mission to make great leaps in data science and artificial intelligence research, for the betterment of all, is rightfully carried out in his name. And we, like Turing, can see plenty there that needs to be done.
Header image of Alan Turing courtesy of the National Portrait Gallery.
"
https://www.turing.ac.uk/people/researchers/martin-oreilly,"
Martin is Director of Research Engineering at the Turing. His focus is on using good software engineering practices to increase the impact of research software and data science analyses by making these reusable, reliable and robust. He also has a strong interest in reproducible research, and is working to improve the tools and working practices available at the Turing to make it easier for researchers to work reproducibly.
Martin also has an interest in enabling researchers to work safely, securely and productively with sensitive data, leading work on the Institute's cloud-based secure research environment, as well as a project evaluating techniques for generating synthetic datasets, to understand if these can effectively replace more sensitive datasets for data science analyses.
Martin has a PhD in computational neuroscience from UCL, and an MSc in artificial intelligence from Edinburgh. During his postgraduate work his focus was on understanding the brain by simulating it, and developing techniques to understand the connectivity of neurons within the brain from high-resolution microscopy.
Martin's experience developing software, managing software projects and managing technical teams spans both the research and business sectors, having spent several years doing this in the commercial sector between stints in academia.
"
https://www.turing.ac.uk/events/driving-data-futures-technology-and-government-good-bad-and-ugly,"
In the lecture series Driving data futures, the public policy programme of The Alan Turing Institute invites audiences to learn and critically engage with new research at the intersection of new technologies, public policy, and ethics.
In this first event of the series, we will focus on the different ways technology can be used in government and discuss what is “the good, the bad, and the ugly”.
Data science, AI, and statistical modelling have far-reaching and diverse capabilities. They allow for the processing and ordering of vast amounts of structured and unstructured data and find application in scoring, classification, targeting, profiling, even prediction. Such technologies promise a myriad of benefits and public authorities are beginning to take notice. Faced with increasing budget cuts and rising service demands, public bodies are looking to algorithms to help them deliver what they see as a more objective, efficient and accessible services. But when is technology actually used for the common good and how can we ensure that this is always the case?
Two speakers will come present the results of their research on the topic.
Dr Omar Guerrero will give a speech on Policy Priority Inference for Sustainable Development
Guidelines to support international development of developing countries are many. Today, the best example of these guidelines is the Sustainable Development Goals (SDGs) – a set of 17 general goals monitored through 232 development indicators. Achieving these goals is, however, a complex task. There are interdependencies between development goals as well as inefficiencies and uncertainties in policymaking processes (e.g. corruption). A clearer understanding of development is needed to evaluate SDGs, to align environmental policies, to coordinate anti-poverty policies, and to better understand the synergies and trade-offs between development goals.
The research project Policy Priority Inference (PPI) has been established to overcome these challenges. By specifying the policymaking process through a political economy game on a network of spill-over effects, PPI accounts for the network of interdependencies between policy issues (such as the sustainable development goals), as well as well-known political economy problems arising from budget assignment (e.g. corruption). At the core of the project is an agent-computing model that simulates – from bottom up – the observed dynamics of development indicators. This tool allows us to circumvent several limitations of traditional statistical methods (e.g. losing country-specificity from cross-national estimates).
In this talk, Omar will introduce the PPI methodology and demonstrate some of the various applications where it sheds new light on economic policies for development. This is for example the case in estimating policy resilience, studying ex-ante policy evaluation, quantifying policy coherence, and assessing the effectiveness of governance reforms in the fight against corruption. In addition, we will discuss the direction that PPI is taking towards the SDG 2030 agenda and ongoing collaborations with the United Nations Development Programme (UNDP). The talk concludes that agent modelling and data science can find positive applications for international development and data-driven policy-making.
 
Following this, Dr Lina Dencik will speak about Social Justice in an Age of Datafication
Turning away from a ‘big picture’ view, Lina Dencik will speak about the use of data and algorithmic processes for decision-making affecting individuals and social life. Digitally monitoring, tracking, profiling and predicting human behaviour and social activities is what underpins the information order now frequently described as surveillance capitalism. Increasingly, it is also what helps determine decisions that are central to our ability to participate in society, such as welfare, education, crime, work, and if we can cross borders. A recent report from the Data Justice Lab at Cardiff University showcases that such technologies are already being used by local authorities across the UK. How should we understand what is at stake with such developments?
Often, we are dealt a simple binary that suggests that the issue is one of increased (state-)security and efficiency on the one hand and concerns with privacy and protection of personal data on the other. However, it is becoming increasingly clear that we need a broader framework for understanding these developments. This is one that can account for the disparities in how different people might be implicated and that recognises that the turn to data-driven systems is not merely technical, but a distinctly political development.
In this presentation Lina Dencik will advance a research framework for studying datafication that is rooted in a broader concern for social justice. Such a framework, referred to here as ‘data justice’, pays particular attention to the ways in which data processes are uneven, can and do discriminate, create new social stratifications of ‘have’ and ‘have nots’, and advance a logic of prediction and pre-emption that fundamentally transforms political process. In outlining such a framework, Line advances an engagement with data politics, as the performative power of or in data, that considers how the implementation of data-driven technologies in different contexts relate to wider interests, power relations, and agendas.
 
Agenda:
17:15 – Doors open
17:30 – 17:35 – Introduction
17:35 – 18:05 – Omar Guerrero – Policy Priority Inference for Sustainable Development
18:05 – 18:35 – Lina Dencik – Social Justice in an Age of Datafication
18:35 – 19:00 – Q&A
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/royal-holloway-university-london,"
"
https://www.turing.ac.uk/news/publications/stein-points,"
An important task in computational statistics and machine learning is to approximate a posterior distribution p(x) with an empirical measure supported on a set of representative points {x_i}_1^n. This paper focuses on methods where the selection of points is essentially deterministic, with an emphasis on achieving accurate approximation when n is small. To this end, we present `Stein Points'. The idea is to exploit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discrepancy between the empirical measure and p(x). Our empirical results demonstrate that Stein Points enable accurate approximation of the posterior at modest computational cost. In addition, theoretical results are provided to establish convergence of the method.
Chen WY, Mackey L, Gorham J, Briol FX, Oates CJ. Stein Points. International Conference on Machine Learning (ICML 2018).
"
https://www.turing.ac.uk/events/short-talks-cyber-physical-systems-and-monte-carlo-algorithms,"
​
Date: 2 March 2017 Time: 11:00 – 12:30 Watch the live stream.  Recordings will be made available on our YouTube channel following the event.
The Short Talks are an opportunity for Turing researchers to introduce themselves, with a short presentation on their research interests and what they are working on at the Institute. All talks, unless stated otherwise, will be livestreamed and archived on the Institute's YouTube channel. Short Talks are 25-minutes each, followed by a 5-minute Q&A. This session of Short Talks will feature:
For biographies, research interests and more, please visit Turing People.    
​

"
https://www.turing.ac.uk/news/turings-chief-scientist-among-new-royal-academy-engineering-fellows,"
The Royal Academy of Engineering elected 72 leading figures in the field of engineering and technology to its Fellowship at its AGM on 20 September 2022. Among them was Professor Mark Girolami, Chief Scientist at The Alan Turing Institute. 
Professor Girolami is the former Programme Director for Data-Centric Engineering. Under his leadership, the programme saw the delivery of a number of ‘world-firsts’ including the world's first self-sensing 3D printed stainless steel pedestrian bridge in Amsterdam and sustainable and more efficient underground agriculture.

The group consists of 60 Fellows, seven International Fellows and five Honorary Fellows, each of whom has made exceptional contributions to their own sector, pioneering new innovations, leading progress in business or academia, providing high level advice to government, or promoting wider understanding of engineering and technology.
Professor Girolami, said: ""I am extremely honoured to have been elected to the fellowship of the Royal Academy of Engineering by my peers. I have been privileged to have led outstanding teams of engineers and scientists at The Alan Turing Institute where the new discipline of Data-Centric Engineering has been developed and is having a transformative impact in engineering science and practice. This could only be achieved through co-creation across multiple disciplines in both academia and industry. I have been very fortunate to work with highly skilled and passionate colleagues across research disciplines and in all engineering sectors where, together, our ambition has been to make a difference to society.”
This year’s new Fellows reflect the Academy’s ongoing Fellowship Fit for the Future initiative announced in July 2020, to drive more nominations of outstanding engineers from underrepresented groups ahead of its 50th anniversary in 2026. This initiative sees the Academy striving for increased representation from women, disabled and LGBTQ+ engineers, those from minority ethnic backgrounds, non-traditional education pathways and emerging industries, and those who have achieved excellence at an earlier career stage than normal.
The new Fellows will be formally admitted to the Academy at a special ceremony in London on 8 November, when each Fellow will sign the roll book. In joining the Fellowship, they will add their unique capabilities to the Academy’s mission to harness the power of engineering to create a sustainable society and an inclusive economy for all.
Sir Jim McDonald FREng FRSE, President of the Royal Academy of Engineering, said: “I am delighted to welcome such an array of enormously talented people to the Fellowship of the Royal Academy of Engineering. From industry and enterprise to education and government – both national and international – these are some of our most pioneering and distinguished engineers and technologists.""
 
"
https://www.turing.ac.uk/news/new-research-explores-intelligence-analysts-use-machine-learning,"
A new report published this week by The Alan Turing Institute's Centre for Emerging Technology and Security (CETaS) explores the use of machine learning for intelligence analysis, based on in-depth consultation with stakeholders from across the national security community.
The research found that machine learning could help analysts to sift through large amounts of data, helping them to prioritise the information that matters most to help keep people safe. The ongoing global expansion of data presents both risks, and also opportunities. The use of ML offers real potential to simultaneously reduce such risks and to pursue such opportunities.
Machine learning works by identifying connections and patterns across large volumes of data that may otherwise go unnoticed by human operators.
The report offers fresh insight into how intelligence analysts make decisions, and it offers a series of recommendations for the design and deployment of machine learning models to ensure they could be effectively and responsibly adopted within national security.
The CETaS researchers found that in order to use ML effectively, those implementing the technology need to have a good understanding of the analyst’s real-world work environment. And while improving the analyst’s understanding of how ML works is important, the researchers report that it is not enough for analysts to trust the technology.
The researchers recommend that explanations on the use of ML should be tailored depending on the person’s expertise. For example, data scientists or those developing policy or approving the deployment of a ML system require different explanations to those required by analysts and oversight bodies. This is important because current efforts to explain how ML works focus on mathematical explanations that day-to-day users may not understand and do not help reinforce trust in the model.  
This report is one of the first public studies investigating human-machine teaming and the use of machine learning for intelligence analysis within UK national security.
Anna Knack, lead author and Senior Research Associate at The Alan Turing Institute, said: “It’s time consuming for intelligence analysts to work through the vast quantities of data that come across their desks every day. Machine learning can speed up this process, filtering out the irrelevant information helping analysts to act on information much more quickly.”
The use of machine learning to support intelligence analysis presents new challenges such as how to present the right amount of technical information regarding the model's performance to the user, to ensure they maintain the appropriate level of trust in the system. The recommendations in this report are aimed at ensuring the responsible, effective and proportionate use of machine learning for intelligence analysis so national security agencies can effectively deploy this technology to help keep people safe.
"
https://www.turing.ac.uk/people/external-researchers/hannah-j-williams,"
Hannah J Williams is a postdoctoral researcher at Imperial College London working on producing and controlling ultracold molecules for tests of fundamental physics and quantum systems. She leads the Imperial College Women in Physics group and is a member of the departments equality and diversity committee. 
"
https://www.turing.ac.uk/events/humanity-and-technology-conversation-jo-guldi,"
Living with Machines invites you to join us for two events with Professor Jo Guldi, where you will hear first hand from one of the world's leading digital humanists. During this event, you will get insights into recently published historical research on global land rights and land reform and understand more about how the humanities is an area of extreme potential for growth in data science.
Event 1
Jo warns of an age of pseudo-history promoted by GPT-3 and easy algorithms, fuelling nationalism and populism. Jo will contrast the naive use of algorithms with ""hybrid knowledge,"" the exciting domain where data-driven analysis of large-scale textual repositories meets critical thinking from the humanities and social sciences. This event is suited for a cross-disciplinary audience. 
Event 2
Jo will present on her latest book, The Long Land War, which tells a story as old as human history: the global struggle over food, water, land, and shelter. The Long Land War focuses on technology and expertise. This event is open to the public. 
This event is organised by The Alan Turing Institute, Living with Machines and the British Library. This event is virtual and free to attend. 
Event 1: Pseudo History and Digital History: The Dangerous Art of Text Mining
Jo argues that a world awash in text requires interpretive tools that traditional quantitative science cannot provide. Text mining is dangerous because analysts trained in quantification often lack a sense of what could go wrong when archives are biased, incomplete, or evidence the suppressions of the past. Jo's talk will review a brief catalogue of disasters created by data science experts who voyage into humanistic study.
It finds a solution in “hybrid knowledge,” or the application of historical methods to algorithm and analysis. Case studies engage recent work from the philosophy of history (including Koselleck, Erle, Assman, Tanaka, Chakrabarty, and others) and investigate the “fit” of algorithms with each historical frame of reference on the past.
Event 2: The Long Land War: The Global Struggle for Occupancy Rights, 1881-1974
The Long Land War tells a story as old as human history: the global struggle over food, water, land, and shelter. The book follows rent strikes, political movements and ideas from Ireland and India to the United Nations, Latin America, Asia, Africa, and the rest of the developed world, tracing the intersection of popular protest, nationalism, Communist and capitalist ideology, and utopian schemes involving the potential of small-scale technology and large-scale maps. Through struggle, those movements redefined private property as a collective resource whose value was its ability to support food and housing.
Jo Guldi will tell the history of state-engineered “land reform” projects from their triumphant origins in Victorian Ireland to their quiet assassination by the United States in 1974. She introduces land reform as a movement forged by a complex diversity of international actors, among them Irish peasants, Hindu saints, development analysts, economists, and indigenous farmers. Her research examines the success and failure of land reform against the complex interplay of Cold War ideology, United Nations schemes for improvement, World Bank dogma, grassroots activism, and human shortcomings.
The 1974 coup cast poor peoples around the world into a state of dependency on landlords: it has made a world of occupancy rights increasingly difficult to imagine. Today, land use represents a major key to the governance of climate change. We can apply the lessons of the past to the governance of climate change today, but we only have a vanishingly small window in which to do so.

 
"
https://www.turing.ac.uk/people/former-doctoral-students/alex-bird,"
Alex started his doctoral studies at The Alan Turing Institute in October 2016. Alex is registered at the University of Edinburgh under the supervision of Chris Williams. His research interests include recurrent neural networks, state space models, generative models, multi-task learning and approximate inference.
Alex is working on multi-task dynamical systems, sequence models which explicitly model the inter-sequence variation (e.g. of different people, organisations etc.) within a dataset. This endows models such as linear dynamical systems and RNNs with the ability to adapt to particular individuals either via end-user control, or probabilistic inference. Applications include personalised drug response modelling and customisable walking style for motion capture sequences.
"
https://www.turing.ac.uk/events/data-study-group-november-2021,"
Please note that applications have now closed.
Due to COVID-19 the Data Study Group will run remotely over three weeks and will be divided into two stages.
Please apply through Flexi-Grant. Participants are expected to attend the full duration of the event. This is a full time engagement and engaging part time impacts other team members and your learning potential. The Alan Turing Institute is committed to supporting individual circumstances, please do not hesitate to email [email protected] to discuss any reasonable adjustments.
The challenges are:
Please see below for further details on each challenge.
The skills that we think are particularly relevant to the challenges for this Data Study Group are listed under each challenge description below. Please note, the lists are not exhaustive and we are open to creative interpretation of the challenges listed. Diversity of disciplines is encouraged, and we warmly invite applications from a range of academic backgrounds and specialisms.
Plankton plays an essential role in the global carbon cycle and carbon sequestration, regulating the exchange of carbon dioxide between the atmosphere, surface ocean and ultimately the seabed. In particular, the role of zooplankton in the food web is critical as they occupy a central position, often controlling the abundance of smaller organisms by grazing and providing food for many larval and adult fish and seabirds. Plankton is also used in global monitoring efforts providing reliable and sensitive indicators to climate change and ecosystem health. 
The RV CEFAS Endeavour, a multi-disciplinary research vessel, collects millions of plankton images during its surveys through the Plankton Imager system: a high-speed imaging instrument which continuously pumps water, takes images of the passing particles, and identifies the zooplankton organisms present. Due to the nature of the passing particles, images have varying shapes and sizes with a highly-skewed distribution towards smaller particles/images. Of these, over 80% can be classified as detritus (e.g., sand, seaweed fragments, microplastics) which are traditionally manually removed (by-eye) before any analysis, leaving the remaining plankton images.
The challenge here is to develop rapid on-the-fly machine learning methods for automatically classifying plankton species (using information on their shape, features and size) using a manually labelled dataset of 40,000 images.
Useful skills: Image analysis and computer vision, deep learning, imbalanced datasets, keen to expand or explore a new field, Ecology.
 
Clinical trials are currently the gold standard for testing the safety and effectiveness of treatments (drugs or surgical procedures) and diagnostics in clinical care. Ensuring the safety of participants taking part in clinical trials is of utmost importance, both from a legal and ethical standpoint. In addition, data generated as part of clinical trials must be collected in a scientifically robust manner, to ensure the outcome of the trial is reliable and meaningful. 
For large clinical trials, a Clinical Trials Unit (CTU) typically oversees and directs the research, with multiple trial sites (usually hospitals) taking part. Whilst each site is responsible for recruiting the patients and collecting data, the CTU oversees the trial, through central trial monitoring, to ensure that all applicable ethical and regulatory requirements are adhered to. Trial monitoring often involves: (1) Risk-Based Monitoring, which uses performance indicators for each site to determine the extent, timing, and frequency of monitoring visits and (2) visiting both routine and poorly performing sites to conduct a site audit, which includes source data verification. Performance indicators are typically based upon quantitative measures such as percentage of data returned to the CTU and numbers of adverse events reported. With monitoring visits being very time-consuming and expensive, choosing optimal performance indicators and thresholds is a key challenge to ensure the efficient running of a trial.
Using centrally held CTU data, can we use AI and machine learning (ML) approaches to better identify and predict which sites within an ongoing clinical trial are performing poorly or at risk of non-compliance with the trial protocol? The key aim of this challenge is to improve on the current method of selection of which sites to monitor, with the optimal case being that only sites that require corrective action are visited. A proposed method of achieving this is to use ML to investigate the link between pre-visit performance indicator thresholds and monitoring visit outcomes. For example, when considering the performance measure regarding rates of data return, which threshold results in only the sites requiring the greatest corrective action being selected for monitoring visits? In addition, are there better performance indicators than those currently being used that can assist in the prioritisation and planning of monitoring visits?
Time stamped data will consist of: (1) pre-visit quantitative performance indicators such as the number of patients consented to take part in the trial and progression of patients through the treatment protocol in addition to the raw datasets for the trial and (2) post-visit ground truth labels indicating how well a site is performing, along with textual data indicating which sites were visited, issues that were found during the visit, their assigned severity level and any corrective action required. 
What we are looking for:
This work is supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/W006022/1, particularly the AI for Science (for CEFAS) and Health (for MRC CTU) theme within that grant & The Alan Turing Institute.  
 
Applications are being accepted through Flexi-Grant – apply now to avoid disappointment.
Apply now
The Turing's Data Study Groups are popular and productive collaborative events and a fantastic opportunity to rapidly develop and test your data science skills with real-world data. The event also offers participants the chance to forge new networks for future research projects, and build links with The Alan Turing Institute – the UK’s national institute for data science and artificial intelligence.
It’s hard work, a crucible for innovation and a space to develop new ways of thinking.
Read reports from previous Data Study Groups to see challenges and outcomes.
Read our FAQs for Data Study Group applicants.
 
Learn more about being a DSG participant including FAQs
How to write a great Data Study Group application
Queries can be directed to the Data Study Group Team
"
https://www.turing.ac.uk/people/guest-speakers/pui-anantrasirichai,"
Pui is currently a Research Fellow with the Visual Information Laboratory, University of Bristol. Her current research interests include image and video analysis and enhancement, image fusion, medical imaging, texture-based image analysis, and remote sensing. She also specialises in machine learning and decision making. For the last two years, she has been involved in developing an automatic framework to detect global volcanic deformation using InSAR data.
 
"
https://www.turing.ac.uk/people/guest-speakers/angeline-wairegi,"
Dr. Wairegi is an experienced research scientist currently working on issues of Artificial Intelligence (AI) and data-centric matters in the African continent. She is currently the AI Project lead at CIPIT, overseeing all the research studies under the center's AI4D umbrella and is also the co-director of CIPIT's Data Governance Policy Center.
"
https://www.turing.ac.uk/news/minister-artificial-intelligence-and-intellectual-property-visits-ai-uk,"
Parliamentary Under Secretary of State in the Department for Science, Innovation and Technology, Viscount Camrose, visited The Alan Turing Institute’s national showcase event today (Tuesday 22 March).  
The minister arrives at AI UK as The Alan Turing Institute launches its new strategy focusing its efforts on addressing some of the most pressing issues facing the world today in health, the environment and sustainability, defence and national security, and our increasingly digitised society. 

Viscount Camrose visited London’s zero carbon underground farm exhibition showing the work being done at the world’s first underground farm. The farm is shaping the future of urban farming through sustainable and hydroponic farming techniques. And everything happening at the farm is measured and represented in a virtual 3D digital twin at a laboratory in Cambridge to ensure the plants on the farm get exactly what they need at every moment of every day.  

He also spoke to researchers working on a modelling system that leverages data science and AI to determine how land is best used in the UK. This project, which sees The Alan Turing Institute partner with the Geospatial Commission, aims to ensure the UK can meet challenging net zero and biodiversity targets while also delivering infrastructure and economic growth. 

Viscount Camrose was appointed Parliamentary Under Secretary of State in the Department for Science, Innovation and Technology earlier this month.  
"
https://www.turing.ac.uk/news/talkdatatome-introducing-turings-new-video-series-curious-minds,"
This week, The Alan Turing Institute launches #TalkDataToMe, a new video series and social media campaign which aims to help the public understand trending topics related to artificial intelligence (AI) and data science. The series will cover topics such as non-fungible tokens (NFTs), smart home appliances and why a lack of women in data science and AI is a problem for everyone.
The first video in the series addresses the topical question: what is an NFT? Series host Tabitha Goldstaub, chair of the UK government's AI Council and co-founder of CogX, interviews the Turing’s Token Economy Theme Lead, Dr Andrea Baronchelli, to learn more about this issue.
In the video, Andrea discusses how an NFT is a unit of data stored in blockchain and gives some different examples of them. The video also discusses why NFTs are an important development in the world of online ‘ownership’.
It also considers what the future holds for NFTs, with Andrea saying that he “believes NFTs are here to stay because we are moving more and more on a life online and so owning things online is important"".

With AI-driven technologies now playing a central role in our professional, social and home lives, it is crucial that people understand the risks and benefits of engaging with them. This video series has been created to provide accessible and factual information on topics relating to data science and AI.  
The topics were chosen as they appear frequently in the news or the public domain but can be open to misinformation. And for future series, the creators are encouraging the audience to share what topics they would like to see covered.
The launch of #TalkDataToMe also coincides with the release of the dedicated portal on our website for the general public: Learn, Explore and Participate (LEAP). LEAP collects the Turing’s most accessible videos, podcasts, articles and events into one place.
If you have feedback or suggestions for #TalkDataToMe and LEAP. Get in touch at [email protected]. 
"
https://www.turing.ac.uk/research/research-projects/model-criticism-multi-agent-systems,"
A crucial limitation in the current generation of autonomous agents is that they are unable to reason about the adequacy of models used to predict the actions of other agents. This inability may cause an agent to use possibly misleading models of other agents (including humans), without ever realising the inadequacy. This project addresses this limitation by developing new algorithms for statistical hypothesis testing of probabilistic agent models. The algorithms will enable autonomous agents to detect model inadequacy, and to take appropriate action (such as revising models or asking for clarification) if the models are found to be inadequate.
In plain terms, we have one agent which uses models to predict the actions of other agents. Such models can make predictions based on things that happened in the past, such as the pact actions of the modelled agent, and other contextual information.
The principal idea in this project is to view models of other agents as stochastic (random) processes which are amenable to statistical hypothesis testing, using data collected from the observed interaction history. This allows the agent to reason about the correctness of the model hypothesis by comparing the predicted actions of the model with the actual actions taken by the modelled agent, and analysing action frequencies and causal dependencies.
The challenge in this approach comes from the fact that the usual statistical hypothesis testing methods rely on assumptions that do not hold in interactive multi-agent settings. For example, other agents may have complex causal structure in their decision making which maps entire interaction histories to actions, and their behaviours may change as a result of what happened in the past.
There are also important practical aspects, such as how many observations are needed to reliably conduct the hypothesis testing process, and how to automatically optimise the hypothesis testing based on task and environment specifications given to the agent.
A central goal in artificial intelligence research is the development of autonomous agents which can interact effectively with other agents to accomplish tasks in complex environments. The ability to reason about the unknown behaviours of other agents (that is, how they make decisions) is a key requirement in such agents, and a number of methods have been developed to this end (Albrecht & Stone, 2018).
However, a crucial limitation in current autonomous agents is that they lack the ability to reason about the adequacy of models used to predict the actions of other agents. Thus, it is possible that an agent may use inadequate and possibly misleading models of other agents, without ever realising it. Such inadequate models will make incorrect predictions, which may cause an agent that uses such models to make bad decisions when interacting with other agents.
For example, we cannot trust an autonomous vehicle to drive safely if it is unable to realise when its models of other drivers are inadequate, and therefore provide wrong predictions. Safe autonomy in these and other applications necessitates the ability to reason about the adequacy of ones models, such that appropriate action can be taken if the models are deemed inadequate (e.g. trying to improve the models).
This project will develop the foundations of a new statistical theory and algorithms to enable an autonomous agent to decide whether its models of other agents are inadequate. To achieve this, the algorithms perform automated statistical hypothesis tests based on observations such as the past actions taken by the modelled agents. The result of these tests inform the agent whether or not to trust its models of other agents.
Some of the technical questions involved in this research include:
Safety and robustness are crucial aspects for a successful deployment of autonomous systems. In particular, as autonomous systems interact with humans and other actors, they will have to reason about the actions of others by building predictive models of their behaviours.
Safety in this context means that an autonomous system can realise whether its models are inadequate for the task of predicting the behaviours of others, and thus take appropriate action (such as asking for clarification). This project aims to develop algorithms with enable autonomous systems to carry out such reasoning, thereby increasing the safety of autonomous systems and contributing to their successful deployment.
To demonstrate the impact potential of this research, the project will explore two novel applications of the developed algorithms:
An autonomous vehicle (AV) could utilise algorithms such as those developed in this project to decide whether its models of other drivers are adequate. If the AV decides that its models are inadequate in a given situation, it may resort to a conservative driving policy that does not rely on accurate models, thereby enhancing the AV's safety. This application will be explored in close collaboration with UK-based company FiveAI which aims to develop a complete AV system for the UK transportation sector.
The algorithms developed in this project could form the basis of a novel approach for secure remote authentication and key generation in computer networks. The basic idea is that a client and server machine engage in an interactive authentication process based on which the server has to decide whether the client is who it claims to be based on the client's observed actions, and this decision can be made using the algorithms developed in this project.
S.V. Albrecht, P. Stone. ""Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems"", Artificial Intelligence (AIJ), Vol. 258, pp. 66-95, 2018. arXiv link.
S.V. Albrecht, S. Ramamoorthy. ""Are You Doing What I Think You Are Doing? Criticising Uncertain Agent Models"", Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence (UAI), 2015. PDF link.
"
https://www.turing.ac.uk/people/former-researchers/maxine-mackintosh,"
Maxine leads the Diverse Data initiative at Genomics England, which aims to reduce health inequalities in genomic medicine by ensuring patients, regardless of their background, receive the same quality of genomics-enabled personalised medicine, supported by the latest research on people like them. Maxine is an organiser in the Turing’s Health Equity Special Interest Group where she primarily focuses on promoting data diversity in genomics. Maxine is also the co-founder of One HealthTech – a global, volunteer-led, grassroots community that supports and promotes under-represented groups in health innovation. OHT has over 20,000 contributors worldwide across 20 Hubs which have collectively delivered over 1000 events, projects, campaigns and initiatives improving diversity in healthtech. She also set up Data Science for Health Equity, a community of practice that brings together those with expertise in data science and health inequalities to connect and collaborate on cutting-edge domains in health. She has been part of a number of communities and committees including being a Non-Executive Director for the Eastern Academic Health Science Network, a member of the World Economic Forum’s Global Shapers, and the British Computer Society (Health Exec) and the DeepMind Health Independent Review Board.
 
With the increasing digitisation of health, care and life sciences, there is great potential to transform the way we predict, prevent, treat and understand health and disease. However, the data-ification of healthcare and research is not without its risks. Huge swathes of the population are missing in health datasets, we see countless examples of algorithms automating and encoding societal biases in health and we still live in a world with enormous inequalities in resources, funding and skills to support the digital revolution in health.
Maxine’s research interests lie in ensuring data science is as fair and equity-enhancing as possible in health, where she is currently focused on this in the context of genomics. Studies of human genetics have largely been on populations from WEIRD (Western, Educated, Industrialized, Rich, Democratic) countries which has resulted genomic insights that are not generalizable to all populations. Most studies, trials and papers conclude with a call to action to recruit and use more diverse genomes, and yet the proportion of non-European ancestries in genomic studies is diminishing. To address this gap, we must work across the whole pipeline of genomic research and health care delivery, from the populations we work with and the data we collect, to the analyses we carry out and the availability of genetic testing.
Prior to joining Genomics England Maxine’s research looked at applying machine learning methods to health, care and life science challenges, and also explored the ethical impacts these approaches had. Maxine was a Research Associate and Fellow working between The Alan Turing Institute, The Health Foundation and the University of Oxford where she primarily worked with Prof Chris Holmes. Her postdoc focused on exploring the boundaries of what we define as “health” data, and how we can infer how healthy or sick people are, based on their financial, geospatial, social or transactional consumer data. Prior to this she completed a PhD at UCL’s Institute of Health Informatics (and was a Turing Enrichment Student) where her work looked at using novel statistical methods to detect early signs of dementia in electronic health records. She also completed an MSc in Health Policy, Planning and Financing (LSE & LSHTM) and a BSc in Biomedical Sciences, Neuroscience & Pharmacology (UCL).
"
https://www.turing.ac.uk/news/central-bank-digital-currencies-pose-danger-individual-privacy,"
A new type of digital currency which gives more people access to banking services is running the risk of eroding individual privacy, according to researchers in a new comment piece in Nature Human Behaviour out today (Friday 8 July 2022). 
The researchers are calling for a public debate on these privacy concerns around the currency known as Central Bank Digital Currency (CBDC). They say that the rush to issue CBDCs with the absence of a well-informed discussion could drastically reduce the little individual privacy that still remains. 
The researchers also say that failing to address privacy concerns now, could echo issues around the regulation of social media platforms.  
CBDCs, a digital form of the currency issued by a central bank, already exist in many places including the UK, Singapore, South Africa and Canada. They are becoming increasingly popular with 14 countries already piloting them and over 50 have announced that they are in the research and development stage. 
Their wide adoption will create a dramatic change in how much data is generated by everyday transactions. The researchers fear that if the wrong technology is chosen, we could end up with a state - even a democratic one - that knows your identity, your income and your transactions, holding even more power over your life. 
The researchers believe there are three key topics that should be included in any public debate relating to CBDCs. This includes: which features of the CBDC would promote financial inclusion; how the CBDC can ensure a reasonable level of account and transaction anonymity; and how data generated will be processed, stored, and eventually destroyed.  
Dr Andrea Baronchelli, lead author and Token Economy Theme Lead at The Alan Turing Institute, said:
“Central Bank Digital Currencies have the possibility of being more financially inclusive by offering convenience and low transaction costs. However, this comes at the risk of our privacy which we don’t believe should be compromised. 
“We are in a unique position to encourage policymakers to make good design decisions as early as possible - before bad features become entrenched.” 
The piece ‘Central bank digital currencies risk becoming a digital Leviathan’ by Andrea Baronchelli, Hanna Halaburda, and Alexander Teytelboym was published in Nature Human Behaviour at 16:00 (BST) on Friday 8 July 2022. 
"
https://www.turing.ac.uk/people/researchers/iain-styles,"
Iain Styles is a Senior Lecturer in the School of Computer Science at the University of Birmingham. His research interests are in the development of new computational methods for understanding complex biological experiments, with current interest centred on studying the biological role, dynamics, and structure-function relationship of proteins using techniques including single molecule microscopy, mass spectrometry, and alanine scanning mutagenesis. His methodological approaches are drawn from classical image analysis, statistical machine learning, and computational topology which he combines with classical simulation techniques such as molecular dynamics to build biologically interpretable models from large-scale data.
He is currently the co-Director of the EPSRC-funded Centres for Doctoral Training in Physical Science for Healthcare, and Deputy Director (Birmingham) of COMPARE - the Centre of Membrane Proteins and Receptor, an interdisciplinary research centre established jointly by the Universities of Birmingham and Nottingham to develop novel methods for studying single membrane proteins. His research has been supported by funding from EPSRC, the British Heart Foundation, the European Union, and the Dunhill Medical Trust. He was awarded his PhD in Theoretical Condensed Matter Physics from Birmingham in 2003.
Iain Styles’ current research interests are in developing new techniques for understanding high-content experimental data in discovery-driven biology. A major current focus is to take advantage of the central organising principle of compositionality in biology and to construct compositional representations of data that reflect the underlying biological structure. Such a representation would allow us to answer fundamental questions: What are the constituent parts? How are they organised with respect to each other? How does that organisation change over time?
Dr Styles is currently developing new methodological approaches to construct these models that draw on ideas from topology, statistical machine learning, and hypergraph theory. The testbeds for the new methods will be i) mass spectrometry imaging, where we aim to extract parts that correspond to chemical species and, compositions of parts that correspond to different tissue sub-types and ii) single molecule microscopy, where parts correspond to individual protein species, and their compositions to different components of the cell’s structure.
"
https://www.turing.ac.uk/people/guest-speakers/nikita-bhangu,"
Nikita Bhangu is Head of Digital Standards Policy in the UK government's Department for Digital, Culture, Media and Sport (DCMS). She works in the Digital Standards & Internet Governance Team in DCMS which brings together the UK government's global engagement with key internet governance and digital standards bodies. She works on the digital standards policy portfolio which includes standards policy on new and emerging technologies such as AI and quantum. Nikita has a previous background in International Digital Trade, where she recently worked on the key bilateral digital trading partnerships such as the UK-Singapore Digital Economy Agreement, and the development of other digital chapters in UK Free Trade Agreements. She also has strong experience in regulatory work, having worked previously in the UK energy regulator the Office for Gas and Electricity (OFGEM). 
"
https://www.turing.ac.uk/people/enrichment-students/cari-hyde-vaamonde,"
Fascinated by the potential for code and AI to reform how law and justice function, Cari Hyde-Vaamonde is an experienced lawyer and court advocate. Having practised in diverse fields including technology, Cari became increasingly interested in systematic analysis. Her focus on research in the field recently culminated in a UKRI 4-year award to research the impacts of AI in justice settings at King's College London, where she is also a Visiting Lecturer. She is engaged in several interdisciplinary collaborations and presented research at the International Conference on Artificial Intelligence and Law in 2021.
The legal system is on the brink of a crisis of confidence. Pressure on courts is unprecedented, and delays mean that justice is too often not being served. Tools exist that may assist in improving processes, and the legal framework to allow these methods is being put in place, but public trust in human-computer interaction in this context is under-explored. Arguments regarding bias, or technical metrics of accuracy only go so far. The legitimacy of the system is at risk of being undermined if reforms take place without public confidence being maintained. 
To close this gap, the research develops a methodology for empirically testing the effect of algorithms on the legitimacy of the justice system of England and Wales, focusing on two scenarios: the single justice procedure (criminal jurisdiction) and the Affordability Calculator (civil jurisdiction). Incorporating methods from the social sciences to build mathematically-based models in a collaborative way, it focuses on perceptions of legitimacy, acknowledging that there are serious questions regarding the legitimacy of human-only judicial decision-making that this analysis cannot ignore.
"
https://www.turing.ac.uk/news/publications/getting-hearer-old-charting-referring-expressions-across-time,"
When a reader is first introduced to an entity, its referring expression must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how expressions that refer to the same entity develop over time. We track thousands of person and organisation entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a model to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30% absolute improvement over a majority-class baseline.
Ieva Staliunaite, Hannah Rohde, Bonnie Webber and Annie Louis, ""Getting to Hearer-old: Charting Referring Expressions Across Time"", Proceedings of Empirical Methods in Natural Language Processing 2018
"
https://www.turing.ac.uk/people/business-team/anneca-york,"
"
https://www.turing.ac.uk/people/external-researchers/valentin-courgeau,"
"
https://www.turing.ac.uk/research/impact-stories/making-simulations-simpler,"
​
​Simulations are useful in a huge number of disciplines, from medical research to insurance to astronomy. For engineers studying fluid dynamics – the movement of liquids and gases – simulations are an invaluable part of their toolkit. They need to know how different variables, constraints, and design specifications affect the movement of, and stresses caused, by fluids. Setting up such simulations and visualising their results can be extremely complex and require considerable computing power.
Often users wanting to run simulations are experts in their particular industrial or scientific fields, but not experts in the complex simulation software or the computing platforms required to run them effectively. As a result the real-world use of such software in industry is low.
To tackle this problem, software engineers at The Alan Turing Institute, in collaboration with partners at Imperial College and UCL, have developed a user interface which aims to make the simulation process more user-friendly, and link seamlessly with cloud-based supercomputing. The tool has the potential to make running, visualising, sharing, and reproducing simulations simpler, for both academic and industrial communities.
For the past five years Omar Matar, Professor of Fluid Mechanics at Imperial College, has been leading the MEMPHIS programme (Multi-scale Examination of Multiphase PHysics In flowS), which aims to generate the next generation of codes for complex fluid dynamics problems.
One output of the programme is Blue, a set of code for simulating various ‘multi-phase’ fluid flow problems. As Professor Matar says, the team now wants to “transition Blue from an excellent research code to one that could be used by industry”, and do this by creating, “a simple, reliable tool that everybody code use”.
“We bring research code out of the academy and onto the desktop”
James Hetherington, Director of Research Engineering at the Turing
Through funding from Imperial and the Turing’s programme in Data-Centric Engineering in partnership with the Lloyd’s Register Foundation, the Turing’s Research Engineering (RE) team became part of a multi-institution group working to create this tool – a customisable application that could run and visualise a range of different simulations. The RE team worked in collaboration with the MEMPHIS team and its industrial partners, as well as with the Research Software Development Group at UCL.
James Hetherington, Director of Research Engineering at the Turing: “Making software that was developed for research easy to use is a critical part of our [team’s] mission, and a key part of this is developing user interfaces which make the complexity of scientific software accessible. We bring research code out of the academy and onto the desktop”.
Simulations can be costly to run, both in time and money, and have a multitude of different parameters, variations in which can have significant effects on results (see visualisation below). Therefore, the application needed to allow people who have a working knowledge of what they want to simulate but don’t have the specific skills and coding knowledge required to launch simulations, to quickly and easily do so.
This meant a well-designed, customisable user interface (UI) for this application needed to be built, that would help clearly visualise and summarise the simulations and their results, and satisfy the requirements of multiple different users and scenarios.
It was also necessary that the tool work as part of a pipeline that includes seamless, secure connection to supercomputer clusters, like those available at Imperial or in cloud based systems like Microsoft Azure, in order to efficiently run the simulations.
The application is designed to work in browser and be fully configurable to the user’s needs.
“The user can define what kind of UI they want”, May Yong, Turing Research Engineer, explains, “Do they want sliders? Do they want text boxes? Do they want checkboxes? Instead of looking at a load of scripts, it’s a very visual way to set up experiments. So when it comes to something with 60 or more parameters this will make a big difference.”
The UI can include various videos and graphs, which enable the user to not only view and analyse completed simulations, but also monitor experiments as they’re running. As Yong notes, “Very quickly you can see the initial results and whether the experiment is worth continuing. If there’s obviously something wrong you can stop it running right away, rather than checking it in a month’s time.”
Furthermore, as the UI is browser-based it could be shared with others, along with a simulation’s output, so that results subsequently published in papers could be corroborated, and experiments checked and re-run with the same or different parameters.
A proof of concept of the UI, customised for the Blue code, was demoed at an industry showcase for the MEMPHIS programme at Imperial College on 15 September 2017. Present were representatives from the programmes’ collaborative universities – Imperial, UCL, University of Birmingham, and University of Nottingham – and its industry sponsors and partners – EPSRC, P&G, BP, Chevron, and Johnson Matthey.
Richard Craster, Professor of Applied Mathematics at Imperial, told us the demo was “a highlight…admired by several of our industry partners and certainly showed the concept is more than viable”.
The team is now developing an enhanced prototype with the ultimate goal of having an application that industrial partners start using as part of their day to day businesses. The work is contributing to the key challenge of the Turing’s data-centric engineering programme to deliver safer, smarter engineering. Lowering the barrier to entry for running simulations increases opportunities for developing innovative solutions to modern, data-rich engineering problems.
""Lowering the barrier to entry for running simulations increases opportunities for solving modern, data-rich problems""
In line with The Alan Turing Institute’s open and collaborative approach to research and software development, the core code of the application will be made open source. Customisations required on top of this ‘open core’ for particular simulation software and scenarios will be retained as closed source intellectual property by the relevant research groups. For Imperial this will be for the Blue code, and for UCL this will be for BEM++, their platform for solving acoustic and electromagnetic problems.
This open core code means the application could be used for many other domains, such as medical imaging. May Yong sums it up, “You’d just define your UI, have your simulation libraries running and the code would connect the two.”

"
https://www.turing.ac.uk/people/guest-speakers/michael-woollard,"
Michael Woollard is a PhD student in the Radar group at University College London. His research primarily focuses on automatic object recognition (AOR) methods for hyperfine resolution SAR imagery. He obtained his MEng (Hons) in Electronic (with Communications) Engineering from UCL in 2019, during which he was awarded the Rank Taylor Hobson and departmental prizes for his third- and fourth-year projects. Both of these projects addressed challenges in the development of AOR approaches for low-equity airborne SAR systems.
Michael is a regular contributor to meetings of the Electromagnetic Systems Interest Group (EMSIG), and a member of the Association of Old Crows.
"
https://www.turing.ac.uk/news/concern-over-reliance-digital-technologies-student-mental-health-care,"
Students and staff at universities have significant concerns about the impact of relying on digital technologies for mental healthcare, according to new research published today by The Alan Turing Institute. 
Students that participated in this research expressed concerns about the data privacy policies of digital mental health tools and that in-person care was diminishing and being replaced with digital technologies and services. 
Students were also concerned by the lack of empathy offered by technology and the potential exacerbation of social isolation by digital offerings. 
The university mental health administrators that took part voiced concerns over the clinical effectiveness and the management of risks on digital mental health platforms. 
They also expressed concerns that the introduction of digital services can influence who is seen to take responsibility for the mental ill-health of students, as responsibility is shifted from the institution to the individual, and so impacts on how duty of care is understood. 
The findings come from a series of interviews and participatory workshops, which took place with 25 university students and university administrators from across 10 UK universities. 
The crisis in student mental health across UK universities worsened during the pandemic. For many universities, the additional challenges faced by the COVID-19 pandemic have resulted in an increased reliance on digital mental health technologies, from online chatbots to cognitive behavioural therapy (CBT) delivered by smartphones. 
The report provides key recommendations to the university sector on how they can best navigate the complex and radically shifting landscape of digital mental health tools.  
These recommendations include ensuring there are clear and open communications with students around the benefits and risks of digital mental health services and ensuring issues of digital poverty are taken into account when planning for the implementation of technological solutions. 
To build trust in digital mental health technologies, it is essential to show how ethical principles have been considered and included in their design, development, and deployment. 
To address this need, the report authors have set out a positive proposal for a framework and methodology called 'Trustworthy Assurance'. This will help regulators, policymakers, developers, and researchers show how they’ve embedded core ethical principals in the design, development, and deployment of these technologies.  
Dr Chris Burr, lead author and Ethics Fellow at The Alan Turing Institute, said: “Digital technologies are already transforming mental health research and the provision of mental healthcare services. However, the increasing availability of such technologies raises important ethical questions for affected users about their right to privacy and the varying quality of care offered. It also offers significant challenges for regulators and developers about how best to manage the design, development, and deployment of these technologies. The lack of transparency around these technologies at present is contributing to a culture of distrust which impacts vulnerable people getting support. 
“Our research aims to address some of these concerns, which we hope will help make these technologies more responsible and trustworthy. Most importantly, we hope that our work can contribute to improving mental health services for those who need them.” 
Professor Mark Girolami, Chief Scientist at The Alan Turing Institute, said: “With mental health services under more pressure than ever before, more people are turning to digital services and solutions. 
“That’s why this research is so important. It’s essential that anyone providing digital mental health services for people at a potentially vulnerable time in their lives, can be sure that the services are safe and have been created responsibly.” 
Rianna Walcott, a workshop participant and mental health advocate, said: “In my experience digital mental health services can be really beneficial. I found my therapist online using these services.  
“But it’s important that they’re carefully monitored and used with some caution.  There shouldn’t be a one size fits all approach – particularly when it comes to mental health care because different people will use technology differently. What works for me won’t work for someone else. So, I was happy to be a part of this research to try and improve mental health provisions for young people, particularly those who are marginalised at the intersections of race, gender, class, sexuality.  
"
https://www.turing.ac.uk/events/event-type/workshop,"
"
https://www.turing.ac.uk/people/researchers/vincent-straub,"
Vincent J. Straub is a Research Assistant in the AI for Public Services team, within the public policy programme.
Vincent is interested in understanding how to best combine human and machine intelligence to address complex societal and environmental challenges. At the Turing, he is supporting research on the implications of bringing artificial intelligence and machine learning into government and public service.
"
https://www.turing.ac.uk/people/researchers/juergen-branke,"
Juergen Branke is Professor of Operational Research and Systems at Warwick Business School, and Director of Warwick’s Data Science for Social Good programme. He studied Industrial Engineering and Management at the University of Karlsruhe, Germany, where he also did his PhD in Applied Computer Science in 2000. He worked as Researcher and Lecturer at the University of Karlsruhe until moving to the University of Warwick in 2009. Juergen is the Editor-in-Chief of ACM Transactions on Evolutionary Learning and Optimization and Area/Associate Editor of several other journals. He enjoys working with industry and has done various projects with industry in logistics, engineering, and telecommunication.
 
Juergen's research area lies at the interface of machine learning and optimisation and falls into the broader field of prescriptive analytics, i.e., how to use data and models to make optimal decisions. In particular, he works on methods such as metaheuristcs and Bayesian optimisation to tackle problems that involve uncertainty, are dynamically changing over time, or involve multiple, conflicting objectives. 
"
https://www.turing.ac.uk/people/researchers/josh-cowls,"
Josh Cowls is the Research Associate in Data Ethics at The Alan Turing Institute and a member of the Institute’s public policy programme. A former Turing Doctoral Student, Josh pursued a DPhil at the Oxford Internet Institute, University of Oxford, with a focus on the ethics and politics of algorithmic decision-making systems. His research agenda centres on democratic decision-making in the digital era, and he is the author or co-author of work on data ethics, state surveillance, online agenda-setting, and the use of web archives in publications such as New Media & Society, Minds and Machines, and in numerous edited volumes.
Josh is the Convenor of the Turing’s Ethics Advisory Group, a member of the Institute's Data Ethics Group, and sits on the Digital Catapult Machine Intelligence Garage’s Ethics Committee. As part of the public policy programme, Josh contributes to the Institute’s partnership with the Ada Lovelace Institute and its work with the Information Commissioner’s Office on a framework for algorithmic explainability.
"
https://www.turing.ac.uk/events/event-type/conference,"
"
https://www.turing.ac.uk/people/researchers/john-francis,"
John Francis is a Research Assistant on the AI for Public Service team under the public policy programme. Prior to joining The Alan Turing Institute, John worked on numerous Education and Health-related research projects at NORC at the University of Chicago.
John holds an MSc in Social and Geographic Data Science from University College London and a BA in Global Studies and Political Science from the University of Chicago. His research interests now focus on utilizing satellite and aerial imagery to aid public decision-making processes.
John is interested in different ways to extract meaningful information about cities and the environment from satellite and aerial imagery. At the Turing, he is supporting research on different ways to integrate artificial intelligence and machine learning into government and public decision-making practices.
"
https://www.turing.ac.uk/news/publications/mining-uk-web-archive-semantic-change-detection,"
Semantic change detection (i.e., identifying words whose meaning has changed over time) started emerging as a growing area of research over the past decade, with important downstream applications in natural language processing, historical linguistics and computational social science. However, several obstacles make progress in the domain slow and difficult. These pertain primarily to the lack of well-established gold standard datasets, resources to study the problem at a fine-grained temporal resolution, and quantitative evaluation approaches. In this work, we aim to mitigate these issues by (a) releasing a new labelled dataset of more than 47K word vectors trained on the UK Web Archive over a short time-frame (2000- 2013); (b) proposing a variant of Procrustes alignment to detect words that have undergone semantic shift; and (c) introducing a rank-based approach for evaluation purposes. Through extensive numerical experiments and validation, we illustrate the effectiveness of our approach against competitive baselines. Finally, we also make our resources publicly available to further enable research in the domain.
Tsakalidis, A., Bazzi, M., Cucuringu, M., Basile, P., & McGillivray, B. Mining the UK Web Archive for Semantic Change Detection. Proceedings of INTERNATIONAL CONFERENCE RECENT ADVANCES IN NATURAL LANGUAGE PROCESSING, https://doi.org/10.17863/CAM.43534
2.9 MB
"
https://www.turing.ac.uk/people/doctoral-students,"

Turing Doctoral Students register for their doctorate at one of our university partners, and are hosted at the Institute offices.
A number of students already enrolled on a doctoral programme are at the Institute on placements of 6-12 months through the Turing's Enrichment Scheme.
Find out more about studentships at the Turing
2022 Enrichment Students
2019 Doctoral Student Cohort
2018 Doctoral Student Cohort
2017 Doctoral Student Cohort
2016 Doctoral Student Cohort
Students who have completed their studies
"
https://www.turing.ac.uk/work-turing/jobs,"

View and apply online for current vacancies via our careers site (opens a new window)
We are committed to making sure our recruitment process is accessible and inclusive. This includes making reasonable adjustments for candidates who have a disability or long-term conditions. Please contact us at [email protected] to find out how we can assist you.
Find out more about accessibility and facilities at the Turing.
For more information on the Institute’s commitment to equality, diversity and inclusion please visit our equality, diversity and inclusion page.
View our privacy notice
"
https://www.turing.ac.uk/events/bigscience-philosophy-building-open-ai,"
Dr Margaret Mitchell, Dr Yacine Jernite, and members of the BigScience Data Governance group will join speakers from The Alan Turing Institute to lead a discussion on their approach to building a Large Language Model (LLM) in the open with researchers around the world.                                                                                
The BigScience model, which finishes training in July 2022, has pioneered a new approach to building open & responsible AI, inspired by open science creation schemes such as CERN and the LHC, in order to facilitate the creation of large-scale artefacts for the entire research community. This effort addresses the current models of data governance and LLM development by big technology giants, which poses problems from research advancement, environmental, ethical, and societal perspectives, and issues raised in a paper co-authored by Margaret: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?       
In this open conversation with the Turing community, Margaret, Yacine, and members of the BigScience team will present on BigScience’s model for data governance. Leaders from Turing’s tools, practices, and systems (TPS), public policy, and AI programmes will respond and address open questions with regards to building AI through distributed, international collaborations.
             
Register now
By registering for this event you are agreeing to the events code of conduct. 
"
https://www.turing.ac.uk/events/cifar-ukri-cnrs-ai-society-principles-practice,"
Express your interest is now closed
This event will bring together leading researchers from Canada, the UK, and France to explore the technical underpinnings of topics including AI fairness, interpretability and privacy, as well as policy issues including online harms.
The workshop will aim to establish closer links between the research communities in the three countries and forge new collaborative projects. While we intend to focus on technical work, of course it’s vital to connect this to the real world.
We are very pleased also to be hosting sessions with legal and policy experts from each of the countries who will compare the regulatory and policy environments in each jurisdiction.
A detailed agenda will be shared shortly. The programme will comprise keynote presentations and lightning talks, as well as breakout sessions. We hope to explore collaborative projects in detail during the breakout sessions.
Please note that this working meeting is for an advanced professional audience and places are limited. We are seeking delegates from a range of disciplines working on themes relevant to AI & Society such as algorithmic fairness, interpretability, privacy or on policy issues such as online harms, either in academia or industry, with an affiliation to an academic institution, government organisation or sector relevant company.
 


"
https://www.turing.ac.uk/news/publications/node-wise-pseudo-marginal-method,"
Motivated by problems from neuroimaging in which existing approaches make use of “mass univariate” analysis which neglects spatial structure entirely, but the full joint modelling of all quantities of interest is computationally infeasible, a novel method for incorporating spatial dependence within a (potentially large) family of model-selection problems is presented. Spatial dependence is encoded via a Markov random field model for which a variant of the pseudo-marginal Markov chain Monte Carlo algorithm is developed and extended by a further augmentation of the underlying state space. This approach allows the exploitation of existing unbiased marginal likelihood estimators used in settings in which spatial independence is normally assumed thereby facilitating the incorporation of spatial dependence using non-spatial estimates with minimal additional development effort. The proposed algorithm can be realistically used for analysis of moderately sized data sets such as 2D slices of whole 3D dynamic PET brain images or other regions of interest. Principled approximations of the proposed method, together with simple extensions based on the augmented spaces, are investigated and shown to provide similar results to the full pseudo-marginal method. Such approximations and extensions allow the improved performance obtained by incorporating spatial dependence to be obtained at negligible additional cost. An application to measured PET image data shows notable improvements in revealing underlying spatial structure when compared to current methods that assume spatial independence.
D. Thesingarajah and A. M. Johansen. The Node-wise Pseudo-marginal Method. Statistics and Computing, 32(43) 1-31: 2022.
 
3.1 MB
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups,"

""Team up with a collaborative community of motivated data science experts, to jointly create a pathway to follow-up research and translational activities""
Sebastian Vollmer, Creator of the Turing Data Study Groups

I am from an organisation interested in posing a challenge
I am a researcher interested in being a participant

 
Proud winner of The KE Awards 2021 for Academic Engagement of the Year
Data Study Groups take place throughout the year. Upcoming Data Study Groups will be featured here.
Find out first when the next DSG challenges will be taking place and more – select 'Receive Applied Skills updates'.
"
https://www.turing.ac.uk/research/research-projects/detecting-fringe-behaviour-companies-networks,"
Complicated company ownership structures are often set up to obfuscate the identification of beneficial owners. These strategies are considered to be on the fringe of legitimacy and legality, so detecting them is important to regulators and society in general. We combine economic theory, network analysis, and machine learning to unearth the patterns that such behaviours leave behind in large-scale administrative datasets. This allows us to identify clusters of companies that specialise in such strategies, and to formulate models about their potential adaptation to future legal reforms. This project provides policymakers with guidance on how company ownership evolves over time as clusters that specialise in fringe behaviour learn and adapt to government enforcement mechanisms.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-st-andrews,"
www.st-andrews.ac.uk
"
https://www.turing.ac.uk/research/research-projects/exploring-childrens-rights-and-ai,"
Despite rapid innovation in artificial intelligence that has the potential to impact future generations and their relationship with technology, there is little research done to examine children’s experiences of AI. Our project involves engaging with children to explore what children think about AI, how they would like to see AI developed in the future, and how children can become more involved in shaping AI innovation, policy, and governance. From our public engagement work, we aim to use our findings to inform policymakers to support the development of child-centred AI.

Whether in education, health, or in everyday lives, AI presents specific opportunities and challenges for children. Particularly following the COVID-19 pandemic, children have been more connected than ever, where physical forms of interaction, play, and schooling have migrated online. Some of these changes have been made permanent, with hybrid tools remaining in place. While research has been done to explore how adults engage with AI and algorithmic systems, much less has been done to assess their impact on children’s lives.
As part of our project, we aim to bring together multidisciplinary research expertise and methodologies to examine children’s understanding, knowledge, and engagement with AI and digital technologies. This includes conducting exploratory literature reviews, surveys, and public engagement work within and outside of the classroom to investigate the socio-technical and policy implications of children’s current perceptions of AI.
Bridging the gap between theoretical considerations of children’s interaction with technology and empirical insight into their opinions, aspirations, and questions about AI, our project team aims to support developments on policy, curricula, and the deployment of AI technologies for children to ensure that children’s considerations are involved in decision-making about future uses of AI for their own benefit.

This programme of work is being undertaken through several interrelated research projects: 
In 2020, as a policy pilot partner with UNICEF, we conducted research to pilot UNICEF’s draft guidance on child-centred AI. This research was published by UNICEF as a case study in 2021. Our research demonstrated a lack of research into children’s experiences with AI, and a pressing need to engage with children and young people to underpin their future involvement in the development of child-centred AI. Our research also indicated that many public sector organisations recognise the importance of involving children in the design, development, and deployment of AI systems that use their data, but that they lack the experience, resources, or skills to do so.  
In 2022, we began conducting a review of existing frameworks relating to children’s rights and AI in the UK and internationally. This includes UNICEF’s Policy Guidance on AI for Children 2.0, the World Economic Forum’s Artificial Intelligence for Children Toolkit, the ICO’s Age-Appropriate Design Code, and the UK’s proposed Online Safety Bill amongst others. This in-depth review examines the ways that children’s rights are being advanced and protected in relation to AI and seeks to inform best practice in child-centred AI. The outputs of this research will provide a comparative analysis of existing frameworks, synergies and differences, and challenges surrounding the implementation and enforcement of such frameworks. We recently presented preliminary findings of this research and are set to publish our work in the near future.

Since 2022, we have been collaborating with the Children's Parliament and the Scottish AI Alliance to engage primary school children aged  7-11 across Scotland on AI and children’s rights. A series of introductory workshops have been held with 87 children in four schools across Scotland including Edinburgh, Glasgow, Stirlingshire, and Shetland. Following this, we are working with children from across these schools who will continue to be engaged over the next two years. This project is exploring how children feel about AI, how they would like to see AI developed in the future and how children would like to be involved in the development and governance of AI.  
As part of this project, we travelled up and around Scotland to meet and work with all 87 children teaching them about children’s rights and AI but, more importantly, learning about what matters to them in this area. Our team developed creative and interactive ways to explain foundational AI concepts, for instance how an algorithm works and what machine learning is.  
“I didn’t know before that AI could be used in healthcare to help people”. - P5 Student, Stirlingshire, Scotland 
We have also collaborated with the Children’s Parliament to continue to work with a core ‘AI investigators’ team of 13 children across the four schools. From November 2022 – February 2023 we ran six 90-minute workshops, each of which focused on a different theme of children’s rights and AI. Children shared their thoughts on bias, fairness, where AI may be used in their day to day lives, and how AI may be better developed to uphold children’s rights. They quickly grasped difficult concepts like how algorithms are used in prediction, recommendation, and classification.     
In an additional set of four online sessions, the children had the opportunity to ask us questions based on real-world AI case-studies. We were impressed with the variety and depth of questions asked, such as the limitations of facial recognition for identical twins and improving training data to reduce bias. One case study focused on AI in healthcare where children expressed concerns about accessibility and asked how AI devices can be designed to be more accessible for the elderly and younger individuals. Children explicitly mentioned their ‘right to privacy’ and asked, ""how fair is it for companies to collect information [data] about children?"". Other children said, ""a lot of games try and get children to spend money"" and wanted to learn about how to better safeguard themselves online. Children also showed empathetic considerations pointing to the digital divide and asked, ""is it fair that some children do not have access to AI [devices]?""  
Our project report, summarising the first stage of our engagements with children in Scotland, identifies four key themes drawn from the investigator’s participation: AI & Education, Fairness & Bias, Safety & Security, and The Future of AI. In the next phase of this research, we are continuing to collaborate with the Children’s Parliament and Scottish AI Alliance to explore these themes, and to develop and test new approaches to engaging children in decision-making and policy-making relating to AI in Scotland.  
You can watch the project video below:

 
In 2023, we began our collaboration with the Council of Europe Steering Committee for the Rights of the Child to design, develop, and implement a mapping study exploring the extent to which children’s rights have been considered in the context of AI across member states. This piece of research will provide evidence on existing frameworks in place and gaps to be addressed and will support the CoE Strategy for the Rights of the Child. 
We asked if children think they should have a say in how AI is made and developed and they said:
“Yes. To evolve their [children’s] future and make sure nothing bad’s going to happen…And make their future better. And everyone’s future – better."" - P5 Student, Glasgow, Scotland 
AI is increasingly part of children's day-to-day lives with impacts often realised only in retrospect. Understanding the ways in which children understand, experience, and engage with AI could provide much-needed guidance for policymakers, AI developers, and affected stakeholders to inform future approaches to child-centred AI.
Specifically, our research engaging children across Scotland on AI and children’s rights will contribute towards Scotland’s AI Strategy to ‘Develop Scotland’s approach to AI and children’. It can also help to build out not only protections for children currently set out in the GDPR and Age-Appropriate Design Code, but also empower children in their use and understanding of digital technologies.
Broader applications and benefits of this project include critical research needed to lay the foundation and inform the landscape of children’s rights as they relate to AI, especially as new technologies that directly affect children continue to be developed and deployed.
With this research, the case for children to be included as a key stakeholder group and be meaningfully involved in the development of child-centred AI will be further strengthened and help to ensure their voices are heard. This project also prioritises the diversity and inclusion of minority and marginalised groups as a key component throughout to ensure that future research applications are relevant and beneficial to all children.


Publication of the Stage 1 project report summarising the first stage of engagements with children in Scotland in partnership with Scotland's Children's Parliament and the Scottish AI Alliance, May 2023
Sabeehah Mahomed and Dr Janis Wong presented at Many Worlds of AI on ‘Exploring Children’s Rights and Child-Centred AI’, April 2023 
Dr Janis Wong and Dr Mhairi Aitken, together with Children’s Parliament and 13 children from our Scottish engagement project presented ""Exploring Children’s Rights and AI: Breaking News! At The Scottish AI Summit, Glasgow, 28th March 2023  
Dr Janis Wong volunteered at Acland Burghley School to teach students about data ethics, February 2023 
Our team together with Children’s Parliament conducted six 90-minute online workshops, four in-person workshops, and an additional four 60-minute online sessions with 87 children across 4 schools in Scotland, September 2022 - February 2023 
Morgan Briggs featured on the Children's Parliament podcast discussing our project engaging children across Scotland, December 2022 
Project launch blog with the Children’s Parliament and Scottish AI Alliance, October 2022
Dr Mhairi Aitken and Morgan Briggs presented at Government Data Summit on ‘Public Trust and Data’ with specific reference to children’s rights and AI, July 2022
Dr Janis Wong was part of a Roundtable with the ICO to evaluate the Children’s Code, June 2022
Dr Mhari Aitken and Morgan Briggs co-authored piece, ‘Engaging Children with AI Ethics’ for the Raspberry Pi Foundation, June 2022
Dr Janis Wong judged the UCL Computer Science Coding Curriculum 2022 Hackathon to support technology demonstrations for school children, May 2022
Morgan Briggs taught children about AI for Future Forums workshops (in partnership with the British Science Association), February 2022
Participated in Scottish AI Summit in Edinburgh. Dr Mhairi Aitken presented in panel discussion entitled ‘Why is it important to engage children in AI and how do we do it right?, March 2022
Publication of case study resulting from months of research in partnership with UNICEF, November 2021
Blog Post on Scotland AI Strategy’s website entitled ‘Is the public sector ready for child-centred AI?’, November 2021
Dr Mhairi Aitken delivered a seminar on “AI Ethics and Engagement with Children and Young People” as part of the Raspberry Pi Foundation, Computing Education Seminar Series, September 2021
"
https://www.turing.ac.uk/people/researchers/siddharth-narayanaswamy,"
Siddharth N. (Sid) is a Reader in Explainable AI in the School of Informatics at the University of Edinburgh, a part-time Senior Research Fellow at The Alan Turing Institute, and a Visiting Fellow at the Department of Engineering Science at the University of Oxford.
He was previously a Senior Researcher in Engineering at the University of Oxford and a Postdoctoral Scholar in Psychology at Stanford. He obtained his PhD from Purdue University in Electrical and Computer Engineering.
Sid's interests are broadly cross-disciplinary, with his research being motivated by problems found at the intersection of machine learning, computer vision, natural-language processing, cognitive science, robotics, and neuroscience.
His current focus is on unsupervised learning of structured representations from perceptual data, and establishing common ground between machines and humans through interaction, with implications for building AI and ML systems that are interpretable and explainable by design.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/vwv,"
www.vwv.co.uk
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-bath,"
University of Bath website
"
https://www.turing.ac.uk/news/publications/efficient-programmable-random-variate-generation-accelerator-sensor-noise,"
We introduce a method for non-uniform random number generation based on sampling a physical process in a controlled environment. We demonstrate one proof-of-concept implementation of the method, that doubles the speed of Monte Carlo integration of a univariate Gaussian. We show that we must measure and compensate for the supply voltage and temperature of the physical process to prevent the mean and standard deviation from drifting. The method we present and our detailed empirical hardware measurements demonstrate the feasibility of programmable non-uniform random variate generation from low-power sensors and the effect of ADC quantization on the statistical qualities of the approach.
J. T. Meech and P. Stanley-Marbell, ""Efficient Programmable Random Variate Generation Accelerator from Sensor Noise,"" in IEEE Embedded Systems Letters, doi: 10.1109/LES.2020.3007005
353.7 KB
"
https://www.turing.ac.uk/news/publications/near-real-time-social-distancing-london,"
During the COVID-19 pandemic, policy makers at the Greater London Authority, the regional governance body of London, UK, are reliant upon prompt and accurate data sources. Large well-defined heterogeneous compositions of activity throughout the city are sometimes difficult to acquire, yet are a necessity in order to learn 'busyness' and consequently make safe policy decisions. One component of our project within this space is to utilise existing infrastructure to estimate social distancing adherence by the general public. Our method enables near immediate sampling and contextualisation of activity and physical distancing on the streets of London via live traffic camera feeds. We introduce a framework for inspecting and improving upon existing methods, whilst also describing its active deployment on over 900 real-time feeds.
Near Real-Time Social Distancing in London, James Walsh, Oluwafunmilola Kesa, Andrew Wang, Mihai Ilas, Patrick O'Hara, Neil Dhir, Theodoros Damoulas, arXiv:2012.07751.
"
https://www.turing.ac.uk/news/publications/convergence-rates-class-estimators-based-steins-method,"
Gradient information on the sampling distribution can be used to reduce the variance of Monte Carlo estimators via Stein's method. An important application is that of estimating an expectation of a test function along the sample path of a Markov chain, where gradient information enables convergence rate improvement at the cost of a linear system which must be solved.
The contribution of this paper is to establish theoretical bounds on convergence rates for a class of estimators based on Stein's method. Our analysis accounts for (i) the degree of smoothness of the sampling distribution and test function, (ii) the dimension of the state space, and (iii) the case of non-independent samples arising from a Markov chain.
These results provide insight into the rapid convergence of gradient-based estimators observed for low-dimensional problems, as well as clarifying a curse-of-dimension that appears inherent to such methods.
Oates CJ, Cockayne J, Briol F-X, Girolami M. Convergence Rates for a Class of Estimators Based on Stein's Method. Bernoulli, 2018.
"
https://www.turing.ac.uk/people/researchers/andrea-pizzoferrato,"
Andrea is working on a seed funded project on Predictive Graph Analytics and Propagation of Information networks. Andrea has previously been at the University of Warwick where he was completing his mathematics PhD and was also a Teaching Assistant.
"
https://www.turing.ac.uk/events/ai-and-sustainable-cities,"
This is a scoping workshop activity centred on the theme of ‘Cities’ that examines the role of data science and artificial intelligence in paving the way for sustainable cities. Aspects of sustainability in cities include mobility, energy, and climate with central pillars of equality, security, and data privacy. Two-thirds of humanity are expected to live in cities in 2050. These are cultural and business centres but are also major consumers of natural resources and producers of emissions. To make cities more resource-efficient and environment friendly, these different aspects of sustainability need to be considered jointly instead of the currently silo-ed approach. The workshop is structured under four themes:
Each theme will feature a keynote speaker, two talks and 2 lightning talks; two themes will be covered each day of the workshop. Speakers will be invited from academia, industry, and governmental authorities to present diverse efforts that aim to instigate sustainability in UK cities. With a unique opportunity to convene diverse stakeholder visionaries of future cities from around the UK and across disciplines, the second day of the workshop will foster a panel discussion that is founded on central principles of sustainability
Register now
"
https://www.turing.ac.uk/people/business-team/jules-manser,"
Jules has been an integral part of the Data Study Group since its inception in 2016.
The variety of challenges and organisations coming through the Data Study Group make this an incredible opportunity. It is a very flexible entry into data science research with academia.
If you want to get in touch about the Data Study Group programme, please email the team on [email protected]
Externally, Jules has an interest in music production and runs a recording studio.
"
https://www.turing.ac.uk/people/guest-speakers/gao-feng,"
Dr Feng Gao is the co-founder and managing director of Open Data China, the very first civic group and social enterprise in China working on data transparency and digital rights. Under Feng's leadership, Open Data China actively advises governments and other data holders on how to build an open and transparent data agenda and set up a community for data users to make their voices heard. In 2015, Open Data China co-launched Shanghai Open Data Apps (SODA), a competition-based open data innovation model. Since then Feng leads the daily operation of SODA and its sister programs in Chongqing, Shenyang, and other cities. Overall, the SODA model helps unlock more than 60 datasets from 30 government agencies and private companies, generate nearly 1000 prototypes and applications, cultivate several startups with over 1000 million dollars in investments. Feng's publications and talks on open data are featured in Paris Innovation Review, Financial Times China, and International Open Data Conference. He was Yale World Fellow (2019) and a recipient of Study UK Alumni Award (China) on Professional Achievements. He holds a PhD in Computer Science from the University of Southampton, and a BE in Computer Science from Beijing University of Posts and Telecommunications.
"
https://www.turing.ac.uk/events/turing-lectures-accelerated-discovery,"
Introducing the first Turing Lecture of 2022, Dr Alessandro Curioni joins The Alan Turing Institute's landmark event series to discuss enabling artificial intelligence (AI) to accelerate the path to discovery. 
Esteemed Dr Curioni is Vice President of IBM Research (Europe and Africa), regarded as an internationally recognised leader in high-performance computing and computational science. He leads IBM's global research strategy in Accelerated Discovery and Security. 
In this Turing Lecture, Dr Curioni will explore the new AI- and quantum-powered age of computing, describing its potential to accelerate discoveries up to 10 times faster and with 10 times less cost than in the past. This will lead to breakthroughs across business, policy-making, and everyday life. 
 
The Turing Lecture will detail how the world is witnessing a revolution in computing. Exploring how traditional methods can be enhanced by AI to solve problems by boosting emerging quantum technologies.
Today, you can see an increase in companies and labs using AI to accelerate discovery. For example, if a scientist is working on a new chemical to create safe, sustainable batteries for storing renewable energy, deep neural networks could sift through a multitude of patents, academic papers, and other publications to see what has already been done on the topic.
Next, AI would automatically generate hypotheses based on the data it found, to expand the search for new molecular designs. Based on the hypothesis, high-performance computers and quantum computers would simulate a new molecule. This simulated molecule would then be confirmed or refuted during increasingly autonomous lab tests. Finally, AI would assess the result, identify anomalies and extract new knowledge. New questions would surface and the loop would continue.
Therefore to shift the paradigm of scientific discovery, we need to enable AI, hybrid cloud computing, and – eventually – quantum computing to converge.

 
"
https://www.turing.ac.uk/people/researchers/kirstie-whitaker,"
Kirstie discovered the wonder of brain imaging at the University of British Columbia during a masters degree in Medical Physics. She completed a PhD in Neuroscience at the University of California, Berkeley in 2012 and joined the Turing Institute as a Turing Research Fellow in 2017 from a postdoctoral fellowship at the University of Cambridge in the Department of Psychiatry.
In 2020, she was promoted to Programme Lead for Tools, Practices and Systems, and in 2021 to Programme Director, reflecting the growth of this cross cutting programme. Kirstie is committed to realising the TPS community's mission of investing in the people who sustain the open infrastructure ecosystem for data science.
Kirstie is the lead developer of The Turing Way, an openly developed educational resource inspire, train and enable researchers and citizen scientists across government, industry, academia and third sector organisations to apply open source practices to their work. She is also the chair of the Turing Institute's Ethics Advisory Group.
Kirstie is a passionate advocate for making science ""open for all"" by promoting equity and inclusion for people from diverse backgrounds, and by changing the academic incentive structure to reward collaborative working. She is a Fulbright scholarship alumna and was a 2016/17 Mozilla Fellow for Science.
Kirstie was named, with her collaborator Petra Vertes, as a 2016 Global Thinker by Foreign Policy magazine.
You can follow her and her dog's adventures on Twitter @kirstie_j.
Adolescence is a period of human brain growth and high incidence of mental health disorders. Kirstie's research uses magnetic resonance images to understand changes in the brain's structure and function that underlie the emergence of schizophrenia and depression. Her work considers the brain as a network and investigates how different brain regions work together. She is particularly passionate about ensuring that work is reproducible and can be replicated in independent data sets. Her focus at the Turing Institute is to improve the generalisability of research findings so they may be translated to the clinic and used by public health policy makers.
"
https://www.turing.ac.uk/taxonomy/term/1043,"
"
https://www.turing.ac.uk/news/partnership-between-mi5-and-alan-turing-institute-revealed,"
MI5 and The Alan Turing Institute have today made public their ongoing partnership, which brings together expertise from both organisations to apply artificial intelligence (AI) research to confront national security challenges. 
The partnership forms part of The Alan Turing Institute defence and security programme which was launched in 2017 to ensure a safe, secure, and prosperous society, including the application of AI ethics to defence and security projects.
MI5 has chosen to unveil this partnership today to enable closer working with The Alan Turing Institute and their academic stakeholders. 
MI5 often draws on new technologies to help identify and mitigate threats to UK national security and keep the country safe, including the use of data science and AI. 
As the national institute for data science and AI, The Alan Turing Institute specialises in the development of high performing, ethical AI and offers MI5 the opportunity to make the most of their research and expertise in developing its own AI tools. It also means MI5 will be able to contribute more openly to the development of AI in the UK for the wider public benefit.
MI5 Director-General, Ken McCallum said: “It is great to announce MI5’s partnership with The Alan Turing Institute today. 
“The UK faces a broader and more complex range of threats, with the clues hidden in ever-more fragmented data. MI5 has a long and proud history of innovation and use of cutting-edge technology in an ethical way; artificial intelligence is another example of that and a vital capability in MI5’s toolkit. 
“By working with The Alan Turing Institute, MI5 can draw on world-leading expertise to ensure our use of AI keeps pace with the challenges we face, and enables us to keep the country safe.”
Sir Adrian Smith, Institute Director at The Alan Turing Institute, said: “AI offers new ways to analyse increasing volumes of fragmented and complex data, providing new insights about that data and improving analytical processes. 
“And so we’re delighted to announce this important partnership today with MI5 which will help our organisations work together more closely and ensure AI and data science continues to play an important part in national defence and security.”
Part of the collaboration supports MI5 in keeping abreast of state-of-the-art approaches in AI, that can be used to respond more flexibly and more quickly to threats to UK national security. 
MI5 is already using AI to keep the country safe. For example, it uses AI to help identify dangerous weapons. AI technology is used to scan and triage images which could contain weapons before being passed to experts to assess the need for further action.
Top image credit: Alina Constantin / Better Images of AI / Handmade A.I / CC-BY 4.0


"
https://www.turing.ac.uk/events/provenance-security-machine-learning,"
120.9 KB
Recent proposals for securing systems against sophisticated attackers, such as Advanced Persistent Threats (APTs), include wholesale monitoring of system activity down to the level of individual system calls, sometimes in a causal, graph-based representation called provenance.  While this level of detail should ensure malicious activity is recorded, and that the sources and effects of such activity can be understood after-the-fact, it is challenging to locate such activity against a background of high-volume, high-velocity activity, with high variability in the structure and meaning of data obtained from different sources.  Automatic, reliable detection of realistic APT behavior in provenance traces (with an acceptable false-positive rate) appears to be an open problem.
Until recently, progress in this area has been hindered by the absence of publicly-available datasets.  In the US, a recently-concluded DARPA research programme on Transparent Computing has produced publicly-available datasets including realistic APT behavior in provenance traces recorded on a variety of mainstream operating systems.  However, these datasets are not easy for the broader research community to reuse, due to large scale (each day of activity can require more than a gigabyte), and hetereogeneity.  Moreover, attacks are highly imbalanced (often consisting of under 0.01% of the data) and ground truth information that could be used for training is usually not available, rendering supervised machine learning techniques ineffective.  Relevant techniques for semi-supervised or unsupervised machine learning or outlier/anomaly detection may require adaptation to deal with the large scale of the data or its complex structure. 
This workshop aims to bring together researchers with expertise in security, data management, and machine learning, each of which bear on this challenge.  The workshop will also involve participants in the DARPA Transparent Computing programme who can share experience and understanding of the problem and the available datasets.  Breakout sessions will be organised to enable participants to contribute to a research vision and agenda for future work in this area, which will form the basis of a workshop report.
This event is suitable for individuals who are interested in the following areas of data science and AI:
The topics of this workshop cut across several challenge themes at the Turing, particularly Defence & Security.  Participants from a range of backgrounds are welcome to apply to attend; if space is limited, participants will be selected so as to ensure diversity of backgrounds and perspectives.
Agenda for ""Provenance, security & machine learning""
The programme will include breakout sessions, tentatively on the following four topics, which may be adjusted on the day based on audience interest:
Apply to attend now close

"
https://www.turing.ac.uk/AlanTuring,"
“We can only see a short distance ahead, but we can see plenty there that needs to be done.”
This is what Alan Turing, the pioneering mathematician, computer scientist, philosopher, code-breaker and icon of the 20th Century, said of the challenge of developing ‘machines that can think’ in his seminal paper ‘Computing Machinery and Intelligence’ in 1950. Seven decades later, and computing, machine learning and artificial intelligence (AI) have not only blossomed, but exploded.
Today, the horizon has transformed, but Turing’s words still hold true. This Institute, named in his honour, is committed to tackling the plenty that needs to be done to ensure data science and AI changes the world for the better.
To find out more, read the article below and watch the video where we asked the Institute's community what Alan Turing means to them:

Throughout his life, Alan Turing’s fearless approach to daunting problems helped him break new conceptual ground. From his time at Cambridge, when he published papers now recognised as the foundation of computer science, through his vital work at Bletchley Park cracking German codes – shortening the Second World War by years – to his exploration of the notion of artificial intelligence and his fascination with the application of mathematics to the biological world.
At the Institute we aim to adopt a similarly ground-breaking, multi-faceted approach to our research.
Despite being a singular genius, Turing was also a great collaborator, both with the hundreds of women and men at Bletchley Park, and throughout his career working with other mathematicians, engineers and scientists.
The biography Alan Turing: The Enigma by Andrew Hodges includes the following quote from Turing, which sums up the spirit in which the Institute operates: “The isolated man does not develop any intellectual power. It is necessary for him to be immersed in an environment of other[s]…The search for new techniques must be regarded as carried out by the human community as a whole, rather than by individuals.”
""The search for new techniques must be regarded as carried out by the human community as a whole""
Alan Turing
Turing’s life was tragically affected by the societal norms of his time: despite his pivotal part in ensuring the safety of the nation and saving countless lives, his sexuality resulted in him being defined as a security risk, and he was harassed by police surveillance up until his untimely death in 1954.
Though we now live in a more progressive and open society, at the Institute we recognise the importance of actively ensuring anyone in ‘the human community’ can contribute effectively to changing the world through data science. We do this through our commitment to equality, diversity and inclusion, demonstrated by events such as ‘Gamechangers for diversity in STEM’.
On Turing’s influence on the modern world of data science Vinton Cerf, Chief Internet Evangelist for Google, says: “His practical realisations of computing engines shed bright light on the feasibility of purposeful computing and lit the way towards the computing rich environment we find in the 21st Century.” Our programme in Data Science at Scale continues this legacy, identifying the ways in which computers and algorithms can be better designed to fulfil a huge range of purposes and tasks. And our Research Engineering team, which likes to think of itself as an echo of the Bletchley Park ‘Hut 8’ group led by Turing, helps the Institute develop practical data science tools.
The mathematical foundations strand of our Data-centric Engineering programme also recognises that delivering reliable and robust data science solutions requires rigorous theoretical research and practices. It’s a notion which aligns well with the ‘from first principles’ approach Turing often adopted in his work.
Turing’s revolutionary ideas in cryptography were developed in service of public safety and security, and the Institute’s programme in Defence and Security is continuing this purpose. For example, we have multiple projects looking at ways to store sensitive data, such as health records, in the cloud, in a way that not only allows the data to remain encrypted, but also makes them accessible to publicly beneficial research, without compromising anyone’s privacy.
In a talk broadcast by the BBC in 1951, Turing said: “I believe that the attempt to make a thinking machine will help us greatly in finding out how we think ourselves.” Today, ‘thinking machines’ are ubiquitous, from speech-recognition software to algorithms used in criminal justice systems. Serious consideration needs to therefore be given to how human mindsets – our social norms and subconscious biases – affect the development and implementation of these systems.
Our programmes in Artificial Intelligence and Public Policy, and our Data Ethics Group, all explore how we can ensure that algorithmic systems are built to contribute reliably and fairly to the public good, and how they affect the way we interact with our world.
In his later career, Turing became fascinated with the challenge of understanding biological pattern formation in nature. He proposed an explanation in terms of chemical interactions and developed equations for them. Decades later, scientists confirmed his theory. Ian Stewart, Emeritus Professor of Mathematics at the University of Warwick explained in a BBC documentary: “In an area where mathematics had never been used before, suddenly the door was opened.”
This value of applying mathematics and computer science to other fields can be seen throughout the Institute’s research, particularly in the Data Science for Science programme and projects such as ‘Living with machines’ in collaboration with the British Library, which is bringing together data scientists with historians, computational linguists, and archivists to think critically about the interdependence of humans and machines in modern society.
Alan Turing’s legacy and influence is felt the world over, and we make no claim that our Institute is sole heir to his mantle. For example, the Turing Trust, a charity run by Alan Turing's family, does fantastic work supporting education by reusing old ICT equipment that anyone in the UK can donate, providing the opportunity for students around the world to use digital technology for the first time.
What we are is a proud torch-bearer for the man often dubbed the ‘father of computer science’ and we believe that our mission to make great leaps in data science and artificial intelligence research, for the betterment of all, is rightfully carried out in his name. And we, like Turing, can see plenty there that needs to be done.
Header image of Alan Turing courtesy of the National Portrait Gallery.
"
https://www.turing.ac.uk/events/turing-lectures-science-movement,"
Humans spend a lifetime learning, storing and refining a repertoire of motor memories appropriate for the multitude of tasks we perform, whether that's playing the piano, learning sign language or peeling a banana. However, it is unknown what principle underlies the way our continuous stream of sensorimotor experience is segmented into separate memories and how we adapt and use this growing repertoire.
In this lecture, Professor Daniel Wolpert will review his team’s work on how humans learn to make skilled movements, focusing on the role of context in activating motor memories. He will then present a principled theory of motor learning based on the key insight that memory creation, updating and expression are all controlled by a single computation–contextual inference.
Unlike dominant theories of single-context learning, Wolpert’s team’s repertoire-learning model accounts for key features of motor learning that had no unified explanation, and predicts novel phenomena, which they confirm experimentally. These results suggest that contextual inference is the key principle underlying how a diverse set of experiences is reflected in motor behaviour.

"
https://www.turing.ac.uk/people/researchers/jack-roberts,"
Jack is a Senior Data Scientist in the Applied Research Centre for Defence and Security (ARC), transforming the latest developments in deep learning and data science into actionable reports and reproducible proof-of-concepts for real-world challenges.
He originally joined the institute in March 2019 as a member of the Research Engineering Group. In that role he worked on projects in the intersection between software engineering and data science, mostly within the healthcare and urban analytics programmes.
Other activities Jack has been involved in include, developing and teaching the Turing's Software Engineering and Data Science courses, participating and organising Data Study Group challenges and AIrsenal (an algorithm to play fantasy football).
Prior to the Turing, Jack worked at CERN in Geneva, Switzerland. He also completed a DPhil and postdoc in Particle Physics with the University of Oxford.
"
https://www.turing.ac.uk/people/external-researchers/alessandro-di-nuovo,"

"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/public-health-scotland,"
www.scot.nhs.uk
www.isdscotland.org
"
https://www.turing.ac.uk/news/publications/core-periphery-structure-directed-networks-0,"
Elliott, Andrew, Angus Chiu, Marya Bazzi, Gesine Reinert, and Mihai Cucuringu. ""Core-Periphery Structure in Directed Networks."" arXiv preprint arXiv:1912.00984 (2019). Accepted in Proceedings of the Royal Society, Series A 
2.3 MB
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/can-we-automate-uks-planning-system-using-ai,"

Can we automate the UK’s system for handling planning applications? In a Data Study Group challenge put forward by Agile Datum we trained machine learning models on UK planning application documents to classify and detect floorplans in applications, with the aim of speeding up the process of validating submitted planning applications using deep learning.
The workshop, which was part of the Turing’s December 2019 Data Study Group, was proposed by Agile Datum, a UK based company specialising in using data science in agile applications for public and private sector organisations. Data Study Groups are collaborative hackathons, organised by The Alan Turing Institute, bringing together organisations from industry, government, and the third sector, with talented multi-disciplinary researchers from academia. This case study contains extracts from the full report published on the Turing’s website.
Over 3.5 million planning applications are submitted in the United Kingdom each year. That is not surprising considering that every construction project in the UK, including building or extending a house, fitting new windows on a listed building, or chopping down a tree, requires the submission of complicated forms and technical drawings. Once submitted, each of these documents needs to be manually validated and approved by a planning officer.  
As a result, each application requires 30-60 minutes of council time to validate whether the information in the forms and drawings is correct. This manual validation represents around 250,000 person hours of administration per year.  This creates large backlogs and a lack of information for applicants, leading to additional calls and emails to chase progress, which further increases workload on planning officers.
Over one third (1.2 million) of the submitted applications are rejected, often because the planning system in the UK remains complex and fragmented and thus, inefficient. For example, as of yet there is no standardised format for applications. Each council has its own application forms which have different fields of entry and there are no standard templates for submission of the drawings. The plans and forms are all submitted in a PDF format containing various layouts, formats and content.
Despite the wide diversity of applications, independent research led by Agile Datum shows that over 80% of rejected applications occur due to 12 common mistakes. These errors can be categorised as issues in the application forms or in the planning drawings of the existing and proposed work. These include incomplete sections of forms, incorrect application fee, the description of work does not match the drawings, missing or incorrect floorplans, missing or incorrect elevations (façade plans), incorrect or missing scale in drawings, north arrow missing in drawings as well as missing or incorrect site plans.
If AI methods could detect these common errors before the application gets to the stage of being reviewed by a planning officer, then the planning process could become much more efficient and streamlined.
In this Data Study Group, we used artificial intelligence (AI) and machine learning (ML), to detect and classify drawings and application forms, in order to automatically detect common mistakes. This automation could greatly reduce the amount of time it takes a planning officer to validate and approve a planning application, thus leading to a faster, more streamlined planning process. The challenge goes beyond identification of errors, by detecting individual components within an application, and offers a method for creating a database of classified drawings and a suggestion for a collective standardised planning application form.
The report developed from the Data Study Group at the Turing outlines the methodology used to perform this.  
All planning application documents are publicly available and are generally separated in two categories: the text forms and the drawing plans. 
To perform text analysis, we created a dataset with PDF file metadata, a dataset of (computer selectable) text directly extracted from the PDFs and a dataset of text generated from the image dataset using optical character recognition (a computer vision technique for identifying characters in images).  
For the drawings, we created a dataset of PDF pages converted to images. By doing this, we were able to create a searchable database, of all planning forms, including handwritten ones, in order to automatically detect errors in text forms. 
Planning drawings in the UK are not standardised and can vary from having one drawing per page to multiple drawings per page, being computer generated or hand drawn, and using varying scales and colours. Thus, the classification of individual components of the documents becomes a challenge. Drawings are also usually flattened in PDFs, meaning that each page can contain one or many images of floorplans and elevations.  
Regardless of format, each planning application should contain the following five components: floorplan, siteplan, elevation plan, section, north arrow and a textbox. Being able to detect these components is a first step towards automatically verifying whether an application contains the common errors mentioned earlier, such as a missing north symbol, that would result in an immediate rejection of the application. The challenge was to investigate whether a computer vision model could in fact allow us to automatically detect these different objects within the applications. 
This first required us to create a training dataset. We manually produced the training dataset, by labelling a sample of different drawings in real-life applications. We created labels for each of the five key categories mentioned above: a. floorplan, b. siteplan, c. elevation plan, d. section, e. north arrow, f. textbox.
Detecting floorplans
After creating the training dataset, we had to use object detection, which is a technique in image processing that deals with detecting semantic objects in digital images or videos (such as kangaroos, cats, humans or cars).  While Image Localization will specify the location of a single object in an image, Object Detection specifies the location of multiple objects. Finally, Image Segmentation will create a pixel wise mask of each object in the images.

To perform object detection, we used the deep learning framework Keras and the Mask-RCNN library (mrcnn) to fine tune a Mask-RCNN pretrained model on our training data to detect the components within the planning drawings. R-CNN (Region-based Convolutional Network), initialises small regions in an image and merges them with a hierarchical grouping.  The Mask Region-based Convolutional Network (Mask R-CNN), detects different objects in an image or a video. You give it an image, and it outputs the object bounding boxes, classes, and masks. The model was pretrained on the MS COCO dataset (a large-scale dataset for object detection). 
This tutorial by Jason Brownlee is a great resource for providing further details relating to the setup of such a pipeline. 
Even though we were only able to label only 30 images given the timeframe, the results were promising. The model was able to detect drawings and separate them from text blocks, or north arrows. The bounding boxes were also quite accurate in most cases, engulfing each of the drawings separately. 
Figures 3, 4 and 5 provide example predictions made by the model on two test images and on two training images respectively. The left image in each pair is the ground truth for that example (i.e. the bounding box labelled by the human) and the image on the right is the model’s predicted bounding boxes. Each class is represented by a different colour. 
The first and the second image shows an example of a successful prediction. The model accurately predicted 4/5 drawings, with relatively accurate bounding boxes in the first case, while in the second the model accurately detected the site plan and successfully categorised the north arrow symbol. The labels and drawing title are also successfully categorised. The second figure is an example of a false prediction. The model recognised half of the drawing and part of it was misclassified as drawing title. 
The results show that the object detection process can provide a successful method for the digitisation of planning applications, though the model may benefit from a larger training dataset as well as hyperparameter tuning. The detection of discrete elements such as symbols or individual drawings, can speed up the evaluation process by automatically eliminating planning applications with common mistakes, such as the missing north symbol, or a missing floorplan. 
The current developed model also does not yet recognise whether a plan is an existing floorplan or a proposed one, and neither can it separate between floorplans of different floors (i.e. if the proposed building has more than one floor). However, when planning officers look at a drawing, they can read the labels assigned to each plan. In order for the model to distinguish between the proposed and existing plans, we could give the model another feature of the “title text” found near the plan (usually labelled as “proposed” or “existing”) so that it could learn to associate them. If we then perform OCR on these title texts, we get a semantic text of the label and thus, each allocated drawing, providing a dataset of classified drawings. 
Conclusions
This work was a proof of concept, a first step towards identifying components in these planning applications that many applicants trip over.  There can be further future uses of ML models in this context, such as models being able to automatically identify and highlight the changes between existing and proposed plans. In this report, we presented a prototype based on the Mask R-CNN framework to perform image detection and segmentation. There are lots of techniques for semantic segmentation of images these days, in particular using clustering features of images given by pretrained model (which don’t necessarily require further training of a model or a labelled dataset). 
However, this work has been a demonstration of assisted AI to support and improve the UK’s planning application services. 
The planning support system in the UK, until this day, remains decentralised. Each local authority receives planning applications separately, and the review is performed manually. AI can aid in this context by speeding up the evaluation process, not by taking over the existing system, but rather by quickly detecting common mistakes, digitising and categorising drawings, and creating a searchable database of plans and text. Further to this though, this work can lead to creating a digital 3D database of the UK’s building stock. The challenge would be to take the classified drawings, convert them to an editable semantic form, and reconstruct a 3D building from the 2D drawings. This can provide a platform for an urban digital twin, a virtual replica of a city of past and new buildings, with the possibility of offering insights into performance and potential problems in the context of the smart city.
Our thanks to Agile Datum and the research team for their work on this case study.
 
"
https://www.turing.ac.uk/events/nature-reviews-physics-machine-learning-theoretical-and-experimental-high-energy-physics,"
Photo by Michael Dziedzic on Unsplash
Machine learning is no longer restricted to data analysis. It is currently used in theory, experiment and simulation. This is a sign that AI is becoming pervasive in all traditional aspects of research. However, are theorists, experimentalists and computational scientists aware of each other’s problems and the solutions developed to tackle them? Are researchers working in different areas of physics aware of developments in other areas? Machine learning has been used in experimental high energy physics since the 1990s, later enabling the data analysis that made possible the discovery of the Higgs boson. Today machine learning is not only an integral part of the data acquisition and analysis workflows in high energy physics experiments, but it also provides new tools for theorists. Therefore, machine learning is expected to make a large contribution to the ongoing search for new physics.
Chair - Professor Adrian Bevan

In this event, we will hear from Associate Professor James Halverson and Dr Jennifer Ngadiuba.
Associate Professor James Halverson - Machine Learning for Theoretical Physics
Machine learning has now also been applied to theoretical high energy physics, for instance in the construction of particle physics theories or in string theory. Unexpected deep connections between neural network theory and quantum field theory and the framework underlying particle physics, have all been uncovered and are currently being explored.
Dr Jennifer Ngadiuba - Machine Learning for High Energy Physics
Particle physics experiments are constantly increasing in both sophistication of detector technology and intensity of particle beams. The resulting big-data challenge faced by next generation of HEP experimental setups will require
machine learning to be deployed at every stage of the data processing up to the real-time subsytems characterized by stringent system constraints in terms of computational speed, resource usage, and energy consumption. Modern ML algorithms provide promising solutions thanks to their capacity of extracting the most useful physics information from highly complex data in a scalable way when full parallelisation is exploited on suitable hardware. This talk will cover a few examples of applications of ML at the Large Hadron Collider and at neutrino experiments.


"
https://www.turing.ac.uk/people/researchers/malvika-sharan,"
Malvika is a senior researcher at the tools, practices and systems research programme. She is leading on Open Research at The Alan Turing Institute and establishing a growing team of community managers. She is a co-lead investigator of The Turing Way project – a community-led handbook on data science, that aim to make research reproducible, collaborative, ethical and inclusive for researchers around the globe.
Malvika advocates for inclusive and collaborative community spaces to enable purposeful human-centred adoption of open research practices, tools and culture. Learn more about her in the Turing Spotlights post and her homepage.
"
https://www.turing.ac.uk/research/interest-groups/social-data-science,"
The group seeks to address the challenges associated with the vast quantities of new and heterogeneous data that are increasingly becoming available through two primary themes: developing foundational theories of human behaviour at diverse social and temporal scales; and identifying methodological challenges and solutions to enable social data science to deliver robust and credible results in key application domains.
This interest group in social data science aims to create a critical mass of social scientists, data scientists and social data scientists, and to build relationships with data infrastructure and training investments and policy makers through regular meetings involving academic, commercial, NGO, and government stakeholders.
Click here to request sign-up and join
[email protected]
Robin Williams, University of EdinburghElla McPherson, University of CambridgeMelinda Mills, University of OxfordAndy Hudson-Smith, UCLNoortje Marres, University of WarwickCelia Lury, University of Warwick
"
https://www.turing.ac.uk/people/business-team/jessie-pearce,"
"
https://www.turing.ac.uk/people/researchers/valery-otieno,"
Valery Atieno Otieno is a research assistant on the Women in Data Science and AI project, within the public policy programme. She completed her MSc in Social Science of the Internet from the University of Oxford and was a Weidenfeld Hoffmann scholar. During her studies, she researched Black culture on Instagram, exploring how image culture on the platform provides a medium for Black Aliveness, an alternative and necessary form of digital activism. She also holds a BA (cum laude) in History with a minor in Africana Studies from Pomona College, California.
Valery is interested in the use of transdisciplinary methods to investigate the implications of technology on minoritized communities, and the counter-hegemonic narratives of resistance manifested within everyday life. At the Turing she supports research on the gendered practices mediating the data science and AI fields and the impact of marginalisation in building a more gender-inclusive technoculture.
"
https://www.turing.ac.uk/people/guest-speakers/francisco-villaescusa-navarro,"
Francisco Villaescusa-Navarro is a Research Scientist at the Flatiron Institute in New York City and a Visiting Research Scholar at Princeton University. Francisco is the main architect of the Quijote simulations, the largest set of N-body simulations generated to-date and one of the main developers of the Cosmology and Astrophysics with Machine Learning Simulations (CAMELS) project.
Francisco works in the intersection between cosmology and machine learning. He uses artificial intelligence techniques to develop accurate theoretical models to maximise the scientific return of cosmological missions, aimed at determining the laws and constituents of the universe.
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-university-sheffield-advanced-manufacturing,"
With the advent of the new industrial era, modern manufacturing equipment is expected to be more flexible, sustainable and operative with minimum human interference which requires the supporting process monitoring system to be smarter and more intelligent, e.g., carrying out automated machine health checks using embedded sensors. This challenge focused on the investigation of using modern data science and AI techniques to analyse the multiple sensor measurements to monitor the status of the machining process.
Process monitoring allows the integrity of a machining operation to be gauged through the sensor measurement. This methodology can help to identify issues with the component, cutting tool, or machine tool before the component undergoes final inspection. The benefits of identifying issues in-process, rather than a final inspection, include limiting further damage to the component or machine tool and preventing additional components from being machined before identification of the issue.
Installing such a system can therefore provide significant savings to a manufacturer in terms of scrap, machine tool maintenance, and downtime. Techniques for process monitoring of machining operations through sensor signals are well-established in the literature. Many solutions utilising such techniques have now been commercialised and are available to the industry. However, most of the commercial systems are often based on static limits or signal trending.
To gain further insight into the machining process, the nature of the signal beyond simple level/amplitude is expected to be examined through time-series analysis techniques, such as fast Fourier transform, and spectral entropy calculations. The use of sensor fusion, i.e., evaluating combinations of sensor signals rather than in isolation, has also been demonstrated to be able to achieve more robust monitoring and fault identification. Finally, rather than simple trending or setting static limits, machine learning techniques could be employed in determining the optimal indicators and setting tolerances.
Data Study Group team. (2022, September 13). Data Study Group Final Report: The University of Sheffield Advanced Manufacturing Research Centre. Zenodo. https://doi.org/10.5281/zenodo.7075713
8.8 MB
"
https://www.turing.ac.uk/blog/hacking-23-years-government-history-example-uk-government-web-archive,"
Web archives provide a key resource for the public. They allow us to access a wide range of data reflecting all areas of a society but, as they are large and meticulously maintained datasets, they can be daunting and difficult to navigate. 
In December 2019, The Alan Turing Institute and The National Archives co-organised a Data Study Group challenge. Data Study Groups (DSGs) are events hosted by the Turing, which bring together some of the top talent from data science, artificial intelligence, and wider fields from across the world, to analyse real-world data science challenges.
The culmination of that work is now available to read via the published Data Study Group report ‘Discovering topics and trends in the UK government web archive’ here.

This DSG brought together a group of international, multi-disciplinary researchers in a hackathon format, to explore new ways of accessing the UK Government Web Archive (UKGWA). The UKGWA is a collection of UK central government information published on the web and managed by The National Archives, (the official archive and publisher for the UK Government and for England and Wales). The web archive includes videos, tweets, images and websites dating from 1996 to present, housing more than five billion resources, with hundreds of thousands of page views per month. The data covers a wide range of topics, from health to international policy.
The DSG provided a chance to apply innovative data science methods to current challenges related to accessing and researching this vast resource. The size, variety of file formats, and complexity of this vast collection makes the discoverability of its content challenging to the user and to the search service. The study group’s aims were to learn what approaches could be used to assist in understanding the UKGWA data, and to learn the most viable approaches for improving data search for users.  
The collaboration between The National Archives and The Alan Turing Institute brought together computational and collections expertise that investigated potential future applications with the UKGWA and opened further avenues for related research, as well as providing practical recommendations for improvements to service delivery.
In brief, the main research challenge included the use of natural language processing, deep learning language models and clustering to enhance search and discovery in this vast archive, either through browsing by subject area, or finding similar material to subsets of documents. One of the aims was to build an algorithm capable of identifying similar documents and inferring the likely topics that they cover (e.g. “climate change” or “healthy living”). 
The research questions and high-level aims of this challenge were to investigate approaches for improving resource discovery within the UKGWA collection, using approaches led by evidence from users. The project team’s aim was, through experimentation, to investigate approaches for improving resource discovery within the UKGWA collection.
The main outcomes of the collaboration appear in a 50-page report. The report is a detailed presentation of (i) the data; (ii) the research problems and methods; and, most importantly, (iii) recommendations for implementation of some of the findings in order to improve services and access to the UKGWA.
David Beavan, the Principal Investigator of this DSG challenge said: “It was such a pleasure to see this collaboration come to fruition over the course of the week. Initiated by Turing Fellow, Barbara McGillivray and expertly facilitated by Fazl Barez, it consisted of an interdisciplinary team, with wide-ranging expertise only the Turing could draw together. I also want to acknowledge the hard work both Turing and The National Archives teams dedicated to preparing for this.”
Engagements such as this show that arts, humanities, and culture are important and growing parts of the Turing’s activities. David Beavan believes “Successful AI and data science is dependent on the humanities, and in particular humanities ways of thinking. They let us work with conflicting influences, ambiguities and perspectives, while focusing on people, heritage and our culture.”
Most importantly, this DSG helped cement a truly deep and lasting collaboration with The National Archives. It gave fresh new directions to challenges, suggesting innovative research and development paths to explore. This has been a springboard for additional project proposals and collaborations, the DSG was just the beginning.

This was highly collaborative and interdisciplinary accomplishment; it would not have been possible without everyone’s dedication. Read the published Data Study Group report ‘Discovering topics and trends in the UK government web archive’ here. 
You can learn more about The Alan Turing Institute’s Data Study Groups here. If your corporate, public, third sector or academic organisation has a data science or AI challenge that needs tackling, Data Study Groups may be of interest to you. We welcome new challenges. Find out more.
"
https://www.turing.ac.uk/events/methodological-challenges-and-approaches-covid-19-epidemiology,"
A virtual event, hosted by The JBC-Turing-RSS Laboratory.
Introduced by by Johanna Hutchinson, Director of Data and Data Science at the Joint Biosecurity Centre and Professor Chris Holmes, Director of the Turing-RSS Laboratory
In this seminar, Professor Lipsitch will discuss the challenges of understanding COVID-19 transmission, clinical course, and prevention. For transmission, he'll discuss the challenges of household and other secondary attack studies as described in a recent review, as well as some ways to address these challenges. For clinical course, he'll describe the design challenges and main findings of work on a large US health insurer's data base. Last, he'll describe the challenges of observational, post-approval vaccine evaluations and the approach used by the team with which he collaborated in Israel to assess the effectiveness of BNT162b2 mRNA Covid-19 Vaccine in a Nationwide Mass Vaccination Setting.
"
https://www.turing.ac.uk/people/guest-speakers/matthew-rice,"
Matthew Rice is a Senior Policy Officer at the Information Commissioner's Office, the UK's Data Protection Authority. His focus is on supporting controllers adopting innovative and novel processing activities, ensuring that data protection by design is built in at an early stage in solution development. Current interests include geospatial data and technology in the built environment, and Online Safety Tech.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/systems-theory,"
"
https://www.turing.ac.uk/events/digital-identity-focus-evolving-use-cases-risks,"
Exploring behaviours and the systems opportunities for purpose-built trustworthy identity systems. Hosted by the Trustworthy Digital Infrastructure for Identity Systems project, this workshop is designed to advance collective understanding of use cases, cultural considerations and how behaviours are evolving with the implementation of national digital identity systems, and technology generally. Discussions will inform research into understanding the threats and risks that will inform trustworthy systems design opportunities for managing them.
This event is part of the Trustworthy Digital Infrastructure for Identity Systems project. Read more about this project.


"
https://www.turing.ac.uk/about-us/frequently-asked-questions,"

Billions of gigabytes of data are generated globally every day. Data science is the drive to turn this data into useful information, and to understand its powerful impact on science, society, the economy and our way of life.
The study of data science brings together researchers in computer science, mathematics, statistics, machine learning, engineering and the social sciences.
In this film we asked a range of Turing researchers, together with industry and government thinkers about what data science means to them – and why it is important to all of us.

 
There is no accepted definition of artificial intelligence or ‘AI’ but the term is often used to describe when a machine or system performs tasks that would ordinarily require human (or other biological) brainpower to accomplish, such as making sense of spoken language, learning behaviours or solving problems. There are a wide range of such systems, but broadly speaking they consist of computers running algorithms, often drawing on data.
In popular culture artificial intelligence is often viewed as sentient machines, thinking and behaving like a human.
In reality, much AI is computers which are trained to perform tasks independently and which are already present in much of our lives. For example, there has been much publicity about the use of AI in decision-making, for example in the legal system. The AI in this example is driven by machine learning tools, which have taught a computer to make decisions based on the data presented to it.
 
Machine learning is a branch of artificial intelligence that allows computer systems to learn directly from examples, data and experience. The Royal Society's recent report on machine learning provides more background.
 
Many of the foundational tools and methods of data science underpin machine learning and artificial intelligence.
Data science, AI and machine learning require many of the same skills in mathematical modelling, computer science, statistics, mathematics.
They are all long-standing areas of science which have experienced a major resurgence (in the case of AI and machine learning) or emergence (in the case of data science) thanks to increases in computer power, the ability to capture and make sense of data, theoretical break-throughs, and investment.
They are all set to have a major impact on the world we live in, and provoke important ethical and societal questions.
 
Research at the Turing is channelled around a number of ambitious challenges which represent areas in which AI and data science can have a game-changing impact for science, society, and the economy.
Find out more about our challenges, programmes of research, projects, and interest groups and the core capabilities which underpin them.
 
The process to develop the Institute’s strategy was a collaborative one; we engaged with individuals from across the Turing community and our partner organisations through a series of workshops, meetings and co-design sessions, along with relevant stakeholders from the wider research landscape. We sought further feedback of the draft strategy through a consultation phase with our network, refining our thinking prior to full publication in March 2023.
Given the scale of resources that other nations can bring to bear, we took it as a basic organising principle that we needed to focus our efforts on areas where:
Taking initial direction from the priorities of the Office for Science and Technology Strategy (now part of the Department for Science, Innovation and Technology) and applying the tests outlined above led us to select the following areas on which to focus the Institute’s science and innovation efforts:
These areas form the foundation of our developing Grand Challenges and their associated Missions. Supporting these efforts will be our core capabilities. Collectively, our aim is to provide the end-to-end interdisciplinary pathway that will enable ideas to be taken through to impact and for data science and AI to solve problems of national and global significance.
 
See our dedicated page here.
 
Visit our Learn, Explore and Participate (LEAP) page to discover all the different ways you can engage with the Turing.
We run regular Turing Lectures featuring guest speakers discussing the latest trends in data science. We host workshops and seminars for researchers and technical audiences, as well as more general events open to all. Sign up for our newsletter mailing list to be kept informed.
We aim to engage with as many researchers as possible with an interest and expertise in artificial intelligence and data science. This page lists opportunities available to researchers from across the UK (and in some cases the globe), whether you’re early in your career or a senior researcher.
We also regularly advertise for talented researchers, students, data scientists and business operations staff to join us at the Turing. Look out for these on our opportunities page.
"
https://www.turing.ac.uk/research/harnessing-power-digital-twins/turing-research-and-innovation-cluster-digital-twins,"
Together, The Alan Turing Institute and its partners have invested more than £26m in digital twin research and innovation across a range of dynamic projects, including developing foundational theory and applications in the engineering, environmental and social sciences. This represents one of the largest and most exciting portfolios of academic digital twin research and innovation in the UK. 
To build on this activity, we are now establishing a new Turing Research and Innovation Cluster in Digital Twins (TRIC-DT). 

While digital twin technology has proven to be powerful in a range of areas, current digital twins are often highly bespoke, and their development and deployment to address real-world problems requires specialised expertise and computational infrastructure. This has created a barrier that can limit use. As with other powerful computational technologies, realising the potential of digital twins requires a robust, user-friendly implementation open to all.
The primary objective of the TRIC-DT is to democratise access to digital twin technology by providing open and reproducible computational and social tools freely accessible to the UK research and innovation communities. This vision will be achieved by establishing knowledge exchange between a central Turing Innovation and Impact Hub and a network of collaborators across the academic and private sectors.
Research and innovation activity will focus on solving significant societal challenges and generating tangible societal benefits in three interrelated areas:
These are in partnership with a small number of key research and industrial development partners. The purpose of these focus areas is to provide compelling case studies that illustrate the power of digital twins and act as exemplars to help support the evolution of a national digital twins research and innovation community.
As the national institute for data science and artificial intelligence, we are well-positioned to accept the challenge of developing the nation’s digital twins capability. The Turing will take a lead role in democratising digital twin technology, drawing on its broad, multidisciplinary academic expertise, to enable the widespread use for societal and economic benefit.
The three focus areas will deliver a set of outstanding case study projects, enabling digital twins to move from a bespoke technology to a more easily adoptable industrial technology. The TRIC-DT will be organised to support this by ensuring a flow of information and shared expertise between the focus areas. The organisational model includes programme management and partnership development support.
 
Additionally, a vibrant Innovation and Impact Hub (the Hub) based at the Turing will deliver and demonstrate impact from the TRIC-DT research portfolio.
The Innovation and Impact Hub is responsible for supporting the TRIC-DT’s goal of democratising access to digital twin technology by providing open and reproducible computational and social tools freely accessible to the UK research and innovation communities. As such, a primary role and objective of the Hub will be to coordinate and consolidate research activity in the three challenge areas of the TRIC-DT, and ensure flow of tools, methods and techniques between areas, as appropriate.
The Hub will achieve this objective using a variety of complementary activities, organised into four broad (and interconnected) themes:
Each of these themes and their associated activities will be accompanied by a set of appropriate measures to help track progress towards the goals of the TRIC-DT, and the tasks and themes will be regularly evaluated to ensure they are in alignment with objectives of the TRICT-DT.
The Hub will consist of:
A 2-day workshop discussing AI and sustainability centred on the theme of ‘Cities’ that examines the role of data science and artificial intelligence in paving the way for sustainable cities. Dr Cassandra Gould Van Praag, Senior Research Community Manager will be speaking on the TRIC-DT Innovation and Impact Hub. 
This Open for Business event on 7 June was part of an INI programme on the mathematical and statistical foundation of future data-driven engineering and brought together the community to discuss the latest research and innovation in digital twinning for engineering, providing an opportunity for networking and help foster cross-disciplinary connections.
Prof David Wagg and Prof Keith Worden from the TRIC-DT leadership opened and chairing this engaging session.
This Senior Stakeholder Forum builds upon a roundtable conversation last March, where it was recommended that leaders in the environmental digital twin development space re-convene to share progress, align priorities and establish an ongoing Senior Stakeholder Forum for a UK Community of Digital Twins. Chaired by Professor Stephen Belcher, Chief Scientist at the Met Office.
A Pilot IMFe: Piloting an Information Management Framework for Environmental Digital Twins, led event.
As part of the TRIC-DT there has been a programme of international visiting scholars, to help develop partnerships and cross sector research development. On 13 March three of the TRIC-DT visiting scholars joined the Co-Director team for a networking session to support the partnerships ambition.
Delivered with support from the Turing, Imperial College British Heart Foundation Centre of Research Excellence, Wellcome Trust/EPSRC Centre for Medical Engineering..
Bringing together clinicians, industry and academics, providing an opportunities to move towards a common understanding of, and expectations for, a digital twin and identify opportunities to move this technology from a research area into a clinical application.
Attended by over 100 delegates from across industry, academia and government.
A hybrid workshop hosted by Faculty of Engineering, University of Sheffield and AMRC exploring digital twins in practice, supported by Turing.
To find out more, please contact [email protected].
Harnessing the power of digital twins
"
https://www.turing.ac.uk/events/rsf-seminar-series-dexter-semi-automated-epidemiology-platform-electronic-health-record,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility (RSF) project.
Event title: DExtER, a semi-automated epidemiology platform for electronic health record research
Speaker: Professor Krishnarajah Nirantharakumar, Professor in Health Data Science and Public Health, Institute of Applied Health Research, University of Birmingham
The NHS provides healthcare to 66 million people, with over a million people utilising NHS services every day. Each of these encounters is an opportunity to learn however, this data is often poorly accessible and not in research-ready formats. 
 
In this session, we will learn more about University of Birmingham’s system, DExtER (PMID: 32856160), a semi-automated system for efficient, transparent and reproducible research. This brings together experts in epidemiology, data science and software engineering. Studies which used to take months, can now be undertaken within days. For example, DExtER enabled the analysis of >70,000 serum testosterone measurements in young women, identifying increased risk of non-alcoholic fatty liver disease and diabetes in women with androgen excess. This led to a major new experimental medicine study (DAISy-PCOS) stratifying risk and developing novel treatments to prevent metabolic complications in women with polycystic ovary syndrome, a lifelong condition affecting 1 in 10 women worldwide. DExtER will also support the recently awarded MuMPreDiCT consortium that will study the effect of multiple long-term conditions on pregnancy outcomes.
 
In the session, we will also hear about how such an approach has the potential to:
This talk is part of an ongoing series of events hosted by the AIM RSF, which aims to share information and learning on aspects of interest to the MLTC research community. 

"
https://www.turing.ac.uk/blog/non-fungible-tokens-can-we-predict-price-theyll-sell,"
Non-fungible tokens, or NFTs, have been around for several years, but interest in them has rapidly increased in 2021 with a number of high-profile sales, including the $69.3 million Christie’s sale of the NFT associated with Everydays: the First 5000 Days – a digital artwork by the US artist Beeple. Despite concerns about the environmental impact of the underlying technology, NFTs racked up sales of $2.5 billion in the first half of 2021 – an all-time high. But why do some NFTs sell for so much? That question is at the heart of a new Turing-funded study (part of the Turing’s token economy theme) that we recently published in Scientific Reports – the first comprehensive, quantitative overview of the NFT market.
First, the basics. An NFT is a unique unit of data that can be used to certify the ownership of a digital asset. NFTs are created using blockchain technology – usually the Ethereum blockchain – and they can represent any digital object, including photos, artworks, memes, music, digital collectibles and even tweets. There are many NFT marketplaces, where sellers either sell through an auction or a ‘buy now’ option. The novelty of NFTs compared to ‘physical’ art objects is that digital assets can in theory be replicated and/or downloaded an unlimited number of times, and can therefore be ‘owned’ by anyone. The idea behind NFTs is therefore to establish which of the many copies is the ‘original’, and who owns it.
To map the evolution of the NFT market, we looked at a dataset of 4.7 million NFTs exchanged by more than 500,000 buyers and sellers between 23 June 2017 and 27 April 2021. Items included digital art as well as other collectibles such as digital cards and items used in video games (e.g. weapons and clothing). There was a large variability in the prices of the analysed NFTs: only the top 1% of objects traded for more than $1,500, with 75% of objects selling for less than $15.

In order to find out what determines the sale price of an NFT, we developed a machine learning model that considers three factors: 1) visual characteristics, 2) previous sales of related NFTs, and 3) the popularity of the buyers and sellers.
Visual features of the digital assets were assessed using sophisticated computer vision techniques that extract the visual properties of the objects, such as their colours and shapes, and look for similarities. Previous sales were assessed by considering the market history of items from the same collection (items exchanged on the NFT market are organised in collections that, in most cases, share some common features). Finally, the popularity of traders in the NFT network was assessed by considering the total number of purchases and sales made by each trader.
We found that all three factors play a role in determining the price of an NFT. Previous sales of related NFTs are consistently the most important feature, explaining up to 50% of the variability in NFT prices. Visual features are also important determinants of the price, increasing the performance of our machine learning model by 10-20%. Adding data about the popularity of traders further increases the performance of our model by 10%. Together, these three factors can explain up to 70% of the variability in NFT prices.
These results suggest that it might be possible to accurately predict NFT prices – a direction we are planning to explore. We have also made our data freely available so that others can perform their own analysis. Better price prediction would help to encourage a more mature financial industry around NFTs, reducing investment risks connected to the unpredictable volatility of NFTs, both for private investors and at the systemic level.
NFTs are revolutionising the way in which digital content is produced and exchanged. Our study will stimulate further research on NFT production, adoption and trading across a broad array of disciplines, including economics, law, cultural evolution, art history, computational social science and computer science.
Read the paper:Mapping the NFT revolution: market trends, trade networks, and visual features
 
Video and top image created by Mauro Martino
"
https://www.turing.ac.uk/research/research-projects/discovering-polarity-change-word-meanings,"
Words can change meaning, and polarity, quite fast. Spectral clustering methods and natural language processing can offer a solution to this problem. They allow for improvements to be made to the performance of sentiment analysis tools, which inform whether a text is talking positively or negatively about a person or an organisation.
In natural language processing, sentiment analysis (SA) identifies a text's polarity. Existing approaches to SA do not account for a time factor. This is an important limitation, as the polarity of words can often change quickly. For example, the word ‘sick’ has recently acquired the meaning of ‘cool’ alongside the previous one of ‘unwell’. Being able to identify and react to this change is crucial for keeping SA systems up to date.
Existing SA research usually requires large amounts of labelled data. Crucially, by transferring domain knowledge from ongoing interaction with Thompson Reuters, this project is using financial data to efficiently produce sentiment-labelled data.
In algorithmic research, this project also seeks to further state of the art in spectral clustering methods – grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). The project will be extending constrained clustering to an online setting, where constraints arrive incrementally.
The main challenge of incremental clustering is how to efficiently update a clustering so that it satisfies both the new and old constraints, as opposed to re-clustering the entire data set from scratch. A related problem involves incorporating user feedback in the clustering loop, a task common in active learning where the algorithm chooses constraints and queries the user.
Utilising the JISC UK Web Domain Dataset 1996-2013 (JISC-UK), which is hosted at the British Library, to develop a system for polarity change detection which can improve existing sentiment analysis.
The great advantage of working with a massive archive of language data covering a limited time period is that the change in words’ polarity can be tracked at a very high level of granularity, and thus enable sentiment analysis systems to react promptly to the ever-changing nature of today's language.
Through collaborative development of clustering algorithms beyond the state of the art, the combination of the project researcher’s skills in natural language processing, mathematics, and history, will ensure the best use is made of this unique dataset.
The real-world impact of this project concerns the improvement of tools to mine the sentiment of texts. This is a very active area in industry. From a scientific viewpoint, the project contributes to the Turing’s research on mathematical representations and understanding human behaviour.
Concerning the wider research landscape, the polarity change detection system produced will be able to be used by the sentiment analysis community as a pre-trained resource.
Questions in computational social science will also be addressed, such as establishing the communication flows between online communities and mainstream media, and the creation of online identities via their use of polarity of words (i.e. by using certain words in a positive versus a negative sense).
March 2018: Project received seed funding from The Alan Turing Institute.
"
https://www.turing.ac.uk/people/enrichment-students/richard-brown,"
Richard Brown is a doctoral researcher at Northumbria University in Newcastle investigating perceptions of risk and socioeconomic inequalities in health. His PhD is supervised by Dr Gillian Pepper and Dr Liz Sillence. Richard also works as a researcher within the Psychology department at Northumbria exploring a range of behavioural health issues using both quantitative and qualitative methods. 
After studying philosophy at the University of Warwick, Richard worked as a corporate solicitor in the North East of England. He then enjoyed several years as an English teacher in Hue, Vietnam and Aranda de Duero, Spain before returning to the UK to pursue a career in academia. Richard started his PhD in January 2021 and hopes to continue carrying out health research upon completion.
 
Richard’s research in association with Turing will aim to analyse data associated with online health misinformation. Little attention has been given to misinformation concerning non-communicable diseases, such as cancers and cardiovascular disease, with most misinformation research centring around vaccines and the spread of COVID-19. For health communicators, the ability to ‘cut through the noise’ is fundamental to providing effective health messaging. Applying a range of data science methods to study the message characteristics that amplify content retransmission on social media is vital for understanding its proliferation online. Therefore, Richard’s research will aim to use a range of natural language processing techniques to investigate misinformation relevant to the modifiable behavioural health risk factors that are driving the rising prevalence of certain lifestyle diseases.
"
https://www.turing.ac.uk/people/researchers/mark-birkin,"
Mark Birkin is Professor of Spatial Analysis and Policy in the School of Geography, University of Leeds. He has longstanding interests in mathematical modelling of urban and regional systems including geodemographics, microsimulation, agent-based modelling, and spatial decision-support systems.
Mark has a notable track record of collaboration, including ten years as an executive director of Geographical Modelling and Planning (GMAP) Limited. In this time, GMAP developed from occasional consulting projects into a market analytics business with 120 employees and global reach, working with household name partners such as Ford Motor Company, Asda-Walmart, HBoS, Exxon-Mobil and GSK. An ethos of collaboration with external partners in business and the public sector continues in his current role as Director of the Consumer Data Research Centre (CDRC), a national investment within the ESRC Big Data Network. He is also PI for the ESRC Centre for Doctoral Training in Data Analytics which will coordinate sixty postgraduate research projects in tandem with external partners.
Since 2014, Mark has been Director of the Leeds Institute for Data Analytics (LIDA). Having started as a partnership between CDRC and the MRC Medical Bioinformatics, LIDA now supports 40 projects and programmes with more than £50M of funded research, bringing together over 200 researchers from across all eight faculties at the University.
He is a Fellow of the Academy of Social Sciences and a Fellow of the Royal Geographical Society.
The ambition of the CDRC is to create scientific insights on all aspects of society which are richly informed by the preferences and behaviour which are revealed in everyday patterns of consumption. This research is informed through partnership and engagement with more than 150 external organisations. Mark’s personal research interests include geodemographic profiling for neighbourhoods, microsimulation of individual behaviour patterns, and decision-support systems for retail and business organisations.
LIDA’s Integrated Research Campus provides tools for data management and analytics within a secure and accredited environment. This infrastructure supports more than £50M of funded research projects involving all eight faculties of the University, including National Centres for Consumer Data Research and Medical Bioinformatics.
"
https://www.turing.ac.uk/people/spotlights/omar-guerrero,"

I work on economic problems where policy interventions (like inheritance taxes, affirmative action policies, housing market regulations, etc.) could benefit from using computational tools for their design and evaluation. Sometimes, this involves simulating artificial economies, developing methods to quantify unobservable behaviour, or studying complex networks and big data.
At the Turing, I am developing an AI tool that will help governments to allocate public spending across hundreds of interdependent policy issues; something critical to achieve the Sustainable Development Goals by 2030. I am also creating computational tools for designing housing market interventions, a highly debated topic in the UK due to its stark inequalities.
I hope that my research provides social scientists and policymakers with tools to analyse economic problems from angles that are inaccessible through conventional methods.
The impact that my project on policy priority inference has achieved. Although I started with modest expectations, I’m pleased to see my methodology is now in the process of being adopted by different governments­­. Whenever we present it to high-level government officials, they are very receptive and see the value of adopting it as part of their planning toolkit. In fact, the project is currently being promoted by international organisations, including the United Nations Development Programme. I would have never imagined this when I started working on this topic three years ago.­
Economics, computing, policy
In the social sciences, it is not uncommon to ascribe to a set of ideas and methods and, then subsequently view every problem through those lenses. Interdisciplinary research often challenges these preconceptions, and this – unfortunately – can translate into an excessive scepticism that permeates into editorial boards, funding bodies and hiring committees. So getting support for this kind of work can be extremely difficult.
The Turing is one of the very few places in the world that encourages truly interdisciplinary research. It has been an ally of disruptive ideas from its research community and has put an impressive amount of effort in generating real-world impact with them. In my personal experience the Turing's public policy programme, as well as support teams such as communications, IT, research services and events planning, are always there to back me up.
I can fence
Yoshihiro Togashi
"
https://www.turing.ac.uk/about-us/impact/providing-covid-19-expertise-uk-government,"
The Turing-RSS Health Data Lab, a partnership established in August 2020 between the Turing and the Royal Statistical Society, has provided invaluable insights to the government’s UK Health Security Agency, which is responsible for public health protection in the UK.
The Lab aims to develop statistical and machine learning techniques to answer policy-relevant questions about COVID-19. It is made up of over 35 people in research, leadership and operational roles from more than 10 institutes, including Imperial College London, MRC Harwell and the University of Oxford.
“The Alan Turing Institute and the Royal Statistical Society came together at pace in response to the COVID-19 pandemic, providing world-class independent research and modelling expertise to the UK government.”
Johanna Hutchinson, Director of Analytics and Data Science, UK Health Security Agency
In December 2021, Lab researchers published a paper in Nature Microbiology describing a statistical framework that combines multiple sources of COVID-19 test data to provide more accurate estimates of local virus prevalence. This was followed in February 2022 by a statistical analysis from the Lab, published in The Lancet Regional Health, which found that deprived areas in England with higher proportions of non-White people were associated with higher COVID-19 infection rates. However, the strength of this association varied over the course of the pandemic and for different ethnic subgroups, highlighting the importance of continual monitoring when developing policies aimed at eliminating health inequalities.
Other work at the Lab has included a project looking at the potential of diagnosing COVID-19 and other diseases by acoustically analysing someone’s speech or coughs, and a statistical model that uses incomplete COVID-19 test data to estimate (‘nowcast’) the total number of positive tests. Meanwhile, a recent paper describes the Lab’s overarching approach to statistical modelling, setting out a framework that will allow other research teams to rapidly build effective, data-driven models in response to future health emergencies.
“The partnership has played a highly valuable role in developing and further enhancing the data science and advanced analytical capabilities within the UK Health Security Agency, both in responding to COVID-19 and in tackling new and existing threats to UK health.”
Johanna Hutchinson, Director of Analytics and Data Science, UK Health Security Agency
Further reading:
This piece first appeared in The Alan Turing Institute’s Annual Report 2021-22Top image: Sergey Zaykov / Shutterstock
"
https://www.turing.ac.uk/events/turing-lecture-data-science-or-data-humanities,"

This event is now full, however you can join remotely by visiting our YouTube channel on the day.
Melissa Terras is the Professor of Digital Cultural Heritage at the University of Edinburgh‘s College of Arts, Humanities, and Social Sciences, leading digital aspects of research within CAHSS, and Director of Research in the new Edinburgh Futures Institute. Her research focuses on the use of computational techniques to enable research in the arts, humanities, and wider cultural heritage and information environment that would otherwise be impossible.
With a background in Classical Art History and English Literature (MA, University of Glasgow), and Computing Science (MSc IT with distinction in Software and Systems, University of Glasgow), her doctorate (Engineering, University of Oxford) examined how to use image processing and machine learning to interpret and read deteriorated Ancient Roman texts. She is an Honorary Professor of Digital Humanities in UCL Department of Information Studies, where she was employed from 2003-2017, Directing UCL Centre for Digital Humanities from 2013. Books include “Image to Interpretation: An Intelligent System to Aid Historians in Reading the Vindolanda Texts” (2006, Oxford University Press) and and “Defining Digital Humanities: A Reader” (Ashgate 2013) which has been translated into Russian and Chinese.
She is a Trustee of the National Library of Scotland, serves on the Board of Curators of the University of Oxford Libraries. is a Fellow of the Chartered Institute of Library and Information Professionals, and Fellow of the British Computer Society. You can generally find her on twitter @melissaterras.

What are the opportunities, issues, and rewards for researchers developing data-led approaches to answer research questions in the Arts and Humanities? How can we build and utilise appropriate computational methods for the analysis of our past and present societies? What possibilities and barriers are there in working in this crossover point from data science to the humanities? And how can the humanities contribute to development of data science approaches? From the development of Handwritten Text Recognition for archival material, and the mining of millions of words of historical newspaper archives, this talk will showcase a range of innovative international research projects, whilst also giving pointers on how others can approach this interdisciplinary space successfully. In addition, it will raise issues of how tricky yet rewarding “interdisciplinary research” – which we are all now being encouraged to do – can be.
14:00 - 14:30 - Registration
14:30 - 14:35 - Introduction (Barbara McGillivray)                          
14:35 - 15:25 - Data science or data humanities? (Melissa Terras)
15:25 - 16:00 - Q&A
16:00             - Close

"
https://www.turing.ac.uk/people/business-team/katrina-payne,"
"
https://www.turing.ac.uk/people/guest-speakers/ian-diamond,"
Professor Sir Ian Diamond FBA FRSE FAcSS became National Statistician on 22 October 2019.
Sir Ian previously served as a non-executive member of the UK Statistics Authority Board from June 2018 to August 2019.
Sir Ian was Principal and Vice-Chancellor of the University of Aberdeen from 2010 to 2018. He was previously Chief Executive of the Economic and Social Research (ESRC) Council and, from 2004 to 2009, Chair of the Research Councils UK Executive Group (2004-2009).
Before joining the ESRC, Sir Ian was Deputy Vice-Chancellor at the University of Southampton, where he had been for most of his career. Sir Ian is Chair of British Universities and Colleges Sport, Chair of Edinburgh College of Further Education, Chair of Plan International UK and a Board member of UK Research and Innovation. He chaired the Welsh Assembly Government of the Higher Education Review for Wales reporting in 2016.
Sir Ian was elected to the UK Academy of Social Sciences in 1999, is a Fellow of the British Academy (2005), a Fellow of the Royal Society of Edinburgh (2009) and holds honorary degrees from the universities of Cardiff, Glasgow and Curtin.
"
https://www.turing.ac.uk/people/enrichment-students/emma-li,"
Emma is a PhD student at the UKRI CDT in Accountable, Responsible and Transparent AI at the University of Bath. Her work is focussed on the interpretability of Machine Learning and AI for more informed responsible usage of deep learning algorithms. Her aim is to bridge the gap between the domain of machine reasoning and the space of human understanding for all users by creating improved explainability methods. She holds an MMath in Maths and Computer Science and worked as a Data Scientist prior to her PhD.
Emma's research will attempt to further the landscape of deep learning interpretability methods using an interdisciplinary approach across the fields of causal discovery, Bayesian Machine Learning, cognitive science, and ethics. Her research interests centre on improving responsible usage of deep learning algorithms to complement human skills and experience. Aside from Interpretable and Explainable AI, these interests include causality, human understanding and explanation synthesis, and the development and enactment of AI policy and regulation.
"
https://www.turing.ac.uk/news/turing-experts-respond-google-ai-chatbot-story-media,"
Experts from The Alan Turing Institute have been responding to reports this week that a Google engineer has been put on leave after saying an AI chatbot has become sentient.
The news sparked debate about whether machines could think and feel for themselves, and the Turing’s Professor Mike Wooldridge and Dr Adrian Weller offered their knowledge to a range of online and broadcast media outlets on the subject matter.
In a piece for inews Professor Wooldridge, who was extensively quoted, commented on the limitations of the company’s LaMDA (language model for dialogue applications) saying that “these machines are doing impressive things but actually you don’t have to dig very hard to find out their limits.”
Additionally, in an interview for ITV News at Ten, Professor Wooldridge said, “what’s it’s doing is just trying to generate, based on vast quantity of text that it’s seen, things that sound plausible”.
Mike also featured in interviews for Times Radio and the Daily Mail.
The Turing recently announced an expansion of its AI work. This new overarching work strand will be headed up by Professor Wooldridge, who has been a member of the Turing community since 2018. As Professor of Computer Science at the University of Oxford, he was Head of Department from 2014-21. He currently holds a UKRI Turing AI World-Leading Researcher Fellowship.
In an article for the Daily Mail, Dr Weller, a recent recipient of an MBE for his ‘outstanding services to digital innovation’, almost everyone would agree that it is not sentient, it can produce text output which might superficially suggest it might be – until you take time to dig further with more probing questions.”
Adrian also featured in interviews with BBC World at One on BBC Radio 4, BBC Radio Kent and the New Scientist.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/microsoft,"
microsoft.com/en-gb
"
https://www.turing.ac.uk/partnering-turing/international,"

The Alan Turing Institute’s research ranges from theoretical to applied. As the national institute of data science and AI, we are in a unique position to convene collaborations across UK industry, academia, and government.
The Institute is looking to complement and augment the UK’s capacity in data science and artificial intelligence through international collaboration. Our aim is to engage with international partners and their expertise to advance world class scientific and technological innovation and utilise data science and AI for social good in a way that is mutually beneficial for the UK and its collaborators. 
Our approach to international engagement is based on the principles of open-sourced research, research excellence and the co-design of collaborative projects with partners over contract or consultancy research. Organisations who are interested in engaging with the Turing and who share our principles and vision, should refer to our programme pages for information on our current research. 
For more information about the Institute and to discuss potential collaboration, please contact us via [email protected]. For delegation visits, we work in a busy environment but we try to accommodate requests when possible. 
 
"
https://www.turing.ac.uk/people/researchers/animesh-chaturvedi,"
PhD from Indian Institute of Technology Indore (IIT Indore)
Ph.D Research Project: System Evolution Analytics: Pattern mining and Deep learning of an Evolving system
M.Tech Research Project: Automated Web Service Change Management AWSCM - A Tool
- Worked as Visiting faculty of Cloud Computing (course material) at IET-DAVV for 22 hours in 15 days. The course is conducted to teach final (4th) year undergraduate students.
- Worked as a ""Senior Student Research Associate"" & ""Project Associate"" at Indian Institute of Technology - Kanpur
- Research intern in Motorola Mobility - Bangalore. Developed Big Data Analytic system on Hadoop and Storm.
Volunteer IEEE CloudCom 2014 and 36th IEEE/ACM - ICSE 2014
Animesh is working on Hate speech project. 
Reviewed
ACM KDD 2020, HyperText 2020, CIKM 2020
Selected in HLF 2019 as 200 Young Researcher.
Selected for Schloss Dagstuhl Seminar.
Reviewed
IEEE Trans. on Cloud Comp.,
IEEE Trans. on Services Comp.,
IEEE Trans. on Big Data,
IEEE Trans. on ET Computing,
IEEE Trans. ET Computational Intelligence,
IEEE Trans. on Cognitive and Developmental Sys.,
IEEE Sys. J.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/goldsmiths-university-london,"
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-exeter,"
"
https://www.turing.ac.uk/people/external-researchers/gobe-hobona,"
Dr. Gobe Hobona is the Director of Knowledge Management for the Open Geospatial Consortium (OGC). The OGC is an international industry consortium of over 525 companies, government agencies and universities participating in a consensus process to develop publicly available interface standards for geospatial technologies. Gobe is responsible for the OGC’s knowledge management strategy towards creating an environment that enables the OGC membership to identify, create, represent, share, distribute, and adopt insights and experiences across programmes. His other roles within the OGC include being Chair of the OGC Naming Authority and the OGC Architecture Domain Working Group. Prior to joining OGC staff, he was the Consultancy Team Leader and Head of Applied Research at Envitia. During his time at Envitia, he was the technical lead of a multi-year geospatial research project sponsored by a major UK government ministry. He has also previously held postdoctoral research posts at Newcastle University and the University of Nottingham. He is a professional member of the Royal Institution of Chartered Surveyors (RICS) and the Association of Computing Machinery (ACM).
"
https://www.turing.ac.uk/people/researchers/mihai-cucuringu,"
Mihai received his PhD in Applied and Computational Mathematics (PACM) at Princeton University in 2012, supervised by Amit Singer. His thesis was on the low-rank matrix completion problem and several distance geometry problems with applications to sensor network localization and three-dimensional structuring of molecules. During 2013-2016 he was a CAM Assistant Adjunct Professor in Computational Applied Mathematics at UCLA. During Fall 2014, he was a Research Fellow at the Simons Institute for Theory of Computing at UC Berkeley, in the program Algorithmic Spectral Graph Theory.
Mihai's research interests concern the development and mathematical analysis of algorithms for large networks, certain inverse problems on graphs, and big data analysis, with applications to various problems in engineering, machine learning, finance, and biology. Particular areas of interest are spectral and SDP-relaxation algorithms and applications, the group synchronisation problem, ranking from noisy pairwise comparisons, lead-lag relationships in multivariate time series, clustering, core-periphery structure in networks, multiplex networks, dimensionality reduction and diffusion maps (with an eye towards heterogeneous data and nonlinear time series), spectral algorithms for analysis of signed graphs and correlation networks. The above problems share an important feature: they can all be solved by exploiting the spectrum of their corresponding graph Laplacian.
"
https://www.turing.ac.uk/news/publications/synthesis-data-instrumented-structures-and-physics-based-models-gaussian,"
A recent development which is poised to disrupt current structural engineering practice is the use of data obtained from physical structures such as bridges, viaducts and buildings. These data can represent how the structure responds to various stimuli over time when in operation, providing engineers with a unique insight into how their designs are performing. With the advent of advanced sensing technologies and the Internet of Things, the efficient interpretation of structural health monitoring data has become a big data challenge. Many models have been proposed in literature to represent such data, such as linear statistical models. Based upon these models, the health of the structure is reasoned about, e.g. through damage indices, changes in likelihood and statistical parameter estimates. On the other hand, physics-based models are typically used when designing structures to predict how the structure will respond to operational stimuli. What remains unclear in the literature is how to combine the observed data with information from the idealised physics-based model into a model that describes the responses of the operational structure. This paper introduces a new approach which fuses together observed data from a physical structure during operation and information from a mathematical model. The observed data are combined with data simulated from the physics-based model using a multi-output Gaussian process formulation. The novelty of this method is how the information from observed data and the physics-based model is balanced to obtain a representative model of the structures response to stimuli. We present our method using data obtained from a fibre-optic sensor network installed on experimental railway sleepers. We discuss how this approach can be used to reason about changes in the structures behaviour over time using simulations and experimental data.
Gregory A, Lau D-H, Girolami M, Butler L, Elshafie M. (2018) The synthesis of data from instrumented structures and physics-based models via Gaussian processes. arXiv:1811.10882
"
https://www.turing.ac.uk/collaborate-turing/internships,"

Applications for the Turing Internship Network are now closed.
Please ensure to carefully review the call document and the role descriptions linked below before you apply. We encourage applicants to check out the section on how to apply on this page and this guidance for tips on how to write an application. 
 
You can find a recording from the online information session here.
 
 
Launched in 2020, the Turing Internship Network is a national engagement scheme between our business partners and doctoral students across the UK and Ireland who are studying any topic with a data science and/or AI focus. The Turing's role is to facilitate and convene, pairing internship projects put forward by industry with talented doctoral students. The business partners host, supervise, and provide a salary for the successful interns.
Testimonial from one of our 2022 interns at Accenture.
 
Due to an increasing interest for students in acquiring industry experience and business awareness, and for the business sector to address emerging data challenges using state-of-the-art academic methodology, we are keen to ensure everyone benefits from the Institute’s unique position in convening businesses with academics. We are therefore extending our training leadership goal to support employment opportunities that tackle real-world challenges.
In this 6th round of recruitment, we are working with 6 TIN partners to offer new exciting internship opportunities where students can apply their knowledge and skills to different projects such as the detection of harmful memes online, reinforcement learning for the security of computer networks and systems and much more.
Please find below the descriptions of the roles that are open for applications:
 
 
 
Students must meet all the following eligibility criteria to be considered for the scheme: 
Additional eligibility requirements may apply for specific internships and those are specified in the role descriptions.
 
Applications for the Turing Internship Network are now closed. 
When applying, please read carefully the call document and these guidelines on how to write your application. If you are an international student studying in the UK on a visa, please read the Information for International Students document.   
 
We invite you to share with your supervisors information about the programme that can be found in this document. 
For information on how we handle your personal data collected through the application form, please read our privacy notice.
 
 
In the first 5 recruitment rounds, we worked with 9 TIN partners and 9 TIN-DSG partners, and we successfully placed around 60 interns.
The internship roles advertised in the past cover a wide range of research areas such as data science and AI applied to finance, defence & security, environment and sustainability, the legal domain etc, and data science and AI in public policy.
Some examples of previously advertised roles are:

 
The Turing Internship Network provides a fantastic paid opportunity for students to apply their academic skills on real-world problems. In addition, a Turing internship provides students an excellent setting to expand their professional networks and to identify prospective employment opportunities following graduation.
Internships are provided and hosted by the TIN partner/collaborator who requires data-driven academic expertise for their business challenges.
Find out more about the TIN experience from Akhil Ahmed, intern at Accenture until April 2023.
 
""The Turing Internship Network provides an excellent opportunity to gain industrial exposure and to map academic research experiences to real-world challenges. I have a better understanding of the industrial challenges through this internship. It is a great experience to be able to solve real-world problems using my academic research skills.""
Pauching Yap,TIN intern at TRL
To find out first when new calls are going out, please join our mailing list.

TIN attracts high-quality interns to work on your data-driven business challenges. The Institute is well-placed in the UK national landscape to convene businesses and industry with high calibre academics.
The Institute will manage all processes and recruitment between you sending the role descriptions and up to interviewing the shortlisted candidates.
For more information about the scheme, please read the TIN brochure.
 
We are keen to work with a diverse portfolio of TIN partners and therefore we welcome all expressions of interest as long as the work the successful intern(s) would be doing involves some methods or concepts from mathematics, statistics, data science, computer science, AI, data ethics and/or technology.
If you would like to find out more about this scheme and would like to express interest on behalf of your organisation, we encourage you to complete the form below. A member of our Internship Network team alongside a Partnership Business Lead will contact you within five working days to discuss your interest and suitability for promoting internship roles through the TIN.
 

We hope that the Turing Internship Network will attract a diverse pool of both partners and applicants. All applications will be considered on their merits and project suitability. Applicants can be of any nationality (unless otherwise stated in the specific internship requirements) providing they have the right to work in the UK and applicants from under-represented groups are encouraged to apply. We welcome applicants from all UK-based universities, or who have taken a career break prior to their PhD. We welcome interested applicants to let us know what work arrangements would suit them best.
We are encouraging our existing and prospective business partners to consider developing inclusive internships (e.g. part-time and flexible working options, accommodating interns on a student visa, remote work, etc.) and this commitment is very important to the Turing when deciding who to work with through business partnerships.
The Alan Turing Institute is a Disability Confident organisation. As part of this commitment, we recognise that there may be individual circumstances that we need to be aware of. We aim to accommodate specific needs and personal circumstances, but we are reliant on applicants sharing this information with us. 
To discuss an adjustment to the application process please contact our team directly at [email protected]. There is also a section on the application form where applicants may make us aware of individual circumstances. 
We will treat any information you disclose to us as sensitive and will handle it in line with the Data Protection Act 2018. You can find out more information about how we handle your personal data in our transparency notice (available on the application portal).
If you would like to find out more about the previous in-house internship programme, please see testimonials and more.
"
https://www.turing.ac.uk/people/former-researchers/peter-foster,"
Peter Foster is a Research Data Scientist at The Alan Turing Institute. He obtained a PhD in Electronic Engineering from Queen Mary University of London, where he investigated statistical techniques for determining musical similarity. Subsequently, he worked on two industry-focussed postdoctoral projects at Queen Mary. Prior to joining The Alan Turing Institute in January 2020, he worked in the financial technology sector, investigating machine learning solutions for biometric-related applications.
At the Turing, Peter is interested in delivering reproducible insight based on statistical machine learning techniques. He is currently involved with projects related to the Defence and Security, and Finance and Economics programmes.
"
https://www.turing.ac.uk/people/external-researchers/lars-nesheim,"
"
https://www.turing.ac.uk/people/guest-speakers/mick-ridley,"
Mick has worked in the advertising industry for the past 27 years for both media agencies and media owners. During that time he has worked extensively with audience data from traditional research sources through to mobile network data. He has over the past 6 years worked at Global Media with a large amount of his time spent in support of their advertising contract with TfL. During that time he has built a close relationship with TfL, building systems to help understand how customers navigate the network providing public and commercial benefit.
"
https://www.turing.ac.uk/people/enrichment-students/peiling-yi,"
Peiling is a Ph.D. Candidate in the Cognitive Science Research Group, Department of Computer Science, Queen Mary University of London. Her research interests lie in building novel deep transfer learning and fair machine learning algorithms for context-based adolescent cyberbullying detection models. She holds a master's degree in software engineer and has worked as a software engineer and project manager for several global software companies
During the enrichment period, Peiling will mainly focus on improving the generality of context-aware cyberbullying detection systems and investigating biases in these current models. The approach to building these models will be directly linked to the Multi-Agent Systems, Robust Machine Learning, Natural Language Processing and Machine Learning, and Dynamic Systems Interest Groups, which face the challenge of ""making algorithmic systems fair, transparent, and ethical."" Their research will explore how to make models behave fairly and how to explain their debiasing behavior, which is also the focus of the Fairness, Transparency, and Privacy interest group.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/epcc,"
www.epcc.ed.ac.uk
"
https://www.turing.ac.uk/blog/updates-algorithm-underlying-nhs-covid-19-app,"
In my previous blog post, I described the NHS COVID-19 app’s technical roadmap – specifically related to the app’s ability to differentiate between high and medium/low risk encounters with an individual that tests positive for COVID-19, using Bluetooth Low Energy (BLE) signal strength data, and all of the uncertainties that BLE presents.
Since writing that blog post, the app has been downloaded (an incredible) 19 million times. In line with recent research, the app is very much expected to be working alongside other non-pharmaceutical interventions to reduce infections and deaths. We are working hard to evaluate and validate the app’s impact. Like our earlier research findings (in which we observed significant decreases in incidence and R on the Isle of Wight immediately after the launch of the Test and Trace programme, including the app), we will publish research when we have statistically robust conclusions to share.
In this blog post, I want to provide an update on the changes to the risk scoring component of the app.
The NHS COVID-19 app uses Google/Apple Exposure Notification (GAEN) Application Programming Interface (API) to perform decentralised contact tracing – ensuring that the app is privacy preserving, meaning that it can be optimised for health outcomes, whilst ensuring that the app users’ sensitive data remains cryptographically secure, located on device, and not able to be passed to anybody (see this BBC news article too). To perform risk assessment on a user’s device, GAEN API has two Bluetooth data modes available: mode 1 and mode 2. Let’s have a brief look at them, and what the new techniques do to make use of mode 2 data.
In mode 1, the GAEN API provides signal attenuation information to the app, in the form of a three-bin histogram, to allow each country to define a risk calculation in line with their national definition. (We loosely term these three bins as “near”, “medium”, “far”, and use 2m and 15mins as the baseline definition of high risk, for England and Wales.) The height of each histogram bin corresponds to the number of seconds spent in that bin over a 24-hour period, based on any interaction with a device whose owner has subsequently tested positive. The width of each histogram bin, partitioned within signal attenuation, is defined by the health authority.
For the NHS COVID-19 app, the selection of the bin width, through the specification of two attenuation values, has been performed through statistical optimisation: for all the different forms of signal attenuation and signal multipath, what’s the configuration that optimises receiver operating characteristic area under curve (ROC AUC), and so only notify those that are high risk. To select these values, we have simulated hundreds of thousands of scenarios, and performed large numbers of real-world experiments, allowing us to search across the 5-dimensional parameter space (containing the partition of the histogram and associated weights), and so mathematically select the optimal values. Using this principled approach, we arrived at the same conclusion as our German app colleagues, and so use the same configuration.
In order to understand the performance of the app in the real world using these parameters, we performed many experiments (on a bus, in an office, in a pub, in a conference hall, and so on) and determined that the current app (running on 19 million devices) has an AUC of approximately 0.72 – which is “good” (see this (unrelated) reference for such terminology). With the latest release, for users that decide to continue with the mode 1 app (i.e., those that have not upgraded their device’s operating system so can’t use mode 2), we are further reducing the risk threshold, to make the app more sensitive to high-risk encounters.
Google and Apple have released an update to their API, which moves away from the mode 1 histogram approach (Google documentation). The GAEN API now provides health authorities with time sequenced attenuation values, with the attenuation value being a “raw” measurement at a particular point in time, rather than being aggregated into histogram format. This means that we have several more pieces of information to use (in a fully privacy preserving manner), and can better approximate transmission risk.
Using a sequential Bayesian process – an Unscented Kalman Smoother – we can compute a posterior distribution over distance given time sequenced attenuation data. Intuitively, this algorithm exploits the fact that two individuals within an encounter are very unlikely to have their distance fluctuate rapidly as a function of time (i.e. two individuals are unlikely to be 2m then 10m then 3m then 8m then… apart over a 15 min window). Exploiting this prior information, coupled with a complete characterisation of the conditional uncertainty in the likelihood function, as detailed in our preprint, allows us to compute the aforementioned posterior distribution, and so compute a more accurate risk score. (Note that we considered sequential Monte Carlo algorithms for inference, which were deemed to be too computationally expensive and difficult to verify in the development time we have available. We also considered simpler algorithms such as the extended Kalman smoother, which was deemed not to be as robust as the unscented variant, due to the well-known limitations of the first-order Taylor series approximation that such an algorithm makes.)
With these algorithms deployed, we’re able to obtain an AUC of around 0.85 – which is deemed to be “excellent”. We believe that we are the first app to use mode 2 data in this way, and our source code for the algorithms are available, should anybody wish to offer suggestions, or should other countries want to use (and build on) the work that we’ve done.
It also turns out that using the mode 2 API completely removes the “ghost notification issue”, which has caused concern for some users. Now, if the app notifies you, then it will only be for high-risk encounters - in which case you should follow the instructions provided, and help to keep your family, friends, and community safe.
Along with colleagues at The Alan Turing Institute, I continue to offer independent scientific advice to the NHS COVID-19 app, to help to produce an app that offers maximum benefit to citizens in England and Wales, and the rest of the world. In research terms, we’re exploring the epidemiological impact of the app, the utility of the inclusion of (privacy preserving) indoors / outdoors indicators, adaptive Bluetooth sampling, Bayesian device calibration, and a set of other potential improvements that could make this app even better. I welcome input from interested collaborators.
I would like to thank our collaborators for the interesting and informative conversations and comments to date, from organisations including Google, Apple, MIT, University of Liverpool, University College London, Glasgow University, University of Oxford, University of Cambridge, and NCSC – and all colleagues across NHS Test and Trace.
"
https://www.turing.ac.uk/research/research-projects,"

"
https://www.turing.ac.uk/people/guest-speakers/skye-mccool,"
With a background in science and environmental education, and a lifetime of experience in visual design and communications, Skye finds her niche in using creative tools to facilitate complex processes and conversations. She is a trained graphic facilitator, and designs, manages and delivers public and stakeholder engagement, with a commitment to accessibility and inclusivity.
"
https://www.turing.ac.uk/research/environment-and-sustainability,"
To address the climate and biodiversity crisis and the need for greater sustainability.
High-impact environmental events such as heatwaves, flooding and droughts are increasing in both frequency and magnitude. This threatens our planet, including the four billion people who live in cities and the rich biodiverse natural ecosystems from which life on earth relies on. Never has our planet experienced such rapid increase in the extraction and use of natural resources, including fossil fuels and critical minerals now used by industry. The path we are on is unsustainable and evidence indicates that we could be heading towards a sixth mass extinction. We understand the causes: they include climate change, land use and pollution. It is not too late to turn this around.  
To act on this we need to slow the pace of global heating, predict and mitigate climate impacts, and provide clear evidence to develop national and international policy to meet net zero targets.  
Environmental and sustainability datasets come in all shapes and sizes from past climate signals and fossils buried in rocks and ice; monitoring our planet and infrastructure using in-situ and remote sensors including imagery from cameras and satellites; and the output from highly detailed and complex state-of-the-art process-based computer model simulations. 
To understand and predict future change we need to harness knowledge to best effect, to maximise the usefulness of our existing observations and sensor networks and provide intelligence for future sensor placement. 
We must also leverage solutions and ideas seamlessly from across disciplines, bridge and integrate data across spatial and time scales, and deploy solutions in a timely manner to help secure our planet for future generations. 
We believe that truly interdisciplinary research will catalyse the next big step-changes in environment and sustainability innovation. To deliver the change we need will require diverse teams and skills to co-design and co-develop solutions. 
Our expertise in applying data science and AI across a broad range of science and engineering makes the Turing uniquely placed to play a central role in tackling and averting the climate and biodiversity crisis in order to help society and life prosper in a net zero world. No single institute is as well-equipped as us to drive forward the development of robust open-source and community-led AI algorithms, data science and digital pipelines to democratise the use of these tools to safeguard our planet.   
We need to break-down silos and foster strong and lasting collaborations to create adaptable tools and develop state-of-the-art digital methods for automated knowledge-informed and data-driven environmental monitoring and forecasting.  
Our expertise in digital twinning enables us to advance efforts to optimise the infrastructure in which our society relies on, and model interventions to support pathways to decarbonisation, sustainable use of natural resources, and provide trustworthy evidence for policy making. 
‘Climate change and biodiversity loss are defining challenges of our time. And the new strategy of the Alan Turing Institute rightly identifies as a grand challenge in environment and sustainability. By harnessing the UK’s world leading expertise in data science alongside other centres of excellence in the UK, such as the Met Office, the Alan Turing Institute will play a key role in addressing environment and sustainability.’ 
Professor Stephen Belcher, Chief of Science and Technology, Met Office  
‘Scientific collaboration and the sharing and unlocking of data is critical if we are to find solutions to climate instability and biodiversity loss. Working with organisations like The Alan Turing Institute will be key to driving solutions to the planetary emergency, with positive environmental outcomes towards sustainability, as reflected in their future strategy.’ 
Dr Tim Littlewood, Executive Director of Science, Natural History Museum 
The Institute is developing a set of missions to sit under the Environment and Sustainability Grand Challenge, to be launched soon. 
We need collaborators to help us to solve this Grand Challenge. Sign up to our newsletter to stay in touch with the latest news.

"
https://www.turing.ac.uk/people/research-associates/louise-coutts,"
"
https://www.turing.ac.uk/about-us/impact/asg,"

The Alan Turing Institute was awarded £38.8 million over five years in 2018 through UKRI’s Strategic Priorities Fund. AI for science and government (ASG) is delivered in partnership with the Engineering and Physical Sciences Research Council (EPSRC) and in collaboration with a number of other research councils.
The biggest challenges that our societies face – from the COVID-19 pandemic to social and economic instability, overpopulation, and the climate emergency – involve complex interconnections between environmental, healthcare, social, economic, political and engineering systems. Addressing these grand challenges therefore requires both deep disciplinary expertise and ways to convene and combine expertise from different disciplines, strongly connected to problem owners.
Traditionally, this has been a substantial challenge. However, by providing the tools and methods to handle, combine and model large, disparate datasets, modern data science and AI tools have the potential to transform this process, with myriad benefits to society. The vision of ASG is to demonstrate – via a diverse range of activities organised and presented as an integrated whole – how AI and data science can be used to effectively address significant societal challenges and transform health, science, engineering, and government.
Themes, projects, events and associated programmes can be explored below.


Here you will find a collection of one page briefing notes that provide an overview of the impact achieved to date on projects funded by the AI for Science and Government (ASG) programme.
A digital twin for London’s underground farm (PDF)
Accessible training for diverse data users (PDF)
Colouring Cities: a worldwide network to inform buildings research and policy (PDF)
EnergyFlex: using simulation to make our homes more energy-efficient (PDF)
IceNet: faster, more accurate sea ice forecasting with a new AI-based tool (PDF)
Raphtory: deeper network analysis (PDF)
scivision: making computer vision tools more accessible (PDF)
SPARRA: using AI to predict emergency hospital admissions in Scotland (PDF)
The National Health Index: developing a reliable measure of the nation’s health (PDF)
The Turing Way for responsible research (PDF)
Working with Cefas to create cutting-edge software for surveying sea life (PDF)
In consultation with the programme’s External Advisory Board, the Management Board agreed in July 2020 to fund several new and strategic research initiatives to build on research outcomes from the first two years of the ASG programme and address important issues for the UK government and public sector made more urgent since the onset of the COVID-19 pandemic. Below provides a description of the new research initiatives and will be further detailed as the projects associated with them progress. 
Measuring the impact of policy interventions related to COVID-19 and building resilience against future shocks.
The COVID-19 crisis has highlighted how vulnerable societies and governments are to shocks. This sensitivity is largely due to the propensity to design policy for narrow siloes relating to sectors and government departments, without adequate consideration of the interdependencies between them or proper appreciation of the interconnected nature of local and global societies. It has become evident that resilience in one policy area (e.g. health) can come at the cost of resilience in another (e.g. the economy). The overall aim of this research is to develop a better understanding of resilience in interconnected health, social, and economic systems and to use this understanding to identify robust policy measures.
Find out more about shocks and resilience
Digital twins are now well established in a number of domains and are increasingly being linked into ‘ecosystems of digital twins’ (EDTs), but foundational challenges remain. We seek to develop new methods, tools and underpinning foundations to build well-defined EDTs which are spread across spatial and temporal scales, addressing specific use cases in engineering, health, commerce, economics, urban infrastructure and community modelling, as identified by relevant commercial and government stakeholders.
Find about more about ecosystems of digital twins
Environmental models: Bridging the spatial scales, from surface sensors to satellite sensors.
Satellite sensors can now provide an amazing level of detail of the Earth surface, yet with sparse and imperfect ground-truth sensors to validate them, and due to their relatively short record (a few decades) their usefulness when used on their own is somewhat limited. To make new leaps in understanding environmental change and to improve prediction we must find intelligent ways to combine satellite data with surface sensors and the output from physics-based environmental simulators (e.g., climate models). To bridge these spatial scales and various modalities we are creating a team of scientists and engineers to build and deploy toolkits driven by real-world case studies. This effort will create the foundations for building UK research capacity in developing Digital Twins of the natural environment.
The impact of climate change on agriculture.
Agriculture is highly vulnerable to climate change, with important socio-economic implications for food sustainability and greenhouse gas emissions. We can begin to model this by bringing together data from plant science, hydrology, soil science, insect population dynamics, economics, consumer behaviour and climate models. In this way we will contribute to the sustained development of a national crop modelling platform which will continue to develop at scale beyond the duration of this particular project.
Find out more about environment and sustainability
This theme is aligned with the urban analytics programme.
This theme is aligned with the data-centric engineering programme.
This theme is aligned with the health and medical sciences programme.
This theme is aligned with the public policy programme.
This theme is aligned with the data science for science and humanities programme.
Researchers at The Alan Turing Institute, Science Technology Facilities Council and their collaborators, have access to the PEARL computing service for research falling under this theme. Click here for more information.
This theme is aligned with the tools, practices and systems programme, and involves collaboration with the Research Engineering Group.
The ASG programme is governed by its Management Board which includes its Theme Leads as members as well as representatives from EPSRC, Delivery Partner of the investment, and independent members from the Turing’s University Partner Network. In April 2021, two new leadership roles within ASG were appointed on an initial 12 month interim basis: Professor Jonathan Rowe as Programme Chair, and Professor Ben MacArthur as Programme Director.
The ASG programme also has an External Advisory Board which offers strategic advice to the Management Board. Members were invited based on their independence from the ASG programme and their range of scientific, industry and government expertise. The Chair of the External Advisory Board is Dame Wendy Hall and Deputy Chair is Professor Tom Rodden.
"
https://www.turing.ac.uk/research/research-projects/topological-hierarchies-complex-image-data,"
Imaging is a cornerstone of modern biology, and generates huge quantities of rich data. Understanding what this data tells us about the structure and function of the underlying biological system is a challenging and complex task. This project is developing new methods to interpret these complex images, drawing on recent advances in machine learning and computational topology to attempt to understand the sample in terms of its component 'parts' and how these combine together to form more complex structures. Developing an understanding of the biology in these terms could reveal important information about the structure-function relationship with potential long-term applications in drug discovery.
Biological systems are fundamentally compositional in nature - simplistically we have molecules composed from atoms; organelles from molecules; cells from organelles; tissues from cells. This hierarchical organising principle is reflected in how a sample appears when it is imaged, and complex images are built up from increasingly involved combinations of quite simple building blocks, or parts. This idea has been used in computer vision to learn 'compositional hierarchies' of complex visual scenes.
This project will extend these ideas and combine them with ideas drawn from topology, which provides a formal framework that will allows us to understand how the parts are connected with each other.
Modern imaging techniques that are now widely used to understand complex biological samples can generate huge quantities of extraordinarily rich and informative data. The ability to understand what the images tell us about the sample is currently limited by the computational methods that are essential to the analysis of the images.
This project will develop a new approach to analysing these images that will allow us to understand how the complex biological sample - for example a living cell, or a piece of tissue from a biopsy - is composed from simpler parts that combine together in certain ways that control how the sample behaves. The new approach will improve understanding of what the fundamental parts are and how they combine together in different ways under different conditions to give rise to different functions. This understanding is crucial to working out how the parts, their relationships, and the function are connected to each so that researchers can discover and understand the processes that control our bodies.
This could, in turn, allow us to determine what is different between healthy and unhealthy samples and, in the long-term, to develop therapies that specifically target the key processes that cause disease and don't interfere with normal function. 
Advanced imaging techniques are being used in academic biology labs around the world, and in the biotechnology and pharmaceutical industries. This project will develop a new methodological approach that will enable these labs to probe the sample's constituent parts and their interactions in a new way. This could provide valuable new information about the underlying biological mechanisms that are controlling the sample's behaviour. In turn, this could allow them to understand how to control specific mechanisms or interactions that lead to the abnormal behaviour associated with disease.
[email protected]
"
https://www.turing.ac.uk/people/researchers/antonella-delmestri,"
Dr Antonella Delmestri is the data science lead for observational research at the Centre for Statistics in Medicine, Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences (NDORMS), University of Oxford. Antonella studied Computer Science at the University of Udine (Italy) and obtained a PhD in Information and Communication Technologies at the University of Trento (Italy). After several years of developing firmware and software for electronic devices, in 2006 Antonella moved to the UK to join the Clinical Trial Service Unit (CTSU) at the University of Oxford. She joined NDORMS in 2013 and the Centre for Statistics in Medicine in 2019. In the same year she also embraced the European Health Data and Evidence Network (EHDEN) collaboration and the Observational Health Data Sciences and Informatics (OHDSI) international initiative. Antonella is a member of the Women in data science (WIDS) Oxford initiative and an advisor in the WiDS Oxford Committee.
Antonella Delmestri’s research focuses on real-world data (RWD) sources and their automated processing, which involves data engineering, data mining, advanced curation, and common data modelling. Among her contributions there is Curator, a clinical data science tool, which has allowed for the delivery of more than twenty observational studies so far. Antonella is interested in measuring the impact of algorithmic automation on RWD management, its role in the statistical process and its impact on medical research and public health. Related interests are the assessment, measurement, and enhancement of RWD quality, research reusability, consistency, reproducibility, and transparency.
"
https://www.turing.ac.uk/news/publications/does-better-governance-guarantee-less-corruption,"
Corruption is an endemic societal problem with profound implications in the development of nations. In combating this issue, cross-national evidence supporting the effectiveness of the rule of law seems at odds with poorly realized outcomes from reforms inspired in such literature. This paper provides an explanation for such contradiction. By taking a computational approach, we develop two methodological novelties into the empirical study of corruption: (1) generating large within-country variation by means of simulation (instead of cross-national data pooling), and (2) accounting for interactions between covariates through a spillover network. The latter (the network), seems responsible for a significant reduction in the effectiveness of the rule of law; especially among the least developed countries. We also find that effectiveness can be boosted by improving complementary policy issues that may lie beyond the governance agenda. Moreover, our simulations suggest that improvements to the rule of law are a necessary yet not sufficient condition to curve corruption.
Guerrero, Omar A and Castañeda Ramos, Gonzalo, Does Better Governance Guarantee Less Corruption? Evidence of Loss in Effectiveness of the Rule of Law (2019). Available: https://arxiv.org/abs/1902.00428
"
https://www.turing.ac.uk/people/external-researchers/sarah-mcgough,"
Sarah McGough is an epidemiologist and data scientist at Roche specialised in machine learning, causal inference and advanced analytics. Her research leverages large volumes of patient-level data to deliver impactful clinical, molecule and disease insights and enable personalised healthcare.

Prior to joining Roche/Genentech, she obtained her PhD and MSc in epidemiology from Harvard University where she focused on outbreak forecasting and real-time public health surveillance using a range of machine learning, statistical, and Bayesian techniques. During this time she was also a graduate researcher in the Computational Epidemiology Lab and Machine Intelligence Lab at Boston Children’s Hospital.
"
https://www.turing.ac.uk/events/global-local-environmental-exploration-data-science-and-ai-innovations,"
World Environment Day (WED) is celebrated annually on 5th June and encourages awareness and action for the protection of the environment.
We invite you to join us in exploring environmental Data Science and AI research and its applications of the present and future. Unpacking the environmental issues that affect us all, the day will feature research into sustainability and the environment.
Topics to be discussed are the existing and prospective challenges we face of our Global to Local Environment Exploration (GLEE) and the critical need to invest in a sustainable future, maximise all our research capabilities and examine the environment we create, work and live in. Please join us to discuss with some of the UK's leading Data Science & AI researchers and create connections in this rapidly expanding, cross-disciplinary landscape.
We encourage all UK Universities wanting to expand their Data Science and AI Network and who are looking to join the Alan Turing Institute University Network in 2023, to join us. You can make connections with colleagues, researchers and meet potential new partners from a national ecosystem and of course, ask questions.
09.30 - 9.40
Open and welcome
Housekeeping and information and days schedule. Please use the QR code for information and the schedule located in event areas
09.40 - 10.15
Keynote Doctor Adam SobeyData Centric Engineering, The Alan Turing Institute
10.15 - 10.45
Keynote Doctor Jonathan Smith
IceNav: risk aware in-ice ship navigation using neutral operator machine learning emulation
10.45 - 11.10
Break
Please vote in the poster competition and visit Frontier
11.10 - 11.50
Keynote Professor Theo Damoulas
Digital Twin of London Air Quality
11.50 - 12.30
Keynote Professor Ruchi Choudhary
Simulation-based methodologies for the decarbonisation of buildings; uncertainty quantification in building simulation models; the integration of novel building technologies and renewable energy supply systems
12.30 - 13.30
Lunch in Radcliffe restaurant
Please vote in the poster competition and visit Frontier
13.30 - 14.10
Keynote Professor Amin Al-Habaibeh
Design and innovation of intelligent systems and sustainable built environment technologies
14.10 - 15.35
Panel Discussion
15.35 - 15.45
Networking opportunity to make connections
15.45 - 16.00
Close
16.00 - 17.00
Garden Summer Punch, Pimm's and Petit Fours in the outdoor spa area
Register now 
 
"
https://www.turing.ac.uk/people/guest-speakers/alessandro-curioni,"
Dr. Alessandro Curioni is an IBM Fellow, Vice President of IBM Research Europe and Africa and Director of the IBM Research lab in Zurich, Switzerland. He is responsible for IBM corporate research in Europe and leads IBM’s global research strategy in Accelerated Discovery and Security. Dr. Curioni is an internationally recognized leader in the area of high-performance computing and computational science, where his innovative thinking and seminal contributions have helped solve some of the most complex scientific and technological problems in healthcare, aerospace, consumer goods and electronics. He was a member of the winning team recognized with the prestigious Gordon Bell Prize in 2013 and 2015. His primary research interests currently include accelerating the rate of discovery with AI, quantum computing, and novel computing paradigms. He is a member of the Swiss Academy of Technical Sciences.
"
https://www.turing.ac.uk/news/publications/tracking-abuse-twitter-against-football-players-2021-22-premier-league-season,"
Online abuse against prominent sportspeople, such as football players, is a growing concern. To help understand this issue, we have launched a new project analysing tweets directed at Premier League Footballers with an account on Twitter. The analysis was run over a period of 165 days (~ 5 months), from the start of the 2021/2022 season (13 August 2021) to the winter break (24 January 2022). We did not analyse online abuse in the Women’s Super League, the highest league of women’s football in England. The dynamics and patterns of abuse experienced by women players require their own interrogation in dedicated research.
Twitter is the focus of this report for three reasons. First, Twitter is a large and widely-used platform, and many Premier League football players are active on it. Second, several players have reported being abused on Twitter before, such as during the Euro 2020 finals, which makes it relevant for this research. Third, unlike most platforms, Twitter makes data available for academic research via its free to use API, making this type of analysis possible. This research is not intended as a reflection or commentary on Twitter’s trust and safety practices. We did not investigate who saw each tweet, how many times they were viewed, how long abusive posts stayed online or what safety measures were applied by the platform.
This report is quali-quantitative in nature, comprising manual review of 3,000 tweets by experts; creation of a new machine learning tool that can automatically assess whether tweets are abusive; and large-scale data analysis of 2.3 million tweets. This report was commissioned by Ofcom as part of the Online Harms Observatory, a new analytics platform from The Alan Turing Institute generously supported by the Department for Digital, Culture, Media and Sport (DCMS).
If you have questions about this report or would like more information about The Alan Turing Institute’s research, reach out to Pica Johansson ([email protected]).
The public policy programme works alongside policy makers to explore how data-driven public service provision and policy innovation might solve long running policy problems and to develop the ethical foundations for the use of data science and artificial intelligence in policy-making. Our aim is to contribute to the Institute's mission – to make great leaps in data science and artificial intelligence research in order to change the world for the better – by developing research, tools, and techniques that have a positive impact on the lives of as many people as possible.
The Online Harms Observatory is a new analytics platform from The Alan Turing Institute’s public policy programme. It combines large-scale data analysis and cutting-edge machine learning developed at the Turing to provide real-time insight into the scope, prevalence and dynamics of harmful content online. It aims to help policymakers, regulators, security services and civil society stakeholders better understand the landscape of online harms. Initially, it will focus on online hate, personal attacks, extremism and misinformation. The Observatory is supported by the Department for Digital, Culture, Media and Sport (DCMS).
This report was commissioned by Ofcom, in relation to its upcoming role as the UK’s Online Safety regulator. It is one output of a larger Turing project utilising the new Online Harms Observatory. In-kind support was given by The Alan Turing Institute.
2.4 MB
"
https://www.turing.ac.uk/people/doctoral-students/naya-yerolemou,"
Naya is a final year doctoral researcher in mathematics at The University of Oxford and The Alan Turing Institute, under the supervision of Professor Vidit Nanda and Professor Ulrike Tillmann. She holds a Masters in Mathematics from The University of Warwick and has previously worked as a software developer at an investment bank. Her research focuses on topological data analysis.   
Naya is a Tutor at St. Catherine's College, Oxford in pure and applied mathematics. She has also been a Teaching Assistant for Category Theory and Topology & Groups.
She organised the Maths in Society speaker series at Oxford, which aims to highlight how advanced mathematics is used to solve problems in areas such as healthcare, the future of energy and urban planning.
Naya is interested in developing new algebraic-topological methods to extract information from datasets across different scientific disciplines. Topological methods offer a unique way to understand qualitative information about the structure of data, which is particularly important when the structure is not well-understood a priori.
Fewer that 1% of know protein structures listed in the Protein Data Bank are known to be knotted, and understanding their folding mechanisms and the functional advantages of the occurrence of knots is an open question in biology. One stream of Naya's research, led by Agnese Barbensi, is applying tools from knot theory to provide topological shape descriptors of these proteins, and to study the topological untangling of such structures.
Naya has also worked with Daniele Celoria on developing a link between discrete Morse theory, combinatorics and knot theory. The main output of this research is a generalisation of two results, one from combinatorics and one from knot theory, captured in a single result called the generalised Clock Theorem. 
Naya worked on a collaboration led by Professor Kathryn Hess in the field of computational neuroscience. Modelling the brain as a directed graph, they are seeking to understand the space of subgraphs explored by the brain after inputting different stimuli. This project combines topological data analysis with network science and was motivated by data and simulations from the Blue Brain Project.
 
"
https://www.turing.ac.uk/events/rsf-seminar-series-good-health-all,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility project. 
Speaker: Dr Alisha Davies, Theme Lead for The Alan Turing Institute’s AI for Science and Government Health (ASG) programme
Event title: Good health for all - What do we need to know about health inequalities?
In England, someone born today in one of the most deprived areas has a life expectancy of 7-9 years shorter than someone born in the most affluent areas. These patterns are similar across other parts of the UK and globally.
Addressing health inequalities recognised as a key priority by the World Health Organisation and within the UN Sustainable Development Goals. “Reducing health inequalities is a matter of fairness and social justice” (Marmot, 2020).
In this session, Alisha will discuss factors that contribute to inequality across the health domain and will address existing challenges and the potential to achieve gains. There will be an opportunity to discuss how we can apply data science and AI to better address this challenge.
Whilst these tools are advancing early detection, prediction and management of multiple long-term conditions and models of personalised medicine, they are progressing slower with regards to health inequality. It is important to question why this is the case, and what opportunities exist in meeting this challenge.
"
https://www.turing.ac.uk/taxonomy/term/1254,"

The Turing-Roche strategic partnership was established in summer 2021. The goal of this five-year partnership in advanced analytics between the two organisations is our North Star: 'enabling the generation of insights to better understand patient and disease heterogeneity and its relevance to clinical outcomes at an unprecedented level of precision in order to improve clinical care'. Our partnership will cover multiple activities to investigate how and why diseases affect people differently and why they may respond differently to treatments.
This knowledge share series aims to bring together members of both Roche and Turing's networks, as well as the wider scientific community, to showcase partnership updates and research, knowledge share and hear different academic and industry perspectives on data science topics to gain insight and help build new connections and collaborations.
This series will be loosely based around areas related to our North Star but we also want to explore other areas of data science that are of mutual interest to Roche, The Alan Turing Institute and beyond – so everyone is welcome.
Join the Turing-Roche Slack Workspace. 
If you are interested in being a speaker at one of our events or want to give feedback on a knowledge share event, then please do get in touch with Vicky Hellon, Community Manager for the partnership ([email protected])
 
"
https://www.turing.ac.uk/people/researchers/james-geddes,"
James is Principal Research Data Scientist at the Turing, leading the data science side of the Group. He is interested in understanding how new ideas in machine learning can be applied to real-world challenges. He is especially keen to find ways to reduce the surprising amount of manual “data wrangling” that all projects seem to need.
James has a PhD in physics from Chicago. He has experience as a data scientist in industry and government, in building analytical models that are transparent and understandable, and teaching data science.
"
https://www.turing.ac.uk/news/publications/how-do-people-feel-about-ai,"
1.9 MB
In late 2022 The Alan Turing Institute and the Ada Lovelace Institute conducted a nationally representative survey of over 4,000 members of the British public, to understand their awareness, experience and attitudes towards different uses of artificial intelligence (AI), including views and expectations on how these technologies should be regulated and governed.
Artificial intelligence (AI) technology and its widespread use in many aspects of daily life is developing at a rapid pace. It is therefore crucial to understand how people currently experience the many applications of AI, including people’s general awareness of these technologies, how they perceive the benefits and concerns, and whether their attitudes differ across key demographic factors. To effectively inform the design of policy responses, it is also important to understand people’s views on how these technologies should be governed and regulated.
The Alan Turing Institute and the Ada Lovelace Institute partnered to conduct a new, nationally representative survey of the British public’s attitudes towards, and experiences of, AI.
Understanding people’s perspectives on data and AI is a key component in ensuring these technologies are developed to align with societal values, and are able to address societal needs. We hope this research will help AI companies and policymakers to understand and address the public’s diverse opinions and attitudes towards AI, so that technology developed in the future works better, for everyone.
The full report can be viewed and downloaded here.
This report was co-authored by The Alan Turing Institute (Professor Helen Margetts, Dr Florence Enock, Miranda Cross) and the Ada Lovelace Institute (Aidan Peppin, Roshni Modhvadia, Anna Colom, Andrew Strait, Octavia Reeve) with substantial input from LSE’s Methodology Department (Professor Patrick Sturgis, Katya Kostadintcheva, Oriol Bosch-Jover)
"
https://www.turing.ac.uk/people/external-researchers/ernst-hafen,"
Ernst Hafen is a Professor of Systems Genetics at ETH Zurich (Institute of Molecular Systems Biology) and former President of ETH. He has made seminal contributions to the field of developmental and cell biology and has received several prizes. In addition to over 30 years of academic research, he has founded and advised several biotechnology companies and is the president of the BIO-TECHNOPARK Schlieren-Zurich. As a trained geneticist, he has a strong interest in human genetics and personalised medicine. He posits, that an individual’s control over his or her personal health data, will be a key asset for better and more effective health care. In 2012, Ernst Hafen acted as a founding member of the Association Data and Health and is the president of MIDATA.coop (https://www.midata.coop), which he co-founded in 2015. Citizen-owned personal data cooperatives enable citizens to securely store, manage and control access to their personal data and form the basis for a fair and sustainable personal data economy.
"
https://www.turing.ac.uk/people/external-researchers/adam-steventon,"
Adam Steventon is Director of Data Analytics at The Health Foundation. He leads the Foundation’s Data Analytics for Better Health strategy, which aims to ensure that everyone in the UK benefits from advances in analytics and data-driven technologies such as artificial intelligence.
Before joining the Health Foundation, Adam was a senior analyst at the Nuffield Trust, where he led some of the first projects bringing together UK health and social care data. In 2013, he was a Harkness Fellow in Health Care Policy and Practice at Yale University.
Adam studied mathematics at Cambridge University and the Massachusetts Institute of Technology. He has a PhD in health services research from the London School of Hygiene & Tropical Medicine.
"
https://www.turing.ac.uk/blog/turing-podcast-how-data-science-and-machine-learning-are-answering-questions-about-human,"
In the latest episode of The Turing Podcast, our hosts Bea Costa Gomes and Ed Chalstrey are joined by Robert Foley, Professor of Human Evolution at the University of Cambridge and a Turing Fellow. The episode focuses on his  ‘Palaeoanalytics’ Turing project, and considers how our understanding of human evolution has changed in recent decades.
 Analysing the stone tools used by ancient humans can provide a multitude of insights into how our ancestors lived and interacted with their environment. However, this analysis is a highly skilled and time-consuming task – it can take over 15 minutes per tool, and some samples consist of hundreds of thousands of tools.
The Palaeoanalytics project is developing AI and machine learning approaches to ‘palaeo-data’ (data from the deep past), which will provide new ways of automatically recognising and categorising the sizes and shapes of stone tools, as well as other prehistoric artefacts such as the fossils of extinct animal and human species.
Podcast guest Robert Foley said: 
“There is a vast amount of evidence, stone tools and of publications on the topic and I wanted to find a way to bring that together. Analysing stone tools is interesting but quite a slow process – what can we do to automate this and up the game?” 

Listen to the podcast here, or subscribe via Spotify, ApplePodcasts, Stitcher, Podbay, Podbean, iHeartRadio or Listen Notes. 
"
https://www.turing.ac.uk/people/research-engineering/david-salvador-jasin,"
David is a Research Data Scientist at the Research Engineering Group. He has a background in mechanical and aeronautical engineering.
During his PhD at Loughborough University (in partnership with Rolls Royce plc), he performed high-fidelity numerical simulations of realistic gas turbine combustion chambers and developed statistical methods for the analysis of fuel-air mixing and smoke emissions.
David also holds a Master of Research in Gas Turbine Aerodynamics from the University of Cambridge. David is interested in the use of data science and machine learning for the development of complex engineering systems.
"
https://www.turing.ac.uk/people/researchers/ben-macarthur,"
Ben MacArthur is professor of quantitative biomedicine at the University of Southampton. In accordance with his interdisiplinary research interests he holds a joint between the Faculty of Medicine and Mathematical Sciences. At the Turing he is Deputy Programme Director for Health and Medical Sciences.
He obtained a PhD in applied mathematics (Southampton, 2003) before training in experimental cell biology, first in the Faculty of Medicine at Southampton (2003-2008) and then at Mount Sinai School of Medicine, USA (2008-2010).
He is a visiting professor at the International Research Centre for Medical Sciences at Kumamoto University, Japan.
Ben's work combines computational, mathematical and experimental methods to better understand human health and disease. In particular, he uses machine learning methods to analyse and combine complex biomedical datasets, including time-series, genomics/proteomics, single cell expression, and heterogeneous clinical datasets. He is also interested in how complex patterns of social interactions (for example, from social networks) interact with physiology to affect human health, and well-being.  His work often combines machine learning with mechanistic mathematical modelling to better understand the causes of any observed patterns.  
 
 
"
https://www.turing.ac.uk/news/publications/uncovering-vote-trading-through-networks-and-computation,"
We develop a new methodological framework for the empirical study of legislative vote trading. Building on the concept of reciprocity in directed weighted networks, our method facilitates the measurement of vote trading on a large scale, while estimating the micro-structure of trades between individual legislators. In principle, it can be applied to a broad variety of voting data and refined for various specific contexts. It allows, for example, to study how vote trading in a specific legislative assembly varies over time. We validate our method with a computational model in which we control the level of vote trading. Finally, we demonstrate our framework in an analysis of four decades of roll call voting in the U.S. Congress.
Guerrero, Omar A and Matter, Ulrich, Uncovering Vote Trading through Networks and Computation (November 4, 2016). Available at SSRN: https://ssrn.com/abstract=2864421 or http://dx.doi.org/10.2139/ssrn.2864421
"
https://www.turing.ac.uk/academics-researchers/,"



In-person tickets are now sold out. You can still secure your place at AI UK through the live stream and watch the fantastic showcase, which will host over 150 expert speakers across three stages: conversation, research and impact. Participants can get involved in sessions by asking questions via Slido, taking part in polls and chatting online with other like-minded data science enthusiasts.
Live streaming AI UK means the flexibility to design your agenda around existing commitments. With an affordable ticket price and a plethora of talks, roundtables and panels, virtual attendance is ideal if you want to stay up to date with the cutting edge of data science and AI.
Head to the Research stage for world-leading keynote speakers, check out the Lightning Talks area for inspirational and thought-provoking flash- talks, or head to the Workshop arena for discussions and debates led by experts, both from the Turing and the wider sector.
Whether you’d prefer to dive in with one of our interactive workshops and roundtables, network with innovators at the live exhibits or settle in at the Research stage to discover the future of data science and machine learning, AI UK 23 offers non-stop opportunities to get involved.
The programme of speakers, discussion chairs and workshop leaders is second to none. World-leading academics, industry pioneers and research innovators from across the vast spectrums of data science, artificial intelligence and machine learning are ready and waiting to engage with delegates and exchange ideas at AI UK 23.
From talks on cutting-edge updates in AI, through live discussions and debates featuring world-leading thinkers, to bite-sized lightning talks and myriad interactive displays of state-of-the-art developments, AI UK provides endless opportunities for immersion in the latest and greatest in UK AI.
With dedicated networking and community-building spaces and events across both days of AI UK 23, there will be plenty of opportunities to meet fellow academics and researchers, as well as leading policy experts from every area of the AI ecosystem.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.


"
https://www.turing.ac.uk/partnering-turing/industry,"

Academia has a track record of supporting the research and development of new products and techniques. The emergence of data science and AI represent a new frontier in scientific and technological innovation, and there remains great potential impact in many sectors of industry, third sector and government.
The Turing, as the centre of a connected network of world-class academic expertise, has an important role to play as a trusted partner in this rapidly-changing scientific landscape.
We have flexible models of engagement, for larger corporates to small enterprises, ranging from one-week Data Study Groups and 3 to 6 months internships, through to multi-year research programmes and targeted collaborative research projects. Guided by our challenge areas and collaboration principles, our engagement with partners and collaborators focuses on an exchange of talent, skills, and knowledge, with industry and academics learning from each other and sharing best practice.
We are interested in challenges that allow data science and AI to be applied to real-world problems, as well as difficult cross-disciplinary problems that have the potential to yield positive transformation across sectors. Our researchers, data scientists, and software engineers can provide novel solutions to industry challenges, drawing upon the latest developments in mathematical, statistical, computational, engineering, and social sciences.
To find out more on how to set up a collaboration email [email protected].
The Alan Turing Institute works with a wide number of organisations, including government, charities, companies and universities to undertake world-leading research to change the world for the better.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/ridgeview,"
www.ridgeview.co.uk
"
https://www.turing.ac.uk/people/guest-speakers/isaac-kohane,"
Isaac Kohane, MD, PhD is the inaugural Chair of the Department of Biomedical Informatics and the Marion V. Nelson Professor of Biomedical Informatics at Harvard Medical School. He develops and applies computational techniques to address disease at multiple scales from whole healthcare systems as living laboratories to the functional genomics of neurodevelopment with a focus on autism.
Kohane's i2b2 project is currently deployed internationally to over 120 major academic health centers to drive discovery research in disease and pharmacovigilance (including providing evidence on drugs which ultimately contributed to boxed warning by the FDA).
Dr. Kohane has published several hundred papers in the medical literature and authored a widely-used book on Microarrays for an Integrative Genomics. He is a member of the Institute of Medicine and the American Society for Clinical Investigation.
"
https://www.turing.ac.uk/people/researchers/deborah-morgan,"
Deborah Morgan is a Research Assistant in the AI for Public Services team, within the public policy programme. She is also a PhD researcher within the Accountable, Responsible and Transparent AI CDT at the University of Bath.
Deborah is interested in AI regulation and her PhD research is exploring the use of regulatory sandboxes for AI. She is also interested in how AI systems can be responsibly used within complex systems of public services. At the Turing, she is supporting research to improve public services and outcomes for citizens through the responsible and effective adoption of AI by government.
"
https://www.turing.ac.uk/events/ai-decarbonisation,"
Photo by Sam LaRussa on Unsplash
The Alan Turing Institute is pleased to host this important and timely online seminar on AI for decarbonisation in collaboration with the Department for Business, Energy and Industrial Strategy (BEIS). 
This morning event connects industry decision makers with world-leading researchers, to explore current efforts in and the potential for AI in decarbonisation. Presentations and discussion will be themed on machine learning in energy industry design, electricity systems, digital twins and transport & logistics. 
Please note this event will take place online, due to the RMT Union Rail Strikes taking place in early November. 

 
Welcome and introduction
Research applications and case studies
Facilitated conversation between audience and researchers
 
 
The event will take place online on a browser-based platform. You must register to receive the joining information. 
Final joining information, including the link to join the event, will be sent the day before to your registered email address only.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-hospitals-birmingham,"
www.uhb.nhs.uk
"
https://www.turing.ac.uk/people/enrichment-students/ilias-moutsopoulos,"
Ilias Moutsopoulos is a first year PhD student reading Bioinformatics at the University of Cambridge, under the supervision of Dr Irina Mohorianu. He holds a BA/MA degree in Mathematics and an MSc in Systems Biology, also from the University of Cambridge. 
Ilias has a breadth of experience working on a variety of projects and research topics, both within and outside academia. He also has a keen interest in helping others through open science, volunteering, and mentoring and always strives to make the world a better place.
Coordinated, flexible, yet robust gene regulation is essential in all organisms and ensures the required adaptability for survival. On a systems level, the collection of all regulatory interactions can be represented/summarised in a gene regulatory network (GRN) of genes, transcription factors, and regulatory RNAs.
Ilias uses a variety of approaches to study and compare these networks by applying supervised and unsupervised learning approaches to tackle some of the current issues of GRN identification and classification. He is particularly interested in the effects of noise, network comparison and integration of information from different systems
"
https://www.turing.ac.uk/people/enrichment-students/aras-selvi,"
Aras Selvi is a PhD candidate at Imperial College London (Department of Analytics and Operations, Business School) as a member of the group “Models and Algorithms for Decision-Making under Uncertainty” supervised by Professor Wolfram Wiesemann. His research interests include robust and distributionally robust optimisation, machine learning, computational privacy, and their intersections. 
Aras is a member of the Computational Optimisation Group (Department of Computing), Data Science Institute, and Imperial Business Analytics. He serves as a reviewer in leading optimisation and machine learning venues including SIAM Journal on Optimization, Management Science, NeurIPS, and ICML. He is also an academic mentor on the Imperial’s Data Spark programme where his past projects used data analytics to improve operational efficiencies and commercial performance for a global energy company as well as derived analytical tools to prevent global nuclear proliferation for a major policy institute. He supervises student-led projects on deriving analytical tools to facilitate the decision-making (under uncertainty) processes of major global non-profit organisations. He works as a teacher or teaching assistant for graduate-level optimisation and machine learning modules at Imperial College London, London School of Economics, and London Business School.
Aras holds a BSc degree from Ozyegin University (Turkey) in Industrial Engineering with a minor in Computer Science, and MRes degrees from Tilburg University (the Netherlands) and Imperial College London, both in Operations Research.
 
Aras is interested in developing efficient algorithms to solve hard optimisation problems arising in privacy, machine learning, and business. 
In his ongoing research projects during his Turing affiliation, he has two main themes.  The first theme is on designing optimal algorithms in differential privacy. He works on research questions including:
-    There are several parameters of query functions used in differential privacy, including the smooth sensitivity, that are NP-hard to compute in many cases. Can one obtain convex optimisation-based tight approximations for them?
-    The literature on the design of differentially private machine learning algorithms relies on the post-processing and composition properties. Can one apply optimisation techniques to rely on these properties as little as possible so that we control the total amount of noise used in the whole process? 
The second theme is on designing distributionally robust machine learning models, especially over Wasserstein balls. In one of his previous works, with his co-authors, he worked on Wasserstein logistic regression with mixed features and showed that although the formulation of this problem amounts to an optimization problem of exponential size, it admits a polynomial-time solution scheme. As there are many machine learning problems whose Wasserstein distributionally robust formulations have stayed intractable so far, Aras is interested in exploiting special structures these problems admit in order to develop efficient solution (or approximation) algorithms.
 
"
https://www.turing.ac.uk/people/external-researchers/simon-cameron,"
"
https://www.turing.ac.uk/events/creative-machine-oxford,"
6.5 MB
Creative Machine Oxford is a prestigious symposium and showcase of the latest creative and research applications of artificial intelligence (AI) at the intersection of AI and creativity across the domains of digital art, neuroscience, scientific discovery and music.
 
The Symposium has been developed by Goldsmiths, University of London, in partnership with Jesus College Oxford, and is generously supported by The Alan Turing Institute. 
The event brings together leading experts from the worlds of computing, AI, art, robotics, and virtual and augmented reality, including speakers from King.com, Deepmind, Oxford Computational Neurosciences, MIT Technology Review, Spotify, and the V&A, for a day of talks and networking opportunities themed around AI and creativity. Explore topics such as ‘AI and visualisation in scientific discovery’, ‘Can AI be creative?’ and ‘Curating AI and digital art’. There will be demonstrations of some of the latest creative AI technologies, and a digital art exhibition, plus a keynote panel discussion on the impact and implications to society of AI in the 21st century, hosted by Professor Sir Nigel Shadbolt, Jesus College Principal and Professorial Research Fellow in Computer Science at the University of Oxford.
Co-Chairs are Prof William Latham, Prof Frederic Fol Leymarie, Dr.Janina Schupp and Dr. Steve Taylor.
The event is free to attend, and open to all members of the Alan Turing Institute.
https://www.creativemachine.io/
Register now
"
https://www.turing.ac.uk/work-turing/turing-university-network-call-2023,"
Applications will be made on behalf of institutions, not individuals, and are therefore limited to one per university.  
The Alan Turing Institute is delighted to invite applications from universities across the UK to join the Turing University Network.  
In line with the goals of the new Institute Strategy, the Turing is building a structured and inclusive university network representative of the wider data science and AI community in the UK. The network provides all UK universities with an interest in data science and AI the opportunity to engage and collaborate both with the Institute and its broader networks.  
Universities who join will be part of a national data science and AI network that does not exist elsewhere in the UK. The network is designed to be open, with minimal barriers to both entry and bureaucracy, and will provide enablers such as mechanisms to connect across the ecosystem, interesting problems and routes to impact.  
The Turing University Network launched as a pilot in April 2023 and we are pleased to now launch an open call for new members. Full information can be found in the call documents below.  
""We are delighted to invite universities from across the UK to join the Turing University Network, as we strive to build a structured and inclusive community, representative of the broader data science and AI landscape. Joining this national network offers universities the opportunity to engage and collaborate with the Institute and its extensive networks, creating a unique platform for innovation and impact in data science and AI.""
Professor Mark Girolami, Chief Scientist, Alan Turing Institute
Applicants wishing to apply must complete the following steps: 
Read the Turing University Network Call Document which includes the eligibility criteria. 
Complete and submit an application by Tuesday 25th July 2023 at 17:00 BST within the Turing’s Flexi-Grant portal. If you have not already done so, all applicants must first register on the system and provide basic details to create a user profile. 
 
We have produced a Turing University Network Call 2023 Frequently Asked Questions document that can be used for reference when completing your application. 
If, after reading the Call Document and FAQs, you have questions about the information listed above please email [email protected].   
 
"
https://www.turing.ac.uk/people/researchers/ivan-tyukin,"
Professor Ivan Tyukin is an expert in the mathematical foundations of Artificial Intelligence (AI) and learning systems, Machine Learning, mathematical modelling, adaptive systems, inverse problems with nonconvex and nonlinear parameterization, data analytics, and computer vision.
Professor Tyukin has been awarded a prestigious Turing AI Acceleration Fellowship to lead innovative and creative AI research with transformative impact. He is a member of the IFAC Technical Committee on Adaptive and Learning Systems, an Editor of Communications in Nonlinear Science and Numerical Simulations, and is a member of the All Party Parliamentary Group on AI's data governance task force which examines the economic, social, and ethical implications of AI. He has a broad network of academic and industrial collaborators includes strategic UK sectors such as public safety and security, health technologies, space and Earth Observation, and manufacturing.
Professor Tyukin’s recent work focuses on creating a theory and practice for developing AI systems that are provably robust, resilient, certifiable, trustworthy, human-centric and data-driven. The theory will enable the creation of new-generation AI systems which are certifiably stable, secure, adaptive and maintainable. These systems will be prepared to handle the challenges of adversarial attacks and data inconsistences, uncertainties, and bias inherent within any empirical data. This will enable new gold standard methods and tools in tasks that are currently heavily reliant upon non-deterministic human input to have long-term transformation.

"
https://www.turing.ac.uk/node/1793,"
"
https://www.turing.ac.uk/research/research-projects/ai-control-problems,"
Complex control problems arise in the operation of critical infrastructure, including electricity, gas, water and transportation. This project is developing ‘rangl’, an AI competition environment for practitioners (both novice and experienced) to apply classical and machine learning techniques and expert knowledge to UK-centric problems. The project will work with industrial challenge holders to develop insights into the leading classes of solution.
Infrastructure systems, including electricity, gas, water and transportation, must operate reliably at acceptable cost in the context of ageing infrastructure and new technological possibilities. At the same time a greater amount of measurement and forecast data is becoming available. There is a growing need for appropriate AI controllers to leverage this data. Artificial intelligence, with its speed, scale and accuracy, offers transformative potential in applications to these problems. However, controllers for critical infrastructure should be robustly assessed in an appropriate simulation environment.
This project is developing a competition platform in which challenge environments are formulated in the reinforcement learning framework using the “agent-environment loop”. At each timestep the controller (agent) chooses an action based on the current observation, and the challenge environment returns a new observation and a reward. The aim is to create controllers with intelligent characteristics, capable of handling both quantifiable and unquantifiable uncertainty and encoding expert knowledge. Competition entrants document their controllers, helping to develop insights into the leading classes of solution.
'RangL' is a competition platform created at The Alan Turing Institute as a new model of collaboration between academia and industry. Through integration with OpenAI Gym, rangl offers a user-friendly environment to develop learning approaches to data-driven control problems. Anybody can propose a rangl challenge, compete in a challenge by designing a controller, or contribute an ‘off-the-shelf’ AI controller for users to customise.
The platforms assess user-submitted algorithms for specific tasks, helping the best classes of solution to emerge; a proven mechanism for realising the potential of AI. 
1 March 2022 – Pathways to Net Zero winners announced
The Pathways to Net Zero RangL challenge took place from 17 to 31 January 2022. After considering both the leaderboard and the executive summaries, the selection panel chose three joint winners:
Additionally, Team AIM-Mate were highly commended for their efforts. The Net Zero Technology Centre, who sponsored the challenge, will hold a webinar on 28th March 2022 with the winning teams. The final leaderboards can be viewed at the challenge repository. Thank you to all who participated.
22 November 2021 – Qualitative and quantitative evaluation
As the environment had reached a suitable state of development, this week the group discussed possible evaluation criteria for the challenge. The RangL project aims to collect best practice in the application of RL (and also other optimisation approaches) to control problems in industry. Thus while it would be possible to use only leaderboard scores, we decided to ask participants to submit a one-page executive summary of their approach, which would be considered in the overall evaluation.
8 November 2021 – Lagrangian formulation
In RL agents learn from experience. However if rewards occur only after a long sequence of actions, it can be difficult for an RL agent to associate this long sequence with the eventual reward. An example is a game of chess, if the reward is simply 1 for winning and 0 for losing. Similarly, if problem constraints are handled by awarding a large negative penalty when a constraint is breached, this can make reinforcement learning challenging. To address this, the group experimented with a Lagrangian reformulation to transform the constrained problem to an unconstrained one by modifying the reward function. As a result, the constraint on job numbers was replaced by adding a reward term proportional to the number of jobs created at each step.
18 October 2021 – Correlations 
Typically a spreadsheet will not include any model of randomness – in other words, it is deterministic. In reinforcement learning it can be straightforward to include randomness, and this is one of the strengths of RL.  
We will leave the choice between deterministic (without randomness) and stochastic (with randomness) modelling for another blog post. The interesting point for us this week was correlations: how we describe the extent to which random factors tend to move together. In deterministic modelling there is simply no need to think about how, for example, the prices of natural gas price and blue hydrogen (which is derived from natural gas) move together. In contrast, in stochastic modelling different correlations could even drive different solutions. Fortunately, in this project we have the luxury of discussions with some of the creators of the Integrated Energy Vision (the spreadsheet model on which the challenge is based), so correlations can be chosen in an informed way. 
4 October 2021 – Direct actions 
Today the group agreed a modification to the challenge outline: the RL agent will now directly specify the rate of deployment of offshore wind, blue and green hydrogen. This is achieved by allowing the RL environment to interact directly and repeatedly with the IEV spreadsheet model, both simplifying the approach and increasing its transparency and interpretability. Initial results from training an RL agent with this environment were shared and sense-checked. 
20 September 2021 – Project video
Having decided the general shape of the challenge, today the project group agreed to begin working in an agile fashion through a GitHub project board. 
Excitingly, RangL was also invited to be part of the Net Zero Technology Centre's virtual showcase ""Road to Glasgow: Destination Net Zero"" at the 26th UN Climate Change Conference of the Parties (COP26) in Glasgow in November 2021. A virtual exhibition booth will include ""meet the developers"" live sessions and a project video explaining the Pathways to Net Zero challenge.  
19 July 2021 – Reward functions 
In Reinforcement Learning the agent learns to maximise the rewards it receives. In this way the reward function is an integral part of the problem statement, and this week’s efforts centred around finalizing its exact form. Given the aims of the study, the project group decided to include the cost of total carbon emissions in the reward alongside UK energy sector profits (that is, energy revenues minus capital, operating and decommissioning costs). 
The reward function can also be used to place constraints on acceptable solutions. While the shift to zero-carbon technologies can lead to increased employment in the long term under our modelling, it is important to ensure that job numbers are also managed in the short and medium term. 
It was agreed that recent volatility in energy market prices highlights the importance of incorporating randomness in the RL environment. In addition to reflecting real-world uncertainty, this allows the agent to learn to adapt under a variety of scenarios. 
1 July 2021 – Pathways to Net Zero RL environment
RangL aims at applying reinforcement learning (RL) to solve real-world industrial problems by involving participants from the wide AI community. Today, our focus was therefore on developing an appropriate RL environment for the Pathways to Net Zero challenge. 
The objective is to find optimal deployments for technologies such as offshore wind, blue and green hydrogen, and carbon capture and storage. These technologies will be instrumental in reaching the UK’s target of net zero carbon by 2050.
After brainstorming we opted to take Breeze, Gale and Storm as baseline scenarios from which others can be built. An agent will interact with the RL environment by choosing a mix of those scenarios and also by varying the speed with which they are implemented. For instance, earlier deployment reduces lifetime emissions but generally implies higher capital costs. Solutions will also need to meet some non-monetary constraints, e.g. balancing job creation in new technologies against the loss of roles in decommissioned infrastructure. We will also work with the Net Zero Technology Centre and ORE to extend the Integrated Energy Vision appropriately, so that lifetime emissions and their social cost can be considered in the RL reward function.
21 June 2021 – Pathways to Net Zero project kick-off
The RangL competition platform exists to accelerate progress in data-driven control problems for industry, and today marks the real beginning of that journey.
The purpose of the Net Zero Technology Centre is to develop and deploy technology for an affordable net zero energy industry, and the Offshore Renewable Energy (ORE) Catapult is the UK’s leading technology innovation and research centre for offshore renewable energy.
Today, colleagues from the Net Zero Technology Centre, ORE Catapult and RangL team gathered to make a start on the Pathways to Net Zero challenge. The agenda was focused on first introducing the competition platform and then understanding the Net Zero Technology Centre / ORE Catapult Integrated Energy Vision (IEV) model, on which the challenge will be based.
The IEV is the result of a major modelling exercise undertaken collaboratively by the Net Zero Technology Centre and ORE Catapult, and addresses the UK's vision of achieving net zero carbon emissions by 2050 for the North Sea offshore energy industry. The range of possibilities is illustrated by three imagined pathways, Breeze, Gale and Storm, each addressing the four main technology pillars of offshore energy: offshore wind, oil and gas, hydrogen, and carbon capture and storage (CCS).
The Pathways to Net Zero challenge aims to build on the IEV by first allowing a range of intermediate pathways between Breeze, Gale and Storm, then defining a criterion to measure the quality of each pathway in a specific sense. Challenge participants will be invited to apply reinforcement learning, or any other approach of their choice, to find the ‘best’ pathway. The challenge will be made more realistic and difficult by the inclusion of uncertainty over future parameters such as energy revenues and technological progress.
18–25 January 2021 – First RangL Challenge
From 18 to 25 January 2021 the RangL team fulfilled a long-held ambition: to run a generation scheduling challenge. The problem involves using continually updated forecasts for energy demand and renewable energy generation to schedule, and so to minimise, the use of fossil fuels. It is challenging partly because the observation space is large — at each step, the agent is given forecasts for all time periods — and also because the forecasts are updated as new information arrives, so are guaranteed to be superseded by better ones.
This ‘look-ahead mode’ generation scheduling was one of the first motivations for RangL, when the project was conceived in early 2019 during the Mathematics of Energy Systems research programme at the Isaac Newton Institute in Cambridge. While not directly connected, it’s interesting to note that the forthcoming special issue of Philosophical Transactions of the Royal Society A based on the MES programme has an article by Peter Glynn and Jacques de Chalendar on theoretical aspects of this kind of problem (titled “On incorporating forecasts into linear state space model Markov decision processes”).
The competition itself was heavily oversubscribed, with applicants from Argentina, Denmark, the Netherlands, Italy, France and the UK, drawn from academia, industry and the third sector. We’d like to thank all participating teams, who generated a fantastic atmosphere on our Slack channel throughout the week. It must have been good, as one competitor even joined the RangL team. The winners were team zeepkist with members from the Intelligent Electrical Power Grids group at TU Delft and Tennet, the Dutch power system operator. The final scores, and zeepkist’s winning code (which used RL), are here in the challenge repository.
We recently argued on the Turing blog that as the world reopens following the pandemic, we will need to make more flexible, responsive and data-driven decisions. Hopefully this first challenge illustrates a small part of the potential role that reinforcement learning can play.
"
https://www.turing.ac.uk/people/guest-speakers/arnoldo-frigessi,"
Dr. Arnoldo Frigessi is director of the Oslo Center for Biostatistics and Epidemiology and the center for research-based innovation 'Big Insight' a consortium of partners from academia and the public and private sectors.
Frigessi develops new methods and stochastic models to study principles, dynamics and patterns of complex dependence. Currently, he has research collaborations in genomics, personalised medicine, infectious disease modeling including COVID-19 and preference learning.
Frigessi is elected member of the Norwegian Academy of Sciences and Letters and knighted Cavaliere Ordine al Merito della Repubblica Italiana.
 
"
https://www.turing.ac.uk/people/external-researchers/desmond-higham,"
Des Higham is Professor of Numerical Analysis in the School of Mathematics at the University of Edinburgh.
He has research interests in stochastic computation, with applications in network science, data science and machine learning.
He is a SIAM Fellow and editor-in-chief of SIAM Review.
"
https://www.turing.ac.uk/people/guest-speakers/susan-morrison,"
Susan Morrison is a writer, broadcaster and comedian with two grown-up children and a very patient husband. She's rubbish at organising, has a complete lack of self-discipline, is utterly terrible at most things house-work related, but she does like ironing. Denies she is obsessed with the sinking of the RMS Titanic, but she is.
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-modern-topics-missing-data,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
This event will be diving into the topic of missing data- a common problem that arises in many fields that can significantly complicate analysis. 
During this event you will hear from Dr Robin Mitra, Senior Lecturer in Mathematics from Cardiff University and Dr Chris Harbron, Expert Statistical Scientist and Advanced Analytics Lead at Roche.
Robin will be giving us an introduction to missing data, what problems it can cause and how we can deal with it. An illustration will also be provided, involving a study on survival times after receiving an organ transplant.
Chris will introduce us to the concept of 'structured missingness', lay out some of the main challenges and emerging areas for research around this concept and share some of the work the partnership has started in characterising the different types of missing data.
As well as having a Q&A at the event, we hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace which you can join here. 
You can watch a recording of this event here. 

Registration is now closed.
By registering for the event, you are agreeing to the events code of conduct: Events code of conduct | The Alan Turing Institute
"
https://www.turing.ac.uk/people/researchers/sabeehah-mahomed,"
Sabeehah Mahomed is a Research Assistant in Data Justice and Global Ethical Futures under the Ethic’s Theme of the Turing’s Public Policy programme. She currently works across projects on the ethical and responsible design of artificial intelligence (AI) technologies. These include researching and analysing the context of children’s rights as they relate to AI through a series of engagements with schools across Scotland as well as projects with the UK’s Office for Artificial Intelligence and the Council for Europe.
Sabeehah holds a MSc in Digital Humanities (distinction) from the Department of Information Science/Studies at University College London (UCL). Her master’s thesis focused on public awareness and perception of racial bias in AI systems, which was presented at the 'Ei4Ai' (Ethical Innovation for Artificial Intelligence) Conference in July 2020, hosted by UCL and University of Toronto. Sabeehah also has a postgraduate Bachelor of Arts Honours Degree (Cum Laude) and a BA Degree (distinction) from the University of South Africa and University of Johannesburg. 
Sabeehah's main interests lie in the ethical governance and innovation of AI systems, data privacy & protection, and using technology for social good. She brings an interdisciplinary academic background with a keen focus on the humanities, human rights, and societal issues particularly for vulnerable, marginalised and minority groups.
Prior to joining the Turing, she conducted fieldwork research within the Somali migrant community in South Africa focusing on social dynamics as it relates to religion, integration, and international relations. This was later published by the Institute for Global Dialogue (IGD). Sabeehah also has experience in entrepreneurship, business, and education where she established a Mathematics education centre in South Africa, providing quality education to over 2000 students for 7 years and creating employment for over 40 individuals. 
 
A selection of appearances and talks:
"
https://www.turing.ac.uk/events/complexity-and-token-economy,"
53.3 KB
Cryptocurrencies are currently the most developed example of the forthcoming Token Economy. While most of the attention has been so far on the design aspect of the next token, surprisingly little is known about the dynamics generated by the interaction between developers, users, traders and regulators of cryptocurrencies. Complex Systems Science offers the conceptual and technical tools to make sense of the emergent behaviour of the cryptocurrency ecosystem, and to anticipate the potential feedback loops between the latter and the broader society.
 
Please note that this event is not managed by The Alan Turing Institute directly; the registration link will direct you to a third-party page. Please address any queries to Andrea Baronchelli.
The event will bring together experts from academia and the industry to talk about how Complexity can help make sense of what is happening in the cryptocurrency ecosystem, and how open data from cryptocurrencies can help test and advance theories of Complexity. The event will include a number of invited talks aimed to be accessible to the intended audience of researchers and practitioners from academia, industry, and public agencies.
City, University of London 

Northampton Square
London
EC1V 0HB

"
https://www.turing.ac.uk/research/impact-stories/optimising-worlds-first-underground-farm,"
This is an edited version of a piece first published by the University of Cambridge in March 2021.
Thirty-three metres below London’s Clapham High Street is the world’s first underground farm. It’s shaping the future of urban farming.
Stacked racks of fresh green leaves thrive under banks of LED lights – peashoots, basil, coriander, parsley, salad rocket, pink radish, mustard plants – the fragrance of the ‘microgreens’ filling a former World War Two air raid shelter under south London.
A post-war plan to join the tunnels to the London Underground system never happened and, in 2015, the deserted subterranean space sprouted new life when co-founders Richard Ballard and Steve Dring decided it was a perfect site to grow food while reducing the carbon footprint of transport and supply.
An unheated tunnel without natural light seems an unlikely place for a farm and yet plants grow extremely well here – yielding a harvest roughly every 10 days all year long. Either side of the 2.5m-high passage, the plants grow without soil (‘hydroponically’) on wool carpet cut-offs that were destined for landfill, and watered from tanks housed on a lower level.
“Growing Underground is a farm that feeds the city from within the city,” explains Ballard. “We sow, pack and grow on site, taking the harvest to New Covent Garden Market less than a mile away for distribution across the capital, reducing food miles, pollution and food waste.”
Zero-carbon food is at the core of the Growing Underground vision – which means paying close attention to what happens to the plants below ground as well as above ground.
Down in the tunnels, a team of engineers and data specialists has been helping the farmers to optimise crop performance and reduce energy use. They are led by Ruchi Choudhary, a Group Leader in the Turing’s data-centric engineering programme who is also Reader in Architectural Engineering at the University of Cambridge’s Department of Engineering.
Together, they’ve reduced the time it takes to grow some crops by 50% and all crops by an average of 7%, and increased yields by 24%. Meanwhile, the crops are grown using less space and water than conventional greenhouse growing, no pesticides and 100% renewable energy.
This can only happen if every element of the farming process is carefully measured and tweaked and measured again. The plants on this farm get exactly what they need at every moment of every day thanks to the power of data – and a digital twin looking out for its sibling from a laboratory in Cambridge.
The underground farm grows 12 times more per unit area than a traditional greenhouse grows in the UK, but farming crops without sunlight is a much more energy-intensive process.
From the outset, Ballard and Dring knew that for the enterprise to live up to their sustainable vision, they needed not just to use 100% renewable energy but also to find ways to cut energy use while maximising crop growth.
“Whenever we saw an opportunity to mash two potatoes with one fork, we did it. We want to be sustainable at every stage of the process. Optimising crop performance for the energy we used seemed an obvious step forward,” says Ballard.
“Smart farming was a new area for us and we were one of the first people to do it in this way – so having Cambridge and the Turing really pushing forward on the data side really sold us on the benefits of capturing and using information to solve some of the sustainability challenges we faced.”
Four years ago, Melanie Jans-Singh – a PhD student at Cambridge’s Department of Engineering – began installing sensors in the tunnels to capture everything you might ever want to know about how this garden grows. Back in Cambridge, she and colleagues set about building a digital twin.
“From day one, Richard and Steve bet on data to help them, and we’ve assisted them from the start of their data journey,” explains Jans-Singh, who was working with Ruchi Choudhary’s ‘Digital twins for the built environment’ group at the Turing. “We learned about their farm at the same time as they did.”
There are 25 sensors measuring 89 variables transmitting to eight Raspberry Pi loggers in the tunnels. The data is warehoused on a server in Cambridge and sent over Wi-Fi to an online horticultural data platform.
Nutrients, water, lights, heat, CO2, airflow, humidity. Even crop growth is tracked minutely by the farm operators. All of this is measured and compared with how the crops perform and then represented in a virtual 3D representation of what’s happening on the ground.
“This virtual representation is the digital twin,” she explains. “What the digital twin shows is better than being in the tunnel in person – it can monitor, learn, feed back and forecast information that will make the real-life twin work better.”
“Having Cambridge and the Turing pushing forward on the data side really sold us on the benefits of using information to solve the sustainability challenges we faced.”
Richard Ballard, co-founder of Growing Underground
At the same time, Turing and Cambridge research associate Rebecca Ward set about analysing the ‘physics of the farm’ – everything from the transfer of heat through the tunnel to the way plants use it to transpire, releasing water through their leaves by evaporation.
“A model based on data can only inform on what the data shows,” explains Ward. “But we wanted the digital twin to cope with unexpected conditions that had never been encountered before – like a very hot day outside. We’ve accomplished this by linking the data to a model based on the laws of physics.”
“We found that the tunnel environment was mainly affected by external weather through the extraction fans and the LED lights, which together consume 95% of electricity,” explains Jans-Singh. “We help the growers tweak conditions – such as tailoring light and heat use – so that the plants grow the best in terms of taste, nutrients and yield with least waste of resources.”
If there is any doubt about how the plants are growing at a certain spot, the researchers can look at their measurements of the different environmental variables to identify the precise conditions in that area. When the plants are growing better in one place than another, the instrumentation helps them to work out why.
The digital twin doesn’t just help with day-to-day decision-making. Its added value over a simple monitoring and visualisation system is the fact it can integrate forecasting models, suggest operational changes for the day ahead and then feed back on how this went.
“The farm managers tend to check the dashboard and sensor data at the beginning and end of the workday,” says Jans-Singh. “So we’ve set the system to provide a forecast at 4am ready for the first workers arriving at 6am to help with decision-making for the day, and at 4pm to warn about possible conditions that will happen overnight just before the workers leave. This might mean reducing ventilation if the farm is likely to be too cold, temporarily adding a heater in a specific location, or trialling different light settings.”
“All the time we’re trying to improve yield but while we’re doing this we can tweak different elements of the light spectrum to make plants grow in a different way to increase sugars and starches, and therefore their nutritional value,” adds Ballard.
The researchers work closely with Growing Underground’s Data Platform Lead Jakob Thomas, who explained that the digital twin has another very important use: it’s guiding the growers in how to build another farm – this time in a disused warehouse that will supply an even larger area of London.
“The digital twin allows us to simulate a hypothetical farm so well that in the engineering and planning phase we can pinpoint much more precisely what equipment we need to keep the environment stable and use the least amount of energy,” he explains.
“That’s pretty important because the systems that are required for heating, ventilation and cooling are quite expensive to build in the first place. We want to be sure we don’t overinvest in equipment that is overpowered and producing unnecessary carbon for the space, but we also want to prevent underinvesting and needing to retrofit.”
“Digital twins are increasingly being considered for optimising energy use in agriculture but our study is the first time a digital twin has been shown to mirror an urban farm through real-time streams of data,” says project leader Ruchi Choudhary, whose research group at the Turing is funded by both the Lloyd’s Register Foundation and UK Research and Innovation (via the Turing’s ‘AI for science and government’ (ASG) programme). “We’re using open-source software and economical platforms like Raspberry Pi so that our system will be transferrable to other bespoke environments.”
Choudhary is an expert at integrating data and models to support the energy efficiency of urban built environments: “As awareness of the need for sustainable food production has risen, so has the drive to incorporate green infrastructure within cities. Urban integrated farming offers an excellent mechanism to contribute to meeting the UK targets of net zero carbon emissions by 2050.”
The World Resources Institute estimates that feeding 10 billion people sustainably by 2050 will require closing a 56% food gap between crop calories produced in 2010 and those needed in 2050. Part of the answer must be to farm in non-traditional spaces, says Ballard.
“Over the next few years we’ll see an exponential growth in the tech behind LEDs, Internet of Things, cloud computing and wireless sensors. But the real game changer will be cheap renewable energy and battery storage. Then we’ll really start to see large vertical farms in and around urban areas growing staples like wheat and maize.
“We realise that growing microgreens won’t address world hunger but we feel that what we are doing is a step in the right direction of food security.”
Ward agrees: “It’s the ambition of the business that’s so fantastic: to grow crops in the heart of the city, get rid of all those food miles, and do it in an efficient way with minimum waste. Anything we can do to try and help them achieve their ambition is just great.”
The team has also valued the chance to see research and a real-world context working together, as Thomas explains: “Connecting the tremendous amount of academic work that goes into creating something like a digital twin to the practice of providing high-quality food locally and sustainably has been really rewarding for the whole team. We learn from each other.”
As the growers continue with their plans to create other farms, they are justifiably proud of what they have achieved so far. They also know there can’t be many farmers who can answer the nursery line ‘how does your garden grow?’ with the words ‘with Raspberry Pi, a digital twin and sensors all in a row.’
 
Further reading:
 
Top image: Betsy McCue / Zero Carbon Farms
"
https://www.turing.ac.uk/people/research-engineering/katriona-goldmann,"
Katriona is a research data scientist at the Alan Turing Institute. She previously completed a PhD at Queen Mary University of London whereby she developed personalised medicine models in autoimmune diseases. During her PhD, she also participated in the Turing's enrichment scheme to develop her work in machine learning with collaborators at the institution. 
Prior to her doctoral studies, Katriona also studied Astrophysics (MPhys) at the University of Edinburgh and Biomedical Engineering (MSc) at the University of Strathclyde.
Katriona is a member of the Research Engineering Group (REG) at the Turing Institute. While her background is in physics and bioinformatics, she now works on monitoring biodiversity using AI in collaboration with the UK Centre for Ecology & Hydrology. Her work focuses on using computer vision and edge processing to develop on-device machine learning models to accurately classify species of insects in the field.
"
https://www.turing.ac.uk/events/secure-computation-rams-revisiting-square-root-oram-and-low-leakage-secure-boolean-queries,"
​
Speaker: Mariana Raykova (Yale University, USA) Date: 22 August 2017 Time: 14:00 - 15:00 Venue: The Alan Turing Institute.  Watch event video online.
Hiding memory access patterns is required for secure computation, but remains prohibitively expensive for many interesting applications. This talk presents two works addressing this question: a new oblivious RAM (ORAM) construction and a secure computation scheme using ORAM in the context of Boolean database queries. Many prior works have developed ORAM solutions, which provide asymptotic benefits for secure computation, but require large database sizes to improve the concrete efficiency of naïve linear scan. The talk presents a new construction that shows how the classical square-root ORAM of Goldreich and Ostrovsky can be modified to overcome these problems, even though it is asymptotically worse than the best known schemes. The presentation also covers a new encrypted search scheme that reduces the leakage of current Boolean queries solutions. This solution is based on a hybrid approach, which integrates ORAM techniques with the efficient search index structure of the Blind Seer system. Biography Mariana Raykova is an Assistant Professor at Yale University with research focus in cryptography and security. She obtained her PhD from Columbia University in 2012 and was a postdoc at the cryptography group at IBM Research Watson. Prior to joining Yale she was a research scientist for two years at SRI. Her work develops new cryptographic techniques that facilitate the use of private data while protecting the privacy of the data and guaranteeing verifiability. Her research interests include secure computation, verifiable computation and obfuscation. Beyond constructing new cryptographic protocols, her work also focuses on implementations that leverage the newly developed cryptographic tools and optimize the efficiency of the resulting systems both at algorithmic and systems level aiming for realistic loads.
​
"
https://www.turing.ac.uk/news/announcing-ai-uk-2021-celebration-uks-ai-talent,"
Nearly a year has passed since we had to cancel the first ever AI UK conference because of the COVID-19 crisis. At The Alan Turing Institute, we’d spent a year planning the event, lining up the very best in UK AI talent, so the decision was made with heavy hearts.
But we are delighted to be able to announce the 2021 AI UK conference, taking place on 23-24 March 2021. The event will be virtual, but the idea remains the same: to showcase the UK’s most exciting and pioneering research in the science and applications of AI and machine learning.
For all the tremendous difficulties of the past year, AI research has continued to develop apace, and its emerging applications have continued to astound us. In June 2020, we saw the release of GPT-3, a language model from the OpenAI research lab that uses deep learning to generate human-like text that gives even the most sceptical of AI pundits pause for thought. In October, autonomous car company Waymo announced that it was opening its fully driverless taxi service to the public. And in November, DeepMind’s AlphaFold system made huge progress on protein folding – one of the most important practical problems in biology.
The pandemic itself has seen a flurry of work by AI researchers and data scientists, including a host of projects at the Turing. And the crisis has essentially made us all data scientists, scrambling to make sense of every day’s mass of statistics and news reports as we try to figure out what they mean for our families and communities.
AI UK will reflect on the role that AI has played in the scientific response to the pandemic. It will explore the new ideas and directions that have emerged in AI, and how these will shape our post-pandemic world.
The world today is unimaginably different to the world in which we planned last year’s event. But AI remains now, as it did before the pandemic, the most exciting and important area of contemporary science. AI UK will be an essential event for understanding the current state of the UK’s AI research, and where it will be heading over the next decade.
 
"
https://www.turing.ac.uk/people/enrichment-students/thao-do-ngoc,"
Thao is a computational social scientist and development professional with research interests focused on governance and public policy, ICT4D, ethics of AI, digital justice, human trafficking and child protection. She is currently a PhD student at the ART-AI centre, University of Bath. Her research examines the accountable, responsible, and transparent usage of AI in anti-trafficking tools, supervised by Dr Emma Carmel, Dr Alinka Gearon, and Dr Julian Padget. She holds an MA in Governance and Development at the Institute of Development Studies, University of Sussex, UK.  
She has ten years of experience in the international development and humanitarian response sector, working with grassroots organisations, INGOs, UN agencies, and research institutes. She serves in advisory, research and director roles in programs supported and funded by various agencies such as FCDO, IDS, UNICEF, UNHCR, and SIDA. She is also leading a development initiative in Vietnam - Share for VN Development – a platform for sharing knowledge and experiences for development practitioners and scholars in the Development studies field in Vietnam. She has worked on different development projects in the US, UK, India, Tunisia, Cambodia, Vietnam, and Uganda.
With her interdisciplinary background in International Development and AI, she is interested in exploring the potential of data science and AI in researching and resolving social development issues as well as their ethical and responsible usage in global South context, especially concerning the marginalised and underserved population.  
Thao’s research aims to evaluate the effectiveness and societal, ethical and social implications of existing AI-based anti-trafficking tools based on an analytical framework built on AI ethics and human rights standards. Using Vietnam as a case study, she is working towards assessing and enhancing the application of data analytics and AI techniques in anti-trafficking tools, considering the socio-economic, cultural, and linguistic characteristics of the global South context. The results will provide valuable insights into her recommendations for designing and implementing an accountable, responsible, transparent, victim-centred, and context-driven AI-based human trafficking response system. 
Joining the interdisciplinary team at the Turing community, Thao is excited to learn more about the latest data analytics and AI techniques and regulatory frameworks on data governance and AI ethics standards in the anti-trafficking field. During her time at the institute, she aims to develop a prototype of a language model to detect child trafficking in Vietnam by analysing information from social media and online newspapers in Vietnamese languages. Having hands-on experience working directly with training data and algorithms, she hopes to gain insights into the sources of potential risks and harms associated with each technique. The empirical evidence could provide insightful inputs for designing and deploying participatory and context-driven AI tools in less-resourced contexts and contribute to the emerging literature on AI ethics in the global South context.
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/databases,"
"
https://www.turing.ac.uk/people/guest-speakers/michele-ceriotti,"
Michele Ceriotti received his Ph.D. in Physics from ETH Zürich. He spent three years in Oxford as a Junior Research Fellow at Merton College. Since 2013 he leads the Laboratory for Computational Science and Modeling, in the institute of Materials at EPFL, that focuses on method development for atomistic materials modeling based on statistical mechanics and machine learning. 
Michele is one of the lead developers of several open-source software packages, including http://ipi-code.org and http://chemiscope.org, and proudly serves the atomistic modeling community as an associate editor of the Journal of Chemical Physics, as a moderator of the physics.chem-ph section of the arXiv and as an editorial board member of Physical Review Materials. 
"
https://www.turing.ac.uk/news/publications/optimal-thinning-mcmc-output,"
The use of heuristics to assess the convergence and compress the output of Markov chain Monte Carlo can be sub-optimal in terms of the empirical approximations that are produced. Typically a number of the initial states are attributed to ""burn in"" and removed, whilst the remainder of the chain is ""thinned"" if compression is also required. In this paper we consider the problem of retrospectively selecting a subset of states, of fixed cardinality, from the sample path such that the approximation provided by their empirical distribution is close to optimal. A novel method is proposed, based on greedy minimisation of a kernel Stein discrepancy, that is suitable for problems where heavy compression is required. Theoretical results guarantee consistency of the method and its effectiveness is demonstrated in the challenging context of parameter inference for ordinary differential equations. Software is available in the Stein Thinning package in both Python and MATLAB.
Riabiz M, Chen WY, Cockayne J, Swietach P, Niederer SA, Mackey L, Oates CJ. (2020) Optimal Thinning of MCMC Output. arXiv:2005.03952.
"
https://www.turing.ac.uk/news/publications/key-challenges-participatory-governance-ai-public-administration,"
As artificial intelligence (AI) becomes increasingly embedded in government operations, retaining democratic control over these technologies is becoming ever more crucial for mitigating potential biases or lack of transparency. However, while much has been written about the need to involve citizens in AI deployment in public administration, little is known about how democratic control of these technologies works in practice. This chapter proposes to address this gap through participatory governance, a subset of governance theory that emphasises democratic engagement, in particular through deliberative practices. We begin by introducing the opportunities and challenges the AI use in government poses. Next, we outline the dimensions of participatory governance and introduce an exploratory framework which can be adopted in the AI implementation process. Finally, we explore how these considerations can be applied to AI governance in public bureaucracies. We conclude by outlining future directions in the study of AI systems governance in government.
Wong, J., Morgan, D., Straub, V. J., Hashem, Y., & Bright, J. (2022). Key challenges for the participatory governance of AI in public administration. https://doi.org/10.31235/osf.io/pdcrm
 
420.4 KB
"
https://www.turing.ac.uk/research/research-areas/algorithms/complexity-algorithms,"
"
https://www.turing.ac.uk/events/workshop-firm-level-supply-networks,"
Supply chains underpin much of the functioning of the economy, including critical sectors such as food, medicine, defense and engineering. Increased complexity, the adoption of lean management principles, and interdependence have resulted in increased vulnerability to disruptions that often spread through global networks, halting the flow of materials and services. Supply chain risk has been identiﬁed as a top business risk factor in a variety of industrial sectors and has been prioritised by numerous governments.
The climate crisis and increased geopolitical instability are expected to exacerbate the challenges faced by production networks. Although there are already long-established academic fields that investigate supply networks, research is scattered across multiple disciplines, ranging from Manufacturing Engineering and Supply Chain Management to Complex Systems and Macroeconomics. To tackle modern challenges, however, the models that focus on understanding the supply chains of a single firm are too constrained in scope, and traditional macroeconomic models are too aggregate.
We are specifically interested in reconstructing firm-level supply networks, so that awareness can be generated both at the micro (firm) level and at the macro (country) level. Underpinning this exploration is an understanding of datasets, data science methods and how they complement each other. For example how can longitudinal datasets can be used alongside periodic reconstruction.   
In this interdisciplinary workshop, we will explore how “micro” and “macro” perspectives on firm-level supply networks can be reconciled, providing the foundations for a research agenda devoted to the development of realistic models based on global firm-level supply networks datasets. We are specifically interested in reconstructing firm-level supply networks when data is unavailable, by relying on an understanding of data sources, data science methods, and domain knowledge in supply chain management and economics.
The event will kick off with speakers giving different perspectives on data availability, followed by technical talks on reconstruction methods. We will then discuss how reconstructed networks can be used to inform practitioners, for example to prepare for different scenarios of supply network disruptions. The workshop will also have more interactive sessions, to identify research topics ripe for exploration and perform sandbox style group activity to define the way forward
In order to generate a coherent community and meet the goals of this workshop, registration for this event will be invitation-only in the first instance. The list of invited applicants includes a number of Turing Fellows and Turing-affiliated researchers.
The remaining spaces will be filled through an open invitation. If oversubscribed, the organisers will assess/accept applicants to ensure the goals of the workshop are met.
Interested participants should contact the organisers, Alexandra Brintrup and Francois Lafond to register for this event.
"
https://www.turing.ac.uk/people/external-researchers/paola-tubaro,"
Paola Tubaro is currently a Research Scientist at CNRS, Paris, France, after a career as a Reader at the University of Greenwich, London. An economist and sociologist, she researches the effects of digital technologies on organizations, labor relationships and social cohesion.
"
https://www.turing.ac.uk/people,"

Click on a group to find out more and find who you're looking for.
"
https://www.turing.ac.uk/research/research-projects/spatio-temporal-causal-inference,"
Despite the recent explosion of activity in causal inference (drawing conclusions about causal connections in data), little research has been undertaken into how to perform causal inference in settings that have spatio-temporal relationships, such as urban traffic. The project aims to extend recent methodologies in causal inference to this setting including propensity score (estimating the effect of an action) and instrumental variable methods, with focus on the urban traffic setting.
Causal inference is difficult in observational studies due to issues with selection bias and confounding (when certain variables influences both the dependent variable and independent variable causing spurious associations). Methods to offset these issues include considering conditional probability. The project aims to extend these methods to more general settings.
The project aims to investigate the use of advanced causal inference techniques in application for datasets with spatio-temporal correlations. 
A major issue in the spatio-temporal setting is that recently developed advanced causal inference techniques rely on strong assumptions, including the assumption of no interference (i.e. that a treatment, or action, applied to one unit does not affect the outcome of another), and this is often violated in a spatio-temporal settings. The aim is to develop inference methodologies which account for this interference. 
Extending causal inference methodologies to a spatio-temporal setting would be useful in a wide variety of applications.
This project is part of the Data-centric engineering programme's Grand Challenge of 'Monitoring Complex Systems'.
Whilst the methodologies developed will be applicable in a wide range of applications, the transportation setting is of particular interest. The methods developed would enable the answering of questions such as how traffic interventions, such as the introduction of speed limit zones, improve safety where potential 'covariates' (related, independent variables such as road type and traffic intensity) vary over space and time.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/royal-veterinary-college,"
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/numerical-analysis,"
"
https://www.turing.ac.uk/people/researchers/morgan-briggs,"
Trained as a data scientist, Morgan currently serves as the Research Associate for Data Science and Ethics within the Public Policy Programme. Morgan has experience leading cross-disciplinary teams and served in a project lead capacity at the Rhodes Artificial Intelligence Lab (RAIL) to assist the World Food Programme Innovation Accelerator with building AI solutions for urban food insecurity. Morgan has worked on a variety of topics related to social data science including food security and remote sensing, renewable energy, and ethical considerations of data science methodologies and digital technologies.
Morgan works on a variety of projects at the Turing including continued, ongoing work on AI explainability, building upon the Turing and ICO co-badged guidance, Explaining decisions made with AI. She is also a researcher on the UKRI-funded project called PATH-AI: Mapping an Intercultural Path to Privacy, Agency and Trust in Human-AI Ecosystems and an international project entitled Advancing Data Justice Research and Practice which is funded by grants from the Global Partnership on AI, the Engineering and Physical Sciences Research Council, and BEIS. Morgan has continued to research topics related to children’s rights and AI stemming from research that was conducted with UNICEF. 
In addition to her role at the Turing, Morgan serves as an Associate Teaching Fellow for the Global Leadership Initiative, a programme created by the Oxford Character Project within the Faculty of Theology and Religion at the University of Oxford. Here, Morgan co-facilitates weekly sessions on topics of virtue ethics and character, as they relate to the technology sector. 
Morgan has a BSB from Wake Forest University where she studied Business Data Analytics and Religion. She also holds an MSc in Social Data Science from the University of Oxford, awarded with distinction. 
Briggs, M., Aitken, M., & Leslie, D. (2021). “Is the public sector ready for child-centred AI?” Scotland AI Strategy Guest Blog Series. https://www.scotlandaistrategy.com/news/is-the-public-sector-ready-for-child-centred-ai
Pauwels, E., Briggs, M., Aitken, M., Leslie, D. (2021). Case Study: Understanding AI Ethics and Safety for Children: The Alan Turing Institute. UNICEF. https://www.unicef.org/globalinsight/media/2321/file  
Wright, J., Leslie, D., Raab, C., Kitagawa, F., Ostmann, F., & Briggs, M. (2021). Privacy, Agency, and Trust in Human-AI Ecosystems Interim Report (short version). https://www.turing.ac.uk/sites/default/files/2021-12/path_ai_report_final_version.pdf
Advancing Data Justice Research and Practice Team. Interim Report. (2021). Prepared for the 2021GPAI Summit. https://gpai.ai/projects/data-governance/data-justice/advancing-data-justice-research-and-practice-interim-report.pdf
Leslie, D. & Briggs, M. (2021). “Project ExplAIn enters its next phase.” https://www.turing.ac.uk/blog/project-explain-enters-its-next-phase
Briggs, M. (2021). SatDash: An Interactive Dashboard for Assessing Land Damage in Nigeria and Mali. In ACM SIGCAS Conference on Computing and Sustainable Societies (COMPASS) (COMPASS ’21), June 28-July 2, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3460112.3471949
Leslie, D., Burr, C., Aitken, M., Cowls, J., Briggs, M. (2021). Artificial intelligence, human rights, democracy, and the rule of law: A primer. The Council of Europe. https://www.turing.ac.uk/sites/default/files/2021-03/cahai_feasibility_study_primer_final.pdf
Leslie, D., Burr, C., Aitken, M., Cowls, J., Briggs, M. (2021), Intelligence artificielle, droits de l’homme, démocratie et État de droit. Guide introductif, Conseil de l’Europe. Translation by the Council of Europe.
Leslie, D., Burr, C., Aitken, M., Cowls, J., Briggs, M. (2021), Artificiële intelligentie, mensenrechten, democratie, en de rechtsstaat: Een introductie. The Council of Europe - i.s.m. KDM en ALLAI, Nederlandse vertaling door Valcke, P., Muller, C. en Goetry, B.
Leslie, D. & Briggs, M. (2021). Explaining decisions made with AI: A Workbook (Use case 1: AI-assisted recruitment tool). http://dx.doi.org/10.2139/ssrn.3808512
Leslie, D. & Briggs, M. (2021). Explaining decisions made with AI: A Workbook (Use case 2: Machine learning for children’s social care). http://dx.doi.org/10.2139/ssrn.3808517   
 
"
https://www.turing.ac.uk/events/visualization-data-science-and-ai,"
144.4 KB
The fields of data science and AI involve processes and generate outputs that can be challenging for humans to understand. In many situations it is essential to understand and gain deep insights about these phenomena in order to make decisions in the real world. The role of Data Visualization in these situations is to enhance human cognition, providing faster and more accurate insights.  
There is a real and growing gap between big data and the human ability to comprehend and act upon the analysis of that data. Research in the science and engineering of visualization needs to respond to this challenge in order that data and data products remain comprehensible to people. 
This one day symposium will explore the latest science and engineering in Data Visualization, including a training workshop, demonstrations and presentations from leading UK researchers and innovators, and a forward-thinking keynote from an international speaker.
This event is suitable for individuals who are are interested in the following areas of data science and AI:
Visualization theory and practice cuts across all the challenge themes at the Turing, if you have an interest in visualization come and network with leaders in the field.
Agenda for ""Visualization for data science and AI""
Creating and effectively using visualization techniques in real-world applications, Keynote talk from Jörn Kohlhammer
 
Apply to attend is now closed.

"
https://www.turing.ac.uk/research/research-projects/women-data-science-and-ai,"
Using research to inform policy measures aimed at increasing equity in the data science and AI fields

Using research to inform policy measures aimed at increasing equity in the data science and AI fields
The Women in Data Science and AI project sits within the public policy programme at The Alan Turing Institute. We work alongside policy makers and industry stakeholders, offering actionable insights and recommendations to tackle the multifaceted ethical, economic and governance-related issues stemming from inequalities in AI.

The fields of AI and data science have grown exponentially as the world is increasingly being built around automated systems and 'smart' machines. Yet the people whose work underpins this are far from representative of the society these systems are meant to serve.
As AI becomes ubiquitous in everyday life, the drive for diversity and inclusion in technology is of pressing concern. In particular, the persistent under-representation of women and marginalised groups in data science and AI, including gender data gaps, leads to the encoding and amplification of bias in technical products and algorithmic systems, creating harmful feedback loops.
It is crucial that we get ahead of this now, before flawed technologies become irreversibly integrated into the fabric of society. We address these complex challenges through a three-tiered approach:
A demonstration of gender bias when translating Hungarian to English on Google Translate
A hiring algorithm developed by Amazon was found to discriminate against female applicants
The Apple Card's algorithm was accused of being a 'sexist program' in 2019
An independent audit found that Facebook was withholding certain job ads from women because of their gender
Scientists found music recommendation algorithms had a biased feedback loop
According to a 2020 BBC Investigation, women with darker skin are more than twice as likely to be told their photos fails UK passport rules
 
In collaboration with Diversio, an AI-based diversity, equity and inclusion (DEI) platform, this research explores behavioural and structural inclusion challenges faced by women in technology companies. 
We are using Diversio’s database and employee public review websites’ data to analyse, and therefore tackle, diversity and inclusion pain points (e.g., pay gap, leadership diversity) within the technology industry across Canada, United States and the United Kingdom.
Stack Overflow, the world’s largest question-and-answer platform of programming knowledge, is a key site for establishing technical expertise and reputation. The overrepresentation of men among the site’s elite may therefore compound the barriers to women’s participation in computing.
Using historical posts, answers and comments from the platform’s archive, our research explores how gender mediates reputation and knowledge creation on the platform, with the aim of providing actionable recommendations for inclusivity in online forums.
We developed a tool, the ‘Diversity Dashboard’, to offer insights into gender inclusion within online tech workplaces. The Dashboard was designed for use by technology companies, running internally on top of GitHub and Slack, to measure and highlight in real-time potential differences between the treatment of men and women.
To find out more about the tool please watch the demo video below and read the information sheet.

We developed a new and innovative tool for scraping LinkedIn's profiles to access information regarding individual’s job, skills and educational backgrounds in the data and AI fields. For more information, please reach out via our contact form.
The Women in Data Science and AI’s Hub is a digital platform for women and marginalised groups interested in joining, or already involved in, the data science and AI community. Through our curation of relevant organisations, networks, guides, and educational resources, we aim to encourage increased participation and advance gender diversity within the data science and AI industry.
Visit the Hub
"
https://www.turing.ac.uk/people/research-associates/joshua-rhodes,"
Josh is a historian of eighteenth- and nineteenth-century Britain, focusing on agrarian and industrial transformations. He completed his PhD at the University of Exeter in 2019. Since then, he has taught at the University of Bristol and been a researcher at the Gender, Place and Memory research cluster at the University of Hull. Before joining The Alan Turing Institute, he held an Economic History Society fellowship and was based in the Cambridge Group for the History of Population and Social Structure (Campop) at the University of Cambridge.
Josh’s research examines the causes, timing, and effects of the British Agricultural and Industrial Revolutions. His doctoral research focused on developing new ways of measuring changes to farming and rural society and their relationship to modern economic growth. At the Turing, Josh works on the Living with Machines project, where he is using digitised historical census data to re-assess what we know about industrialisation and how it impacted peoples' lives.
"
https://www.turing.ac.uk/people/researchers/david-leslie,"
David Leslie is the Director of Ethics and Responsible Innovation Research at The Alan Turing Institute and Professor of Ethics, Technology and Society at Queen Mary University of London. He previously taught at Princeton’s University Center for Human Values, where he also participated in the UCHV’s 2017-2018 research collaboration with Princeton’s Center for Information Technology Policy on “Technology Ethics, Political Philosophy and Human Values: Ethical Dilemmas in AI Governance.” Prior to teaching at Princeton, David held academic appointments at Yale’s programme in Ethics, Politics and Economics and at Harvard’s Committee on Degrees in Social Studies, where he received over a dozen teaching awards including the 2014 Stanley Hoffman Prize for Teaching Excellence. He was also a 2017-2018 Mellon-Sawyer Fellow in Technology and the Humanities at Boston University and a 2018-2019 Fellow at MIT’s Dalai Lama Center for Ethics and Transformative Values.
David is the author of the UK Government’s official guidance on the responsible design and implementation of AI systems in the public sector, Understanding artificial intelligence ethics and safety (2019) and a principal co-author of Explaining decisions made with AI (2020), a co-badged guidance on AI explainability published by the Information Commissioner’s Office and The Alan Turing Institute. After serving as an elected member of the Bureau of the Council of Europe’s (CoE) Ad Hoc Committee on Artificial Intelligence (CAHAI) (2021-2022), he was appointed Specialist Advisor to the CoE’s Committee on AI where he has led the writing of the zero draft of its Human Rights, Democracy and the Rule of Law Impact Assessment for AI, which will accompany its forthcoming AI Convention. In his advisory role with the CoE, David led his team in writing a Primer to support the meaningful participation of citizens and civil society organizations in the stakeholder outreach of the CAHAI’s Feasibility Study, published March 2021 and translated into French and Dutch. He and his team were then asked by the CoE to carry out research into the Impact Assessment Instrument to be submitted to the Council of Ministers as part of the CoE legal framework on AI and published the 335-page Human Rights, Democracy, and the Rule of Law Assurance Framework for AI Systems  submitted in September 2021.  As part of his international work, he also serves on UNESCO’s High-Level Expert Group steering the implementation of its Recommendation on the Ethics of Artificial Intelligence, a first-of-its-kind document adopted by the 193 Member States of the Organization.
David is on the editorial board of the Harvard Data Science Review (HDSR) and is a founding editor of the Springer journal, AI and Ethics. He is also Principal Investigator of a UKRI/ESRC-funded project called PATH-AI: Mapping an Intercultural Path to Privacy, Agency and Trust in Human-AI Ecosystems, which is a research collaboration with RIKEN, one of Japan’s National Research and Development Institutes founded in 1917. More recently, he has received a series of grants from the Global Partnership on AI, the UK Engineering and Physical Sciences Research Council, and BEIS to lead a project titled, Advancing Data Justice Research and Practice, which explores how current discourses around the problem of data justice, and digital rights more generally, can be extended from the predominance of Western-centred and Global North standpoints to non-Western and intercultural perspectives alive to issues of structural inequality, coloniality, and discriminatory legacies. This project yielded collaborations with research teams from Aotearoa (New Zealand), Australia, China, India, Pakistan, Uganda, Cameroon, Kenya, Brazil, Chile, and Bolivia, and ultimately the publication, in March 2022, of 12 project reports from the field as well as a Project Documentary, an Integrated Literature Review, Data Justice Stories: A Repository of Case Studies, Data Justice in Practice: A Guide for Policymakers, Data Justice in Practice: A Guide for Developers, and Data Justice in Practice: A Guide for Impacted Communities.
David was a Principal Investigator and lead co-author of the NESTA-funded Ethics review of machine learning in children’s social care(2020). His other recent publications include ‘The Ethics of Computational Social Science’, (2023) published In Handbook of Computational Social Science for Policy for the European Commission Joint Research Centre/Centre for Advanced Studies, ‘Artificial intelligence and the heritage of democratic equity’ (2022) published by The Venice Commission of the Council of Europe, the HDSR articles “Tackling COVID-19 through responsible AI innovation: Five steps in the right direction” (2020) and “The arc of the data scientific universe” (2021) as well as Understanding bias in facial recognition technologies (2020), an explainer published to support a BBC investigative journalism piece that won the 2021 Royal Statistical Society Award for Excellence in Investigative Journalism. David is also a co-author of Mind the gap: how to fill the equality and AI accountability gap in an automated world (2020), the Final Report of the Institute for the Future of Work’s Equality Task Force and lead author of “Does AI stand for augmenting inequality in the COVID-19 era of healthcare” (2021) published in the British Medical Journal. In his shorter writings, David has explored subjects such as the life and work of Alan Turing, the Ofqual fiasco, the history of facial recognition systems and the conceptual foundations of AI for popular outlets from the BBC to Nature.
 
David’s recent and upcoming invited lectures, talks, and public appearances include:
David's current research focuses on digital ethics, algorithmic accountability, explainability, and the social and ethical impacts of machine learning and data-driven innovations. In his wider research, David studies the moral and ethical implications of emerging technologies. In particular, he is keen to question how the biospherically and geohistorically ramifying scope of contemporary scientific innovation (in areas ranging from AI and synthetic biology to nanotechnology and geoengineering) is putting pressure on the conventional action-orienting categories and norms by which humans, at present, regulate their behaviour.
"
https://www.turing.ac.uk/research/research-programmes/defence-and-security,"
The defence and security (D&S) community – represented by the Ministry of Defence (Defence Science and Technology Laboratory [Dstl] and Joint Forces Command), GCHQ and MI5 – are collaborating with The Alan Turing Institute to deliver an ambitious programme of data science and artificial intelligence (AI) research to deliver impact in real world scenarios. 
Following the signing of a collaboration agreement and a year of knowledge exchange the D&S programme was launched in 2017. Since then, programme personnel has grown across research, leadership, and support. 
The programme’s vision is to undertake multidisciplinary data science and AI research to ensure a safe, secure, and prosperous society:
The programme carries out this vision through traditional academic research (e.g. DSO Partnership, AI for Cyber Defence Research Centre [AICD]), capability development (e.g. Applied Research Centre for Defence and Security [ARC]) and policy analysis (e.g. Centre for Emerging Technology and Security [CETaS])
The D&S programme is solving challenges across four key areas:
Existing projects:
Existing projects:
Existing projects:
Existing projects:
Starting with the identification of three core research projects, the programme’s activity has rapidly expanded. Additional research projects in our remaining two key challenge areas will begin before the end of the year, and several interdisciplinary research themes have evolved that cut across the Turing’s strategic partners. Our activities are driven by the programme’s goals of delivering world-leading research with real-world impact.
Every computer network is vulnerable to cyber criminals and hackers. Detecting attackers within a network is a complex task, and even skilled operators struggle to keep track of them all.
To bolster network defences and speed up response times, computer security organisations are increasingly looking to autonomous systems. Researchers led by the Turing’s Vasilios Mavroudis and Chris Hicks are exploring a technique called reinforcement learning – a field of artificial intelligence in which computer algorithms learn by solving problems through trial and error, with the goal of maximising a specified reward.
In 2022, the team won the first Cyber Autonomy Gym for Experimentation (CAGE) challenge, run by The Technical Cooperation Program. The team has made its code publicly available so that others can benefit from and expand upon this work.
Read the full case study as seen in the 2021-22 Alan Turing Institute Annual Report
An announcement was made that the programme’s long-term work has been bolstered by a number of shorter, strategically important projects supported by funding from GCHQ. Each up to six months in duration, these projects aim to demonstrate immediate, meaningful impact, and address the key challenges that frame the defence and security programme.
The projects are focusing on a diverse range of applications including understanding hacker communities, adversarial machine learning, encryption, modelling of civil conflict, topological data analysis, and utilising game theory in cyber security. The projects are expected to yield academic impact through publications, and real-world impact through software, which will be released for use and further development.
The Manufacturer, the UK’s premier industry publication for providing manufacturing news, articles, and insights, published an article about the announcement.
Read the Manufacturer article – “Alan Turing Institute to help combat global cyber security challenges”
 
Work produced by Turing Fellow Dr Weisi Guo is aiming to understand the mechanics that cause conflict and identify multi-scale population areas that are at risk of conflict. The research utilises the latest developments in complex networks and spatial interaction theory to model the effect of multiplexed regional-global interactions.
Findings have shown that ‘crossroad’ towns and cities where there are few other routes correlate strongly with data on violence; including terrorism, war between states, and gang violence. The work is building an evidence base which aims to help sustainable global development of infrastructure in order to reduce conflict.
Dr Guo was interviewed by BBC News about his work, the article going into depth about the potential ramifications of the work and the role of the Turing in the research.
Read the BBC article – “Can mapping conflict data explain, predict and prevent violence?”
 
Turing Fellow Dr Weisi Guo and Director of Special Projects Sir Alan Wilson have written a comment piece for Nature, explaining how using artificial intelligence to predict outbursts of violence and probe their causes could save lives.
The piece details existing research being conducted into the forecasting of conflict, including the work of Guo and Wilson at the Turing. The piece identifies three things that will improve conflict forecasting: new machine-learning techniques; more information about the wider causes of conflicts and their resolution; and theoretical models that better reflect the complexity of social interactions and human decision-making. The piece goes on to propose that an international consortium be set up to develop formal methods to model the steps society takes to wage war.
Read the full Nature comment piece
For more information, please contact Tracey Peterson ([email protected]) and Hushpreet Dhaliwal ([email protected]).
"
https://www.turing.ac.uk/research/research-projects/marine-and-maritime-group,"
The oceans transport 90-95% of goods, supporting world trade and enabling efficient transport. We use our oceans to generate power, provide us with natural resources and for leisure. These vessels and structures will need to be designed, operated, and decommissioned safely and efficiently. Data science and artificial intelligence will play a large role in this future. 
We will develop new tools and techniques for marine and maritime, help companies deploy effective data-centric systems and help educate those in the industry on how to make the most of these approaches. The result will be a wider uptake of data science, machine learning and artificial intelligence leading to safer ships and reduced emissions from operations.
The research will be used across the marine and maritime industries, including shipping, offshore platforms, renewable energies, ports, autonomous systems and leisure industries. The group has been involved in a number of advances in the development and uptake of machine learning and artificial intelligence in the maritime field to date. Three practical examples are:
Compared with existing optimisation solutions on the market, T-VOS has been proven to increase fuel savings and reduce emissions by 5%, improve time savings by 7%, and deliver Time Charter Equivalent improvements by 8%. The savings have been validated directly by vessel owners and represent average savings over existing solutions.

 
The floor plan for a new yacht designed using a genetic algorithm, developed by the group lead. The design is compared to one generated by a team of designers from Olesinski, on the left. The software reduces the layout design time from two weeks to two days, allowing a focus on the detailed design and reducing the time spent on more repetitive tasks.

 
Every year, the marine industry relies more heavily on computational fluid dynamics (CFD) simulations, but these simulations are still far too slow for real-time operations use, or even open-ended design. By combining a simplified two-dimensional flow model with a deep convolutional neural network (CNN), our group has developed a three-dimensional flow prediction tool with 95-98% generalization accuracy which is 2-3 orders of magnitude faster than standard CFD.

 
Adam Sobey, [email protected]
 
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/spatial-analytics,"
"
https://www.turing.ac.uk/programme/,"



The Conversation stage can be found on the third floor in the Britten. The Research stage can be found on the fifth floor in the Windsor. The Impact stage can be found on the sixth floor in the Mountbatten.
View our floorplan
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
Science Writer and Columnist, Freelance & Financial Times
Ethics Research Fellow, The Alan Turing Institute
Research Project Manager, AI for Multiple Long-term Conditions, The Alan Turing Institute
Executive Director, Regulatory Risk, Information Commissioner's Office
Head of AI and Big Data, Institute of Global Helath Innovation, Imperial College London
Computational Ecologist, Centre for Ecology and Hydrology
Community Manager (Tools, Practices and Systems), The Alan Turing Institute
Science Director, Intelligent Data Ecosystems, Rothamsted Research
Programme Director for Urban Analytics at The Alan Turing Institute
Professor of Biostatistics, Imperial College London
Postgraduate Researcher, Alan Turing Institute
Assistant Professor, Imperial College London
CEO, Code First Girls
Senior Scientific Software Engineer, Met Office
Director, Sense about Science
Principal Data Scientist, STFC Hartree Centre
Deputy Director, Department for Science, Innovation and Technology
Principal Research Engineer, NATS
Director at Big Brother Watch
Professor of Economics, De Montfort University
: Assoc Director of ASG; Director of International, The Alan Turing Institute
Professor of Practice in Modern Slavery Law and Barrister, King's College London & One Pump Court Chambers, London
Programme Manager (AIM RSF), The Alan Turing Institute
Director of EUREKA Robotics Centre, Reader in Robotics & EdTech, EUREKA Robotics Centre, Cardiff Metropolitan University
CEO, National Biodiversity Network Trust
Research associate, The Alan Turing Institute
Senior Researcher, University of Cambridge
Student, University of Cambridge
Head of Industry Developments, Ofcom
Director, Scientia Scripta Communication Services Ltd
PhD candidate, Imperial College London
Research Associate, The Alan Turing Institute
Programme Manager, The Health Foundation
Professor of Art and Computational Culture, Centre for the Study of the Networked Image, London South Bank University
Turing AI Acceleration Fellow and Director of The Bath Institute for the Augmented Human (University of Bath)
Marconi Professor of Communications Systems, University of Cambridge
Met Office Principal Fellow
Superintendent, West Midlands Police
Director Tech and Innovation, techUK
Health Theme lead AI for Science and Government, The Alan Turing Institute / Public Health Wales NHS Trust
Enrichment Student, The Alan Turing Institute
CTO and Co Founder, digiLab
Group Head of Data & AI Ethics, Lloyds Banking Group
Co-Director, Public Policy Programme, The Alan Turing Institute
Co-Founder and CEO, We and AI
Turing Fellow
Global Lead for Responsible AI, Accenture
Chief Executive, City of Bradford Metropolitan District Council
Research Associate, The Alan Turing Institute
Research Assistant, The Alan Turing Institute
Research Assistant, The Alan Turing Institute
Director of Public Policy at Wayve
Social Data Science PhD candidate, University of Oxford and The Alan Turing Institute
Chief Scientist, The Alan Turing Institute
Executive Chair, Engineering and Physical Sciences Research Council
Post Doctoral Research Associate, The Alan Turing Institute
Assistant Director, Open Climate, Creative Commons
Deputy Director, Data Science Campus, Office for National Statistics
PhD, 4th year, University of Oxford, The Alan Turing Institute
Director, Natural History Museum London and Chairman, The Alan Turing Institute
Chair, CEN-CENELEC Joint Technical Committee 21 (Artificial Intelligence)
Writer, presenter, comedian.
CReDo (climate resilience demonstrator project) Strategic Engagement Lead, Connected Places Catapult
Turing AI Acceleration Fellow and Professor in Natural Language Processing, Kings College London
Professor of Geocomputation, University of Glasgow
Senior Research Software Engineer, The Alan Turing Institute
Programme Director for Health and Medical Sciences, The Alan Turing Institute
Director of Analytics & Data Science, UK Health Security Agency
Professor & Senior Principal Researcher, University of Maryland & Microsoft Research
Director for Technology & Analysis, Foreign, Commonwealth & Development Office
Technology Policy Director, Ofcom
Professor of Pervasive Computing, Nottingham Trent University
Director of AI and Data Economy, UKRI
Chief Digital and information Officer, London Borough of Camden
Chief Data Officer, NHS National Services Scotland
Professor, University of Oxford
Senior Lecturer, Manchester Metropolitan University
Director of Ethics and Responsible Innovation Research at The Alan Turing Institute and Professor of Ethics, Technology and Society, Queen Mary University of London
Turing AI Fellow, The Alan Turing Institute
Turing AI Acceleration Fellow, The Alan Turing Institute
Emeritus Professor, New York University; Founder, Geometric Intelligence; Author, Rebooting AI
Director of Public Policy Programme, The Alan Turing Institute
Head of Skills, The Alan Turing Institute
AI and Justice and Human Rights Theme Lead, The Alan Turing Institute
Associate Director, National Engineering Policy Centre, Royal Academy of Engineering
Chief Executive Officer, Quanterra Systems
Associate Professor in Statistics, UCL
AI Policy Adviser, Financial Conduct Authority
Research Associate, The Alan Turing Institute
PhD Student, University of Bath
Professor, School of Biomedical Engineering and Imaging Sciences, Kings College London & Co-Director (Health) Turing Research and Innovation Cluster: Digital Twins, The Alan Turing Institute
Associate Professor in Energy and AI, UCL
Author and Faculty Associate, Arizona State University
Head of Research and Analysis, Centre for Science and Policy, University of Cambridge
Head of AI Governance and Regulatory Innovation, The Alan Turing Institute
Public Engagement Manager, The Alan Turing Institute
Research Project Manager, The Alan Turing Institute
Assistant Professor, AI Centre, University College London
Head of AI Unit and OECD.AI, OECD Digital Economy Policy Division - OECD
Enrichment Student, The Alan Turing Institute
Director of Technologies, Lloyd's Register Foundation
Research Associate, The Alan Turing Institute
Director of Innovation, The Alan Turing Institute
Senior Teaching Fellow, Faculty of Natural Sciences, Imperial College London
Director, Hartree Centre
MP, Parliamentary Under Secretary of State at the Department for Science, Innovation and Technology
PhD candidate, Imperial College London and The Alan Turing Institute
Research Associate, The Alan Turing Institute
Senior Researcher - Open Research, TPS, The Alan Turing Institute
Associate Director, Digital Ocean, National Oceanography Centre & TRIC Co-Director (Environment), The Alan Turing Institute
Chief Executive Officer, The Royal Academy of Engineering
Doctoral Student, University of Bristol and The Alan Turing Institute
Senior Research Fellow, University of Bristol
Institute Director and Chief Executive, The Alan Turing Institute
Research Application Manager, The Alan Turing Institute
Managing Director, Accenture
Programme Director for Finance and Economics + Turing Fellow, The Alan Turing Institute
Senior Scientist, European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI)
Enrichment Student, Turing
Professor of Nonlinear Dynamics, University of Sheffield & Co-Director (Infrastructure), Turing Research and Innovation Cluster, Digital Twins, The Alan Turing Institute
Director of Research in machine learning, University of Cambridge & Programme Director for Safe and Ethical AI, The Alan Turing Institute
Programme Director for Tools, Practices and Systems, Alan Turing Institute
Research Data Scientist, The Alan Turing Institute
Head of DAO, ESG and Group BI & Analytics, HSBC
Director of Foundational AI Research, The Alan Turing Institute
Professor of Mechanical Engineering, University of Sheffield & Co-Director (Infrastructure) Turing Research and Innovation Cluster, Digital Twins, The Alan Turing Institute
Research Fellow, The Alan Turing Institute
Sustainability Specialist at diginex
Community Manager, The Alan Turing Institute
“One of the most striking aspects of the event was how broad a research area AI is; it was inspiring to see how many different directions one could take. I will continue to recommend the event to people interested in exploring data science further.”
“This has to be the best AI-focused event I’ve attended, thank you to all who worked to put it together. The event highlights that we all have a lot to do to ensure that the AI sector in the UK thrives, but we are doing a lot of work already and the country is really trying to take a leading role, which is fantastic to see.”
“I found it interesting, engaging and thought-provoking. It was particularly fascinating to hear the illuminating perspectives of speakers from Asia, Africa and the Global South.”
“I wanted to congratulate the AI UK team for the excellent level of representation in the event. As a mature woman PhD student from a minority community, I could find someone who reflected me in some way in the various panels. Well done!”
“Besides the motivation, excitement and hope from all the inspirational speakers, AI UK has also re-confirmed my decision to pivot my career and I’m very much looking forward to a role in data science for the public good!”


"
https://www.turing.ac.uk/about-us/impact/reducing-carbon-footprint-shipping-industry,"
The oceans are the superhighways of international trade, with about 80% of global goods carried by sea. But this comes at an environmental cost: the shipping industry emits around a billion tonnes of greenhouse gases – mostly CO2 – per year. This is about 3% of the global greenhouse gas emissions caused by human activity.
Shipping companies are looking for ways to reduce their carbon footprint, and one potential solution lies in optimising ships’ routes so that they burn less fuel. To this end, a team led by the Turing’s Adam Sobey has developed voyage optimisation software that plans the most fuel-efficient route through the waves. This is a collaboration with UK company Theyr, which specialises in supplying high-resolution weather and ocean data to the maritime sector.
Theyr’s data feeds directly into the software, which uses what is known as a ‘genetic algorithm’, inspired by Darwin’s theory of evolution by natural selection. The algorithm creates a population of possible routes, and then mathematically combines (‘mates’) pairs of the most successful routes (i.e. those which arrive on time while using lower amounts of fuel and avoiding poor weather/ocean conditions). By repeating this process over multiple ‘generations’ of routes, the algorithm quickly arrives at an optimal solution.
The software is now being used by Euronav, a crude oil tanker company that has a fleet of 75 ships, and the researchers estimate that it will reduce fuel use by around 5% compared with other routing algorithms. Large vessels typically burn 50-75 tonnes of fuel per day, so this could save as much as £2,000 per day per vessel. And with this, of course, comes a significant environmental benefit: reduced fuel means reduced emissions.
“By optimising our shipping routes, this innovative software has the potential to significantly reduce both our fuel costs and our environmental impact. We are now rolling it out across our entire fleet of tankers.”
Patrick Declerck, Operations Manager, Euronav
This piece first appeared in The Alan Turing Institute’s Annual Report 2021-22Top image: Euronav
"
https://www.turing.ac.uk/events/rsf-seminar-series-tres-environment-collaborative-health-data-research,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility project. 
The diversity of data and its scale and complexity are important factors to note when undertaking research related to the study of multiple long-term conditions. Demographics, socioeconomic indicators, and the condition data, when linked, provide a powerful, dynamic, and flexible research asset for the investigator. Allied to the realisation of the research benefits of such linked data is the need for hosting in a catalytic environment that securely fulfils the enabling function for the realisation of the research equity the data holds.
The presentation will outline the approach of the ‘Trusted Research Environment’ to provide publicly acceptable secure data linkage, the provision of a research environment replete with multiple technological data tools, an ability to support ‘team science’, and interoperability to allow the realisation of the potential of the data across this field of research. The growth of research in multiple long-term conditions requires environments to respond and ensure that platforms efficiently enable this research and learn and adapt to any challenges this scale of research may present.
"
https://www.turing.ac.uk/about-us/impact/anti-bias-test-implemented-amazon,"
A test for detecting bias in AI and machine learning systems, developed by researchers at the Turing, has been adopted by Amazon for its cloud computing platform, Amazon Web Services (AWS).
The test, called ‘Conditional Demographic Disparity’, was first proposed in a 2020 paper by Sandra Wachter, Brent Mittelstadt (both Turing Fellows at the time) and Chris Russell (former Group Leader in Safe and Ethical AI at the Turing). It is a metric that gives a measure of inequality within a dataset, and so can flag up discrimination in, for example, job recruitment processes, automated loan approval, healthcare access and university admissions. The strength of the test is that it incorporates the standards of fairness used in European courts of law, and it also accounts for underlying factors which might be driving the bias, making it useful for detecting ‘intersectional’ discrimination where multiple factors are at play.
“The paper that proposes this test is a delight to read, and it clearly lays out the legal and ethical foundations of the work. Machine learning researchers using Amazon Web Services are now using this test to help them identify bias in their datasets and model predictions.”
Sanjiv Das, Amazon Scholar at AWS and Terry Professor of Finance and Data Science at Santa Clara University
Amazon has included the test as part of its SageMaker Clarify service, which provides machine learning developers using AWS with tools to detect and measure biases in their datasets and models, helping them to understand their models’ predictions and pinpoint issues of inequality. It’s a major success for the researchers behind this test, as their work is now in the hands of those who are developing the AI systems of the future.
 
This piece first appeared in The Alan Turing Institute’s Annual Report 2020-21Top image: Kristi Blokhin / Shutterstock
"
https://www.turing.ac.uk/people/external-researchers/yoshio-matsumoto,"
"
https://www.turing.ac.uk/people/former-doctoral-students/alexis-bellot,"
Alexis is a second year PhD student at the University of Cambridge, supervised by Professor Mihaela van der Schaar. Prior to Cambridge, Alexis graduated from an MSc in Statistics from Oxford and a BSc in Mathematics from Imperial College London.
Alexis’ research interests lie at the intersection of machine learning and healthcare. He strives to develop algorithms and predictive models that can help clinicians give individualised care to their patients. His current research is focused on hypothesis testing and transfer learning, especially in how they relate to high-dimensional data and survival outcomes, both frequent in medical applications.
"
https://www.turing.ac.uk/news/publications/scoring-predictions-extreme-quantiles,"
A. Gandy, K. Jana and A. E. D. Veraart (2018). Scoring predictions at extreme quantiles. Work in progress.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/quantexa,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/northumbria-university,"
"
https://www.turing.ac.uk/people/former-researchers/flora-roumpani,"
Urban-regional planning, urban modelling and simulations, spatial analysis and visualisation. Her PhD and current work focuses on the integration of urban modelling and analytics in proceduraly generated 3D cities, for the development of online “what-if” scenarios for planning and participation.
Other research areas include remote sensing, virtual & augmented reality, IoT, as well as creative technology and interactive installations for exhibitions.
She is involved in the award winning projects “Remap Lima” for participatory mapping projects with drones, in collaboration with DPU and CASA in UCL and a network of 49 organisations from Lima, Peru; the ""Little Sparks Hospital"", coordinating and developing VR applications for children undergoing surgery in collaboration with NHS; as well as the CASA ""Drones4Good"" workshop for young people in collaboration with UCL Engineering.
Her visualisation work has been featured in several mainstream media, such as the Guardian, Fast Company, Urbanista and Archdaily. 
 
2018
Provost's Engineering Engagement Award for the Drones4Good public engagement workshop, in collaboration with CASA and UCL Engineering.
AGBI Innovation in Anaesthesia Critical Care and Pain Award 2018 for the Little Journey application. By Dr. Chris Evans, Ramani Moonsinghe, Flora Roumpani, Alexandros Bertzouanis, Laura Henry.
2013
Greek Architectural Talent 2008-2013 (GRE A T) Project title: “New Athens City Museum”. Publication: “41” Great Edition, Heracles GCCo, Athens, 2013. Design team: Roumpani Flora, Eleni Takou
2010
Honorable Mention in OEK - SIKA European Competition 2010 for Communal Sustainable Residences. Project title: ""RESI_GEN”. Design Team: Anthi Valavani, Flora Roumpani, Maria Markella Kolotourou, Alexandros Bertzouanis.
2006
Selected work for the 5th exhibition of Greek Architecture (SANA). Publication: ""Man in Space - 5th exhibition of Greek Architecture, Papasotiriou, Patras 2006"". Design team: Flora Roumpani, Alexandros Berzouanis.
 
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-cambridge,"
"
https://www.turing.ac.uk/research/research-projects/impact-climate-change-agriculture,"
Agriculture is highly vulnerable to climate change and it is likely that in fifty years’ time the crops we currently grow in the UK will no longer be viable to meet consumer demand. Agriculture also has important socio-economic implications for food sustainability and greenhouse gas emissions.
We urgently need to begin investigating alternative strategies for agriculture that will require bringing together the best available models for the key aspects of the most important UK arable crops. This project will pioneer the development of methodology that will integrate data from plant science, hydrology, soil science, insect population dynamics, economics, consumer behaviour and climate models to form an integrated national crop modelling framework to support new policy or management practices, by offering simulations of their impact under different climate change scenarios.
This project is supported entirely by public funds, through Wave 1 of the UK Research and Innovation Strategic Priorities Fund, under EPSRC Grant EP/T001569/1.
The project combines multidisciplinary expertise from across the Turing community and external partners covering a variety of areas including agriculture, environmental science, policy, meteorology, and ecology. Agriculture is highly vulnerable to climate change, with important socio-economic implications for food sustainability and greenhouse gas emissions. In 50 years time it is likely that the crops we currently grow in the UK will no longer be viable to meet consumer demand, due to changes in the climate. We urgently need to begin investigating alternative agricultural models, based on multiple sources of data. We can begin to model this by bringing together data from plant science, hydrology, soil science, insect population dynamics, economics, consumer behaviour and climate models to form an integrated national crop modelling framework. Bringing together the best available models describing the key aspects of agriculture for the most important UK arable crops will support new policy or management practices, by offering simulations of their impact under different climate change scenarios. Such a framework will accommodate a number of different models:
The project will also evaluate changing public attitudes and consumption in relation to perceptions of environmental change, sustainability, and the economics of agricultural production.
To build an integrated national crop modelling framework, using currently available models and data to allow the testing and development of new policies or management practices prior to implementation. This modelling framework will support interventions aimed at ensuring the UK’s continued food security.
"
https://www.turing.ac.uk/people/vicky-hellon,"
Vicky Hellon is the Community Manager for the Turing-Roche Partnership. She will be working closely with both organisations to build an engaged and sustainable community around the partnership, with a focus on supporting researchers to embrace and embed Open Science practices.
Vicky has a BSc in Biomedical Science from the University of Sheffield and previously had roles in Open Access publishing at Springer Nature and F1000Research. At F1000Research she worked in researcher outreach, particularly in Bioinformatics communities and also delivered Open Access publishing services for funders and institutions, most notably as project lead for HRB Open Research and NIHR Open Research. More recently she worked at Health Data Research UK, building a community around the newly launched Health Data Research Innovation Gateway, a portal to help researchers discover and request access to UK health datasets.
She is passionate about supporting forward-thinking changes to the academic publishing system and engaging with researchers from a range of backgrounds.
"
https://www.turing.ac.uk/research/research-programmes/data-centric-engineering/data-centric-engineering-people,"

The programme management team drives forward the strategic direction of the programme and manages the operation of the programme, its projects and its partnerships on a day-to-day basis.
Contact the programme management team on [email protected]
The Advisory Board, made up of foremost academics, provides independent advice on the strategic direction of the programme, helping to ensure that it is meeting societal and sector needs and working towards achieving the highest levels of real-world impact.
The Grand Challenge and Strategic Leaders are all senior academics, each leading on the implementation of projects to meet the overarching challenges and strategic foci of the programme, which have been selected to ensure the programme delivers the greatest impact.
The programme has adopted a ‘Group Leader’ structure, in which established and rising academics are appointed to lead teams of researchers undertaking foundational research in specific engineering and data-science areas.
The Group Leaders are also tasked with attracting new funding to the emerging field, establishing wider scientific collaborations and engaging with industry and policymakers to translate research into practice.
This 'open engagement’ approach is central to the programme's aim to achieve a broad and solid foundational base for data-centric engineering research and practice.
The programme also engages with a wide number of academics at all levels and works with partners across industry, government and the third sector.
To see who is engaged with each project please visit the project webpages.
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/operations-research,"
"
https://www.turing.ac.uk/research/research-areas/machine-learning/applications-machine-learning,"
"
https://www.turing.ac.uk/people/business-team/daniel-allen,"
"
https://www.turing.ac.uk/people/george-aristidis-elder,"
George is in his second year MRC funded PhD at Queen Mary University of London. As part of the Bessant lab and Cutillas lab at Barts, he is currently focusing on novel bioinformatics applications in the field of proteomics.
Previous work at the Institute of Cancer Research on metabolomics via mass spectrometry informs his interdisciplinary approach to work. He completed his MEng degree in Biomedical Engineering at the University of Glasgow.
Georges’ research focuses on developing logic-based models that overcome the limitations of existing tools used in phosphoproteomics. The diverse and highly complex nature of chemical phosphoproteomics research produces a high volume of data. Thus, there is unprecedented need to develop tools and methodologies to explain and rationalise these results. Using an appropriate background knowledge base and accompanying rules based model he aims to analyse, in an explainable and interpretable manner, these types of data sets.

"
https://www.turing.ac.uk/blog/green-cities-why-were-using-ai-map-urban-trees,"
Cities across the world, including London, Singapore and Nairobi, are harnessing the power of trees to help fight climate change. Tree planting is a go-to initiative for cities looking to invest in the environment because trees can help reduce urban temperatures and clean the air of harmful pollutants, while also improving residents’ quality of life.
However, research has shown that the effect of urban trees depends on their exact location. For example, trees planted along some roadways can impede airflow, preventing the dispersal of pollutants and decreasing the local air quality for residents. Meanwhile, trees planted thoughtfully around buildings can have a cooling effect, reducing energy usage in hot months, although this effect depends on the type of tree and the landscape. To fully understand the environmental impact of a city’s trees, researchers therefore need accurate maps of the ‘urban tree canopy’ (the trees’ locations), which is something we’re working on in the Turing’s ‘AI for public services’ team.
Mapping the urban tree canopy is no easy task, however. Surveying techniques can be used to count trees in a few selected areas before estimating the canopy for the whole city, but these maps are usually low-resolution and also lack an easy method of validation. Other maps have relied on citizens reporting the locations of trees, but while this approach can work well in small areas, it often suffers from a lack of uptake and coverage.
A more accurate method is to use AI tools, particularly machine learning (ML). The city of London, for example, worked with Breadboard Labs (a tech company focused on environmental issues) to create ML algorithms that segment aerial images into areas labelled as trees. ML methods represent an important advance over prior techniques, as they allow for very detailed canopy estimates over any size region. But there is still room for improvement. For one, error-prone humans are usually required to manually annotate detailed images. Additionally, current methods are only able to estimate tree cover, not the height component of trees. This is critical because taller trees tend to have a greater environmental effect than shorter trees, with numerous shorter trees often unable to match the impact of a single tall tree.
At the Turing, we are pioneering an improved ML method for measuring the urban canopy, demonstrating its potential by applying it to the trees of Chicago. The secret to our method is that it combines imagery from planes and satellites with highly accurate lidar data to measure both the tree cover and the canopy height, while also removing the need for human annotation.
Lidar is a technique that involves bouncing light beams off objects to measure distances, and it can be used to generate a 3D model of the environment accurate to within a few centimetres: perfect for mapping trees in a city. The downside is that using lidar to map trees over a large area is expensive – both in time and money – which limits how often it can be implemented. Chicago, for example, last collected lidar data in 2017, and the distribution of trees will have changed significantly since then.
Our solution was to develop a computer model that uses a neural network – a ML technique inspired by the way the biological brain works. We trained this neural network on Chicago’s 2017 lidar data, as well as aerial and satellite imagery from the same year, until it could accurately predict the city’s 2017 tree cover and canopy height. Once trained, we could then use our model to estimate the 2019 tree canopy from more recent aerial and satellite imagery. Our analyses showed that tree cover in Chicago’s neighbourhoods varied widely, with tree covers ranging from less than 1% to 52% and average canopy heights ranging from only a few metres to over 20 metres.
By combining multiple sources of data, our technique can create a more holistic map of the urban tree canopy. We aim to further refine this methodology so that we can apply it to different climates and begin coordinating with local authorities so that other towns and cities can benefit from this work.
With better maps of urban trees, researchers and local authorities will be able to build a clearer picture of the impacts of greenery on residents, prioritising planting where it is needed most. Ultimately, cities need more reliable and useable data on all aspects of their infrastructure, so that decisions can be made with more confidence to generate lasting benefits for communities and the environment.
Find out more:
This work was carried out by the ‘AI for public services’ team, part of the public policy programme at The Alan Turing Institute. Our team aims to help policy makers harness data science and AI to inform decisions and improve public services. To find out more about our work, or discuss possible collaborations, please get in touch.
 
Top image: Drop of Light / Shutterstock
"
https://www.turing.ac.uk/people/former-enrichment-students/iacopo-iacopini,"
Iacopo Iacopini is a second year PhD Student in Mathematics at Queen Mary University of London within the Complex Systems and Networks Research Group, where he works under the supervision of Vito Latora and Elsa Arcaute (CASA, UCL). He is also collaborating with the Dynamical Systems and Statistical Physics Group as part of the LoBaNet project. Before, he was a Researcher at ISI Foundation (Turin) and a Data Science Intern at United Nations - UPU (Bern).
Iacopo's research interests include Complex Networks and Data Science, with a particular focus on spatial and social networks and their data driven application to the study of human behaviour in the urban environment. His main PhD project deals with the mathematical modelling of urban systems and the interplay between spatial interaction models and mobile phone data.
"
https://www.turing.ac.uk/news/publications/reporting-guidelines-clinical-trials-evaluating-artificial-intelligence,"
As artificial intelligence moves into the realm of clinical trials, consideration is needed on whether the current CONSORT and SPIRIT reporting statements are sufficient to ensure transparency. This commentary describes work by an international group of researchers who form the CONSORT-AI and SPIRIT-AI Steering Group to develop new reporting guidelines for AI-driven interventions in clinical trials.
Liu, X., Rivera, S.C., Faes, L. et al. Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed. Nat Med 25, 1467–1468 (2019) doi:10.1038/s41591-019-0603-3
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/city-university-london,"
www.city.ac.uk
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-synthetic-data,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
This event will be exploring the topic of synthetic data- which is artificially created data that simulates accurate, real-world settings. These datasets allow researchers to generate insights without having to access sensitive data.
During the event we'll be hearing from Florimond Houssiau, Research Associate at the Alan Turing Institute and Marta Battle and Finn Janson who are both Data Scientists at Roche.
Florimond will be discussing how to evaluate the privacy of synthetic data: synthetic data has been proposed as a solution to using and sharing sensitive data without privacy issues, as records in a synthetic dataset are not linked to real persons. Recent research has however shown that this is not enough in itself to protect the privacy of users. In this talk, he will present a toolbox being developed at Turing to comprehensively evaluate privacy risks of synthetic data generators for a wide range of datasets.
Marta and Finn will discuss how to unlock access to patient data, by generating synthetic data that leaks no information about individual patients. The talk will focus on the state of the technology, its challenges and how they could be overcome. This includes approaches to quality and privacy evaluation, and in particular, challenges in generating high-quality longitudinal datasets.
We will be hosting a Q&A at the event where we hope to get insights from the audience on how to innovate and solve problems in synthetic data generation. We also hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace which you can join here.
You can watch a recording of this event here. 

 
 
"
https://www.turing.ac.uk/people/spotlights/pamela-ugwudike,"

I am currently researching AI ethics and accountability, with a focus on data bias and the implications for criminal justice. This forms the basis of the Turing project I am leading, which is exploring the predictive mechanisms underpinning a machine learning algorithm used by some police services to forecast future crime locations. The project’s primary objective is to identify possible conduits of bias and corrective measures. To do this, the project uses a computational model that replicates the algorithm to run large-scale tests and observe correlations between data inputs and outputs (predictions).
Similarly, another project I’m leading is investigating the relationship of AI and criminal justice. The study, funded by the Web Science Institute (WSI) at the University of Southampton, is exploring how platform algorithms integrated into Online Social Networking Sites (ONSs) interact with broad social factors to structure discourses about crime and punishment.
Leading two research projects on the benefits of digitisation for criminal justice services initially led me to become interested in AI ethics and accountability. I found that data-related issues can prompt predictive algorithms to entrench the belief that social categories such as race and social status are somewhat linked to risk of recidivism. An example is the tendency of some predictive algorithms to obscure problems such as racial bias and infer from patterns in datasets that high rates of arrest are risk predictors. An additional example is the inference that structural problems, for example, poor access to education and employment are also risk predictors.
I find the interdisciplinarity of my current projects particularly exciting, as they bring together researchers from social, computer, and data science disciplines. This makes it possible to bridge gaps between the technical and the social, demonstrating how innovative interdisciplinary methods can be when used to study pressing social problems.
AI ethics, data bias, digital society.
Reading! My preferences are historical fiction and literary/narrative non-fiction. I am also a keen runner.
"
https://www.turing.ac.uk/people/beatriz-costa-gomes,"
Bea is currently a Post-Doctoral Research Associate at the Alan Turing Institute, working in the Data Science for Science programme.
However, the first contact with the Turing was as an Enrichment Student, starting in October 2019. She was a PhD student at the University of Manchester, where she started her final year of a PhD in Bioimage Analysis. She is the creator of the ALFRED software (Advanced Labelling, Fitting, Recognition and Enhancement of Data), a tool to analyse pathological mutations in neurons. Before moving to the UK in 2016, Bea completed her integrated Master’s degree in Biomedical Engineering at the University of Coimbra, in Portugal, where she is originally from. Her master’s thesis was on computational modelling of neuronal growth. She has always been interested in applying computational methods and solutions to further understand neurodegenerative diseases.
Bea is passionate about bridging the gaps between computer science and biology. In her free time, Bea bakes as a stress-relief and is a wannabe stand-up comedian. As a science communicator, Bea is one of the hosts of the Turing Podcast.
Bea is currently working in the Molecular Structure from Images Under Physical Constraints by applying machine learning methods to the analysis of CryoEM images.
Bea is also involved in the development of computer vision tools for scientific imagery (scivision).
President's Doctoral Scholar Award, The University of Manchester
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/simulation,"
"
https://www.turing.ac.uk/events/turing-lecture-better-living-through-trusted-data,"
View the full video:

 
Data, AI, and social media echo chambers can feel scary, but if harnessed correctly they can dramatically improve our quality of life. The potential for improvement comes first from better scientific understanding of our human minds and bodies, and second from a more open and shared understanding of society, government, and our day-to-day lives.
The key to achieving these positive results is aggressive pursuit of a new, broad science of human life to unify the traditional and narrow sciences, and making data a trusted and safe resource for everyone. We are building such systems today, and are changing ""business as usual"" for governments around the world, as well as beginning to unify fragmented social and computational sciences.

Biography
Professor Alex ""Sandy"" Pentland directs the MIT Connection Science and Human Dynamics labs and previously helped create and direct the MIT Media Lab and the Media Lab Asia in India. He is one of the most-cited scientists in the world, and Forbes recently declared him one of the ""7 most powerful data scientists in the world"" along with Google founders and the Chief Technical Officer of the United States.
He has received numerous awards and prizes such as the McKinsey Award from Harvard Business Review, the 40th Anniversary of the Internet from DARPA, and the Brandeis Award for work in privacy.
He is a founding member of advisory boards for Google, AT&T, Nissan, and the UN Secretary General, a serial entrepreneur who has co-founded more than a dozen companies including social enterprises such as the Data Transparency Lab, the Harvard-ODI-MIT DataPop Alliance and the Institute for Data Driven Design. He is a member of the U.S. National Academy of Engineering and leader within the World Economic Forum.
Over the years Sandy has advised more than 60 PhD students. Almost half are now tenured faculty at leading institutions, with another one-quarter leading industry research groups and a final quarter founders of their own companies.
Together Sandy and his students have pioneered computational social science, organizational engineering, wearable computing (Google Glass), image understanding, and modern biometrics. His most recent books are `Social Physics,' published by Penguin Press, and 'Honest Signals', published by MIT Press.
Interesting experiences include dining with British Royalty and the President of India, staging fashion shows in Paris, Tokyo, and New York, and developing a method for counting beavers from space.
"
https://www.turing.ac.uk/people/guest-speakers/alastair-denniston,"
Alastair Denniston is a consultant ophthalmologist (eye specialist) at University Hospitals Birmingham leading research into the use of health data research and artificial intelligence to improve patient care in the ‘real world’. He is Professor at the University of Birmingham, and part of the Biomedical Research Centre for Ophthalmology at Moorfields Eye Hospital/UCL.
Alastair has particular interest in how we can ensure that the innovation within the broad field of ‘artificial intelligence’ is translated efficiently but safely to benefit patients. This includes improving the reporting standards of trials (CONSORT-AI and SPIRIT-AI), helping define the regulatory framework for AI in healthcare, and working with HDRUK and other relevant organisations to support the best of these innovations right through the implementation pathway.
He is Director of INSIGHT, the HDRUK Health Data Research Hub for Eye Health which is focused on eye disease and its application to wider health, including diabetes and dementia. It will use anonymised large-scale data and advanced analytics, including artificial intelligence, to develop new insights in disease detection, diagnosis, treatments and personalised healthcare.
"
https://www.turing.ac.uk/taxonomy/term/173,"

""Team up with a collaborative community of motivated data science experts, to jointly create a pathway to follow-up research and translational activities""
Sebastian Vollmer, Creator of the Turing Data Study Groups

I am from an organisation interested in posing a challenge
I am a researcher interested in being a participant

 
Proud winner of The KE Awards 2021 for Academic Engagement of the Year
Data Study Groups take place throughout the year. Upcoming Data Study Groups will be featured here.
Find out first when the next DSG challenges will be taking place and more – select 'Receive Applied Skills updates'.
"
https://www.turing.ac.uk/people/business-team/karen-cordier,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/rolls-royce,"
www.rolls-royce.com
"
https://www.turing.ac.uk/news/data-science-and-ai-glossary,"

There is a lot of jargon in data science and AI. We’ve created this glossary for non-specialists who want to find out more about these topics without the technical language. We also hope that it will be a useful resource for journalists and policy makers, as well as researchers in areas that intersect with data science and AI. This is an ongoing project, so we will regularly be reviewing the list of terms and definitions. If you would like to suggest any terms, please get in touch.
A sequence of rules that a computer uses to complete a task. An algorithm takes an input (e.g. a dataset) and generates an output (e.g. a pattern that it has found in the data). Algorithms underpin the technology that makes our lives tick, from smartphones and social media to sat nav and online dating, and they are increasingly being used to make predictions and support decisions in areas as diverse as healthcare, employment, insurance and law.
Unfairness that can arise from problems with an algorithm’s process or the way the algorithm is implemented, resulting in the algorithm inappropriately privileging or disadvantaging one group of users over another group. Algorithmic biases often result from biases in the data that has been used to train the algorithm, which can lead to the reinforcement of systemic prejudices around race, gender, sexuality, disability or ethnicity.
A wide-ranging field of research that deals with large datasets. The field has grown rapidly over the past couple of decades as computer systems became capable of storing and analysing the vast amounts of data increasingly being collected about our lives and our planet. A key challenge in big data is working out how to generate useful insights from the data without inappropriately compromising the privacy of the people to whom the data relates.
A software application that has been designed to mimic human conversation, allowing it to talk to users via text or speech. Chatbots are mostly used as virtual assistants in customer service, but there are also chatbot therapists and even chatbot politicians.
A field of research that uses computers to obtain useful information from digital images or videos. Applications include object recognition (e.g. identifying animal species in photographs), facial recognition (smart passport checkers), medical imaging (spotting tumours in scans), navigation (self-driving cars) and video surveillance (monitoring crowd levels at events).
An umbrella term for any field of research that involves the processing of large amounts of data in order to provide insights into real-world problems. Data scientists are a diverse tribe, ranging from engineers, medics and climatologists to ethicists, economists and linguists.
A field of research that applies data science techniques to engineering problems. It often involves collecting copious amounts of data about the object being studied (this could be a bridge, a road network, a wind turbine, or even an underground farm), and then using the data to develop computer models for analysing and improving the object’s design and functioning (see ‘digital twin’).
A form of machine learning that uses computational structures known as ‘neural networks’ to automatically recognise patterns in data and provide a suitable output, such as a prediction or evidence for a decision. Deep learning neural networks are loosely inspired by the way neurons in animal brains are organised, being composed of multiple layers of simple computational units (‘neurons’), and they are suited to complex learning tasks such as picking out features in images and speech. Deep learning thus forms the basis of the voice control in our phones and smart speakers, and enables driverless cars to identify pedestrians and stop signs. See also ‘neural network’.
Synthetic audio, video or imagery in which someone is digitally altered so that they look, sound or act like someone else. Created by machine learning algorithms, deepfakes have raised concerns over their uses in fake celebrity pornography, financial fraud, and spreading false political information. ‘Deepfake’ can also refer to realistic but completely synthetic media of people and objects that have never physically existed; or sophisticated text generated by algorithms. See also ‘generative adversarial network’.
A computer model that simulates an object in the real world, such as a jet engine, bridge, wind turbine, Formula One car, biological system, or even an entire city. Analysing the model’s output can tell researchers how the physical object will behave, helping them to improve its real-world design and/or functioning. Digital twins are a key tool in the field of data-centric engineering.
A machine learning technique that can generate data, such as realistic ‘deepfake’ images, which is difficult to distinguish from the data it is trained on. A GAN is made up of two competing elements: a generator and a discriminator. The generator creates fake data, which the discriminator compares to real ‘training’ data and feeds back with where it has detected differences. Over time, the generator learns to create more realistic data, until the discriminator can no longer tell what is real and what is fake.
An artificial intelligence system inspired by the biological brain, consisting of a large set of simple, interconnected computational units (‘neurons’), with data passing between them as between neurons in the brain. Neural networks can have hundreds of layers of these neurons, with each layer playing a role in solving the problem. They perform well in complex tasks such as face and voice recognition. See also ‘deep learning’.
Software and data that are free to edit and share. This helps researchers to collaborate, as they can edit the resource to suit their needs and add new features that others in the community can benefit from. Open source resources save researchers time (as the resources don’t have to be built from scratch), and they are often more stable and secure than non-open alternatives because users can more quickly fix bugs that have been flagged up by the community. By allowing data and tools to be shared, open source projects also play an important role in enabling researchers to check and replicate findings.
A machine that is capable of automatically carrying out a series of actions. The word ‘robot’ was coined by Czech writer Karel Čapek in his 1920 sci-fi play Rossum’s Universal Robots, but the idea of self-operating machines goes back to antiquity. Modern robots typically contain programmed computers and exhibit some form of artificial intelligence. They can include ‘humanoids’ that look and move like humans, industrial robots used in manufacturing, medical robots for performing surgery, and self-navigating drones.
Data that is generated artificially, rather than by real-world events. It is especially useful for research in areas where privacy is key, such as healthcare and finance, as the generated data can retain the original data’s statistical properties, but with any identifying information removed. Synthetic data can also be used to augment a dataset with additional data points, often to help an artificial intelligence system to learn some desirable property; or to train algorithms in situations where it is dangerous to get hold of the real data, such as teaching a self-driving car how to deal with pedestrians in the road.
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/asymptotic-statistical-methods-theory,"
"
https://www.turing.ac.uk/research/projects/global-urban-analytics-for-resilient-defence/,"
"
https://www.turing.ac.uk/research/research-projects/data-science-music,"
This project will demonstrate the application of data science and AI techniques to music analysis, for application in digital musicology, creative industries and cultural heritage. Specific focus is on addressing music analysis, multimodal analysis of performance data and music in the archive.
'Music Information Retrieval' has been a thriving domain for over a decade, and exemplifies many aspects of best practice - for example in analytics, community engagement, approaches to reproducibility and data sharing, and infrastructure (e.g. running code remotely over datasets that cannot be shared directly). As such it provides well defined challenges in data science and in working with archives it's ready for the application of emerging data science and AI techniques, and it also promotes useful practices for data scientists in other domains.
The music industry is now digital almost end-to-end, and new recordings are generated all the time. Feature extraction can help throughout the life cycle of music from composition, recording and production to distribution, consumption and re-use. Hence this work can be applied by both music producers and consumers.
There are also creative applications, for example using AI, algorithmic composition and sonification, as well as algorithmic enhancement of instruments.
[email protected]
"
https://www.turing.ac.uk/people/guest-speakers/nina-desranges,"
Master's student in Sociology and Anthropology at the Federal University of Rio de Janeiro (UFRJ), and holds a Bachelor of Arts degree, summa cum laude, from the same institution. Since 2019, she has been linked to the Digital Studies Laboratory (LED-IFCS) and the Latin American Network of Studies on Surveillance, Technology and Society (Lavits), where she researches technology through an anthropological and local perspective. Works as a volunteer in the subnational coding of public policies in Brazil, in the Oxford COVID-19 Government Response Tracker (OxCGRT) project at the Blavatnik School of Government at the University of Oxford. She has an interest in the sociology of technology and experience with digital methods and qualitative research, in particular digital ethnography. At the Institute for Technology & Society of Rio, she is a Researcher in Law and Technology and GovTech.
"
https://www.turing.ac.uk/research/research-areas/algorithms/distributed-algorithms,"
"
https://www.turing.ac.uk/research/research-areas/machine-learning/computer-vision,"
"
https://www.turing.ac.uk/research/research-projects/economic-networks-and-transaction-data,"
This project will build understanding of networked data through a combination of data processing for firm-to-firm transactions data held by Office for National Statistics (ONS), economic modelling and forecasting using data from the whole economic system, and the development of methods for efficiently reducing the dimensionality and complexity of network-generated data.
When analysing high-dimensional panels of macroeconomic or financial time series state-of-the-art methods tend to suffer from at least two main limitations.
Firstly, they often consider only linear dependencies. In contrast, real such time series are interconnected through a complex network of dependencies, and this network can display systemic risky behaviour. Secondly, the way in which factors affect data is seldom modelled, and typically only fully pervasive factors that affect all time series with similar magnitude are considered. In contrast, real such time series are collections of heterogeneous time series with different types of series influences by a diverse range of factors. 
Furthermore, traditional theoretical models in economics and finance assume that in large ecosystems the influence of a few individual agents is negligible. Yet in large economic and financial panels there often exist one or more subsets of time series that influence the entire cross-section. For example, with a few time series leading on other time series which can be viewed as lagging behind the leading series. One potential modelling approach is to suppose that the set of agents can be partitioned into clusters, such that within clusters there are no consistent lead-lag relationships, while across clusters there exist stable lead-lag relationships. 
The envisioned research will combine methods from machine learning and network analysis to reveal such hidden factors in the analysis of two types of high-dimensional data sets, with the aim of revealing complex relationships inherent within the data and improving forecasting accuracy. The first type of data is a network capturing pairwise interactions, such as cross-sectoral networks of input-output connections. The second type is a panel of economic time series, where the network structure can be examined by considering the correlations between different data streams.
The project aims to produce a dataset of firm-to-firm transaction flows in the UK to provide high frequency/high geographic resolution indicators to help understand the impact across the UK on issues such as:
Alongside this, the project aims to provide methodological improvements for working with network and high dimensional economic data in order to provide accurate and informative aggregates for policy dimensions and forecasts.
The outputs of this project will provide policy makers with detailed economic indicators at high levels of frequency and regional specificity. These indicators could be used to explore:
Tony Zemaitis[email protected]
"
https://www.turing.ac.uk/people/researchers/cagatay-turkay,"
Cagatay Turkay is an Associate Professor at the Centre for Interdisciplinary Methodologies at the University of Warwick, UK. Before that he was first a Lecturer and then a Senior Lecturer at the Department of Computer Science at City, University of London between 2014 to 2019. He received his PhD degree in Visualisation from University of Bergen, Norway in 2014 and served as a visiting research fellow at Harvard University in 2013.
His research has been supported by national (EPSRC, NERC), international (EC H2020) and industry funded projects. He frequently publishes his research on visualisation journals such as IEEE TVCG, CGF, and IEEE CG&A, as well as journals in machine learning and data mining, and also recently co-authored a coursebook titled “Visual Analytics for Data Scientists’. Turkay has been in editorial roles for journals such as ACM Transactions in Interactive and Intelligent Systems, Computers and Graphics, and IEEE CG&A and co-organising and chairing events such as IEEE VIS, BioVis and EuroVA to name a few. Cagatay has been awarded the EuroVis Young Researcher 2019 award and named a EuroGraphics Junior Fellow in 2019. 
Cagatay’s research investigates the interactions between data, algorithms and people, and explores the role of interactive data visualisations and other interaction mediums such as natural language at this intersection. He designs techniques and algorithms that are sensitive to their users in various decision-making scenarios involving primarily high-dimensional and spatio-temporal phenomena, and develops methods to study how people work interactively with data and computed artefacts. 
"
https://www.turing.ac.uk/news/publications/performance-midi-score-conversion-neural-beat-tracking,"
Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S.
L. Liu, Q. Kong, V. Morfi, and E. Benetos, ""Performance MIDI-to-score conversion by neural beat tracking"", in 23rd International Society for Music Information Retrieval Conference (ISMIR), Dec. 2022.
253 KB
"
https://www.turing.ac.uk/people/guest-speakers/robert-winston,"
Lord Winston is Professor of Science and Society and Emeritus Professor of Fertility Studies at Imperial College London. In the 1970s he developed gynaecological surgical techniques that improved fertility treatments. He later pioneered new treatments to improve in vitro fertilisation (IVF) and developed pre-implantation diagnosis. This allowed embryos to be screened for genetic diseases and has allowed parents carrying faulty genes to have children free of illnesses such as cystic fibrosis.He now runs a research programme at the Institute of Reproductive and Developmental Biology at Imperial College that aims to improve human transplantation.
Robert Winston has over 300 scientific publications about human reproduction and the early stages of pregnancy.Robert Winston is also Chairman of the Genesis Research Trust – a charity which raised over £13 million to establish the Institute of Reproductive and Developmental Biology and which now funds high quality research into women’s health and babies.
"
https://www.turing.ac.uk/people/guest-speakers/curtis-langlotz,"
Dr. Langlotz is Professor of Radiology and Biomedical Informatics and Director of the Center for Artificial Intelligence in Medicine and Imaging (AIMI Center) at Stanford University.  The AIMI Center comprises over 120 Stanford faculty from 20 departments who conduct interdisciplinary machine learning research that optimize how clinical data are used to promote health.  Dr. Langlotz’s laboratory investigates the use of deep neural networks and other machine learning technologies to help physicians detect disease and eliminate diagnostic errors. He has led many national and international efforts to improve medical imaging, including the Medical Imaging and Data Resource Center (MIDRC), a U.S. national COVID-19 imaging research repository. 
As Associate Chair for Information Systems and a Medical Informatics Director for Stanford Health Care, he is responsible for the computer technology that supports the Stanford Radiology practice, including 7 million imaging studies that occupy 0.7 petabytes of storage. He currently serves on the Board of Directors of the Radiological Society of North America (RSNA) as Liaison for Information Technology. He has founded three healthcare IT companies, including Montage Healthcare Solutions, which was acquired by Nuance Communications in 2016.
 
"
https://www.turing.ac.uk/people/former-researchers/christina-last,"
Christina Last is a Research Data Scientist in the Research Engineering Group at the Turing, where she specialises in building machine learning solutions to understand our built environment. Her work bridges active mobility, urban data science, and machine learning operations. Christina has collaborated with a number of city governments and international organizations on software, data science, and strategic planning, in both developed and rapidly developing urban environments in the US, the UK, and Vietnam. She has led various international research projects, most recently as a Senior Data Scientist collaborating with UNICEF to model air quality during COVID-19 lockdowns using machine learning.
Before joining the Turing, Christina was the Lead Data Scientist at a property technology start up, where she managed the development of the data ecosystem and machine learning algorithms to aid the real estate investment deal flow - from sourcing and purchasing to managing residential real estate. She holds a Bachelors from the Department of Geography at the University of Bristol, and completed a sabbatical at the University of California. In her spare time Christina, devotes her time to building tech communities as the AICamp London Chapter Lead, most recently forming a partnership with Google London.
 
Christina’s work at Turing will combine her interests in urban data science and natural language processing, she will work on Living With Machines, a collaboration between historians, data scientists, geographers, computational linguists, and curators at the British Library; and the Universities of Cambridge, East Anglia, Exeter, and London (QMUL); to examine the human impact of industrial revolution. She will develop toponym matching techniques using deep learning to identify and link key geographic features to textual descriptions from archival documents.
In the future, Christina is interested in modelling pedestrian flows using novel measurement and machine learning analysis techniques in developing urban environments, in order to identify communities to understand where walking and non-motorized transportation options should be prioritized. She aims to develop software and hardware solutions to lay the groundwork for permanent and long-term improvements to walking conditions on the city’s streets.
"
https://www.turing.ac.uk/people/researchers/chris-holmes,"
Chris Holmes is Programme Director for Health and Medical Sciences at The Alan Turing Institute.
He is Professor of Biostatistics at the University of Oxford with a joint appointment between the Department of Statistics and the Nuffield Department of Clinical Medicine through the Wellcome Trust Centre for Human Genetics and the Li Ka Shing Centre for Health Innovation and Discovery. 
Before joining Oxford, Chris was based at Imperial College, London, and also worked in industry conducting research in scientific computing. He holds a Programme Leader’s award in Statistical Genomics from the Medical Research Council UK. In 2016, WIRED UK magazine named him one of the ‘Innovators of the year in AI’.
Chris has a broad interest in the theory, methods and applications of statistics and statistical modelling. He is particularly interested in pattern recognition and nonlinear, nonparametric statistical machine learning methods applied to the genomic sciences and genetic epidemiology.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/dutch-authority-financial-markets-afm,"
The Dutch Authority for the Financial Markets (AFM) is the financial regulator for the Netherlands. While the Dutch Central Bank focuses on the stability of the Dutch financial markets, the AFM is responsible for regulating the behaviour on the Dutch financial markets. As such, its responsibilities include supervising sectors such Accountancy, Insurance, Asset Management, and the capital markets. The AFM’s main objective is contributing to transparent and fair financial markets. Since Brexit, the AFM is supervising the largest European Trading Venues, encompassing both equities, bonds, and commodities. The AFM has around 600 full time employees and is headquartered in Amsterdam.
"
https://www.turing.ac.uk/news/children-are-excited-ai-many-do-not-feel-safe-online-new-research-finds,"
Children in Scotland are excited about the potential for AI to support children’s human rights, but have concerns about how it can be developed and used safely, according to a new report published by The Alan Turing Institute, the Scottish AI Alliance and Children’s Parliament today (Wednesday 17 May).
The report also shows that many of the children do not feel happy, healthy or safe online but that they did have a trusted adult to talk to about these concerns.
Researchers found that the children were familiar with the concept of AI being a part of their everyday lives but did not necessarily understand the detail of where and how it is used. Through this project the children learnt about uses of AI in healthcare, education, policing and entertainment.
Dr Mhairi Aitken, Ethics Research Fellow at The Alan Turing Institute and project lead, said: “AI is increasingly becoming part of children’s day-to-day lives. Our report provides a first step in exploring how children understand, experience, and engage with AI. We hope that it could provide much-needed guidance for policymakers and AI developers to inform future approaches to child-centred AI.”
The children, known as the ‘AI Team’, participated as part of a creative, rights-based project run in collaboration between The Alan Turing Institute, Scottish AI Alliance and Children’s Parliament. 
Phase one ‘Exploring children’s views on AI’ of the project worked with children from four schools across Scotland to find out their current knowledge of AI and what they think future possibilities and risks are for AI. The children had lots of thoughts on some of the specific ways that AI might affect them as children.
Across all school groups, students were confused about which technology uses AI and which does not. Only 11 children out of 87 surveyed were confident or very confident that they knew what AI is and what it does in their everyday lives. 
In particular, the children interviewed wanted to learn about how AI can be fair, or unfair, to children. This ranged from fairness in data collection to the fairness of outcomes decided by an AI system, rather than a human.
They also said they felt concerned about risks associated with AI systems making decisions without enough human involvement. 
A Member of the Children’s Parliament (MCP), aged ten from Glasgow, said: “It is important for children to know about artificial intelligence because it is the future and it is good to learn new things when they affect our lives.”
Steph Wright, Head of the Scottish AI Alliance, said: “The Scottish AI Alliance is committed to ensuring that the voices of children are heard in the development of Scotland's AI future and the completion of the first phase of this project is a great step in this direction.  We look forward to continuing this work with Children's Parliament and The Alan Turing Institute.""
Gregory Metcalfe, Project Lead at Children’s Parliament, said: “It’s been incredibly rewarding introducing children’s voices to the field of AI in Scotland. The children have a lot to say on the topic and we’re really looking forward to making sure their views and opinions are taken up meaningfully as the project progresses.”
To mark the end of this phase of the project, a group of children representing the four classes presented their work at the Scottish AI Summit held in Glasgow at the end of March 2023.
"
https://www.turing.ac.uk/events/turings-cabaret-dangerous-ideas-london,"
What do you get if you combine top academics, contentious research and a comedian compère? Answer: the Cabaret of Dangerous Ideas (CoDI)!
As part of London Data Week, we're bringing CoDI to The Camden Club for a night of edgy, exciting AI and data science-based entertainment with a comedy twist.
Doors open at 19:00 for a 19:30 start. Adult content, suitable for 18+.
Hosted by comedian Susan Morrison, and now in its eleventh year, CoDI is ninety minutes of rapid-fire research from some of the finest minds in the country. In this London special, Turing researchers will take to the stage to challenge ideas around AI, data science and technology. From AI in the justice system to conspiracy theories and beyond, this show is one not to miss.
Wake up Sheeple: Unraveling the Sock Matrix - Anna George
Attention truth seekers! Leave the herd and explore the rabbit hole of conspiracy theories. Prepare for revelations about the infamous flat Earth, climate change, and sock thieves. Get ready for science, laughter, and the secret whereabouts of your lost socks.
The algorithm will sentence you now! - Cari Hyde-Vaamonde
Is replacing judges with AI a horrifying prospect? Could algorithms have more impartiality than judges? Cari Hyde-Vaamonde explores whether technology could help make better decisions and laws for our justice system.
Feeling the AI fear? - Michael Katell
ChatGPT is coming for our jobs! AI faked Drake! Well, there’s good news: The bad news is not what you think it is. The fear is by design. It’s the point!
The Camden Club
72-73 Chalk Farm Road,
Chalk Farm
London
NW1 8AN
"
https://www.turing.ac.uk/people/researchers/thomas-gorochowski,"
Thomas Gorochowski is a Royal Society University Research Fellow and Co-Director of the Bristol BioDesign Institute. Having received an MEng in Computer Science from the University of Warwick and PhD in Complex Systems Theory from the University of Bristol, he has since transitioned into the area of Bioengineering, working across both industry as a Marie Curie Fellow at DSM in the Netherlands, and academia as part of the Synthetic Biology Centre at the Massachusetts Institute of Technology, USA. In 2016, he established the Biocompute Lab at the University of Bristol to better understanding the computational architecture of biological systems from the molecular to ecosystem level with the goal of rationally engineering new functionalities across these scales.
Thomas’ research interests encompass a broad range of scientific areas covering both theoretical and experimental approaches to reprogramming living cells. At the Turing he is aiming to establish a community to explore Data-centric Biological Design that brings together advances in high-throughput experimental methods and data science to accelerate and guide more effective bioengineering practices.
"
https://www.turing.ac.uk/taxonomy/term/1279,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/barking-havering-and-redbridge-university,"
www.bhrhospitals.nhs.uk
"
https://www.turing.ac.uk/people/business-team/patricia-holley,"
I work in the Academic Engagement team and support the work of Interest Groups and the Theory and Methods Challenges Fortnights. Previously, I was the Manager of the Jean Golding Institute for data science at the University of Bristol and I have been working in research administration and project management for over 14 years.
I grew up in Ecuador and worked in the Galapagos Islands studying Nazca Boobies and Waved albatrosses. I also completed a Master’s degree in Biological Sciences at Wake Forest University, North Caroline, USA, where I studied the feeding trajectories of Hawaiian albatrosses during the nesting season.
"
https://www.turing.ac.uk/people/researchers/clare-verrill,"
Clare Verrill is Associate Professor of Cellular Pathology with the Nuffield Department of Surgical Sciences at the University of Oxford and Honorary Consultant Cellular Pathologist with Oxford University Hospitals NHS Foundation Trust. Her sub speciality is urological pathology and has her own research group focussing on digital pathology and AI.  The NHS Cellular Pathology Laboratory in Oxford has achieved the milestone of scanning 100% of surgical histology workload under her oversight. She is Oxford Principal Investigator for Oxford PathLAKE (one of the UK Government’s AI Centres of Excellence) and has worked with computer science partners to develop AI. She is the Principal Investigator of a phase 4 AI in Healthcare study to evaluate Paige Prostate AI in NHS settings. 
"
https://www.turing.ac.uk/research/research-projects/machine-learning-prediction-plant-chemical-production,"
Plants are really good chemical engineers: about 50% of all drugs in current use are natural products or natural product-inspired. Unfortunately, many of these plant natural products cannot be directly accessed, and most have yet to be discovered. Triterpenes are plant natural products that are synthesized from a single linear substrate through an origami-like process by enzymes known as oxidosqualene cyclases (OSCs). This project investigates the use of machine learning methods for modeling this synthesis process, and predicting different possible products.
The existing machine learning approaches to modeling chemical reactions are not sufficient for modeling the process by which the linear substrate 2,3-oxidosqualene is synthesized into different triterpene scaffolds. First, most existing models do not adequately take three-dimensional structure into account. Three-dimensional structure information is essential for our purposes, as it is necessary to explicitly model the process by which the substrate is folded into a new conformation. Second, this setting is unusual in that all reactions begin from the same substrate: it is specifically necessary to model which of many possible configurations is produced, conditional on the presence of a particular oxidosqualene cyclase (OSC).
The first stage of the project will require examining circa 180 triterpene scaffolds which have been identified, with the goal of framing the problem and the data in a manner amenable to building a probabilistic model. A primary challenge will be identifying the granularity at which to model the process, as well as representations used as inputs into a machine learning model. The project hopes to leverage molecular docking software to help inform the model. The work will also investigate the possibilities of directly modelling the conformational intricacies that govern favoured arrow pushes seen in these reaction mechanisms. 
Triterpenes are plant natural products that are synthesized from a single linear substrate, 2.3-oxidosqualene, through an origami-like process by enzymes known as oxidosqualene cyclases (OSCs).  Over >200 different triterpene scaffolds are known. However, while OSC genes can be readily predicted in plant genomes it is not, for the most part, currently possible to predict the nature of the cyclization products of these enzymes. Successful prediction of OSC products based on sequence will require both a deep understanding of triterpene biosynthesis as well as novel machine learning methodology.
This project is expected to enable generation of hypotheses about mechanisms of triterpene cyclisation that can subsequently be tested experimentally. It will also pave the way for prediction of the nature of triterpene scaffolds based on OSC sequences.
Triterpenes have a wealth of applications across the health, agriculture and industrial sectors. The ability to harness the process of triterpene cyclization and to predict the nature of OSC products based on genome sequence will greatly accelerate endeavours to engineer triterpenes for use as drugs and for other applications.
"
https://www.turing.ac.uk/people/guest-speakers/syed-kazi,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/office-low-emission-vehicles,"
www.gov.uk/government/organisations/office-for-low-emission-vehicles
"
https://www.turing.ac.uk/research/interest-groups/low-dimensional-structure-data-and-graphs-models-analysis-and-algorithms,"
Modern data, even though large and high-dimensional, often contains 'simple' structure, with only a few degrees of freedom. Leveraging this structure allows for the design of efficient algorithms to collect, process, and communicate the data. For example, a large but low-rank matrix can be completed after observing relatively few of its entries at random. Alternatively, in social networks, the wealth of interaction patterns can be insightfully represented as a few clusters of individuals. Moreover, the ability of deep learning to generalise for unseen data relies on such low-dimensional structure. Many more such examples exist across a variety of applications and scientific disciplines.
This Interest Group is dedicated to the study and development of tools that can produce low-dimensional representations of large and complex data sets, and to the application of such tools across a variety of domains. Our goal will be to improve state-of-the-art and to enable collaboration between members of the interest group, as well as between researchers across the wider scientific community.
In addition to any theoretical underpinnings, we also strive to develop fast and scalable algorithms for such problems, borrowing tools from a wide range of areas such as: network analysis, graph representation learning, time series clustering and anomaly detection, deep learning, stochastic and distributed optimisation, compressed sensing, (numerical) linear algebra, (high-dimensional) statistics, approximation theory, probability and others. We will also explore specific themes such as generalisation errors for deep neural networks.
An area of particular interest is that of inverse problems on graphs. In order to model complex and heterogeneous data, a prominent approach is to represent the data as a graph. Graphs have received significant attention over the last decade, and numerous methods have been proposed for analyzing their low-dimensional structure. The structural properties of such graphs can be used to model interactions within a network, or to capture geometric and statistical information about the data itself. Areas of focus include instances where the empirical data is incomplete or inconsistent, and one exploits the underlying structure (e.g., low-rank structure) to design scalable algorithms that are robust to noise and sampling sparsity.
Click here to request sign-up and join
[email protected]
Andrea Pizzoferrato, University of BathFlorian Klimm, Max Planck Institute for Molecular GeneticsFrancois Lafond, University of OxfordGiorgos Bouritsas, Imperial College LondonKarel Devriendt, University of OxfordLuca Zanetti, University of CambridgeMartin Lotz, University of WarwickPier Luigi Dragotti, Imperial College LondonStephane Chretien, Turing Visiting researcher, National Physical Laboratory, University of LyonShuaib Choudhry, University of WarwickWei Dai, Imperial College LondonXiaowen Dong, University of OxfordYijie Zhou, University of Warwick
"
https://www.turing.ac.uk/contact-us/how-get-alan-turing-institute,"

The Alan Turing Institute is located on the first floor of the British Library, 96 Euston Road, London NW1 2DB, London, UK.
During the British Library's opening hours all guests can access The Alan Turing Institute via the Library's main entrance on Euston Road. If you have a meeting or event outside of those hours, your host can register you to access the staff entrance.
Once at the Turing, please report to reception to pick up your visitor lanyard.
Header image credit: Brian Walker
 
The Institute is located adjacent to St Pancras International Rail Station and within five minute walk of Kings Cross Rail Station and 10 minute walk of Euston Rail Station.
The nearest underground station is Kings Cross St Pancras Station (five minute walk), which is on the Circle, Hammersmith & City, Metropolitan, Northern (Bank branch), Piccadilly and Victoria underground lines.
There are many nearby services, including 10, 30, 59, 63, 73 and 91.
There is no onsite parking.
The nearest car park is the NCP one in Judd Street, 10 minutes' walk away.
Metered parking (08:30 – 18:30) is available on Ossulston Street (3 spaces only).
There are covered and uncovered areas for bicycles. The best approach from central London is via Tavistock Place and Cartwright Gardens.
 
Enter the British Library via the main entrance on Euston Road.
To access the first floor via stairs take the main staircase to the left of the ticket and membership desk. Once at the large central bookcase display, turn right and head towards the sign for The Alan Turing Institute.
To access the first floor via lift go right after entering the British Library, past the ticket and membership desk and fountain, and turn left at the bookshop. Continue until reaching Lift 6. Take the lift to the upper ground floor (UG). Once on the upper ground floor, exit the lift and take one of the three lifts diagonally across to the first floor. On the first floor, exit the lift, turn left and following the sign for The Alan Turing Institute.
The Alan Turing Institute entrance is illuminated with signage over the doorway, behind the display of the enigma machine. Please report to reception once you arrive at the entrance.
"
https://www.turing.ac.uk/people/guest-speakers/max-welling,"
Max Welling is a Distinguished Scientist within Microsoft Research (MSR) AI4Science, based in Amsterdam. He finished his PhD in theoretical high energy physics under the supervision of Nobel laureate Gerard ‘t Hooft. Max is also a Professor and Research Chair in Machine Learning at the University of Amsterdam where he directs the research group AMLAB. He is a fellow at the Canadian Institute for Advanced Research (CIFAR) and the European Lab for Learning and Intelligent Systems (ELLIS) where he serves on the founding board. His previous appointments include VP at Qualcomm Technologies, Professor at UC Irvine, postdoc at University of Toronto and UCL under the supervision of Geoffrey Hinton and postdoc at Caltech under the supervision of Pietro Perona. 
Max has served as Associate Editor in Chief of IEEE TPAMI from 2011-2015. Since 2015, Max has served on the advisory board of the NeurIPS foundation. Additionally, he has been Program Chair and General Chair of NeurIPS in 2013 and 2014 respectively. He was Program Chair of AISTATS in 2009, ECCV in 2016 and General Chair and Co-Founder of MIDL 2018.
Max was the recipient of the ECCV Koenderink Prize in 2010 and the ICML Test of Time award in 2021. Max is co-author of a number of influential papers that introduced Variational Autoencoders, Graph Neural Networks and Equivariant Neural Networks into the machine learning community.
"
https://www.turing.ac.uk/people/turing-research-fellows/carolyn-ashurst,"
Carolyn is a Senior Research Associate in Safe and Ethical AI at the Alan Turing Institute. Her work is motivated by the question: How do we ensure AI and other digital technologies are researched, developed and used responsibly? Her research into algorithmic fairness seeks to understand the fairness implications of data-driven systems from both theoretical, practical and domain specific lenses. As well as mitigating the impacts from deployed systems, her work in responsible research seeks to understand the role of the machine learning (ML) research community in navigating the broader impacts of ML research.
Carolyn also works to convene technical, policy, and domain experts, for example to ensure that regulators have access to necessary technical expertise. She has facilitated discussions between various communities through workshops co-organised with the ICO, NICE and CDEI. She also sits on a range of advisory boards and working groups, including with the FBI, CSIS, ICO and the Turing Research Ethics process. Previously she worked as a senior researcher scholar at Oxford, and as a data and research scientist in various roles within government and finance. She holds a PhD in maths from the University of Bath.
"
https://www.turing.ac.uk/people/external-researchers/manuela-veloso,"
Manuela Veloso is the Head of J.P. Morgan AI Research & Herbert A. Simon University Professor Emeritus in the School of Computer Science at Carnegie Mellon University, where she was previously Head of the Machine Learning Department. She served as president of AAAI, and the co-founder and a Past President of the RoboCup Federation. She is Fellow of AAAI, IEEE,  AAAS, and ACM. She is an international expert in artificial intelligence and robotics. 
At CMU, she founded the CORAL lab for  research and engineering of agents that Collaborate, Observe, Reason, Act, and Learn. With her students, she contributed planning and learning algorithms for soccer robot teams, and autonomous service robots, the CoBot robots. Realizing the fact that robots, and AI agents, inevitably have perceptual, cognitive, and actuation limitations, she introduced a new symbiotic autonomy, in which robots proactively ask humans for help to compensate for their limitations. With her students, their robot soccer teams were RoboCup champions multiple times, and their CoBot robots symbiotic-autonomously traverse the corridors of the university buildings for more than 1,000 kms. 
In 2018, Manuela Veloso joined JPMorgan Chase to create and lead a new AI Research group. She has since gathered a team of close to 80 AI PhD and MS researchers focusing on AI in Finance, in the areas of financial crime, data sharing, multiagent market systems, responsible AI, and process automation. 
Manuela Veloso has co-authored more than 400 publications and has now advised 45 PhD students, who are leaders in academia and industry.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/health-and-safety-executive,"
"
https://www.turing.ac.uk/events/public-dialogue-location-data-ethics,"
The Alan Turing Institute invites you to attend a webinar to discuss the findings from an independent Public Dialogue on Location Data Ethics with a panel of leading data experts.
The report was commissioned by the Geospatial Commission, an expert committee in the Cabinet Office, with support from the UKRI’s Sciencewise programme, and was delivered by public dialogue and data specialists Traverse and The Ada Lovelace Institute.
This event will cover the launch of the independent public dialogue report and will provide a chance for speakers to reflect on some of the key findings and explore further opportunities and challenges for location data ethics. At the end of the event, there will be an opportunity to network.
The public dialogue consulted 85 members of the public over four workshops and the report identifies their aspirations and concerns in relation to location data, as well as suggesting how to mitigate ethical risks and grow public trust.
The report includes findings on the public’s awareness of location data and its benefits, which was initially low; data security and their feelings of insecurity; accountability and governance of users of data; and consent, with participants generally comfortable with personal data being used in an aggregated form.
 


"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-nottingham,"
"
https://www.turing.ac.uk/people/enrichment-students/alexander-lyttle,"
Alex is an astrophysics PhD student at the University of Birmingham. He is researching hierarchically modelling many stars with machine learning methods to improve inference of their ages, masses, and chemical compositions. More accurate stellar properties help decode the evolutionary history of the Milky Way and exoplanet systems. He uses photometric time series data from NASA missions, such as Kepler and TESS, to study the oscillations of stars, unlocking information about their interiors through asteroseismology.
"
https://www.turing.ac.uk/news/filed-under/new-research,"
"
https://www.turing.ac.uk/people/spotlights/heather-selley,"

I am currently part of The Alan Turing Institute’s Enrichment scheme, where I am working on applying machine learning techniques to satellite imagery to detect structural changes in ice shelves in Antarctica. Mapping these structures and how they change will help illuminate the response of ice shelves to climate forcings (any factors that affect the Earth’s climate, such as warming air and ocean temperatures). I am working with Alison Heppenstall and Nicolas Malleson, both based at the Leeds Institute for Data Analytics (LIDA), for six months.
I went to Iceland on a school trip during my A-Levels and saw a glacier for the first time; I have been fascinated ever since. I was then introduced to the world of satellite imagery and Earth observation during my undergraduate degree – it’s unbelievable how much information it can provide us with. For example, it allows us to view these beautiful areas and to look at how Antarctica is changing from over 16,000 km away.
I hope my research will help improve our understanding and projections of Antarctica’s contribution to global sea level rise and response to changing climate forcings. I also hope it highlights the importance of continued coverage of high-resolution imagery over the polar regions, and that it helps to raise  awareness of the climate crisis.
I recently had my proposal accepted to name nine glaciers in Antarctica.  It feels fantastic to have a permanent legacy of my work studying Antarctica. I hope naming the glaciers raises even more awareness of the climate crisis and the need for action now. In particular, I believe naming a glacier after Glasgow, the location of COP26, highlights that now is the time to decide what we want the legacy of our generations to be. 
COP26 marks an important moment in time. Scientifically, it is still possible to limit global warming to 1.5°C. However, it requires strong, rapid and sustained reductions in greenhouse gas emissions now. The disruptions to our climate system increase in direct relation to our greenhouse gas emissions. My hope is that ambitious concrete actions, timeframes and commitments will be made.
I can be found baking and dancing or fighting to improve mental health and diversity in academia.
 
Find out more about the Glacier naming and view Heather’s session in the Blue Zone at COP26.
"
https://www.turing.ac.uk/people/guest-speakers/daniel-andre,"
Daniel has been involved in Synthetic Aperture Radar (SAR) research during the course of his radar career based at DERA, QinetiQ, Dstl and now Cranfield University, where his is lecturer and head of the Ground-Based SAR Laboratory. He has won the John Benjamin Memorial prize for developments in enhancing SAR image coherence products over mountainous terrain. Current research areas include 3D multistatic SAR image formation, coherence and polarimetry. He has a joint honours BSc in Mathematics and Physics and a PhD in Applied Mathematics, both from the University of Bristol.
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/non-parametric-semi-parametric-methods,"
"
https://www.turing.ac.uk/people/guest-speakers/birgitte-freiesleben-de-blasio,"
Dr Birgitte Freiesleben de Blasio is professor II at the University of Oslo and department director at the Norwegian Institute of Public Health (NIPH) and heads the COVID-19 modelling team at NIPH, a collaboration between researchers at NIPH, University of Oslo, Norwegian Computing Centre and Telenor Research.
Her research is focused on mathematical modelling of infectious diseases to assist policy decision-making and social network analysis related to understanding transmission patterns of infectious diseases.
She has an MSc (1997) and PhD (2002) from the Niels Bohr Institute, University of Copenhagen, Denmark.
"
https://www.turing.ac.uk/people/enrichment-students/timothy-sum-hon-mun,"
Timothy is a PhD student in the Blackledge Lab at the Institute of Cancer Research and part of the Radiotherapy and Imaging Department at the Royal Marsden Hospital. His research interests are on the application of interpretable and reliable AI in the medical imaging domain for cancer research focused on soft tissue sarcomas. His research is funded as part of the international Sarcoma Accelerator, funded by Cancer Research UK, which will collect one of the largest repositories of multi-center sarcoma datasets.
Soft tissue sarcomas (STS) are rare tumours arising in connective tissues and can occur almost anywhere in the body. Approximately 50% of patients will relapse following surgery developing both local tumour re-growth and distant metastatic disease, which will significantly reduce patient survival. Surgeries are highly invasive leading to poor patient quality of life and functional impairment. It is therefore important that we develop techniques that can help decide on which patients are most likely to relapse following surgery so that preventative measures can be adopted early on.  His research with Turing Institute will be to develop a model for predicting whether a patient with STS is likely to relapse following surgery by combining medical imaging data with other modalities using novel deep-learning (DL) approaches.
"
https://www.turing.ac.uk/news/publications/report-where-are-women-mapping-gender-job-gap-ai,"
As AI becomes ubiquitous in everyday life, closing the gender gap in the AI and data science workforce matters. The fields are particularly fast-moving, so it is important to comprehensively map how these gaps are manifest across different industries, occupations, and skills.
This policy briefing from The Alan Turing Institute’s Women in Data Science and AI project is a contribution to this endeavour, charting women’s participation in data science and AI in the UK and other countries. By presenting a new, curated dataset, analysed through innovative data science methodology, we are able to explore in detail the gendered dynamics of data science.
Our research findings reveal extensive disparities between women and men in skills, status, pay, seniority, industry, job, attrition and educational background. We therefore call for effective policy responses if society is to reap the benefits of technological advances.
Full report
Young, E., Wajcman, J. and Sprejer, L. (2021). Where are the Women? Mapping the Gender Job Gap in AI. Policy Briefing: Full Report. The Alan Turing Institute.
Summary
Young, E., Wajcman, J. and Sprejer, L. (2021). Where are the Women? Mapping the Gender Job Gap in AI. Policy Briefing: Summary. The Alan Turing Institute.
3.7 MB
1.9 MB
"
https://www.turing.ac.uk/blog/how-write-great-data-study-group-application,"
The point of this post is to give you some guidance on what we’re looking for when we review the applications. I hope you find it helpful.
Back in November, I headed home from the office early and opened up 130 applications for the Data Study Group which coming up in December 2017.
Some of the applications were outstanding. We had students from mathematics, statistics, bioinformatics, computer science, engineering, political science, psychology and philosophy. There were data scientists working in startups and multinational corporations (and a few companies in between!) It was a joy to read how data science was being harnessed and advanced by so many interesting people.
But (there’s always a but) some of the applications were difficult to score.
I’ll summarise the questions you will be asked in the application form::
For the first two questions we also ask you to share with us some links that provide some evidence for your answers.
We ask about technical skills because we want to deliver the very best data science solutions to real world problems. There are lots of ways to evidence that you have technical skills, but a list of programming languages is not a great one. Think about how the challenge team would benefit from your expertise. What problems have you solved using these skills? Why might they be relevant for a fast-paced data study group week?
A great way to share evidence for your technical skills is to share links to GitHub repositories. We understand that not everyone is able to publicly share the code they’ve written, so a dropbox link, or a link to a google drive file, works very well. The best answers provide specific links to skills that were highlighted in the technical skills answer.
Another top tip from a busy person: check that the links work before you send them. Write up your application outside of the google form and send it to a friend. Make sure they can access all the files you want the reviewers to see.
Making sure all our DSG participants have fun and learn from each other is of equally high priority as our goal of delivering world class solutions.
I noticed when I read the December applications that most people answered a version of “What collaborative or interdisciplinary work have you done?” These answers missed a focus on how your team would benefit from having you there. Just because you have worked with others doesn’t mean you’re good at it. Show us how you will help complete a challenging project in one week.
Here’s my second top tip: clear communication is key. Your application itself is evidence of how well you can convey your expertise to an audience outside of your specialty. Can you bring out the best in others? Have you done this in the past? Can you be creative in how you evidence this understanding of your teammates’ motivations? (I think you can!) Do you have leadership or project management expertise? Make us excited to have you with us for the week!
Your answer to the question of what you want to get out of the Data Study Group contributes to your score in two distinct ways. The first is whether you understand what a data study group entails: are your expectations aligned with what can be achieved during the week? Have you accurately assessed how you can contribute and what you can learn?
We also want to test whether our Data Study Group is right for you. Are you looking to change the world when you graduate from your PhD? Do you want a taster of what industry collaboration would look like? Are the opportunities to work with real world data limited in your current position? Data Study Groups allow us to share the experience that we have very day at the Turing of working with great people from a huge range of backgrounds from around the world.
We look forward to hearing from you with your applications for our next Data Study Group.
"
https://www.turing.ac.uk/research/research-projects/advancing-data-justice-research-and-practice,"
Advancing data justice research and practice is a collaboration between the Global Partnership on AI (GPAI), The Alan Turing Institute, 12 policy pilot partners, and participants and communities across the globe. The project aims to widen the lens of current thinking around data justice and to provide actionable resources that will help policymakers, practitioners, and impacted communities gain a broader understanding of what equitable, freedom-promoting, and rights-sustaining data collection, governance, and use should look like in increasingly dynamic and global data innovation ecosystems.

This project aims to fill a gap in data justice research and practice and provide resources that help policymakers, practitioners, and impacted communities gain a broader understanding of data governance. It is our hope that our resources are open and accessible to anyone interesting in learning more about data justice and how to promote it within their context. The research aims to offer practical guidance and conceptual framings for illuminating how historically rooted conditions of power asymmetry, inequality, discrimination, and exploitation are drawn into processes of data protection, extraction, and use. This includes considerations of equity and data justice informed by affected communities that encompass questions of access to, and visibility and representation in, data used in the development of AI and machine learning (AI/ML) systems. To achieve this, the research proposes six pillars of data justice power, equity, access, identity, participation, and knowledge. These provide a framing on how stakeholders can understand and engage in critical reflection of their data practices. 
The project outputs include (a) an assessment of the current state of research in this area and the identification of gaps to create a forward-looking research agenda and (b) Data Justice in Practice Guides for three target audiences: policymakers, developers, and communities impacted by AI/ML systems. The guidance includes practical questions to consider in the practice, use, and experience of AI/ML systems, with particular emphasis on realising the 2030 Sustainable Development Goals. 
This research includes valuable contributions of many individuals and civil society organisations through consultations and an online participatory platform decidim. The 12 policy pilot partners from around the world made significant contributions, through their own research reports informed by a series of interviews and workshops, and by providing iterative feedback on our research throughout the project.  
The research from this project is a core component of the Global Partnership on AI’s (GPAI) Data Governance Framework. 
As scholars, advocates, and individuals, we are committed to social justice and to revealing the systemic bases of intersectional discrimination in our research practices and life choices. Some members of our team relate to marginalised stakeholders from both a position of kinship and one of solidarity, navigating their own lived experiences and confronting intersectional discrimination. Others reflexively acknowledge their inheritance of legacies of unquestioned privilege along with the limited mindsets that derive therefrom. From such a critical self-acknowledgement of privilege and difference, comes a deep sense of responsibility—namely, the responsibility to marshal the advantages of carrying out research in power centres of the Global North and at well-funded research institutions to serve the interests of those on our planet who are all too often marginalised, de-prioritised, and exploited in the global data innovation ecosystem. We recognise how critically important diversity, equity, and inclusion are to carrying out substantively objective and reflexive research. 
‘Uncovering Data Injustice’, the second episode of the three-part ‘Advancing Data Justice’ documentary series premiered at the AI UK 2023 conference.

A series of short video infographics were created to discuss the concepts developed within the ADJRP project and its output guides.
Data Justice in Practice: A Guide for Policymakers  
Data Justice in Practice: A Guide for Impacted Communities  
Data Justice in Practice: A Guide for Developers  
Advancing Data Justice: Research and Practice - An Integrated Literature Review  
Advancing Data Justice: Research and Practice - Data Justice Stories: A Repository of Case Studies  
Advancing Data Justice Research and Practice: An Interim Annotated Bibliography and Table of Organisations 
International Centre of Expertise in Montréal on Artificial Intelligence (CEIMIA)
GPAI Data Governance Working Group
AfroLeadership, Cameroon
CIPESA, Eastern and Southern Africa
CIPIT, Kenya
WOUGNET, Uganda
Gob_Lab UAI, Chile
ITS Rio, Brazil
Internet Bolivia, Bolivia
Digital Empowerment Foundation, India
Digital Natives Academy, Aotearoa
Digital Rights Foundation, Pakistan
Open Data China, China
EngageMedia, Asia-Pacific
[email protected]
 
"
https://www.turing.ac.uk/about-us/impact/supercharging-sustainable-development,"
The United Nation’s Sustainable Development Goals (SDGs) aim to “to promote prosperity while protecting the planet”. These SDGs address the many global challenges faced by humanity, such as poverty, inequality, access to healthcare and education, climate change, environmental degradation, building resilient infrastructure, creating strong institutions, and many more. All over the world, countries are striving to meeting the targets laid out in the SDGs by 2030 – an ambitious agenda by any measure.
Whatever the country, sustainable development is a transformational process that hinges primarily on one input: the provision of targeted government funding and resources. Progress towards the 17 SDGs are monitored by the collection of data on 231 “development indicators” (DIs) – though not all DIs are collected by all countries. Prioritising which policy issues to pursue for maximum impact is an enormous challenge for governments. That’s because the range of development policy options is countless, often with unanticipated inefficiencies that waste resources. And, crucially, there are complex interdependencies between policies that should be taken into consideration.
Modelling these sorts of complex scenarios effectively is impossible using traditional economics and statistical techniques. But this is exactly the sort of ‘wicked’, long-running policy challenge that cutting-edge data science and artificial intelligence technology can make a huge impact on. The Alan Turing Institute’s Public Policy programme is committed to working with policy makers around the world on data-driven policy innovation to address problems like this. An integral part of the programme is a project led by ESRC-Turing Fellow Omar Guerrero, with his research partner, Professor Gonzalo Castañeda of the Center for Research and Teaching in Economics in Mexico. Together, they have developed a suite of analytical tools that can successfully model the impact of a variety of policy decisions on development indicators.
In collaboration with the United Nations Development Programme (UNDP), this technology, called Policy Priority Inference, is already being adopted by state governments in Latin America to support the effective prioritisation of their public policies to optimise sustainable development. “The results of this project show the potential the Policy Priority Inference model has for providing governments with concrete information on how to increase the effectiveness of public spending and accelerate the achievement of development goals,” says Annabelle Sulmont, Public Policy Project Coordinator for the UNDP office in Mexico. “The model also provides a common language that enables its implementation in other parts of the world, and facilitates sharing and comparing results across regions and countries.”
Clearly, this is just the beginning. The new approach has the potential to supercharge the effectiveness of government-backed sustainable development all over the world, to the benefit of billions of people – and the planet itself.
""The results of this project show the potential the Policy Priority Inference model has for providing governments with concrete information on how to increase the effectiveness of public spending and accelerate the achievement of development goals.”
Annabelle Sulmont, Public Policy Project Coordinator for the UNDP office in Mexico

Decades ago, economists tended to focus on GDP as a measure of development, but this is a blunt, unidimensional measure. Today, the SDGs and their hundreds of associated DIs bring serious multidimensionality to the picture of development, and enormous complexity in how these many DIs are interconnected. For example, investing in industrialisation tends also to produce negative outcomes for the environment, while investing in public transport might also boost education outcomes because more children become able to access school.
These positive and negative ‘spillover’ effects are known in development circles as ‘interlinkages’. Imagine hundreds of development indicators, many linked to a lesser or greater degree, positively or negatively, and you get a sense of the scale of the challenge of development planning. “We cannot properly understand development if we don't understand the trade-off of investing in certain areas,” says Guerrero, who is also a Senior Research Fellow in the Economics Department of UCL.
“We cannot properly understand development if we don't understand the trade-off of investing in certain areas.""
ESRC-Turing Fellow Omar Guerrero
The UNDP coordinates the UN’s activities linked to the SDGs. The UNDP’s Latin America bureau was scouting for new analytical methodologies to tackle with the multidimensional nature of the SDGs when they came across Guerrero and Castañeda’s earlier work, in which they modelled socioeconomic indicators and their interactions. “They found it highly relevant because Latin America is leading in the generation of certain types of data on development indicators, but they had no tools to deal with its multidimensionality and complexity,” says Guerrero. After a series of seminars by the researchers at the UN in New York, the UNDP asked the pair if they could refine their method and apply it to the SDGs. This was music to the ears of the researchers, who got into development economics in the first place, says Guerrero, “because it’s one of the fields in economics that have the most impact”.
The project officially began in March 2019, in collaboration with the UNDP office in Mexico, and the country’s National Laboratory for Public Policy at the Center for Research and Teaching in Economics (CIDE).
Policy Priority Inference builds on a behavioural computational model, taking into account the learning process of public officials, coordination problems, incomplete information, and imperfect governmental monitoring mechanisms. The approach is a unique mix of economic theory, behavioural economics, network science and agent-based modelling. The data that feeds the model for a specific country (or a sub-national unit, such as a state) includes measures of the country’s DIs and how they have moved over the years, specified government policy goals in relation to DIs, the quality of government monitoring of expenditure, and the quality of the country’s rule of law.
From these data alone – and, crucially, with no specific information on government expenditure, which is rarely made available – the model can infer the transformative resources a country has historically allocated to transform its SDGs, and assess the importance of SDG interlinkages between DIs. Importantly, it can also reveal where previously hidden inefficiencies lie.
How does it work? The researchers modelled the socioeconomic mechanisms of the policy-making process using agent-computing simulation. They created a simulator featuring an agent called “Government”, which makes decisions about how to allocate public expenditure, and agents called “Bureaucrats”, each of which is essentially a policy-maker linked to a single DI. If a Bureaucrat is allocated some resource, they will use a portion of it to improve their DI, with the rest lost to some degree of inefficiency (in reality, inefficiencies range from simple corruption to poor quality policies and inefficient government departments).
How much resource a Bureaucrat puts towards moving their DI depends on that agent’s experience: if becoming inefficient pays off, they'll keep doing it. During the process, Government monitors the Bureaucrats, occasionally punishing inefficient ones, who may then improve their behaviour. In the model, a Bureaucrat’s chances of getting caught is linked to the quality of a government’s real-world monitoring of expenditure, and the extent to which they are punished is reflected in the strength of that country’s rule of law.
When the historical movements of a country’s DIs are reproduced through the internal workings of the model, the researchers have a powerful proxy for the real-world relationships between government activity, the movement of DIs, and the effects of the interlinkages between DIs, all of which are unique to that country. “Once we can match outcomes, we can discern something that's going on in reality. But the fact that the method is matching the dynamics of real-world development indicators is just one of multiple ways that we validate our results,” Guerrero notes. This proxy can then be used to project which policy areas should be prioritised in future to best achieve the government’s specified development goals, including predictions of likely timescales.
What’s more, in combination with techniques from evolutionary computation, the model can identify DIs that are linked to large positive spillover effects. These DIs are dubbed “accelerators”. Targeting government resources at such development accelerators fosters not only more rapid results, but also more generalised development. Guerrero has been in talks with the UK’s Department for International Development, which is particularly interested in Policy Priority Inference’s ability to identify development accelerators.
Meanwhile, Guerrero and Castañeda have produced several reports with the UNDP, covering case studies from Mexico and Uruguay, and are now starting a project with Colombia, at the national and city levels. For the project with Mexico, the team first ran workshops with stakeholders from the office of the president, the finance ministry, the national institute of statistics, representatives of six of Mexico’s 32 states, and NGOs.
“The workshops were a tremendous success,” says Guerrero. “When we presented preliminary results for the federal case, they expressed that the model has great potential to support them in their budgeting and planning processes. The Mexican state governments provided us with valuable data to help test the model. They are keen on adopting Policy Priority Inference as part of their toolkit for when they plan budgets, and also when they prepare their own, state-level development plans. It helps them decide whether their goals are realistic or not.”
“The contributions of the researchers and the UNDP office in Mexico were mutually complementary,” Sulmont adds. “The UNDP’s experience in public planning and the 2030 Agenda, and its contact with the federal and state governments, assisted both academics in the adaptation of their complex model to the reality of Mexican public planning processes.”
“Participating in the project is extremely valuable. An exercise of this kind obliges us to be in the state of the art of monitoring and implementation of the 2030 Agenda.”
Mauricio Francisco Coronado García, Evaluation Director, Nuevo León’s Executive Office of the Governor, Mexico
So far, several state governments in Mexico have already expressed interest in adopting Policy Priority Inference. “Participating in the project is extremely valuable,” says Mauricio Francisco Coronado García, Evaluation Director in the state of Nuevo León’s Executive Office of the Governor. “An exercise of this kind obliges us to be in the state of the art of monitoring and implementation of the 2030 Agenda.”
He explains how his government wants to put Policy Priority Inference to use. “The current state administration is in the fourth of six years of government and about to carry out a 20/21 Agenda, a type of an administration-closure planning tool. In this, we could include inputs from Policy Priority Inference to: assess whether the goals set at the beginning of the administration are attainable according to the historical distribution of the transformative budget; make adjustments to the goals considering the above, and; use it to make a budget distribution proposal to the transition team of the next administration, according to the development profile they propose.”
So far, the UNDP’s Latin America bureau has been investing in the tools and the application of Policy Priority Inference in the continent has quickly spread beyond Mexico. If and when the technology becomes widespread in Latin America, the intention is to bring it to other global regions.
“It's very energising,” says Guerrero. “There is nothing else like this, and I'm always excited to see what the science will bring up in the next iteration.” One refinement the researchers are working on has become possible because some national governments are starting to report their expenditure data, says Guerrero. He recently received a three-year grant from the UK’s Economic and Social Research Council to take the Policy Priority Inference framework and merge it with this emerging fiscal data. Once the model can generate outcomes that not only match a country’s historical DI movements, but also tie into its real-world expenditure, confidence in the accuracy of its projections grows even more. 
Guerrero is working on this with the UNDP and the Global Initiative for Fiscal Transparency (GIFT, formerly part of The World Bank). “Since my first chat with Omar about the project, I immediately saw a lot of potential in mutual collaboration,” says Lorena Rivero del Paso, Manager for Technical Cooperation and Collaboration at GIFT. “GIFT supports the data standardization of public finances and that is exactly what the Policy Priority Inference project needs. It’s a perfect complement. The project will be a good use case for the publication of spending open data, where structuring data can lead to better analysis and recommendations, which we would expect will turn into better policies for sustainable development.”
“Government expenditure data will take this technology to a whole new level,” says Guerrero. Policy Priority Inference is not only about government, Guerrero stresses, but also about accountability. “We want to bring these tools to NGOs too, because this is useful for them to assess the actions of governments. NGOs can check if governments are prioritising the right policies.”
Later in 2020, Guerrero plans to create a website with open-source tools, linked to the UN’s database of development indicators, so that interested researchers and analysts can explore their own policy scenarios for their country of interest. There will be tutorials published later this year on how to use the Policy Priority Inference tools in the Python programming language.
The Turing’s mission is to pioneer data science and artificial intelligence in order to change the world for the better. The Institute’s Public Policy programme contributes to this by developing research, tools and techniques that help governments design better policies and more effective public services. This work, with its potential to benefit everyone touched by the UN’s Sustainable Development Goals, is exactly the sort of work the Institute is proud to champion.

"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/automata-algebraic-mathematical-modelling,"
"
https://www.turing.ac.uk/people/guest-speakers/egbert-wits,"
Egbert has been active in the field of international development for nearly twenty years. Specialising in project management, training design and facilitation, community empowerment, and strengthening the capacities of local organisations. He is currently managing a range of research efforts at EngageMedia that investigate how digital rights and AI are affecting human rights and environmental justice issues in the Asia-Pacific. His own PhD candidature research at the University of Newcastle, Australia, focuses on the significant roles of traditional arts practices in rural communities of Central Java today.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/babcock,"
babcockinternational.com
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/knowledge-representation,"
"
https://www.turing.ac.uk/people/external-researchers/nephtis-brandsma,"
"
https://www.turing.ac.uk/people/former-doctoral-students/edward-chuah,"
Edward commenced his doctoral studies at the University of Warwick in October 2016. His current interests lie at the intersection of fault tolerance, distributed systems and data analytics. He  has a general interest in anomaly detection, causal inference, networking and security.
Modern day data centres and HPC systems are comprised of complex combinations of networks, processors, storage systems and operating systems. Recent research has demonstrated the value and significance of combining system failure logs with resource utilisation data for failure diagnosis (and error detection). However, the massive amount of data that large HPC systems generate presents a significant challenge in processing the data for effective failure diagnosis.
Edward's PhD research is addressing the challenge by developing a new data-driven framework for error propagation and failure diagnosis. The framework uses resource usage data and system logs in its analyses. He evaluated different feature extraction methods and correlation algorithms and implemented two diagnostics workflows. The workflows are called CORRMEXT and EXERMEST. CORRMEXT was successful at identifying error propagation and recovery patterns that occurred frequently. EXERMEST was successful at identifying error propagation paths and error recovery attempts that are rare.
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/real-time-computing,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/docker,"
Docker website
"
https://www.turing.ac.uk/research/data-ethics,"

The Alan Turing Institute is committed to using data science and AI technologies for everyone’s benefit, and to protect society against these technologies’ unintended consequences.
This page provides an overview of the wide range of initiatives in the field of data ethics taking place at the Institute.
Made up of a range of researchers specialising in ethics, social science, law, policymaking, and big data and algorithms, the Data Ethics Group works in collaboration with the broader data science community, facilitates public dialogue on relevant topics, and supports the Turing’s workshops and public events relating to data ethics.
The Institute’s Ethics Advisory Group (EAG) is chaired by Kirstie Whitaker, Programme Lead for Tools, Practices and Systems. The groups ensures that research conducted across the Institute confirms to high ethical standards, equivalent to the ethics approval process of a university. The Group consists of Turing academics as well as representatives from the Institute’s Partnerships Team and student body.
The Public policy programme works alongside policy makers to explore how data-driven public service provision and policy innovation might solve long running ‘wicked’ policy problems and to develop the ethical foundations for the use of data science and artificial intelligence in policy-making. Among its many projects, the programme has delivered the UK's official public sector guidance on the ethics and safety of artificial intelligence and has studied the ethics of machine learning applications in children's social care. The programme also works closely with the Data Ethics Group and the Ethics Advisory Group.
The fairness, transparency and privacy interest group, led by Turing Fellow Adrian Weller, includes experts in machine learning, security, causal inference and algorithm design. The group examines issues relating to fairness, transparency and privacy of algorithmic systems, aiming to develop new technical approaches and inform the broader ethical discussion.
The Alan Turing Institute is partnering with the Nuffield Foundation on a new £5 million Ada Lovelace Institute to examine profound ethical and social issues arising from the use of data, algorithms, and artificial intelligence, and to ensure they are harnessed for social well-being. This independent body, launched in 2018, will seek to ensure that the power of data – and the automated technologies that augment it – is harnessed to promote prosperity, both for society as a whole and for different groups within it.
A recent independent review, Growing the artificial intelligence industry in the UK, recommended that The Alan Turing Institute work with the Information Commissioner’s Office (ICO) to develop a framework for explaining the processes, services and decisions delivered by AI. The Institute has welcomed this recommendation, and in conjunction with the ICO, has started work on creating this framework.
Attend our data ethics events or watch past seminars and masterclasses below.
 
 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/jaguar-land-rover,"
www.jaguarlandrover.com
"
https://www.turing.ac.uk/events/turing-lectures-how-much-can-we-limit-rising-seas,"
Dr Tamsin Edwards is a climate scientist and award-winning communicator, specialising in quantifying the uncertainties of climate model predictions, particularly for the Antarctic and Greenland ice sheet contributions to sea-level rise. Tamsin is an IPCC author and co-presenter of the 39 Ways series.
This event is organised in collaboration with the Royal Institution of Great Britain.
Eighty-four scientists in fifteen countries. Twenty-one regions of the world with glaciers or ice sheets. Thousands of computer simulations. How was all this put together, amid a global pandemic, to predict sea level rise for the most recent report of the Intergovernmental Panel on Climate Change?
In this Turing Lecture, climate scientist Tamsin Edwards will discuss leading an international team of scientists and using machine learning techniques in the most comprehensive effort yet to predict 21st century sea level rise.
In the midst of COP27, Tamsin will also provide an update on the emissions pledges made to meet the Paris Agreement (2015), and conduct a live experiment to demonstrate how much global warming we should expect based on our current behaviour, and what actually causes sea level rise (hint: it’s probably not what you think!).
Through thought-provoking discussion and creative demos, expect to come away with a new appreciation of the technology that’s helping us to understand sea level rise.

 
 
This series of Turing Lectures is organised in collaboration with The Royal Institution of Great Britain.

The Royal Institution of Great Britain
21 Albemarle Street
London
W1S 4BS
"
https://www.turing.ac.uk/media/blog/new-defence-and-security-projects-tackling-important-problems-from-cyber-security-to-civil-conflict/,"
"
https://www.turing.ac.uk/people/researchers/alan-brown,"
Alan W. Brown is Professor in Digital Economy at the University of Exeter’s Business School and co-founder of the Initiative in Digital Economy at Exeter (INDEX). After receiving a PhD in Computational Science at the University of Newcastle-upon-Tyne, Alan spent almost 20 years in the USA in commercial high-tech companies leading R&D teams, building leading-edge solutions, and driving innovation in software product delivery. He then spent 5 years in Madrid leading enterprise strategy as European CTO for IBM’s Software group. Most recently Alan co-founded the Surrey Centre for the Digital Economy (CoDE) at the University of Surrey where he led research activities in 4 EPSRC-funded research projects.
The impact of Alan’s research is seen in his consulting activities where he advises several startups, and works with clients in the public and private sector including the UK National Audit Office, Centrica, SAP, Resonate, McLaren, and several UK Government agencies. Additionally, he has published 5 books and numerous papers on software engineering, systems design, and digital business transformation. 
Alan’s research is focused on agile approaches to business transformation, and the relationship between technology innovation and business innovation in today’s rapidly evolving digital economy. His current research projects involve the adoption of digital technology in business, the effect of digital disruption in emerging business models, and the development of new management strategies and practices for data-driven innovation.
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/nonlinear-dynamics,"
"
https://www.turing.ac.uk/news/new-framework-will-help-security-and-law-enforcement-agencies-assess-privacy-intrusion,"
The use of automated analytics – using computers to analyse data and provide insight – is crucial to enabling national security and law enforcement agencies to operate in a world where technology is changing rapidly, according to new research published this week by The Alan Turing Institute’s Centre for Emerging Technology and Security (CETaS).
However, the increased use of automated analytics also raises privacy concerns. National security and law enforcement agencies exist to protect people and institutions from harm. But to do this effectively, their work involves a degree of monitoring and surveillance which, in some cases, can lead to an intrusion of privacy into people’s personal lives.
This research offers recommendations to help better understand and assess the level of privacy intrusion from automated analytics.
The research is based on interviews and focus groups with stakeholders across the UK government, national security and law enforcement, and legal experts outside government.
As part of the research, the authors examined the obligations of national security and law enforcement agencies to keep citizens safe in a challenging operational environment. A crucial part of this is minimising intrusiveness and adhering to the legal principle of proportionality which regulates surveillance activity.
The report offers a new framework to help with this. It focuses on six key factors relevant to proportionality judgements that will help individuals and organisations assess the risk of how automated analytics are impacting privacy intrusion. These are: datasets; results; human inspection; tool design; data management; and timeliness and resources.
The authors anticipate the proposed framework will add another guarantee to existing authorisation and compliance processes. The framework will help provide assurances that all relevant factors have been considered at every stage in the automated analytics life cycle. 
Beyond the national security community, the report seeks to further contribute to the public privacy debate, while highlighting that public expectations of privacy are dynamic.
Dr Marion Oswald, lawyer and Senior Research Associate at The Alan Turing Institute, said: “We need to better understand, map and monitor the risk of multiple, connected, automated systems feeding into each other over an extended period. We hope this framework will be adopted by people across the national security and law enforcement communities, such as analysts, investigators, legal advisers, oversight bodies and judicial commissioners.” 
Ardi Janjeva, lead author and Research Associate at CETaS, said: “Big data analytics and automated systems are becoming much more widespread in society. This means that changing expectations of privacy need to be understood in a more rigorous way, to promote transparency and public trust. That’s why we need more public perceptions surveys of intrusion from automated analytics in different scenarios.” 
Notes:
The report Privacy Intrusion and National Security in the Age of AI: Assessing proportionality of automated analytics is a CETaS research report and is authored by Ardi Janjeva, Dame Muffy Calder and Dr Marion Oswald.
In this report, automated analytics was defined as the use of machine-based methods to analyse data and produce insights. This includes both fully automated systems, and systems that are not fully automated but use some degree of human inspection. Privacy intrusion was understood by the research team through the lens of the European Court of Human Rights Article 8 definition regarding the right to respect for private and family life, home and correspondence. 
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-partnership-review-and-plans-2023,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This month, rather than tackling a data science theme, we will be doing a review of the partnership so far and a look forward to our plans in 2023.
We'll be hearing from our partnership team, including leads Chris Harbron, Expert Statistical Scientist and Advanced Analytics Lead, Roche and Ben MacArthur, Deputy Programme Director for Health and Medical Sciences at the Turing.
The team will give a review of our first year and updates on our current research themes of structured missingness and predictive modelling. We will also hear from our Senior Postdoctoral Research Associates and activities from the community side. Finally the team will look forward to 2023 and discuss some of our plans/new year's resolutions.
You can watch a recording of this event here. 

"
https://www.turing.ac.uk/cookie-notice,"

What's in this notice?
This notice explains how The Alan Turing Institute uses cookies and other tracking technologies when you visit our website www.turing.ac.uk.  Please note that other websites, apps and digital platforms that the Turing uses have their own cookies and cookie notices.
What are cookies?
Cookies are small files that are placed on your computer or device by a website when you visit it. They contain details about your use of the website, and they distinguish you from other users. Cookies are useful because they allow a website to recognise you and, for example, remember your preferences, as well as to improve the website.
Like ours, most websites use cookies and similar technologies such as ‘web beacons’, ‘clear GIFs’ and ‘pixel tags’.
You can find out more about cookies at www.allaboutcookies.org.
Why do we use cookies?
We use cookies:
The main cookies we use are:
See here for a full list of cookies currently in use on our homepage.
How do I change my cookie settings?
You can control your cookie preferences at any time by clicking the ‘C’ in the corner of the website and using the sliders.
Analytics and social cookies are turned off by default. We require your consent to place these types of cookies.
Most browsers also let you turn off either all or third-party cookies. What you can do depends on the browser you are using. However, please be aware if you reject all cookies, this may impair your ability to use the website. The ability to do this is usually found in the options, settings or preferences menu of your browser or mobile device.
You can opt out of using Google’s analytics cookies by visiting Google’s opt-out page at https://tools.google.com/dlpage/gaoptout.
Updating this cookie notice
We may update this notice and so we recommend that you check this page regularly. We aim to notify you of any significant changes by way of a notice on this website or by contacting you if reasonably possible.
 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-surrey,"
www.surrey.ac.uk
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/crop-ontology,"
"
https://www.turing.ac.uk/taxonomy/term/1119,"


The Turing Lectures series features influential figures from the dynamic world of data science and artificial intelligence. These prestigious, landmark lectures lead and shape the national conversation, sparking debate, informing and, of course, entertaining audiences.
Our thought-leaders are renowned experts – often world authorities – each with their unique perspectives. From scientific advisors to high-profile scientists, entrepreneurs, industry insiders, and leading academics, The Turing Lectures prides itself on diversity, and authoritative and an engaging style.
Speaking to packed audiences, the lectures have already included the lead scientist of Google Deepmind’s Machine Learning team, Nando de Freitas; Buzzfeed News Editor, Craig Silverman; computer scientist, academic and social entrepreneur Sue Black OBE; and mathematician, author and broadcaster, Hannah Fry. 
Topics explored in the series are as diverse as the speakers and reflect a rapidly changing world. With subjects as intriguing as the ethical implications of 'deepfakes', AI’s effect on economics and the commercial sector, data trusts, the opportunity of the digital humanities, and the promise of wearable tech.
The reach of The Turing Lectures series goes well beyond its highly engaged live audience and you can discover or rediscover many of our lectures via our YouTube channel. The series continues to grow in scope, ambition and reputation and we look forward to welcoming you.
Sign up to the Turing’s event newsletter for more details.
 

"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-birmingham,"
"
https://www.turing.ac.uk/people/researchers/may-yong,"
May is a Senior Research Software Engineer at The Alan Turing Institute. She has worked across multiple research domains, collaborating with scientists to turn interesting ideas into useable software. Past projects include machine learning on neonatal intensive care data and tools for enabling the exploration of UK government expenditure data.
She has previously worked on creating data standards for antibody therapy experiments at UCL Cancer Institute and worked on amalgamation of heterogeneous multiple sclerosis data at the Imperial Data Science Institute.
May is interested in data interpretation, specifically ways of providing information in perspective in order to see the bigger picture. She is building tools to minimise data ambiguity so that data is interpreted and used the way the data collectors intended.
"
https://www.turing.ac.uk/people/former-doctoral-students/amartya-sanyal,"
Amartya Sanyal is a doctoral student in the Dept. of Computer Science, University of Oxford jointly supervised by Dr. Varun Kanade and Dr. Phil Torr. He graduated with a B.Tech in Computer Science And Engineering with a minor in Linguistic Theory in the top 7% of his class from the Indian Institute of Technology, Kanpur in the year 2017 where he was a student of the Department of Computer Science And Engineering where he worked on projects in non-convex optimization, deep learning and extreme learning. He has interned in the Montreal Institute of Learning Algorithms under Prof. Yoshua Bengio and Twitter Cortex NYC among others where he has worked on developing generalized sequence learning models and adversarial models for natural language modelling.
Deep neural networks have revolutionized the field of machine learning and their use has improved on the prior state of the art significantly in diverse domains such as vision, natural language processing, speech, reinforcement learning, etc. Optimization methods used in the training of these networks can be somewhat ad hoc and lack a unified understanding, primarily because of the non- convex nature of these optimization problems. The work will focus on developing new methods for training networks that involves both changes to optimization methods as well as network design. The research will involve both experimental validation on real-world and synthetic data and mathematical analysis of the proposed methods.
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-spend-network,"
Automated matching of businesses to government contract opportunities
This report presents the output of a week-long collaboration between Spend Network, and lead academics from the University of Manchester and the University of Oxford that attended the Data Study Group at The Alan Turing Institute. Spend Network is a platform that aims to enable efficient public procure-ment. The goal of this work was to match suppliers to tenders. Our approach consists of building vector representations of suppli-ers and tenders and identifying their compat-ibility with the distance between the respec-tive vectors. In building their representations, we make use of both their textual descriptions and the knowledge of previously awarded con-tracts. We find previous contracts informative of future procurement decisions. Our best re-sults use Correlated Topic Models (Blei et al., 2007) for extracting representations of textual descriptions.
Data Study Group team. (2019, November 29). Data Study Group Final Report: Spend Network. Zenodo. http://doi.org/10.5281/zenodo.3558243
Luis Ospina-Forero, University of ManchesterOduwa Edo-Osagie, University of East AngliaHoang Le, Trinity College DublinSilviu Oprea, University of EdinburghBemsibom Toh, Heriot-Watt UniversityErin Clark, Spend NetworkAude Vuilliomenet, University College LondonKai Xu, University of EdinburghYuchen Zhao, University of Southampton
2.8 MB
"
https://www.turing.ac.uk/people/external-researchers/silvia-lui,"
"
https://www.turing.ac.uk/people/senior-leadership/jonathan-atkins,"
Jon has over 25 years of experience gained in roles within technology across a number of industry sectors in startups, not for profit and large corporate organisations. Jon is a Chartered Accountant and comes to The Alan Turing Institute from Lab49, a software consultancy and design company with offices in London, New York, Sydney and Dublin where he led the global finance, operations, facilities and IT teams.
Prior to joining Lab49, Jon worked for a number of years with the GSMA (a global mobile technology company with its headquarters in London and Atlanta) where he was Group Finance Director, CFO of its US based commercial businesses and Treasurer of its public US registered charitable foundation. During that time he led the growth of the business helping it to expand to over 800 staff with presence in over 25 countries including China, India, Brazil, Argentina and Dubai. He also helped deliver the success of what is now the world’s largest and most influential mobile event, GSMA Mobile World Congress, which is held annually in Barcelona and attracts close to 100,000 professional attendees.
"
https://www.turing.ac.uk/learn-explore-and-participate,"

[email protected]
"
https://www.turing.ac.uk/events/turing-lecture-ai-innovative-social-work,"
The lockdowns due to COVID-19 have led to the expansion of our online lives – with many groups and communities flourishing in the communal spaces of social media.  
 
But what about the most vulnerable in our society? Social inequalities have been dramatically highlighted by the necessary adjustments made for COVID-19. In this lecture we ask if AI and data science can help pinpoint the challenges faced by our growing online lives, and help us, or will it harm us in the process?
Professor Patton will explore the intersection of race, AI, empathy and social media and share how his work uses qualitative and computational data collection methods to examine the relationship between youth and gang violence and social media, and how and why violence, grief, and identity are expressed on social media.
In 2018, Professor Patton published a ground-breaking finding in Nature journal, Digital Medicine, which uncovered grief as a pathway to aggressive communication on Twitter. The report was cited in an amici curiae brief submitted to the United States Supreme Court in Elonis v. United States, which examined the interpretation of threats on social media. His lecture will explore the real-world impact these expressions have on the well-being of youth, particularly those from low-income and minority ethnic groups.
15:00 - Introduction
15:05 - Turing Lecture: AI for innovative social work
15:50 - Audience Q&A
16:30 - Close
 

7.2 MB
"
https://www.turing.ac.uk/news/publications/performance-evaluation-machine-learning-good-bad-ugly-and-way-forward,"
This paper gives an overview of some ways in which our understanding of performance evaluation measures for machine-learned classifiers has improved over the last twenty years. I also highlight a range of areas where this understanding is still lacking, leading to ill-advised practices in classifier evaluation. This suggests that in order to make further progress we need to develop a proper measurement theory of machine learning. I then demonstrate by example what such a measurement theory might look like and what kinds of new results it would entail. Finally, I argue that key properties such as classification ability and dataset difficulty are unlikely to be directly observable, suggesting the need for latent-variable models and causal inference.
Flach P. Performance Evaluation in Machine Learning: The Good, The Bad, The Ugly and The Way Forward (2018). 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/aston-martin,"
www.astonmartin.com
"
https://www.turing.ac.uk/research/research-areas/machine-learning/semi-supervised-learning,"
"
https://www.turing.ac.uk/people/researchers/tim-watson,"
Professor Tim Watson FBCS FIET is the Programme Director for our Defence and Security Programme. Appointed in January 2022, Tim is also a Professor of Cyber Security and the Director of the Cyber Security Centre at Loughborough University. With more than thirty years’ experience working with government, industry and in academia, he has been involved with a wide range of programmes, several high-profile projects and has acted as a consultant for some of the largest telecoms, power and transport companies. Tim is an adviser to various parts of the UK government and to several professional and standards bodies.
 
Tim’s research includes EU-funded projects on combating cyber crime, UK MoD research into automated defence, insider threat and secure remote working, and UKRI-funded research, focusing on the protection of critical national infrastructure against cyber-attack. In PETRAS he is the Chair of the Research Excellence Board.
 
"
https://www.turing.ac.uk/news/responding-covid-19-pandemic,"

Experts at The Alan Turing Institute are working hard to address the urgent need for scientific innovation to tackle the spread and effects of the COVID-19 pandemic. This page is a hub for content related to these efforts – keep up to date with live projects, calls for researcher engagement and other relevant activities. You can also sign up to our newsletters to be informed of our latest developments.
"
https://www.turing.ac.uk/people/researchers/federico-nanni,"
Federico is a Senior Research Data Scientist at The Alan Turing Institute, working as part of the Research Engineering Group. He currently acts as REG co-lead for the Data Science for Science & Humanities programme.
He has a background in Natural Language Processing and Information Retrieval, in the context of Digital Humanities.
Prior to joining the Institute, he completed a PhD in History of Technology at the University of Bologna, focusing on the use of web archives in historical research and worked for five years at the Data and Web Science Group of the University of Mannheim.
He also spent time as a visiting researcher at the Foundation Bruno Kessler, Aarhus University and the University of New Hampshire.
Federico is a historian by training and is interested in research projects exploring the intersections between digital humanities, computational social science and natural language processing.
"
https://www.turing.ac.uk/people/former-doctoral-students/nicolas-anastassacos,"
Nicolas started his doctoral studies at The Alan Turing Institute in October 2017. He is registered at the UCL. Having completed a BSc Cognitive Science from the University of Edinburgh and MSc Data Science from UCL, Nicolas looks to combine these disciplines in his research to understand behaviour and social contexts from a data-driven perspective.
Nicolas’s current research interests involve studying human behaviour on both an individual and collective scale, looking to analyze how behaviour changes with respect to the environment and determining optimal ways of behaving using reinforcement learning techniques and game theory. His other interests involve personality-based modelling and developing multi-agent systems.
"
https://www.turing.ac.uk/events/turing-lecture-glimpsing-our-ai-future,"
58.1 KB

Right across the economy start-ups are using machine learning to disrupt incumbents. At the same time, incumbents are using machine learning and artificial intelligence to automate processes and increase productivity. China is challenging the US for AI supremacy. Self-driving cars may soon over-turn a century-old model of urban transport and disrupt the car and oil industries. Helped by AI and machine learning, start-ups are harnessing renewable energy to re-imagine and refashion basic industries and are employing bio-tech to redesign what we eat and how food is supplied, with the first lab-grown meat being served in a Michelin starred restaurant.  Coming rapidly closer is the threat of climate disruption.
In this talk, founding Chair of The Alan Turing Institute Howard Covington will endeavour to grasp these dramatic changes to our world and discuss how the business and technology community can seize the opportunities they offer

Howard’s first career was in the City as an investment banker and asset manager. His second career is in promoting mathematical sciences. He has been a trustee of the Science Museum and also chair of the Isaac Newton Institute for Mathematical Sciences, the UK’s national maths research institute at Cambridge. He is the inaugural chair of The Alan Turing Institute. He is also vice-chair of ClientEarth, an environmental law firm.
This Turing Lecture will be hosted by the Institute's Managing Director for Innovation, Christine Foster.
Join us on Twitter: #OurAIFuture
CodeNode
10 South Place, London, EC2M 7EB
"
https://www.turing.ac.uk/blog/everything-you-need-know-about-chatgpt,"
ChatGPT caused a sensation when it was released last November. The AI-powered chatbot’s realistic, and sometimes surreal, outputs have amazed, delighted, and occasionally appalled users, who have used it to create everything from romantic poems to cake recipes. Analysts have described it as the fastest-growing consumer app ever, with an estimated 100 million unique users in the two months since it launched.
Created by OpenAI, an American AI research lab, ChatGPT is part of their GPT suite of models able to undertake natural language tasks. OpenAI also provide Codex, which translates natural language to code, and DALL·E 2, which allows users to create new images and art from a text prompt. Many new competitors in the space, such as Google’s Bard based on their large language model (LLM) LaMDA, are also rapidly appearing.
ChatGPT falls into a category of algorithms known as generative AI - these are defined as algorithms that can create content which resemble creative outputs such as text, images, video, code, and audio. Large language models like ChatGPT are trained on huge datasets of human language taken from the internet. Users can use text prompts to ask the chatbot questions and it then produces content based on what it has learnt from the data.
Professor Mike Wooldridge, The Alan Turing Institute’s programme director of AI Foundations, says this technology is particularly exciting because “the core abilities of this technology are producing and understanding ordinary, everyday language”. It’s “the kind that people use when they talk to other people. And that's been one of the big targets of AI research ever since the field began”, he added.
Although AI and algorithms have been influencing our leisure time, dating lives and consumer choices for years, for many people Chat GPT has been the first opportunity they've had to strike up a realistic, in-depth conversation with an AI.
The release of ChatGPT isn’t the first time that a large language model or generative AI has made headlines over the last year. Google’s LaMDA AI system made the news when one of its engineers claimed it had become sentient, prompting widespread discussion on whether machines could become conscious. Widespread debates are also taking place in the creative industries over AI image generators, such as DALL·E and Stable Diffusion, with ethical questions being raised about the issues of copyright and creativity.
ChatGPT and other generative AI are increasingly being used in more settings. ChatGPT has already been used to give advice to mental health patients and even to inform a judge’s ruling in a court case . Many experts are pointing to ethical concerns of these technologies. Some are worried that these models are being released before their impacts have been properly considered and before safeguards have been put in place.
According to Professor Wooldridge, one concern is that the technology “doesn’t have any conception of the truth. It gets things wrong in very plausible ways. It’s easy to believe and easy to be taken in by some answers it offers”. A real risk is this content is then used to spread misinformation.
Turing Ethics Fellow Dr Mhairi Aitken adds: “It is really important that we increase public awareness of the limitations of ChatGPT and generative AI more broadly. It is important that people understand that these technologies cannot be relied upon to provide factual or objective information.”
Another worry is that generative AI can produce biased or discriminatory outputs, as the models may be trained on data that has embedded discrimination or limitations. This is then replicated in the content it produces, reinforcing biases. “We need to examine the choices that have been made around which data to include in training datasets and what measures are taken to address bias” says Dr Aitken.
 “A lot of the media attention has been on uses of ChatGPT, for example how students may use it to cheat in assignments, and that places a lot of emphasis on responsible use of the technology. However, more attention should be paid to how these systems have been developed.”
Last year, the Turing started its own programme of work in this area. Led by Professor Wooldridge, the programme’s work includes benchmarking the capabilities and limitations of foundation models with the eventual aim of producing guidance on this technology. The Alan Turing Institute is also hosting a one-day symposium to explore these topics further.
The excitement and hype around technologies such as GPT-3, ChatGPT and Bard is only going to grow over the coming years. These are incredibly powerful and impressive tools, but we need to make sure we keep having conversations about how they developed, how they are used, and where this technology is heading.
"
https://www.turing.ac.uk/people/researchers/ian-hall,"
Ian Hall is a reader of mathematical statistics at the University of Manchester. He gained his PhD at the University of Exeter in vortex dynamics, and subsequently worked for 15 years at Public Health England (PHE). In PHE he lead a mathematical modelling team planning for emergency responses to emerging infectious disease outbreaks. This included developing inferential tools to infer infection hazard area from early cases and planning for outbreaks such as smallpox and pandemic influenza. He is a member of the Department of Health's scientific pandemic influenza modelling subgroup (SPI-M).
His Turing related research involves an interest in understanding population movement at a range of scales, operational research to support outbreak mitigation strategies and real time modelling during outbreaks to forecast future case numbers.
"
https://www.turing.ac.uk/people/researchers/ruth-drysdale,"
"
https://www.turing.ac.uk/research/research-projects/posterior-bootstrap-scalable-approach-bayesian-non-parametric-learning,"
Statistical sampling in a Bayesian context has a speed-accuracy tradeoff: methods to sample from the exact statistical model are slow because they cannot be parallelised; the methods that can be parallelised are fast but only approximate. This project bridges the gap and implements a method that can tune the dial between speed and accuracy. It takes the results from the approximate method and performs statistical sampling that is as close to the exact model as desired, all the while with fast and parallel processing.
Bayesian methods often use 'Markov Chain-Monte Carlo' theory to obtain a posterior sample (the starting sample at each pass through a chain) of the parameters of interest. This chain is sequential and thus cannot benefit from the parallelisation of modern computers. 'Variational Bayes' uses a similar approach but assumes that the parameters of interest are uncorrelated, and thus can sample them faster.
This project sidesteps this sequential paradigm and draws samples from the parameters from a 'frequentist regression' where the data points are a combination of the observed data provided and synthetic data generated from the parameter estimates that needing correction (if any). The weightings given to the data and synthetic data governs the proximity of the sampled parameters to the observed data or to the previous model. Since each sample of the parameters is the result of a frequentist regression, the approach scales well: for example, if the user wants 1 thousand samples and has 1 thousand processors available, each processor can produce one sample very quickly.
The project is developing an open-source package in the programming language R that implements the method. It will be available on CRAN, so any researcher can easily replicate the results and benefit from parallel statistical sampling with adjustable proximity to the exact model.
This methodology can be applied in a number of situations, including:
 
"
https://www.turing.ac.uk/news/turings-public-policy-programme-responds-online-harms-white-paper,"
Back in April, the Department for Digital, Culture, Media and Sport and the Home Office released the Online Harms White Paper. It outlined wide-ranging proposals for tackling numerous online harms, from selling illegal items to cyberbullying, and from terrorist content to pornography. The broad message of the White Paper is commendable: ‘We cannot allow […] harmful behaviours and content to undermine the significant benefits that the digital revolution can offer […] If we surrender our online spaces to those who spread hate, abuse, fear and vitriolic content, then we will all lose.’ (p.3)
The government appears to be determined about cleaning up the internet – the only question is whether the proposals in the White Paper are sufficient to realise this ambitious goal. As part of the Consultation process (now closed), The Alan Turing Institute’s public policy programme has released a publicly available response. Here are our five key questions and insights:
The White Paper advocates creating a new independent regulator. One challenge for creating a new regulator is that online harms are multifaceted and varied, spanning many different domains. We feel a single regulator might struggle to develop enough internal expertise to tackle them all at once and to identify and respond to new risks. There could be communication and coordination problems with too many separate government bodies operating in this space, resulting in a lack of clarity for both citizens and the digital tech platforms. Meanwhile, existing regulators have accumulated much of the expertise that the regulation of online harms will require through their existing dealings with digital platforms. For these reasons, we recommend that a new unit with a specific remit for online harms be established within one of the existing regulators, such as Ofcom or the Information Commissioner’s Office.
The rationale for the White Paper is to protect individuals from online harm. The question, however, is what constitutes an online harm? The White Paper tackles this difficult question by providing a list of harms (on p. 31), but the authors acknowledge that the list is “neither exhaustive nor fixed.” Greater clarity is needed on what constitutes a harm, how the degree of harm is measured, and which harms come within scope of the new regulatory body. In introducing the list of harms, the White Paper also distinguishes between harms with a clear definition and those with a less clear definition (on p. 31). However, some of the harms which have been described as having a ‘clear definition’ are complex and messy phenomena, such as harassment, hate crime, terrorist content or modern slavery. We would welcome greater clarity on what harms will come within the scope of the new regulatory body as well as a framework for deciding what is recognised as a harm.
The White Paper highlights the government’s concerns about imposing restrictions on freedom of expression. However, the question of what freedom of expression means when it comes to online spaces is complex. We would welcome a policy roadmap after the White Paper consultation that provides a discussion on how to balance freedom of expression with the need to protect individuals from harm. Striking this balance will be a challenge, and will require a substantive discussion of what values should be incorporated into the regulatory framework for online harms, with potentially contentious decisions needing to be made regarding constraints on online behaviour.
We welcome the White Paper’s encouragement for greater use of technology, such as artificial intelligence, to moderate content. However, as work in the Turing’s Hate Speech: Measures and Counter-Measures project has found, existing content detection technologies perform suboptimally at accounting for context, evaluating intent and recognising irony, satire and humour. This could have important implications for freedom of expression, and we would welcome further guidance from the government on how users might be able to contest content takedowns.
One of the biggest benefits in establishing a cross-platform body for online harms is that they can develop a joined-up approach to regulation. We would welcome greater emphasis on how harmful content, and the purveyors of such content, move between platforms. Research suggests that content moves from niche extremist platforms, such as 4chan, to big mainstream platforms, such as Twitter and Facebook. Similarly, when prominent hateful figures are banned from mainstream platforms, they often migrate to niche, less well moderated platforms, and might encourage their supporters to migrate with them. We believe that the government can make best use of its unique position by investing resources to understand the dynamics of online harms and how they span and migrate between platforms and communities.
Misinformation is increasingly recognised as one of the biggest issues on the internet, and it is recognised in the White Paper as a harm in need of regulation (pp. 22-24). We welcome this recognition, but note that the White Paper also states, “We are clear that the regulator will not be responsible for policing truth and accuracy online.” (p. 36). We are unsure how misinformation will be addressed without the regulatory unit either taking a position on the truth/falsity of content or mandating another body (such as the platforms or a third party) to do so. We would welcome greater clarity on what is ‘in scope’ of the White Paper’s understanding of misinformation, as the range of different types of false or misleading information is considerable, including (1) explicitly false content, (2) misleading interpretations of facts, (3) partial and one-sided analyses and (4) predictions and opinions treated as facts.
Overall, the Online Harms White Paper marks a bold step forward, both in the UK and internationally, in safely and responsibly regulating the internet. The Alan Turing Institute’s public policy programme response is publicly available. If you have any questions or would like to find out more about our work, email [email protected]. 
"
https://www.turing.ac.uk/people/guest-speakers/noor-shaker,"
Noor is a serial biotech entrepreneur with a track record of achievements in AI having held an Assistant Professorship from Aalborg University.
Noor has published numerous papers in the field of artificial intelligence and its application to drug discovery and is an inventor on a handful of patents. She is passionate about data and AI and on a mission to cure disease with the power of human and machine learning.
She is currently the CEO at GlamorousAI, a biotech company that pushes the boundaries to what is possible with AI to cure debilitating diseases. She is a recognised healthcare leader, MIT innovator under 35 and in BBC 100 women.
"
https://www.turing.ac.uk/people/researchers/florimond-houssiau,"
"
https://www.turing.ac.uk/news/turing-publishes-new-landscaping-report-data-science-ai-and-futures-work,"
Today the Turing publishes a landscaping report: ‘Data science, artificial intelligence and the futures of work’ by researchers Sanna Ojanperä, Neave O’Clery and Mark Graham, for our research programme in Economic Data Science. With technology developing at a great pace, there have been many concerns over how it will impact the jobs of the future and our working lives.
Through this overview, the authors explain that multiple possible futures exist and these futures of work are dependent on our choices, adaptability to changing circumstances and policy decisions. This report was commissioned by the Turing to inform the Institute’s research strategy and its aim to further data science and AI research to address real-world problems.
The authors identify several key challenges, including:
Yet the changing nature and organisation of jobs and the adoption of new technologies also gives rise to new opportunities, such as new jobs in both technology-driven and traditional industries. Widening market access, better governance and regulation models, and improved workplace technology are other areas where changes may lead to positive outcomes.
The changing nature and organisation of work and its diverse impacts on societies will need to be understood through not just best practice applied to new topics, but through exploring new research approaches. There are opportunities for emerging research approaches rooted in data science and AI, such as machine learning, robotics and network science. As the future of work quickly becomes the present, there is an urgent need for scholarship that attempts to understand how to make our new world of work sustainable, equitable and just.
“Our report synthesises key findings about the future of work focusing on three main areas: broad research findings, emerging research directions and innovative data science research directions. While these changes affect economies and societies differently around the world, the challenges and opportunities give rise to research opportunities and open up avenues for collaboration and learning.”
Lead author Sanna Ojanperä
The Turing will publish a series of guest blogs this week exploring different angles of #futuresofwork as well as host a Data Debate: The AI will see you now, with the British Library the evening of Thursday 1 November. Later this autumn/winter we’ll also be announcing a portfolio of research projects on 'The Changing Nature of Work' as part of the Turing-HSBC-ONS Economic Data Science Awards.
"
https://www.turing.ac.uk/people/spotlights/barbara-mcgillivray,"

I aim to better understand and track how words change meaning over time. This can be over long time periods – centuries for languages like Ancient Greek or Latin – right down to just a few years in the case of Twitter data, for example.
Language changes constantly, and words can have different meanings at different points in time. It also depends on our context, who we speak to, and where we come from. Getting an algorithm to understand all these complexities is hard!
Language change not only interests computational linguists and data scientists, but also digital humanists, historians, classicists and social scientists, so I often find myself in projects with people from different disciplinary backgrounds. It’s really challenging and rewarding to engage with questions that these different communities are passionate about.
Becoming Editor-in-Chief of the Journal of Open Humanities Data. Having previously worked in academic publishing, and now working full-time in research, it’s great to be in the position to help make a difference towards an open-data culture in the humanities.
Interdisciplinary. Computational. Humanistic.
I enjoy dancing Argentine tango - which I’ve been doing for fifteen years.
The Language Log is a language blog maintained by Mark Liberman at the University of Pennsylvania. It’s a great way into linguistics matters for non-specialists.
One I like is the evolution of the meaning of the words silly and nice, which originally had opposite meanings to what they have today. Silly originally meant ‘blessed’ or ‘happy’ coming from an old Germanic root meaning ‘luck, happiness’, but over time it went through a series of changes from ‘blessed’ to ‘pious’, to ‘helpless’, ‘innocent’ to ‘pitiable’ to ‘feeble’ to today’s meaning of ’foolish’. Nice, on the other hand, coming ultimately from Latin nescire ‘to be ignorant’ went from meaning ‘ignorant’ through ‘foolish’, ‘shy’, ‘lascivious, ‘extravagant’, ‘elegant’, and finally ‘pleasant, agreeable’, thus. So be careful who you call nice or silly!
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/london-school-economics,"
www.lse.ac.uk
"
https://www.turing.ac.uk/people/researchers/nick-holliman,"
Nick Holliman is Director of CUSP London and Professor in Computer Science at King's College London. He obtained a PhD from the Departments of Computing and Mechanical Engineering at the University of Leeds, sponsored by IBM UK. He worked in the computing industry, researching high quality graphics, computer vision and high-performance computing at Sharp's European Research Laboratories in Oxford, leading to a number of globally impactful products. He then moved to academia, most recently leading the development of research and teaching in the Data Science group at Newcastle University. His current research investigates the use of information theory to create novel visualization methods and empirically validates them worldwide using rigorous methods drawn from psychophysics.
He is one of the main organisers of the Visualization Turing Interest Group.
His research aims to find new ways to create data visualisations that directly address challenges in the human understanding of big data and AI. Starting off in electronic form in a computer, a display device converts information optically to light, this electromagnetic information is detected by the eye and the information finally exists as electro-chemical nerve impulses in the brain. How should we optimise this process so that we can quickly and accurately understand new ideas, and with these ideas take decisions about the world?
His research at Newcastle has focused on exploiting the power of cloud super-computing to create more effective visualisations of urban analytics data. Cloud super-computing allows three orders of magnitude more computing power to be applied to data visualisation problems than would be possible on even the most powerful desktop computer. This is allowing the exploration of visual approaches to representing data that have not been see before, one example is the TeraScope project that is producing a visualisation from city scale to room scale in a single image of one trillion pixels.
As a Turing Fellow he focuses on two directions of research: First to address the challenge of visualising uncertainty, how might we represent values and our confidence in values in immediately understandable ways. Second to address the challenge of how to automate visualisation, big data is too big for a human designer to confidently represent visually, how can we automate this process using stochastic optimisation techniques and perhaps also machine learning.
As well as patents and academic papers his research outputs have included a number of award winning short stereoscopic 3D films on the role of dark matter in the creation of the universe in collaboration with Professor Carlos Frenk and the Institute for Computational Cosmology at Durham University. He is a member of the Association for Computing Machinery, The IEEE Computer Society, the Royal Statistical Society and the Society for Imaging Science and Technology.
This course in data exploration and visualization covers cleaning and structuring datasets, and choosing suitable methods for visualizing them. It also provides theoretical knowledge of the underpinning descriptive statistics and the basics of human perception for cognition.
Learners will acquire skills in data exploration and visualization. By the end of the course they will be able to take raw data sets, clean them, structure them and choose suitable methods for visualizing them. They will also acquire theoretical knowledge of the underpinning descriptive statistics and the basics of human perception for cognition.
This is an introductory course on Data Visualization using Python, suitable for anyone with basic experience of Python programming.
By the end of this course you will be able to:
This course is released under a CC BY 4.0 license.
It was designed by Professor Nick Holliman at Newcastle University.
"
https://www.turing.ac.uk/people/business-team/david-sarmiento-perez,"
David is an Andalusian engineer more interested in asking why than how, which led him from robots to smart cities and from programming to design. After some years in product management pushing for user centred design methodologies, he moved to Edinburgh to study Digital Sociology to go deeper into the social shaping of technology.
"
https://www.turing.ac.uk/news/pick-postcard-penguin-post-office-antarctica,"
Polar-inspired postcards designed by students from Cardwell Primary School in Woolwich, London and two illustrators are en route to Antarctica to the world’s most southerly post office, as part of a project funded by The Alan Turing Institute and Polar Impact.
The three winning schoolchildren’s designs were announced recently at the National Maritime Museum. All designs created for the competition are currently on display at the National Maritime Museum as part of a pop-up exhibition.
The polar landscape provided creative inspiration, with icebergs, penguins, seals and orca whales all featured on the artwork.
The project aims to encourage students to consider careers in polar science and to picture how the polar regions relate to their lives.
The postcards are travelling with polar researchers from the British Antarctic Survey, the Centre for Polar Observation and Modelling and Imperial College London. Some are making the journey on the RRS Sir David Attenborough ship.
From January the illustrators’ postcards will be posted back to schools so students can follow the intrepid journeys of the polar scientists through an interactive QR code. The postcards will also link to resources that will encourage the children to imagine themselves as polar explorers.
Every year over 70,000 items are posted to around 100 countries from Port Lockroy in Antarctica - affectionately known to some as the Penguin Post Office due to the 1,500 resident Gentoo penguins.
The project has been run by Polar Impact, whose aim is to support, connect, and highlight the stories of Black, Asian, Indigenous, people of colour, and minority ethnic professionals in the polar research community.
Premdeep Gill, Enrichment Student at The Alan Turing Institute and Polar Impact founder, said: “Involving more people and perspectives in polar research is essential to ensuring we develop the most robust and inclusive plans to protect our polar regions.”
“We hope this project inspires people to consider polar science as a career option. Being able to see the journeys of polar scientists will help bring to life what polar researchers do and will hopefully encourage them to think that is something they could do too.”
Notes to editor:
This project has been supported by Royal Museums Greenwich, The Satellite Data in Environmental Science – Centre for Doctoral Training (SENSE CDT) and its EDI Champion and National Geographic.
The National Maritime Museum holds the world’s largest maritime collection telling stories of Britain’s epic relationship with the sea, housed in historic buildings forming part of the Maritime Greenwich UNESCO World Heritage Site.
The National Maritime Museum is part of Royal Museums Greenwich which also incorporates the Royal Observatory, the 17th-century Queen’s House and the clipper ship Cutty Sark. This unique collection of museums and heritage buildings are also major centres of education and research. The mission of Royal Museums Greenwich is to enrich people’s understanding of the sea, the exploration of space, and Britain's role in world history.
Header image: Anton Rodionov/Shutterstock.com
"
https://www.turing.ac.uk/people/former-doctoral-students/taha-ceritli,"
Taha is a third-year PhD student registered at the University of Edinburgh working under the supervision of Chris Williams. He received his BSc (2015) and MSc (2017) at the Bogazici University. His research interests include Bayesian inference in probabilistic graphical models with the applications for data analytics.
 
In data analytics projects, a significant amount of time and effort is spent on data wrangling, where the goal is to transform a raw dataset into a clean/standard format so that it can be used with statistical models. Hence, it is crucial to develop systems that can help users through various stages of this process. Taha's studies focus on developing machine learning methods to semi-automate such data normalization tasks.
"
https://www.turing.ac.uk/research/research-areas/statistical-methodology/bayesian-inference,"
"
https://www.turing.ac.uk/people/governance,"

The Institute will soon be opening an international call for new members of the Scientific Advisory Board, and will send out further communications in due course.
"
https://www.turing.ac.uk/events/understanding-impact-pandemic,"
The International Lecture Series: Your monthly invitation to hear from world leading experts in health surveillance research. The series will focus on statistical modelling and machine learning approaches, policy responses and international best practice in responding to national health emergencies.
Presented by The Turing/RSS Lab: Supporting the UKHSA
As the 51st session of the UN Statistical Commission drew to a close on 6 March 2020, it was clear that COVID-19 was rapidly escalating from a regional crisis to a global crisis. The Chief Statisticians of the international statistical system had already begun to think about how support could be provided to countries and how a consolidated and coherent statistical overview of the impact of the coming pandemic could be presented.
Meanwhile, the World Health Organization (WHO) and the United Nations Department of Economic and Social Affairs (UN DESA) were also busy establishing a joint Technical Advisory Group to guide methodological work on calculating excess mortality associated with COVID-19, which would be essential for calculating global mortality and population estimates.
While the technical work presented many challenges, and some interesting opportunities, it also ran headfirst into the national and international politics of mortality – a collision that may have lasting and profound implications for international statistics.  
Opening remarks: Professor Chris Holmes and Dr Johanna Hutchinson (UKHSA)
Presentation by: Dr Steve Macfeely: World Health Organization (WHO)
Q&A
Closing remarks: Professor Chris Holmes
Register here for the July lecture
"
https://www.turing.ac.uk/people/university-liaison,"

Each university in the Turing University Network nominates a Turing liaison as the primary contact person at the university. Each university is also invited to nominate a Turing liaison (academic) as the primary research link between the Institute and the university.  
The primary contact link between the Institute and Turing University Network universities 
 
Cardiff University 
Niki Price 
City, University of London 
Claudia Kalay 
Durham University 
Dr Alan Real
Goldsmiths, University of London 
Emma Price 
Imperial College London 
Alex Gibbs 
Keele University 
Julie Street
King’s College London 
Dr Rose Hepworth
London School of Hygiene & Tropical Medicine 
[email protected] 
LSE 
Jill Beattie
Newcastle University  
Angela Craggs 
Northumbria University 
Lindsay Connolly 
Nottingham Trent University 
Padraig Mulcahy 
Queen Mary University of London 
Emma Grant  
Queen’s University Belfast 
Dr Tom Gibson 
Royal Holloway University of London  
Dr Marcela Acuna-Rivera 
Royal Veterinary College  
Dr Emma Tomlinson 
Teesside University 
Kirsty Metcalfe 
UCL  
Matt Davis 
University of Birmingham  
Jessica Mylchreest
University of Bristol 
John Newby 
University of Cambridge 
Arti Sheth Thorne 
University of Edinburgh 
Neil McGillivray 
University of Exeter 
Dr Emily Paremain 
University of Glasgow 
Neil Findlay
University of Leeds 
Kimberley Wright 
University of Liverpool 
Anthony Mtitimila 
University of Manchester 
Sarah Barton
University of Nottingham  
Dr David Hawker 
University of Oxford 
Kathleen Dolan 
University of Plymouth 
Susan Eick 
University of Reading  
Dr Bonhi Bhattacharya 
University of Sheffield 
Emma Barker 
University of Southampton 
Susan Davies
University of Strathclyde 
Elizabeth Ross 
University of Surrey 
Dr Andrew Rogoyski
University of Warwick 
Rebecca Silver
 
"
https://www.turing.ac.uk/people/enrichment-students/dev-aditya,"
Dev Aditya is a researcher of AI and HCI and an architect of conversational AI models. Apart from his core research work, he also leads the digital transformation of Otermans Institute (OI), an SME, that he has turned from a front-facing training provider to a hypergrowth AI-led digital training and education provider with a focus on conversational AI teaching. His work led to the organization growing from operating in the UK and 3 countries, to 12- countries today - all since 2021. Dev, now 30, is the primary AI-model architect of OIAI- a virtual humanlike on-demand teacher. Apart from his technology role, Dev is a multi-award-winning social innovator and educationist with an incredibly strong passion to eliminate the global skills and employability gap. He was awarded by the UK Prime Minister, named an Under 30 Global Innovator by Innovate UK (for work in AI), and received 4 other national and international recognitions just in 2021. Prior to taking the technological and AI lead in the company, he conducted immersive research across 3200 kilometers in South Asia to help create Otermans Institute's current curriculum and teaching system focused on upskilling underserved learners through transferable skills training to improve their employability. Dev Aditya believes that society has come to a moment in time when the vast majority of underserved persons can be reached, particularly to service education and upskilling, because of the penetration of smart devices and the internet. The way to serve them, he believes, is through AI.
Dev Aditya has a passion to provide one-to-one conversational teaching support to everyone in the world, starting with the underserved populations. One of the larger underserved communities that face significant accessibility challenges is hearing-impaired persons - particularly sign language speakers. For them, subtitles and captions do not work and a lack of sign language translation can become a significant barrier for them, especially in the areas of online and digital learning. Over 5% of the world's population – or 430 million people – require rehabilitation to address their 'disabling' hearing loss and 34 million of them are children. It is estimated that by 2050 over 700 million people – or one in every ten people – will have disabling hearing loss (WHO, 2021), and the majority of them could use sign language as their preferred language and method of accessing digital content given the chance. Using AI, Dev Aditya is creating AI-powered digital avatars and AI models to improve accessibility for sign speakers who according to The Guardian face a ""digital glass wall"".
"
https://www.turing.ac.uk/people/guest-speakers/konstanty-korski,"
Konstanty Korski is a Senior Expert Pathologist and R&D Lead for Digital Pathology in the Roche pharma personalised healthcare (phc) Data, Analytics & Imaging group in Basel, Switzerland.
He graduated from the Poznan University of Medical Sciences (2001) and is board certified in general pathology (2008). Since his graduation, he served as a staff pathologist in the Pathology Department at the Greater Poland Cancer Center in Poznan, Poland, where his main focus was breast and gynecologic pathology, cytopathology, immunohistochemistry and molecular pathology. At that time, in one of his roles he coordinated tumor sample collection process in the collaborative international effort - The Cancer Genome Project (TCGA).
Konstanty joined Roche in 2014 at the Roche Innovation Centre Munich where he focused on the application of digital pathology to tumor tissue analysis and immune microenvironment characterization in the context of early phase clinical trials. In 2020, Konstanty joined the phc group in Basel where he leads the R&D DP team. The team concentrates on the extraction of information from the digital images of the H&E slides and on translating it to predictive/prognostic algorithms using state-of-the-art machine learning techniques. 
He has co-authored more than 30 peer reviewed publications and three patent applications.
"
https://www.turing.ac.uk/people/researchers/anastasia-mantziou,"
Anastasia is a Postdoctoral Research Associate at The Alan Turing Institute supervised by Gesine Reinert and Mihai Cucuringu from the University of Oxford. Prior to that, she was a Research Assistant in statistical cyber-security at Imperial College London. She completed her PhD in Statistics at Lancaster University under the supervision of Dr Simon Lunagomez, Dr Robin Mitra and Professor Paul Fearnhead. Her research interests include network analysis, Bayesian methods and topic modelling. Her research has been applied to networks emerging from various scientific fields such as neuroscience, ecology and computer science (human tracking systems).
Anastasia is currently working on network time series data with application on economics, under the economic networks and transaction data project in The Alan Turing Institute. This is in collaboration with the Office for National Statistics (ONS). This project aims to build an understanding of networked data emerging from firm-to-firm transaction data held by ONS by reducing the complexity of such data, and economic modelling and forecasting. 
"
https://www.turing.ac.uk/people/researchers/jonathan-rowe,"
Jon Rowe is Professor of Natural Computation in the School of Computer Science, and Deputy Pro-Vice Chancellor (Strategic Projects) at the University of Birmingham. He is Programme Director for the Data Science for Science and Humanities programme at The Alan Turing Institute, and is PI and Chair of the SPF funded AI for science and government project. He is an Associate Editor for Theoretical Computer Science journal. He joined the University of Birmingham in 2000 and his PhD in Artificial Intelligence is from the University of Exeter.
Jon's research encompasses the theoretical analysis of nature-inspired algorithms, such as evolutionary algorithms and other randomised search heuristics, along with a range of inter-disciplinary applications from modelling social crowd dynamics to chemical reaction networks to medical imaging techniques.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/mosip,"
MOSIP website
"
https://www.turing.ac.uk/people/doctoral-students/giulia-occhini,"
Giulia is a doctoral student registered at the faculty of Science within the University of Bristol. Prior to joining The Alan Turing Institute, she obtained an MPhil in Linguistics at Leiden University and a joint BA in Modern and Classical Languages and Literatures at the University of Bologna and the University of Upper Alsace.
Giulia believes that methodological innovation is fundamental in order to make sure that the Humanities will remain relevant in the future, and she is particularly excited to be part of this innovation at the Turing. Her research interests lie principally within the fields of Digital Humanities and Natural Language Processing, which she explored during her time as a research assistant at the Leiden University Centre for Linguistics and the Royal Netherlands Institute for Southeast Asian and Caribbean Studies.
Not only cities change, but they also change faster than the secondary data update pace (e.g. surveys). Moreover, cities change in myriads of ways, and therefore official sources of structured, secondary data may not be able to capture these changes. Giulia’s work at the Institute will use methodologies from Computational Linguistics in order to explore how the dynamics of the UK urban system can be detected - or even predicted - by mining internet content. These methods will be implemented with statistical modelling and spatial analysis in order to understand the spatiality of these processes, and will mainly utilise unstructured textual data from The Internet Archive.
The key aim of the project is to push the envelope of quantitative geography methodological tool-kit. While methods from computational linguistics have already been employed in Human Geography, their use has been limited to social media data thus far. Similarly, although business studies have used web mining and data from The Internet Archive before, their scope was rather limited and ignored the spatial signatures of these data.
"
https://www.turing.ac.uk/research/interest-groups/clinical-ai,"
The Clinical AI interest group seeks to bring together health professionals from diverse healthcare fields and data scientists who have a shared interest in clinical AI and to foster this network through interactive events on up-to-date developments in the field and cultivate innovative research projects.
The clinical application of medical artificial intelligence is an area of mass interest to researchers, governments and industry and has the potential to improve clinical care. However, if this potential is to be realised, a substantial effort is required to up-skill the clinical workforce and to educate future generations of healthcare professionals in the development, application and pitfalls of clinical AI.
The Clinical AI interest group will address the following aims:
Challenge: In recent times there has been an explosion in the development of AI algorithms for use in clinical practice. These may be undoubtably useful, but still require robust assessment and evaluation.
Challenge: With rapid developments in AI and the growth of their adoption in the clinical realm it is crucial to identify the future direction of travel and new areas of usage.
Challenge: AI systems can learn and perpetuate existing biases within the health system and widen health inequalities. As more AI systems are developed and adopted, strategies need to be in place to ensure it benefits all patients and public, including those in minoritised and marginalised groups.
Click here to join us and request sign-up
[email protected]
"
https://www.turing.ac.uk/people/researchers/catalina-vallejos,"
Catalina is a Chancellor's Fellow at the MRC Human Genetics Unit, where she leads the Biomedical Data Science research group. 
Before moving to Edinburgh, Catalina was part of the first cohort of Turing Research Fellows. As part of her Fellowship, Catalina was also a Group Leader within the Lloyds Register Foundation-Turing Programme on Data-Centric Engineering. 
Between 2014 and 2016, Catalina was a Postdoctoral Fellow in a joint appointment between the MRC Biostatistics Unit (MRC-BSU) and the EMBL European Bioinformatics Institute (EMBL-EBI), both located in Cambridge (UK). In this position, she was a member of the Statistical Genomics research group (MRC-BSU) and the Marioni group (EMBL-EBI) which are respectively lead by Professor Sylvia Richardson and Dr John Marioni. 
Education
Catalina completed a PhD in Statistics at the Department of Statistics of the University of Warwick, under the supervision of Professor Mark Steel. Her PhD thesis covered theoretical and practical aspects of Bayesian inference and survival analysis. She completed her undergraduate and MSc studies in Chile: BSc in Mathematics (Statistics track) and MSc in Statistics at the Faculty of Mathematics of the Pontificia Universidad Católica de Chile. During her BSc studies, Catalina also completed a Certificate in Economics. Her MSc dissertation project was in the area of long memory time times, under the supervision of Dr Wilfredo Palma. 
Catalina's main area of research is on Bayesian statistical methodology, mostly driven by applications in biomedicine. An important area of Catalina's research programme is to translate the methods she develops into open-source analysis tools that can reach the wider community. Currently, Catalina's group focuses on two areas of application: single-cell genomics and electronic health records. 
In terms of methodology, her main interests include:
"
https://www.turing.ac.uk/people/researchers/katharine-robson-brown,"
Kate Robson Brown is Professor of Biological Anthropology within the Faculty of Arts and the Faculty of Engineering. She obtained her PhD in phylogenetics from the University of Cambridge (Newnham College, 1995), and held the Graham Robertson Research Fellowship at Downing College. She is the Director of the Jean Golding Institute for Data Science and Data Intensive Research at the University of Bristol.
Kate is also chair of the Institute's Research and Innovation Advisory Committee, which steers scientific direction at the Turing.
Kate’s research explores the microstructure of living tissues and their response to changing and extreme environments; innovating methodologies for the capture, computational modelling, analysis and interpretation of data describing complex material and structural characterisation. Biological anthropological applications of these methods include forensic identification, the regulation of hard tissue growth and development, and the study of biomechanical systems in extinct species. Engineering applications include employing the ontogeny of tissue microstructure as a model of programmed transformation in 4D materials, biomimetics in engineering design, and multi-scale modelling of complex hierarchical structures and systems.
"
https://www.turing.ac.uk/people/guest-speakers/ann-copestake,"
Ann Copestake studied at the University of Cambridge, and then worked in Cambridge while completing her DPhil at Sussex, before becoming a Senior Research Engineer at the Centre for the Study of Language and Information (CSLI) at Stanford University. She returned to the Computer Laboratory at Cambridge University in 2000, where she is now Professor of Computational Linguistics and Head of Department. She is also co-Director of the Cambridge Language Sciences Interdisciplinary Research Centre.
Her research concerns the development of computational models of aspects of human languages. In particular, she is interested in how models of sentence meaning (compositional semantics) can be formalized and implemented, and how this interacts with word meaning (lexical semantics) and pragmatics. She has worked with a range of approaches, from typed feature structures to neural networks.
"
https://www.turing.ac.uk/people/former-enrichment-students/charlie-dickens,"
Charlie is a PhD Student in Computer Science at the University of Warwick and is supervised by Professor Graham Cormode. Prior to this he completed his MSci in Mathematics from the University of Birmingham in 2016.
Large datasets are ubiquitous in modern society but a common problem is how to deal with such massive datasets? Charlie’s research is concerned with reducing large datasets into smaller, manageable summaries which can be used to approximate key properties of the data. He is also interested in the related questions of whether these approximations can be performed in a reasonable amount of time and whether they can be used in different models of computation; for example, when data arrives one item at a time and only a small amount of it can be kept. These issues span both theoretical and practical backgrounds and will be of interest to anyone who performs large-scale data analysis.
"
https://www.turing.ac.uk/people/business-team/anastasia-shteyn,"
Anastasia is the Programme Manager for Data-Centric Engineering as of May 2021, having previously been the Programme Manager for Finance and Economics and Data science at scale progammes. Please get in touch at [email protected] if you’d like to find out more about the programmes or explore ways to collaborate.
 
Anastasia is currently on an external secondment (July 2022-May 2023).
"
https://www.turing.ac.uk/people/researchers/alejandro-coca-castro,"
Alejandro (he/him) is a Research Fellow of the Data Science for Science and Humanities and Honorary Member of the Tools, Practices and Systems programmes at The Alan Turing Institute. He holds a PhD in Physical Geography with a MSc. in Environmental Monitoring, Modelling and Management at King’s College London. He has 11+ years of experience in applied artificial intelligence and data science for Earth Systems, Agricultural and Environmental sciences. He has contributed to a wide variety of international institutions in the public, research and industry sectors developing pipelines and tools to process and analyse data (spatial and non-spatial) for decision making.
When he is not too busy doing geeky things, Alejandro enjoys reading sci-fi books, cycling and coffee tasting.
Alejandro's main research includes modelling Earth systems and Environmental phenomena using artificial intelligence and data science. He is currently evaluating probabilistic data-driven models for the intelligent fusion of data from a wide range of sources (satellite, reanalysis, in-situ surface sensors, among others) to predict environmental and climate.
Beside his involvement in scientific and applied research challenges, he is an active contributor and lead in open science education and open-source development. For instance, he is founder of the Environmental Data Science book, a computational notebook community promoting open source software, reproducibility and collaborative research in Environmental science. In addition, he has contributed to software development, tutorials, content and events led by community-driven initiatives including Scivision (core member), the Turing Way (core member), Open Life Science (mentee/mentor) and Pangeo (member).
"
https://www.turing.ac.uk/events/turing-lecture-provably-beneficial-ai,"
Stuart Russell received his BA with first-class honours in physics from Oxford University in 1982 and his PhD in computer science from Stanford in 1986. He then joined the faculty of the University of California at Berkeley, where he is Professor (and formerly Chair) of Electrical Engineering and Computer Sciences, holder of the Smith-Zadeh Chair in Engineering, and Director of the Center for Human-Compatible AI. He has served as an Adjunct Professor of Neurological Surgery at UC San Francisco and as Vice-Chair of the World Economic Forum's Council on AI and Robotics.
His book ""Artificial Intelligence: A Modern Approach"" (with Peter Norvig) is the standard text in AI; it has been translated into 14 languages and is used in over 1,400 universities in 128 countries.
His current concerns include the threat of autonomous weapons and the long-term future of artificial intelligence and its relation to humanity.

Is it reasonable to expect that AI capabilities will eventually exceed those of humans across a range of real-world-decision making scenarios? Should this be a cause for concern, as Elon Musk, Stephen Hawking, and others have suggested? While some in the mainstream AI community dismiss the issue, Professor Russell will argue instead that a fundamental reorientation of the field is required. Instead of building systems that optimise arbitrary objectives, we need to learn how to build systems that will, in fact, be beneficial for us.
In this talk, he will show that it is useful to imbue systems with explicit uncertainty concerning the true objectives of the humans they are designed to help. This uncertainty causes machine and human behaviour to be inextricably (and game-theoretically) linked, while opening up many new avenues for research. The ideas in this talk are described in more detail in his new book, ""Human Compatible: AI and the Problem of Control"" (Viking/Penguin, 2019).
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/epsrc,"
"
https://www.turing.ac.uk/people/business-team/nur-azimah-said,"
"
https://www.turing.ac.uk/people/external-researchers/hannah-fry,"
Hannah Fry is an experienced science presenter, public speaker, bestselling author, and academic known for her joyful ability to bring maths to life for audiences of all interests and abilities. Winner of the prestigious Zeeman Medal recognising her work in engaging the UK public with mathematics, Hannah is a trusted and well-recognised presenter. Her expertise is regularly called upon to develop and host documentaries and her fun and lively radio programmes and podcasts have a large and devoted following. Her ability to engage and enthuse people with even the most complicated topics is equally evident on the page and her books delight readers around the globe.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations,"

The Alan Turing Institute partners with a wide range of organisations, including government, charities, companies and universities to undertake world-leading research to change the world for the better.
Read about how to collaborate with us and view how the Institute is governed.
 
We engage with universities from across the UK on our ambitious projects and programmes
"
https://www.turing.ac.uk/people/guest-speakers/adam-kucharski,"
Adam Kucharski is an Associate Professor and Sir Henry Dale Fellow in the Department of Infectious Disease Epidemiology at the London School of Hygiene and Tropical Medicine.
Hi research focuses on analysis and modelling of epidemic-prone infectious disease. In particular, he is interested in how social behaviour and pre-existing immunity shape transmission dynamics, and how knowledge of such processes can improve surveillance and control measures.
He has previously worked on real-time modelling of infections including Ebola, dengue, and cholera, as well as leading ongoing research into influenza and COVID-19. 
From 2012-17 he held a Medical Research Council Career Development Award in Biostatistics, and prior to joining LSHTM he was a research associate at Imperial College London. He has an MMath in Mathematics from the University of Warwick and a PhD in Applied Mathematics from the University of Cambridge.
photo credit: TED/Bret Hartman
"
https://www.turing.ac.uk/news/where-are-women-mapping-gender-job-gap-ai,"
Published on 8 March 2021, International Women’s Day, this policy briefing from the Turing’s Women in Data Science and AI project presents new research into gender gaps in AI and data science, and the extent and impact of men’s dominance in these fields.
A decade ago, Harvard Business Review named data scientists as “the sexiest job of the 21st century.” Since then hiring in the fields of artificial intelligence and data science has exploded as the world is increasingly being built around smart machines and automated systems. 
Yet the people whose work underpins that vision are far from representative of the society those systems are meant to serve.
Only 22% of data and AI professionals in the UK are women, and this drops to a mere 8% of researchers who contribute to the pre-eminent machine learning conferences.
This is not only a fundamental issue of economic equality, but also about how the world is designed and for whom. Mounting evidence (e.g. Buolamwini and Gebru, 2018; West, Whittaker and Chew, 2019) suggests that the under-representation of women and marginalised groups in AI results in a feedback loop whereby bias gets built into and amplified by machine learning systems. 
Addressing the gender job gap in AI is the first step to ensuring that our technology works for all of society.
Existing data is sparse: Raw, intersectional industry data about gender diversity in the AI workforce is severely limited. The available high-level statistics, however, show there are fewer women working in the data and AI fields in the UK compared to the global average. Currently, women make up an estimated 26% of workers in data and AI roles globally, which drops to only 22% in the UK. Further, in the UK, the share of women in engineering and cloud computing is a mere 14% and 9% respectively.
Diverging career trajectories: There is evidence of persistent structural inequality in the data science and AI fields. Women are more likely than men to occupy a job associated with less status and pay in the data and AI talent pool, usually within analytics, data preparation and exploration, rather than the more prestigious jobs in engineering and machine learning.
Job attrition rates: Women working in AI and data science in the tech sector have higher attrition rates (i.e. leaving the industry) than men.
Self-reported skills: Men routinely self-report having more skills than women on LinkedIn. This is consistent across all industries and countries in the sample in the report. This correlates with existing research into women’s lower confidence levels in their own technical abilities.
The qualification gap: Women in data science and AI have higher formal educational levels than men across all industries. The achievement gap is even higher for those in more senior ranks (i.e. for C-suite roles).
Participation in online platforms: Women comprise only about 17% of participants across the online global data science platforms Data Science Central (‘DS Central’), Kaggle and OpenML. On StackOverflow, women are a mere 8%. Additionally, only about 20% of UK data and AI researchers on Google Scholar are women. Of the 45 researchers with more than 10,000 citations, only five were women.
The report shows the extent of gender disparities in careers, education, jobs, seniority, status and skills in the AI and data science fields. Utilising a new, curated dataset and an innovative methodology, it explores the gendered dynamics of careers in the UK and other countries in fresh detail. 
These original findings add urgency to the drive to improve women’s opportunities in the technology industry and highlights the need for effective policy responses if society as a whole is to reap the benefits of technological advances. 
This work has added urgency since the drive to close the gender gap in the technology industry risks being derailed by the pandemic.
As Professor Judy Wajcman, head of the project added: “Diversity in the fields of data science and AI is essential if we do not want to replicate social biases in technical systems.”
Read the full report: Where are the women? Mapping the gender job gap in AI by Erin Young, Postdoctoral Research Fellow in the Public Policy Programme at The Alan Turing Institute; Judy Wajcman, Principal Investigator of the Women in Data Science and AI project at The Alan Turing Institute, and Anthony Giddens Professor of Sociology at the London School of Economics; and Laila Sprejer, a Data Science Research Assistant in the Public Policy Programme at The Alan Turing Institute.
Cover photo credit: Mike Ngo Photography via Flickr
 
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/get-involved-pi,"

Data Study Group (DSG) are collaborative events bringing together skilled data scientists with Challenge Owners (CO) to tackle everyday business challenges. Principal Investigators (PI) are a dedicated resource to COs, working alongside them to determine the scope of the challenge, define the research questions, and understand the nature of the proposed data set to ensure it is ready for the DSG event. 
There are too many COs that need dedicated academic support to prepare their challenge for a DSG than what the DSG team core can provide. This is a good opportunity for training early career researchers in preparation for overseeing future projects, especially when working with industry. 
A PI will be sourced shortly after Turing partnership development confirms the project is nominally viable for a DSG project. This should have a minimum of two months of preparation time for the scoping and following the DSG event itself, two months for results preparation. 
You can also sign up to our DSG PI mailing list to be notified of our latest opportunities below. Please see more info in our resources section below.

Will be based on your CV and cover letter for selection to interview.
The cover letter (up to two pages) should demonstrate your ability to suggest multiple potential methodological approaches to the challenge being applied for, as well as demonstrate:
If you are not sure about if you have appropriate skills, do get in contact at [email protected]
The Alan Turing Institute is committed to creating an environment where diversity is valued and everyone is treated fairly.  In accordance with the Equality Act, we welcome applications from anyone who meets the specific criteria of the post regardless of age, disability, ethnicity, gender reassignment, marital or civil partnership status, pregnancy, religion or belief or sexual orientation. Reasonable adjustments are available to support candidates through the application and interview process.
 
You can download the full guide to being a DSG PI here. It includes detail on the lifecycle of a challenge, PI responsibilities, data suitability and more FAQs.
 

“Being a DSG PI was a great experience, because it enabled me to partake in one of the most fun activities as a data scientist - serving as a translator between people of different disciplines and backgrounds.
Additionally, this experience has sparked future collaborations with DSTL.
I have two suggestions for future DSP PIs: Firstly, I think that it is very important to meet with the challenge owner one-on-one as early as possible before the DSG in order to help ‘translate’ their questions into the language of data science and help manage expectations.
Secondly, it is helpful to make sure that the data is in a format that would enable DSG participants to jump into data analysis straight away, instead of wasting precious time parsing strange data formats or reformatting tables.
Being a DSG PI is very rewarding, especially seeing all of the innovative ideas that DSG participants come up with!”
Daphne Ezer, Data Study Group PI, University of Warwick
 
8.1 MB
328.8 KB
142.5 KB
"
https://www.turing.ac.uk/taxonomy/term/1252,"

A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC) organised by the AI for multiple long-term conditions: Research Support Facility. Open to everyone across the AIM programme and the broader multiple long-term conditions research community, these talks are designed to share knowledge and stimulate discussion regarding AI and MLTC research.
 
Many people suffer from multiple health conditions, yet most of the evidence base for medical care centres on the treatment of single conditions; surprisingly little is known about identifying and treating multiple long-term conditions. To address this emerging strategic priority for the UK’s National Health Service and research funders, at least £23 million has been directed into the AIM Programme to establish research to better understand and treat MLTCs.
A dedicated Research Support Facility (RSF) has been established to support and facilitate activity across the independently-funded AIM research consortia. Hosted by The Alan Turing Institute in collaboration with the University of Edinburgh and Swansea University, the RSF are holding a series of monthly sessions with thought leaders and subject-matter experts – including experts by lived experience - on the second Tuesday of each month.
Funded by NIHR, a dedicated Research Support Facility (RSF) has been established to support and facilitate activity across the independently-funded AIM research consortia.
Every RSF talk takes place online and is recorded and archived on YouTube. Watch the recordings below.
 
 







"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/leeds-city-council,"
www.leeds.gov.uk
 
"
https://www.turing.ac.uk/news/climate-sceptic-tweets-growing-four-times-fast-pro-climate,"
Climate scepticism is growing four times as fast as pro-climate content on Twitter, according to a new paper published today (Thursday 24 November) in Nature Climate Change by The Alan Turing Institute. 
In an analysis of tweets from 2014 – 2021 during the annual COP conferences, researchers found that tweets by climate change sceptics were shared 16 times more during COP26 than COP21.
The study authors found that this increase in climate sceptic tweets online was fueled by growing “right-wing activity” opposing climate action.
The study showed that overall, polarisation on Twitter in relation to the climate was low during COP21 until COP26 and identified 2019 as a key year that climate scepticism grew on Twitter.
Researchers say that a possible reason for the increase in recent years could be due to a backlash against climate activist groups, like Extinction Rebellion and Just Stop Oil, taking action to draw attention to the crisis.
Professor Mark Girolami, Chief Scientist at The Alan Turing Institute, said: “Acting quickly and effectively on the climate crisis relies heavily on broad international consensus and collaboration. The growth in polarisation online could risk political deadlock if it fuels antagonism to climate action. Policymakers should consider what exactly is causing this increase in scepticism online and find ways to tackle it.”
The authors say that groups that oppose climate action are using accusations of hypocrisy as a way to discredit climate summits and delay climate action. 
But they also found that both climate sceptics and pro-climate groups shared criticism of perceived hypocrisy on Twitter, particularly around the use of private jets. And previous research has shown that this type of content is more likely to go viral online.
Dr Andrea Baronchelli, lead author and Token Economy Lead at The Alan Turing Institute, said: “The significant increase in climate scepticism online is really concerning. Social media can act as an echo-chamber where people’s existing beliefs are reinforced. It’s really important that regulators continue to find ways to ensure that content shared online is accurate. ”
"
https://www.turing.ac.uk/research/research-areas,"

"
https://www.turing.ac.uk/people/former-doctoral-students/helen-oliver,"
Helen started her doctoral studies at The Alan Turing Institute in October 2016. Helen is registered at the University of Cambridge and begun her doctoral studies after three years as a Research Associate at the University of Cambridge Computer Laboratory. She has been a researcher since 2008, and her research interests have converged at the intersection of wearable technology, narrative, privacy and innovation in the Internet of Things.
Helen’s project focuses on the research through design of wearable technology. As more users adopt wearable technology, more data of an ever more personal nature will be generated, raising urgent questions about how that data is handled and who benefits. At the same time, the HCI aspect of these devices is endangering mainstream acceptance: there is a lack of variety in the forms and functions of wearable devices, and a need to integrate wearables into IoT ecosystems. Helen’s research seeks concrete solutions for wearable technology, and the data it generates, to benefit the individual user and society as a whole.  
"
https://www.turing.ac.uk/people/external-researchers/marta-kwiatkowska,"
Marta Kwiatkowska is Professor of Computing Systems and Fellow of Trinity College, University of Oxford, and Associate Head of MPLS. Prior to this she was Professor in the School of Computer Science at the University of Birmingham, Lecturer at the University of Leicester and Assistant Professor at the Jagiellonian University in Cracow, Poland.
She holds a BSc/MSc in Computer Science from the Jagiellonian University, MA from Oxford and a PhD from the University of Leicester. In 2014 she was awarded an honorary doctorate from KTH Royal Institute of Technology in Stockholm.
"
https://www.turing.ac.uk/people/researchers/darren-wilkinson,"
Darren Wilkinson is Professor (Statistics) at Durham University. He studied at Durham University, where he did a degree in Mathematics followed by a PhD in Bayesian Statistics. After a year as a research associate at Durham, he took up a Lectureship at Newcastle, being promoted to Professor in 2007. He began working on problems in statistical bioinformatics and computational systems biology shortly after moving to Newcastle. From 2008-2011 he held a BBSRC Research Development Fellowship to study stochasticity and heterogeneity in model biological systems.
He continues to be motivated by challenging inferential problems arising from life science research, and is active in the biological sciences research community, currently serving on BBSRC's Strategy Advisory Panel for Enabling New Ways of Working. In addition to being an investigator on several large RCUK grants, he currently co-directs Newcastle's EPSRC Centre for Doctoral Training in Cloud Computing for Big Data, building on his existing interests in modelling large and complex data sets in molecular biology and genomics.
His current research interests involve applications of Bayesian statistics to a variety of challenging big data problems in molecular biology and engineering. He is especially interested in parameter inference for dynamic models, on-line inference for high-velocity time series data, probabilistic programming, and the use of approximate models and emulators for rendering computationally prohibitive algorithms for expensive models more tractable.
"
https://www.turing.ac.uk/people/external-researchers/desmond-upton-patton,"
Desmond Upton Patton is a Public Interest Technologist who is a pioneer in the use of social media and artificial intelligence in the study of contextually driven AI, empathy, and violence. Dr. Patton is the founding Director of the SAFElab and Associate Professor of Social Work, Sociology and Data Science at Columbia University.
He is the recipient of the 2018 Deborah K. Padgett Early Career Achievement Award from the Society for Social Work Research (SSWR) and 2017-2018 Fellow at Harvard’s Berkman Klein Center for Internet and Society. He is a Presidential Leadership Scholar and Technology and Human Rights Fellow at the Carr Center for Human Rights at Harvard Kennedy School. His worked is featured in the A&E show: Secret Life of a Gang Girl: The Untold Story.
"
https://www.turing.ac.uk/news/publications/exploring-childrens-rights-and-ai,"
Since 2022, researchers on the Turing's Public Policy programme have been collaborating with Scotland's Children's Parliament and the Scottish AI Alliance to engage primary school children across Scotland between the ages of 7 and 11 on artificial intelligence (AI) and children’s rights. As part of this research, a series of in-person introductory workshops on AI and children’s rights have been held with 87 children in four schools across Scotland, including Edinburgh, Glasgow, Stirlingshire and Shetland.
This report outlines the findings from phase 1 of the project, which focused on ‘Exploring children’s views on AI’. The research explored foundational concepts of AI and machine learning, seeking to address the following questions:
Four themes emerged from phase 1 of this work as consistent areas of interest for children, including:
13.3 MB
This research was conducted in partnership with Scotland’s Children's Parliament and the Scottish AI Alliance.
"
https://www.turing.ac.uk/people/researchers/oliver-strickson,"
Oliver is a Research Software Engineer at The Alan Turing Institute. He has a PhD in physics from the University of Cambridge, where he was based at the Cavendish Laboratory and worked on techniques for producing accurate material models for continuum mechanics simulations based on first-principles atomistic techniques, with a particular focus on shock-waves in condensed matter.
Following this, he worked as a software engineer, writing research software for the numerical simulation of fluid and solid dynamics.
He was involved in several successful industrial collaborations with partners in the mining, defence and oil and gas industries. He also holds a degree in mathematics.
"
https://www.turing.ac.uk/partner-us/government-bodies,"

In pursuit of our mission to make great leaps in data science and AI research in order to change the world for the better, we work alongside policy makers to improve public policy as well as the provision of public services using tools from data science and AI.
The ultimate aim of our work with government is to have a positive impact on the lives of as many people as possible. We are interested in collaborating with policy makers to develop innovative ways to provide government services: from allocating resources in the fairest and most transparent way, to designing personalised public services that are tailored to people’s individual needs and situations.
We are interested in exploring how we can use our expertise in data science, AI, and ethics to inform policy-making across all levels of government, from identifying policy priorities to evaluating policy outcomes.
We tailor our collaborations around the needs of our government partners – our partnerships range from small-scale research projects to multi-year research programmes. We encourage all policy makers with difficult-to-solve policy problems, unwieldy data sets, or an interest in data science and AI to email us at [email protected].
The Alan Turing Institute works with a wide number of organisations, including government, charities, companies and universities to undertake world-leading research to change the world for the better.
"
https://www.turing.ac.uk/people/guest-speakers/ashnah-kalemera,"
Ashnah Kalemera works as a Programme Manager at CIPESA. She coordinates multi-country projects promoting the use of ICT in democratic processes and for citizen participation, including training for citizen journalists, mainstream media and government officials. She holds an MSc in Informatics with a major in Electronic Government (Örebro University, Sweden) and a BSc in Computer and Management Sciences (University of Warwick, United Kingdom), and has work experience from Spain, UK, and various African countries.
"
https://www.turing.ac.uk/people/researchers/phillip-stanley-marbell,"
Phillip Stanley-Marbell is an Associate Professor in the Department of Engineering at the University of Cambridge, where he leads the Physical Computation Laboratory and he is a Faculty Fellow at the Alan Turing Institute for Artificial Intelligence and Data Science in London. Prior to moving to the UK in 2017, he was a researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. From 2012 to 2014, he was with the Core OS organization at Apple (Cupertino, USA) where he led the development of new system components for iOS,  macOS, and watchOS that enable on-device machine learning. The work is captured in eight granted patents for technologies in Apple products and is incorporated into all Apple’s products shipping today. Prior to Apple, he spent several years (2008–2012) as a permanent research staff member at IBM Research in Zürich, Switzerland. He completed his Ph.D. at Carnegie Mellon University (Pittsburgh, USA) in 2007, spending 2006–2008 at Technische Universiteit Eindhoven in the Netherlands. Before his Ph.D., he spent several summers as an intern at Bell Labs: in the Microelectronics division with a group that designed ASICs for telephony applications (1995, 1996) and with the Data Networking division (1999), in a project spun out of the group that created UNIX, doing work with the Inferno Operating System.
 His research focuses on investigating methods to use properties of physical systems to improve the efficiency of computation on data from nature. His research has led to several best paper nominations and awards (IEEE ESWEEK / Transactions on Embedded Computing Systems, ACM Computing Surveys), research highlights in the ACM’s flagship Communications of the ACM journal (CACM, 2021), as well as multiple articles covering his research in the mainstream media (e.g., Fast Company 2019, Wired Magazine 2020). He is the author of over 60 peer-reviewed publications and three textbooks.
His research focuses on investigating methods to use properties of physical systems to improve the efficiency of computation on data from nature.
"
https://www.turing.ac.uk/research/publications/dcms-consultation-centre-data-ethics-and-innovation,"
The creation of the Centre for Data Ethics and Innovation was announced in late 2017. The Government launched its Consultation on the Centre in June 2018, seeking the views of interested groups from across society regarding how the Centre should operate, and which work it should prioritise.
The Institute’s response addresses each question posed by the Consultation, and focuses on two areas that we feel we are particularly suited to advise on: how the new Centre should work, and which issues in data ethics it should focus on.
On how the Centre should work, we raise questions around the overall resources that the Centre will have at its disposal, how it is structured, and to what extent research will be undertaken by the Centre itself versus by outside groups on the Centre’s behalf. We argue that among the Centre’s proposed functions, its mandate to “publish recommendations to Government” and to “provide expert advice and support to regulators” will allow it to differentiate itself in an increasingly crowded field of digital ethics.
On which issues it should work on, we agree that the Centre’s identification of six areas of interest represents a good overview of the current state of the field. We also highlight other areas, including accountability, proportionality, and consent, which we feel are relatively underemphasised in the Consultation, and argue that some of the existing themes require more conceptual clarity.
Our response also includes comments on the legal and political basis of the new Centre, the ways in which it might engage with stakeholders, and how it should deliver its recommendations to Government.
1.8 MB
"
https://www.turing.ac.uk/news/publications/statistical-predictions-trading-strategies-electronic-markets,"
We build statistical models to describe how market participants choose the direction, price, and volume of orders. Our dataset, which spans sixteen weeks for four shares traded in Euronext Amsterdam, contains all messages sent to the exchange and includes algorithm identification and member identification. We obtain reliable out-of-sample predictions and report the top features that predict direction, price, and volume of orders sent to the exchange. The coefficients from the fitted models are used to cluster trading behaviour and we find that algorithms registered as Liquidity Providers exhibit the widest range of trading behaviour among dealing capacities. In particular, for the most liquid share in our study, we identify three types of behaviour that we call (i) directional trading, (ii) opportunistic trading, and (iii) market making, and we find that around one third of Liquidity Providers behave as market markers.
Cartea, Álvaro and Cohen, Samuel N. and Graumans, Rob and Labyad, Saad and Sánchez-Betancourt, Leandro and van Veldhuijzen, Leon, Statistical Predictions of Trading Strategies in Electronic Markets (May 9, 2023). Available at SSRN: https://ssrn.com/abstract=4442770 or http://dx.doi.org/10.2139/ssrn.4442770
"
https://www.turing.ac.uk/research/research-projects/digital-twins-high-value-engineering-applications-dthive,"
Digital twins are radically reshaping most aspects of modern technology. A key objective of a digital twin is that it can faithfully capture time-evolutionary behaviour of the physical twin. Significant value and advantage can then be obtained by monitoring data and anticipating changes that happen over time, e.g. for asset management, scheduling of operation and maintenance, extending operational life and increasing security and resilience, to mention just a few tasks. 
At present, there is only a very rudimentary understanding of how the time-evolution of a digital twin will work. Understanding more completely the dynamics and control of the twin across a range of time scales is one of the central research questions of this project. More specifically, it will focus on high-value engineering applications from aerospace to civil infrastructure.
Digital twins are a fusion of physics-based and data-based models that evolve over time, and are connected to the physical twin via a data flow. Defining how the time evolution occurs will depend on the specific context of the application, but should also have some generic properties. For example, processes that need to happen in real-time are treated very differently from those that can be carried out offline, or at other time-scales.
The scheduling of required processes across multiple time scales concurrently is an important function of the digital twin. In addition to this, enabling the processes to access data between the digital and physical twin, both directly and remotely is an area that is yet to be fully developed. This is something that is being investigated in a parallel project, in partnership with IOTICS who provide data ecosystems.
1: Regional Hub - We provide a regional research hub that has important links with key UK industries such as aerospace, automotive, infrastructure and renewable energy, that will accelerate advancements in digital twin research into direct societal impact.
2: Research Excellence - bringing together expert teams, we act as a centre of excellence for research in digital twins for high-value engineering applications.
3: Cross-disciplinary - bring together researchers from across disciplinary boundaries to drive forward the understanding, development, and integration of digital twin science and technology.
Digital twins are live digital couplings between virtual duplicates of real-world systems and the physical twin.  They are radically reshaping most aspects of modern technology from healthcare to manufacturing and are perceived as crucial to providing solutions to urgent societal issues such as climate change. 
This project will focus on applications relating to aerospace, manufacturing and energy use in buildings. The project will run from October 2021 until March 2022, and is intended to deliver working prototypes in the three application areas defined above. The primary scientific objective is to investigate the time-evolving behaviour of digital twins.
“This project will bring together research experts from across disciplinary boundaries to drive forward the understanding, development, and integration of digital twin science and technology” said project lead Professor David Wagg. “The work being carried out at the Advanced Manufacturing Research Centre and the Faculty of Engineering will provide a regional research hub for digital twins in the North of England, that has important links with key UK industries such as aerospace, automotive, infrastructure and renewable energy”.
At a societal level, digital twins will be a key part of solutions for; green energy (e.g. asset management of offshore wind farms); healthcare (e.g. via personalised medicine); smart cities (e.g. by managing energy consumption) and; more efficient manufacturing (e.g by reducing costs and increasing time efficiency) to name just a few important application areas.
At the University of Sheffield we are seeing first hand the urgent need to respond to research challenges that have an important societal impact. We believe that digital twin technology will have a significant impact on these societal challenges for the UK and wider world.

Figure 1 Hawk Test 1
 

Figure 2LVV Chamber
 

Figure 3 LVV Hawk
 

Figure 4 LVV Chamber
David Wagg - [email protected]
 
The challenge facing industry is the access to “right time data” and security of that data. The problem isn't that the data doesn't exist, it's that we can't securely access it when and where it’s needed.
IOTICS is changing the way the world uses and shares data. Our Dynamic Data Sharing Architecture delivers a decentralized, secure and accessible data ecosystem with the flexibility to grow with your data needs.
Using your existing IT systems and the addition of IOTICSpace, data becomes Findable, Accessible, Interoperable and Reusable. Reducing operational costs and delivering scalable building blocks for your data strategy, without locking you into architectural choices.
"
https://www.turing.ac.uk/people/communications-engagement/natasha-lyon,"
"
https://www.turing.ac.uk/research/theory-and-method-challenge-fortnights/prediction-algorithms-causal-interpretation,"
Prediction algorithms are widely used in several domains, including healthcare, yet neither the parameters nor the predictions, have a causal interpretation. A causal interpretation is desirable when using prediction algorithms for decision support to allow for the prediction of the potential outcome of an individual for each intervention under consideration.
With a rich and growing causal inference literature that focuses on estimating the causal effects of hypothetical interventions, firmly grounded in the potential outcomes framework, there is an opportunity to embrace and integrate these methods to allow a predictive algorithm to become meaningful in a causal sense, and thus allow appropriate use of prediction algorithms to guide decisions. With the anticipated increase of (automated) algorithm-based decisions in coming years, following advances in machine learning and artificial intelligence, there is an urgent need for a greater understanding of how causal reasoning can be integrated in predictive analytics.
This is part of the Theory and Methods Challenge Fortnights in Data Science and AI.
There are methodological challenges to overcome to achieve a fundamentally different approach to prediction. Primarily these concern integration of information across different models that achieve different objectives (e.g. a model that estimates a particular causal effect, versus a model optimised for prediction), and synthesis of information from different data sources (e.g. causal effect of treatment estimated in a clinical trial; prediction algorithm estimated in routinely collected data).
In healthcare and other fields, we are increasingly moving to a proactive rather than reactive approach, which is widely believed to be more efficient and effective. Prediction algorithms are essential tools to underpin this, as they allow resource to be targeted in an evidence-based and effective way, by making use of the increasing volumes of data available to make accurate and precise predictions. However, there is a growing awareness that the information prediction algorithms provide is often insufficient for their intended use. As remarked by Hernán et al. (2018) ‘predictive algorithms inform us that decisions have to be made, but they cannot help us make the decisions’.
With QRISK, the proposed intervention is unrelated to the prediction algorithm, and the efficacy of the intervention is determined independently of the prediction algorithm. Incorporating principles of causal inference in predictive algorithms will provide direct information on the consequences of the intended interventions, and permit decision makers to answer the question ‘what if I do or do not take a certain action’.
To map out the research challenges and the proposed program of work required to deliver prediction algorithms enabled with counterfactual prediction for improved algorithm-based decision support.
Prediction algorithms that can be used more directly for decision support in a range of application areas. In health, it will allow to evaluate ‘what if’ predictions, i.e. predictions of outcomes for patients conditional on a range of possible intervention strategies to allow for better informed decision making. We will also evaluate counterfactual fairness, i.e. how to avoid discrimination in prediction models (and decisions based on them) such as in recidivism prediction. We will also develop methods applicable in public policy, and finance and economics scenarios, i.e. allowing for counterfactual prediction of outcomes of different policies.
This project will map and clarify the research agenda for theory and methods development in counterfactual prediction. This directly maps to the Turing’s overall aim of building a data and AI enriched world for the benefit of all, since counterfactual prediction has the potential to lead to better information available to decision makers, as well as tackling the issue of counterfactual fairness.
There are a number of other domains where incorporating causal inference in predictive algorithms will be important, that will be explored in this TMCF.
Public policy: where AI is used to inform policy making, counterfactual prediction algorithms that allow the implications of ‘what if’ scenarios to be evaluated appropriately are of huge value to policy makers.
Finance and economics: counterfactual prediction algorithms can be applied at both an individual level (e.g. estimating the probability of an individual defaulting on a loan) up to macro-economic policy scenarios.
Counterfactual fairness: in a recidivism model, for example, one may make a decision on sentencing based on the risk of re-offending. In this case, it is crucial that discrimination is avoided (e.g. ethnic origin, or a factor related to it, being a predictor). Kusner et al. (2017) have shown the use of a counterfactual model to tackle this problem. More generally, the challenge of linking causal inference with prediction is important across the fields of AI, statistics, machine learning and data science, to ensure that predictions are explainable, transparent, robust, and used in an ethical and fair manner.
Ultimately we hope this research will make a difference by providing risk prediction tools that do allow for the causal interpretations that users are requesting. We would ultimately economically evaluate the implementation of such tools. Of course, this is a long term plan that will be realised through follow-on work from the fortnight.
University of Manchester
"
https://www.turing.ac.uk/people/external-researchers/ozak-esu,"
Ozak is an electrical engineer, leading national and international engineering projects at Cundall. She is passionate about advancing society through science, technology, engineering, mathematic (STEM) and has devoted the past nine years to STEM outreach and volunteering initiatives that promote equality, diversity, and inclusion.
 

"
https://www.turing.ac.uk/research/research-projects/international-trade-modelling,"
International trade is a complex economic problem. New trade data and computational modelling techniques allow us to develop new types of models that account for nuances and dynamics that standard economic models cannot. Our models enable trade analysts and policymakers to gain deep insight into the potential outcomes of policies such as creating new trade agreements.
"
https://www.turing.ac.uk/people/guest-speakers/tie-yan-liu,"
Tie-Yan Liu is a Distinguished Scientist at Microsoft, leading the Asia team of Microsoft Research AI4Science. He is a fellow of the IEEE, the ACM and the AAIA. He is also an adjunct/honorary Professor at Tsinghua University, University of Nottingham and Hong Kong University of Science and Technology.
Tie-Yan has served as general chair, program committee chair and (senior) area chair for various top conferences. These include WWW/WebConf, SIGIR, KDD, ICML, NeurIPS, ICLR, IJCAI and AAAI. He has also served as an Associate Editor of ACM Transactions on Information Systems, ACM Transactions on the Web and IEEE Transactions on Pattern Analysis and Machine Intelligence.
As a world-renowned machine learning expert, Tie-Yan has made notable contributions in ranking problems (learning to rank), learning from tabular data (LightGBM) and learning under resource constraints (dual learning, LightLDA, FastSpeech). Recently, he has shown great interest in AI for Science.  
Tie-Yan and his team developed Graphormer for molecule representation, which won the KDD Cup 2021 on molecular property prediction and the direct track of Open Catalyst Challenge 2021. They used molecular dynamics simulation to understand SAR-COV-2 and discovered the wedge effect of the NTD of the spike protein, which indicates a potential new drug target for COVID-19.
Furthermore, Tie-Yan and his collaborators applied deep learning to detect new particles from jet data, to effectively solve governing equations of fluid dynamics and to identify non-conservative components in observational data as new physics. 
 
"
https://www.turing.ac.uk/about-us/impact/better-together-people-centred-approaches-driving-forward-data-ethics,"
Imagine you applied for your dream job but didn’t even get an interview. Now, you’ve found out that the company you applied to uses artificial intelligence (AI) to process its job applications. Would you feel mistrustful of the technology used to inform the decision? If all we understand is that our data went into an algorithm, something happened to it and then a decision popped out – a ‘black box’ of AI – how can we really know that we’ve been treated fairly?
In the UK and abroad, The Alan Turing Institute is leading conversations on ethical issues just like these. Projects funded by the Turing’s AI for science and government (ASG) programme are helping to establish best practice in responsible research and innovation (RRI) in data science. All of these projects centre not on the technologies themselves but on the people who have the most to gain or lose from them.
ASG-funded Project ExplAIn is helping organisations to understand how and why they should explain the decisions made by their AI systems. As Research Associate at the Turing Morgan Briggs points out, “it’s about who’s on the other end of the explanation. People want to know how a decision was made about their data using AI.” It’s this broader view of AI, extending beyond the purely technical, that shapes the project’s ‘stakeholder-first’ perspective. To craft ExplAIn’s comprehensive guidance ‘Explaining decisions made with AI’ (2020), the Turing team including Director of Ethics and Responsible Innovation Research, David Leslie, collaborated directly with members of the public, scientists and industry experts in a series of workshops and roundtables. The guidance aims to assist organisations in building ‘explainability’ into AI systems from the start and tailoring explanations to different audiences. It is helping the UK government to set standards in ‘AI assurance’, which will in turn help to ensure that we can trust in AI systems.
Two years on, the team has developed workbooks that use practical examples – like the job application processing example above – to help organisations make use of the guidance. Briggs and Leslie piloted these workbooks in a series of virtual and in-person workshops. Meanwhile, Keeley Crockett, an AI ethics researcher at Manchester Metropolitan University, has been using the guidance in AI ethics workshops for small and medium-sized enterprises in Manchester, as part of a £3 million AI Foundry project – 126 companies have taken part so far. She’s also using case studies developed through ExplAIn as teaching resources to give her master’s students in data and AI ethics a grounding in explainable AI. “We’re getting these master’s students to understand more about explainability as a unique selling point for their future employment within companies,” she says.
Overseas, the guidance has already informed recommendations for explaining AI put together by the US National Institute of Standards and Technology. According to Briggs, “there’s mutual investment and interest in this idea of explainability” and of placing AI in its wider, social context.
Project ExplAIn emerged from official UK government guidance on responsible AI for the public sector, developed by Leslie in partnership with the Office for Artificial Intelligence. This internationally recognised guidance, ‘Understanding AI ethics and safety’ (2019), has been put into practice by at least a dozen different government departments. Its purpose is to help guard against some of the potential harms of AI within public sector settings. When AI is used to help improve cancer diagnosis, for instance, patients need to be clear what the AI system is doing and why, and who is liable for any mistake. Leslie and Research Assistant Cami Rincon are now collaborating with government and public sector partners to update the guidance and, through a bespoke training programme, are providing the tools to embed it within public sector AI projects for societal benefit.
According to Leslie, this guidance was “the wellspring” for the values and principles shaping other Turing projects, including a ground-breaking collaboration with Camden Council that resulted in the first ever data charter co-created by a local authority and its residents. In this community-owned project, Ethics Fellow Chris Burr and Rincon worked with 20 Camden residents to explore key concepts in data ethics, helping the residents to define – through their own deliberations – the principles that now guide how the council collects and uses residents’ data. As the council’s Data Custodian, Brendan Kelly, explains, the charter sits at the heart of all its work with data. “Every data project goes through an ethical assessment that has been written out because of the principles in this charter,” he says. He adds that the residents went away with a better understanding of how data is used in a council setting, and more trust in what Camden is doing with their personal data.
Another group grappling with difficult issues around data science is the data scientists themselves. Progress in science depends on reproducibility: using transparent data and methods so that experiments can be checked and repeated, and evidence built upon. But this isn’t always the case in AI and data science, where code and details of complex models often remain unpublished. The Turing strives to work in a different way, embedding reproducibility into its data projects from the start so that innovative new tools and software can be shared and adapted. One of the central functions of the Research Application Manager, a key Turing role, is to help researchers adopt open-science practices so that the outcomes of their work reach more diverse users.
However, the Turing can’t change the culture of data science alone – that requires a more coordinated effort. So, since 2019, ASG-funded project The Turing Way has been supporting the data science community at large to lay down the principles and practices of reproducible research in an openly available, online handbook. The project shares much the same ethos as ExplAIn, with project co-leads Malvika Sharan and Kirstie Whitaker noting that its ‘sociotechnical’ perspective puts greater emphasis on people than technology.
The Turing Way now has over 350 authors and contributors, with many collaborating during regular online co-working events organised by Community Manager Anne Lee Steele. It is at the forefront of a shift in how data science is done, influencing UK government strategy on reproducibility in data analysis and informing the Health Foundation (which tackles real-world health and social care issues through data analytics) as it adopts more open practices. The Turing Way is also used internationally. Biostatistician and co-founder of the Latin American community MetaDocencia, Laura Ación, uses The Turing Way to explain responsible data use in her work with Argentinian data projects, including ARPHAI, which focuses on using AI and data science in policy-making to tackle epidemics.
Meanwhile, The Turing Way has become a key resource for the US space agency NASA’s $40 million, five-year mission Transform to Open Science (TOPS), which aims to get NASA scientists learning about and applying principles of open science. “As we go and talk to NASA scientists, they ask us for resources. I point them to The Turing Way,” says open science advocate Chelle Gentemann, a Senior Scientist at the Farallon Institute who works on TOPS. As well as showing scientists how to make their work reproducible, she says, it shows them how to make it more inclusive, so that more people can contribute.
TOPS has also put together a diverse team of experts to develop an open science course for scientists – the OpenCore Curriculum – covering aspects such as the ethos of open data, and best practices for data-sharing. “Rather than reinventing the wheel”, as one of the team, Christopher Erdmann, former Assistant Director for Data Stewardship at the American Geophysical Union, puts it, they were able to draw from The Turing Way’s open materials.
Open resources are also at the core of ASG-funded project, the Turing Commons. Targeted not just at data scientists but anyone who needs to understand responsible data use, this incorporates the ‘Citizen’s guide to data’ that Burr designed to support his work with Camden residents. Burr also developed guidebooks and activities on ‘Responsible research and innovation’ (RRI) and ‘Public engagement of data science and AI’ that formed the basis for two live, online training courses attended by at least 50 people worldwide. Mayara Carneiro, a tech lawyer in Brazil, took the first RRI course in November 2021. She says it “helped me to have a better notion of what I should take into account when building ethical guidelines and requirements” for tech companies using AI.
Burr and Research Assistant Claudia Fischer are now refining and expanding the resources to support more interactive and tailored content, and in the tradition of the Turing, they’re doing it collaboratively. According to Burr, they’ve had “significant interest from UK universities” in more openly accessible resources for students and researchers across different specialisms, especially on RRI in data science and AI. “Many universities reach out to us to say, ‘You’re creating free, open-access resources. Can we work together?’,” he says. So, for example, partnering with the University of Edinburgh’s Biomedical AI Centre for Doctoral Training, they are now co-designing biomedical case studies that will be shared via the redesigned Turing Commons platform.
RRI is also an important aspect of existing ASG-funded training offered by the Turing’s Research Engineering Group (REG). As Senior Research Data Scientist Federico Nanni explains, whilst many students learn to code during their PhDs, they don’t always learn “best practices” – how to build software that other researchers can reuse and extend. Nanni says the benefit of delivering the training via the REG is that its members work across multiple academic disciplines, offering diverse perspectives for the 40 students who have so far taken the ‘Research data science’ course.
It’s not just PhD students who need data science training, though. REG Data Research Scientist Lydia France had this realisation when collaborating with the Francis Crick Institute on some training for biomedical researchers. It became apparent that some project leaders were avoiding computational work altogether. “They don’t know where to begin because there is no training,” she says. So France redirected her efforts at senior biomedical researchers, mining the wisdom within The Turing Way to build free-to-access masterclasses on how to manage and supervise open and reproducible data projects.
Now France and Nanni are targeting skills gaps across other subject areas, including the humanities, leveraging the broad expertise within the REG. “There are definitely lots of disciplines that are facing challenges because they are collecting such gigantic amounts of data,” Nanni says. “I think that’s where we can contribute the most.”
So, as the Turing sets the standards for more ethical, reproducible and collaborative data science, it is also delivering the skills needed for this more responsible approach to the field. At the same time, community-led and participatory projects funded by the ASG programme are giving more people the power to influence how data-driven technologies are developed, and how their personal data is used, with public benefit to the fore. Better data ethics means putting people, rather than technology, first.
 
Header illustration: Jonny Lighthands
"
https://www.turing.ac.uk/events/geospatial-standards-data-science,"
Advances in geospatial technologies have led to a significant increase in the role of geospatial data across sectors such as agriculture, aviation, energy, utilities, hydrology, smart cities, government, health and others. As more geospatial data has been collected and analysed to support these sectors, so has the need increased to enable efficient and effective sharing of geospatial data. International standards are a key enabler of such efficient and effective sharing of data. The aim of this workshop is to examine the growth of geospatial data sources and the international standards that enable these sources to be leveraged by data scientists. The workshop will also explore what geospatial standardisation in Machine Learning, Cloud Computing, Discrete Global Grid Systems, Linked Data and other areas will mean for Data Science.
The workshop is being organised in collaboration with the Open Geospatial Consortium (OGC) - an international industry consortium of over 520 companies, government agencies and universities participating in a consensus process to develop publicly available geospatial interface standards. Attendees of this workshop will hear about Geospatial Standards for Data Science and also about the role such standards play within the Defence context.
14:30:    Arrival and refreshments
14:45     Introductory remarks 
14:50:    Geospatial Standards for Data Science - Gobe Hobona (OGC)
15:40:    Break
15:50:    The Defence Context - Paul Cripps (Dstl)
16:40     Discussion - all
16:55     Closing remarks - Mark Birkin (The Turing/Leeds University)
17:00     Networking and further informal discussion
17:30     End

"
https://www.turing.ac.uk/research/research-areas/mathematical-modelling/deterministic-mathematical-modelling,"
"
https://www.turing.ac.uk/contact-us/contact-form,"
"
https://www.turing.ac.uk/research/challenges,"
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/parallel-computing,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/kings-college-london,"
"
https://www.turing.ac.uk/people/researchers/vasilios-mavroudis,"
"
https://www.turing.ac.uk/people/researchers/oliver-davis,"
Oliver Davis is Associate Professor and Turing Fellow at Bristol Medical School and the MRC Integrative Epidemiology Unit at the University of Bristol, where he co-directs the Dynamic Genetics Lab with Professor Claire Haworth. Oliver read Natural Sciences at Cambridge, studied for a PhD in social, genetic and developmental psychiatry at King’s College London (KCL) and held a Sir Henry Wellcome Research Fellowship, visiting the Wellcome Trust Centre for Human Genetics in Oxford and the European Bioinformatics Institute near Cambridge. He held academic positions at KCL and UCL before moving to Bristol in 2015.
Mental health and wellbeing are influenced by a complex interplay of social, genomic and developmental factors. Understanding these patterns and how we can intervene to promote good mental health is one of the greatest and most important challenges of our time.
Oliver’s research in mental health data science and digital epidemiology uses emerging technologies to collect, analyse and visualize data from large population samples around the world, exploring the dynamic relationship between social and genomic factors and wellbeing over the life course. Recent research from the lab includes the spACE project which is mapping geographical variation in social and genetic influences on complex traits, and the EMBERS project which is collecting millions of social media interactions from thousands of participants in epidemiological cohorts to track dynamic social and genetic influences on wellbeing in emerging adulthood.
"
https://www.turing.ac.uk/events/ai4space-connecting-networks-develop-ai-space-science-and-engineering,"
This one-day workshop aims to generate advancements in data science and AI technologies and accelerate their adoption across the Space sector.  It aims to generate increased activity at the Turing Institute in the area of Space, in particular aligned to Spacecraft Systems Engineering and the Data-Centric Engineering programme and to develop a wider network of researchers using and developing Data Science and AI, across Space Science and Engineering.
 
The Universities of Southampton and Strathclyde are jointly organizing this event with the support of The Alan Turing Institute. It will cover two themes: Space Science and Space System Engineering and will look at how the different disciplines can work more closely together going forward.  Each theme will reflect on the AI requirement in each sector, current national activity and how the Turing Institute can provide value.
The co-Chairs are Professor Adam Sobey, University of Southampton and Programme Director for Turing’s Data-Centric Engineering Programme, Dr Annalisa Riccardi, University of Strathclyde, and Professor Francesco Shankar, University of Southampton and Turing Fellow.
 
In order to bring together key Data Science and AI experts from across the network to explore potential collaboration and meet the goals of this workshop, registration for this event will be invitation-only in the first instance. The list of invited applicants includes a number of Turing Fellows and Turing-affiliated researchers.  The remaining spaces will be filled through an open invitation. If oversubscribed, the organisers will assess/accept applicants to ensure the goals of the workshop are met.
Interested participants should contact the organiser Susan Davies at [email protected]      
The keynote talks by Dr Ingo Waldman, UCL and Turing Fellow, and Dr Audrey Berquand, European Space Agency, will be live-streamed and open to anyone to attend online.  Please register via the above links – the closing date for registration is 29 June. 
"
https://www.turing.ac.uk/people-powered-ai,"

In 2022, we funded seven innovative public engagement projects across the UK to boost understanding of AI and data science. People-powered AI, a project led by Dr Annabel Latham and Professor Keeley Crocket (Manchester Metropolitan University), received funding with the aim to build and encourage more diverse voices to interact with researchers and innovators during research and development (R&D) processes and wider public discourse. 
Following a series of successful engagements, leading to positive impact for several key stakeholders, we are pleased to share the terms of reference and case study booklet for anyone seeking to establish their own people's panel for AI. 
The terms of reference document was co-produced by panel members, businesses and researchers involved in the People’s Panel for AI project, led by Dr Annabel Latham and Professor Keeley Crockett

271.9 KB
2.9 MB
"
https://www.turing.ac.uk/research/research-projects/masking-clouds-satellite-imagery,"
Clouds appear ubiquitously in the Earth's atmosphere, and thus present a persistent problem for the accurate retrieval of remotely sensed information. By applying machine learning to the problem this project aims to more accurately detect cloudy pixels in remotely sensed images. Improved detection of cloudy pixels can facilitate more accurate retrieval of climate variables such as sea and land surface temperature.
One of the ways to measure sea surface temperature (SST) at a given point and time is to take a satellite image over a particular region of the ocean. Then using the thermal imaging bands from the the 'Sea and Land Surface Temperature Radiometer' (SLSTR) fitted to SENTINEL-3 satellites, which are measured in units of Kelvin, it's possible to estimate the temperature of a particular ocean pixel. To ensure accurate retrieval this can then be compared with the values recorded from drifter buoys floating on the sea surface by matching the location of the buoy to the pixel in the satellite image. 
However, pixels can often be occluded or contaminated by clouds, which will make the value recorded by the satellite different from the one measured on the ground. Accurately classifying pixels as either clear or cloud contaminated, and then removing the contaminated ones from an image, in effect reduces noise and therefore reduces the uncertainty in the measurements. 
The Sea and Land Surface Temperature Radiometer (SLSTR) on board SENTINEL-3 satellites provide a continuous measurement of both land and sea surface temperatures to high accuracy. Unfortunately, the retrieval of such variables is hindered by pixels which are contaminated by cloud. 
This project aims to use a machine learning approach to improve on existing cloud masking approaches for the SLSTR to produce a high quality, versatile cloud mask. Such a mask will perform well in difficult images where the identification of cloudy pixels is complicated by other phenomena such as sea ice, snow, aerosols, sun glint etc. The project also aims to validate its approach against the retrieval of climate variables from satellite scenes, with the hope of showing increased performance in practical applications. 
With the increasing urgency of the climate crisis it is essential to gather an accurate record of variables affecting the world's climate from which to inform our understanding of the global climate system. 
The ability to remove cloudy pixels, which effectively act as noise, from remotely sensed images has a direct impact on the quality of our climate monitoring. This in turn impacts the projections one can make about how severely climate change will affect the world, which has wide ranging impacts on many environmental, economic, and societal areas. The output of this work will be applied by colleagues in RAL Space and the Centre for Environmental Data Analysis (CEDA) to aid in their future work. 
"
https://www.turing.ac.uk/node/3863,"

The Alan Turing Institute was awarded £38.8 million over five years in 2018 through UKRI’s Strategic Priorities Fund. AI for science and government (ASG) is delivered in partnership with the Engineering and Physical Sciences Research Council (EPSRC) and in collaboration with a number of other research councils.
The biggest challenges that our societies face – from the COVID-19 pandemic to social and economic instability, overpopulation, and the climate emergency – involve complex interconnections between environmental, healthcare, social, economic, political and engineering systems. Addressing these grand challenges therefore requires both deep disciplinary expertise and ways to convene and combine expertise from different disciplines, strongly connected to problem owners.
Traditionally, this has been a substantial challenge. However, by providing the tools and methods to handle, combine and model large, disparate datasets, modern data science and AI tools have the potential to transform this process, with myriad benefits to society. The vision of ASG is to demonstrate – via a diverse range of activities organised and presented as an integrated whole – how AI and data science can be used to effectively address significant societal challenges and transform health, science, engineering, and government.
Themes, projects, events and associated programmes can be explored below.


Here you will find a collection of one page briefing notes that provide an overview of the impact achieved to date on projects funded by the AI for Science and Government (ASG) programme.
A digital twin for London’s underground farm (PDF)
Accessible training for diverse data users (PDF)
Colouring Cities: a worldwide network to inform buildings research and policy (PDF)
EnergyFlex: using simulation to make our homes more energy-efficient (PDF)
IceNet: faster, more accurate sea ice forecasting with a new AI-based tool (PDF)
Raphtory: deeper network analysis (PDF)
scivision: making computer vision tools more accessible (PDF)
SPARRA: using AI to predict emergency hospital admissions in Scotland (PDF)
The National Health Index: developing a reliable measure of the nation’s health (PDF)
The Turing Way for responsible research (PDF)
Working with Cefas to create cutting-edge software for surveying sea life (PDF)
In consultation with the programme’s External Advisory Board, the Management Board agreed in July 2020 to fund several new and strategic research initiatives to build on research outcomes from the first two years of the ASG programme and address important issues for the UK government and public sector made more urgent since the onset of the COVID-19 pandemic. Below provides a description of the new research initiatives and will be further detailed as the projects associated with them progress. 
Measuring the impact of policy interventions related to COVID-19 and building resilience against future shocks.
The COVID-19 crisis has highlighted how vulnerable societies and governments are to shocks. This sensitivity is largely due to the propensity to design policy for narrow siloes relating to sectors and government departments, without adequate consideration of the interdependencies between them or proper appreciation of the interconnected nature of local and global societies. It has become evident that resilience in one policy area (e.g. health) can come at the cost of resilience in another (e.g. the economy). The overall aim of this research is to develop a better understanding of resilience in interconnected health, social, and economic systems and to use this understanding to identify robust policy measures.
Find out more about shocks and resilience
Digital twins are now well established in a number of domains and are increasingly being linked into ‘ecosystems of digital twins’ (EDTs), but foundational challenges remain. We seek to develop new methods, tools and underpinning foundations to build well-defined EDTs which are spread across spatial and temporal scales, addressing specific use cases in engineering, health, commerce, economics, urban infrastructure and community modelling, as identified by relevant commercial and government stakeholders.
Find about more about ecosystems of digital twins
Environmental models: Bridging the spatial scales, from surface sensors to satellite sensors.
Satellite sensors can now provide an amazing level of detail of the Earth surface, yet with sparse and imperfect ground-truth sensors to validate them, and due to their relatively short record (a few decades) their usefulness when used on their own is somewhat limited. To make new leaps in understanding environmental change and to improve prediction we must find intelligent ways to combine satellite data with surface sensors and the output from physics-based environmental simulators (e.g., climate models). To bridge these spatial scales and various modalities we are creating a team of scientists and engineers to build and deploy toolkits driven by real-world case studies. This effort will create the foundations for building UK research capacity in developing Digital Twins of the natural environment.
The impact of climate change on agriculture.
Agriculture is highly vulnerable to climate change, with important socio-economic implications for food sustainability and greenhouse gas emissions. We can begin to model this by bringing together data from plant science, hydrology, soil science, insect population dynamics, economics, consumer behaviour and climate models. In this way we will contribute to the sustained development of a national crop modelling platform which will continue to develop at scale beyond the duration of this particular project.
Find out more about environment and sustainability
This theme is aligned with the urban analytics programme.
This theme is aligned with the data-centric engineering programme.
This theme is aligned with the health and medical sciences programme.
This theme is aligned with the public policy programme.
This theme is aligned with the data science for science and humanities programme.
Researchers at The Alan Turing Institute, Science Technology Facilities Council and their collaborators, have access to the PEARL computing service for research falling under this theme. Click here for more information.
This theme is aligned with the tools, practices and systems programme, and involves collaboration with the Research Engineering Group.
The ASG programme is governed by its Management Board which includes its Theme Leads as members as well as representatives from EPSRC, Delivery Partner of the investment, and independent members from the Turing’s University Partner Network. In April 2021, two new leadership roles within ASG were appointed on an initial 12 month interim basis: Professor Jonathan Rowe as Programme Chair, and Professor Ben MacArthur as Programme Director.
The ASG programme also has an External Advisory Board which offers strategic advice to the Management Board. Members were invited based on their independence from the ASG programme and their range of scientific, industry and government expertise. The Chair of the External Advisory Board is Dame Wendy Hall and Deputy Chair is Professor Tom Rodden.
"
https://www.turing.ac.uk/blog/emoji-are-even-more-language-previously-thought,"
It might come as a surprise but emoji, those increasingly ubiquitous icons representing people, animals, objects and emotions, are the subject of intense academic research. Early research compared emoji to their predecessor, the emoticon, looking at simple statistics such as frequency of usage. Today, researchers are building up a more complete understanding of the role emoji play in human communication, by studying the linguistic properties of emoji in comparison with languages like English. However, research often raises more questions than it answers. 
For example, we know that some emoji are very strongly considered to have a distinctly positive, neutral or negative meaning, just like words too, but there are some which people cannot agree on. Part of this is due to differences in how different platforms such as Android or iOS display emoji, but there is also evidence that who you are (e.g. your gender and age) has an influence on how you interpret the emoji you see as well as how you use emoji.
That the same emoji can have a different meaning depending on who writes it and who reads it, as well as the context it is used in, is a core property of human languages all over the world: polysemy. Examples in English are words like bank, which has (at least!) ten different meanings as a noun and eight as a verb. Semantics is the area of linguistics that examines the meaning of words and there are decades of research on semantic change. This looks at how words acquire or lose additional meanings, but also how one meaning can shift entirely, as in the case of cute which today means something like pretty but in the 18th century meant clever.
One of the difficulties in semantic research is the fact that language has been around a long time but the written record, let alone the digital one, is relatively short and often incomplete. This is where emoji differ - we know exactly when each one was introduced and platforms like Twitter offer academic access to large volumes of people using emoji, making it possible to study an emoji from its birth to the present day. Given that emoji exhibit language-like semantic properties, a natural follow-up is to ask whether emoji also undergo language-like semantic change.
In a new analysis of 1.7 billion tweets covering 2012 to 2018, researchers from the universities of Edinburgh, Essex, Utrecht, Cambridge, Oxford and The Alan Turing Institute measured semantic change in 348 emoji, using computational techniques, such as word embeddings and semantic similarity metrics, which have only previously been used to study more standard written language.
The semantics of most emoji (n=247) have changed relatively little over the years. They are used in similar contexts throughout their lifespan so far, a property they share with words - if the semantics of words changed constantly, it would make them poorly suited to communication! For emoji that did change in some way between 2012 and 2018, four characteristic patterns were detected using machine learning methods.

Some well-known changes in emoji meaning were identified in the results. For example, the frog emoji ? underwent a sudden but temporary change in 2014 when it became associated with right-wing memes and Donald Trump, while the skull emoji ? has gradually come to refer to figurative death, usually from embarrassment, as much as it means literal death. Less expected was the maple leaf emoji ? gradually changing to be used with words related to cannabis and autumn, going through seasonal cycles where it means one more than the other.
While computational methods and machine learning make it easier than ever to explore and understand enormous volumes of data, human expertise is necessary to account for the patterns found in individual emoji. It is hoped that drawing on the knowledge of emoji users all over the world will encourage exploration and discussion of emoji semantics, potentially leading to an understanding of what factors drive semantic change in emoji, which is an on-going avenue of academic research. To aid this, the data is available as an interactive dashboard where anyone can explore how much any emoji's semantics have changed, as well as which words it was most similar to each year and in individual months.
Full details of the study can be found in the paper, which will be presented at the 4th​ International Workshop on Emoji Understanding and Applications in Social Media, part of the 15th International AAAI Conference on Web and Social Media.
Cover photo: Kelvin Yan via Unsplash.
"
https://www.turing.ac.uk/people/external-researchers/miranda-cross,"
Miranda is interested in consumer financial data regulation and governance, specifically privacy and portability issues. She holds a BA in Public Policy from Vanderbilt University, and previously worked at the US Consumer Financial Protection Bureau.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/nottingham-trent-university,"
"
https://www.turing.ac.uk/people/researchers/sebastian-stein,"
Sebastian Stein is an Associate Professor in the School of Electronics and Computer Science at the University of Southampton. He currently holds a UKRI Turing AI Acceleration Fellowship on Citizen-Centric AI Systems. With a background in multi-agent systems, Sebastian is passionate about building AI systems that help solve urgent societal problems, for example in smart mobility, smart energy and disaster response applications. This requires algorithms that consider the preferences and incentives of individuals, and that can be understood and trusted by non-expert citizen users.
"
https://www.turing.ac.uk/sessions/the-future-of-cities/,"


"
https://www.turing.ac.uk/people/efthymia-tsamoura,"
Efi Tsamoura received her BSc in 2007 and her PhD in 2013 with honours both from the Computer Science Department of Aristotle University of Thessaloniki, Greece. Since June 2013 she is a postdoctoral researcher in the Computer Science Department of University of Oxford, working on problems related to querying semantically interrelated resources. Her research interests lie in the fields of data integration, query answering under constraints, distributed query optimisation, multi-objective query optimisation and grid computing. Efi received several distinctions and awards both during her undergraduate and postgraduate studies.  She currently works at Samsung AI Centre.
Efi is one of the co-organisers of the Neuro-symbolic AI interest Group.
"
https://www.turing.ac.uk/people/researchers/mark-freestone,"
Mark Freestone is Senior Lecturer in the Centre for Psychiatry, Wolfson Institute for Preventive Medicine, Queen Mary University of London. He has a first class honors degree in Social and Political Sciences fromt the University of Cambridge, and gained his PhD in Applied Social Science from the University of Nottingham in 2005. His core research interests include: psychosocial treatment of mental health, particularly personality disorder; forensic mental health; the epidemiology of violence; clinical presentation and sub-types of psychopathy; and criminological research relating to prisons and prisoners.
Dr Freestone's work with the Turing will follow three work packages:
"
https://www.turing.ac.uk/people/researchers/anjali-mazumder,"
Anjali Mazumder is the Theme Lead on AI and Justice & Human Rights. Her work focuses on empowering government and non-profit organisations by co-designing and developing responsible and inclusive data and AI methods, tools and frameworks for safeguarding people from harm – particularly those most vulnerable, building resilient institutions and systems, and accelerating the opportunity for inclusive, fair and just services, systems, economies, and communities. She is passionate about fostering multi-disciplinary collaborations and multi-sector partnerships to co-create pathways for innovation that improves services, policy, and actions to safeguard human rights and address humanitarian challenges. Her research interests are in developing integrated Bayesian decision support systems to manage uncertainty with complex data structures, value of evidence, causal reasoning in the wild; expert judgement; detecting bias and algorithmic fairness; socio-technical solutions to harnessing multiple disparate sources of data whilst enabling responsible and inclusive data and AI principles and practices; communicating uncertainty and risk; and safeguarding rights and the Rule of Law.
She has over 15 years’ experience tackling fundamental statistical problems of societal importance – human rights, justice, security, the Law, education, public health & safety – working at the interface of research, policy and practice in the UK, the US, and Canada, fostering multi-disciplinary and cross-sector collaborations. She was appointed to Canada’s National DNA Databank Advisory Committee (2012-2018) and currently serves on the UK Forensic Science Regulator’s fingerprint interpretation subgroup, and the senior management board of the UK’s Policy and Evidence Centre for Modern Slavery and Human Rights. She has also served the Royal Statistical Society in a variety of ways, most recently appointed to the Statistics & Law Section and the Data Science Section committees. She holds a doctorate in Statistics from the University of Oxford and two masters’ degrees in Measurement and Evaluation, and Statistics from the University of Toronto.
"
https://www.turing.ac.uk/research/research-engineering,"

Researchers and practitioners face a common need for high quality tools, practices, methodologies, platforms and systems.
Many domains can benefit from the deployment of cutting-edge algorithms and approaches, but these cannot be effectively applied unless realised as usable software libraries, reproducible analyses and workflows, or high performance computational environments.
The Research Engineering Group (REG) contributes skills in research software engineering and data science in support of other programmes, as well as to its own projects. This model of working ensures that the tools they develop are useful and applicable to a wider range of areas. The team supports professional delivery of impactful research across the Turing's programmes, as well as its own research interests.
The team concentrate mainly on:
To keep up to date with the team, follow us on twitter at @turinghut23 or LinkedIn
Header image: Close up of a Bletchley Park Bombe, designed by Alan Turing and his colleagues to decipher German Enigma machine messages in WW2. Source: mendhak, Flickr, Creative Commons.

 
Read about some of our project highlights. See here for a complete list. 
Research Engineering is involved in many projects across the Turing, collaborating both with academics and strategic commercial partners.
Below is the list of our most recent projects, the full list can be found here.
For a full list of blogs and news associated with REG see here.
The most recent events involving REG are shown below. For a complete list see here. 
Below is the list of our most recent publications, the full list can be found here. 
"
https://www.turing.ac.uk/research/interest-groups/data-science-and-digital-humanities,"
Thanks to digitisation efforts over the past decades, humanities scholars now have access to large digital data collections which can be analysed computationally and quantitatively. Such datasets offer a huge range of opportunities to develop new approaches to answer high-profile research questions.
The main aims of the group are to strengthen relationships and build collaborations at the intersection between data science and digital humanities.
Our goal is to raise the profile of data-driven humanities research at the Turing, open up future collaborations, and strengthen the Turing’s links with organisations such as the British Library, the National Records of Scotland and The National Archives in the UK.
The group will show the key role that can be played by The Alan Turing Institute in the area of digital humanities by demonstrating that data science research can answer questions relevant to the humanities and vice versa, thus benefiting both fields. This will be achieved with meetings, workshops, and joint research projects.
Translating fundamental research in data science into lasting impact in the humanities requires interdisciplinary efforts, through the sharing of perspectives, methods and knowledge. The interest group builds on the organisers’ extensive experience in interdisciplinary research on historical data and brings together people from a range of different disciplines.

 
The aim of this sub-group is to stimulate discussions around new topics, publications, events and debates at the many intersections between humanities and data science. We are looking forward to establishing an open and welcoming venue for exchanging opinions across disciplines, research interests and methodologies. 
We rely on the broad range of expertise of group members to set the topic of each meeting and to start the discussion by offering a variety of points of view. We welcome anyone at Turing interested in taking part in the discussion or simply in following the debate while enjoying a cup of tea.
Chair: Federico Nanni (Turing)
This group is composed of members who are involved in delivering and supporting digital humanities teaching and training whether through standalone courses and degree programs or as part of a broader research skills training program across different degrees. An increasing number of institutions are offering teaching provision in the digital humanities, though we recognise that this has reached differing levels of maturity in different locations.
The aim of the sub-group is to provide a forum for the exchange and discussion of the practical and theoretical challenges including institutional barriers related to DH teaching. In particular, we are interested in exploring the following:
We draw on group members' varied experiences of supporting teaching at their institutions to identify successful strategies. We propose regular face-to-face events for colleagues to discuss and debate these issues and promote broader networking.
Chair: Anne Alexander (Cambridge)
Members: Bea Alex (Edinburgh), Anne Alexander (Cambridge), Giovanni Colavizza (Turing), James Cummings (Newcastle), Rachele De Felice (UCL), Fiona Douglas (Leeds), Leif Isaksen (Exeter), Ewan Jones (Cambridge), Anouk Lang (Edinburgh), Nora McGregor (BL), Thierry Poibeau (ENS), Pip Willcox (National Archives)
The aim of this working group to explore opportunities to submit a DH-themed ""challenge"" for a Turing Data Study Group. 
Chair: Barbara McGillivray 
Members: Adam Farquhar (British Library), Anne Alexander (Cambridge), David Beavan (Turing), Eirini Goudarouli (The National Archives), James Freeman (Bristol), Lise Jaillant (Loughborough), Mark Bell (The National Archives), Matthew Grenby (Newcastle), Mia Ridge (British Library), Nick Holliman (Newcastle), Stephen Roberts (History of Parliament). 
This working group aims to write a white paper with recommendations for how to support research at the intersection between data science and humanities. The white paper is available:
McGillivray, Barbara et al. (2020). The challenges and prospects of the intersection of humanities and data science: A white paper from The Alan Turing Institute. Figshare. dx.doi.org/10.6084/m9.figshare.12732164
Chair: Barbara McGillivray
Members: Beatrice Alex (Edinburgh), Sarah Ames (National Library of Scotland), Guyda Armstrong (Manchester), David Beavan (Turing), Arianna Ciula (King’s), Giovanni Colavizza (Amsterdam), James Cummings (Newcastle), David De Roure (Oxford), Adam Farquhar, Simon Hengchen (Gothenburg), Anouk Lang (Edinburgh), James Loxley (Edinburgh), Eirini Goudarouli (The National Archives, UK), Federico Nanni (Turing), Andrea Nini (Manchester), Julianne Nyhan (UCL), Nicola Osborne (Edinburgh), Thierry Poibeau (CNRS), Mia Ridge (British Library), Sonia Ranade (The National Archives, UK), James Smithies (King’s), Melissa Terras (Edinburgh), Andreas Vlachidis (UCL), Pip Willcox (The National Archives, UK)
There are two main ways to be involved in the group: members and affiliates.
Members. To ensure continuity across the group, we aim to keep the number of members from each Turing partner institution to two and the number of members from other institutions to one. We understand that circumstances can change, so we ask members to confirm their interest at the beginning of every calendar year. The group organises member meetings every second Thursday of the month, from 11 to 12:30, remotely via Zoom. These meetings are an opportunity to discuss group-internal matters and to decide on the focus areas for the group. We ask members to attend at least half of the group meetings in a year, to volunteer to chair some of the meetings and to follow up on specific actions arising from the discussion. If your institution already has two members (in the case of a Turing partner) or one (in the case of all other institutions), please contact them (see list below). If your institution doesn’t yet have the maximum number of members and you have enough time and commitment, you are welcome to join the group as a member by contacting the group convenors ([email protected]).
We keep affiliates updated on the main activities of the group via our mailing list, which anyone is welcome to join. Affiliates are also welcome to join one of the group’s working groups (see above for more information) by contacting the respective chair. If you can’t join as a member but you’re interested in joining as an affiliate, please contact the group convenors ([email protected]).
 
The group is involved in the research network Cyclades (Corpora and Computational Linguistics for Digital Humanities); an international research network at the intersection of Natural Language Processing (NLP) and Digital Humanities (DH). The network is made of 8 partners from 4 different counties (Lattice, the SciencesPo médialab and the French national Library – BnF for France; the University of Cambridge, the Turing and the British Library for the UK; the Göttingen Centre for Digital Humanities for Germany and the Stanford Literary Lab for the USA. The goal of the network is to develop new areas of research (and more specifically new techniques for text analysis), to adapt tools to new problems and to evaluate the solutions proposed in an original way, both qualitative and quantitative.
Turing H&DS were partners in the ""Towards Large Scale Cultural Analytics in the Arts and Humanities” AHRC funded project [AH/W007533/1], as part of the Scoping Future Arts & Humanities Led Research programme (also known as AHRC iDAH).”
Please subscribe to our mailing list for updates.
If you are interested in the group, please contact the convenors at the following address: humanitiesanddatascienceconvenors AT turing DOT ac DOT uk
David Beavan, the TuringBarbara McGillivray, King's College LondonFederico Nanni, the TuringAndreas Vlachidis, UCL
 
Charlotte Tupman, University of ExeterClaire Reddleman, University of ManchesterDimitris Mylonas,  New College of the HumanitiesEirini Goudarouli, The National ArchivesFraser Sturt, University of SouthamptonJames Cummings, Newcastle UniversityJames Freeman, University of BristolJoanna Taylor, University of ManchesterJohn Stell, University of LeedsJulianne Nyhan, UCLLeif Isaksen, University of ExeterLorna Hughes, University of GlasgowLuca Scholz, University of ManchesterMegan Gooch, University of OxfordNicola Osborne, University of EdinburghPip Willcox, The National ArchivesRachel Saunders, University of NottinghamSarah Ames, National Library of ScotlandThomas Irvine, University of SouthamptonTiago Sousa Garcia, Newcastle University
 
Andrea Nini, University of ManchesterAnne Alexander, University of CambridgeAlan Blackwell, University of CambridgeAlbert Meroño Peñuela, King's College LondonArianna Ciula, King's College LondonAnouk Lang, University of EdinburghBrett Greatley-Hirsch, University of LeedsEwan Jones, University of CambridgeGabriele Salciute Civiliene, King's College LondonGenevieve Liveley, University of BristolGuyda Armstrong, University of ManchesterIacopo Ghinassi, Queen Mary University of LondonJack Pink, University of SouthamptonJames Loxley, University of EdinburghLuca Marinelli, Queen Mary University of LondonMaria Liakata, University of Warwick and The Alan Turing InstituteMatthew O. Grenby, Newcastle UniversityMiguel Rodrigues, UCL and The Alan Turing InstituteRachele De Felice, UCLRebecca Roach, University of BirminghamRichard Ward, University of ExeterSimon Hosie, Food Standards AgencyStephen Roberts, History of ParliamentThierry Poibeau, CNRS and University of CambridgeTim EvansImperial College LondonTom van Nuenen, King's College London
62 KB
What is the role of the Arts and Humanities in the age of Data Science?
Wednesday 5 September 2018, 18:00-20:30 BST, Edinburgh, UK
Find out more
"
https://www.turing.ac.uk/research/research-projects/double-trouble-gene-regulation-polyploids,"
Many crop species have multiple copies of their genome, and each copy of the gene can develop mutations that lead it to be expressed in different tissues and developmental stages. This project is developing machine learning tools to predict where and when each copy of the gene is expressed. This will allow the researchers to transfer experimentally-validated models from well-studied species with simple genomes (such as Arabidopsis or 'thale cress') into important crop species with multiple copies of their genomes (such as broccoli, cabbage or cauliflower).  
This project is using a mixture of supervised and unsupervised learning techniques. The tricky bit comes from the fact that there are multiple gene copies that have very similar regulatory sequences, but very different expression patterns. Another challenge comes from balancing the need to develop interpretable models with the need to make very predictive models.
The aims of the project are to:
This work is important for agriculturally-relevant plants, such as cabbage, broccoli, cauliflower, turnips, and Brussels sprouts, which are part of the Brassica family. Brassicas are closely related to the model plant species Arabidopsis or 'thale cress', which is well-studied.
In Arabidopsis, we know the function of many genes which might be good targets for directed breeding programs to increase yield or resilience to environmental stress. However, Brassicas have many copies of each gene, so it is unclear which of these gene copies should be the primary target of directed breeding initiatives. If we can predict when and where each gene copy is expressed, we can identify which gene copy is more likely to be relevant for breeders.
For instance, lets say we know a gene in Arabidopsis that helps control how much water is released from leaves, affecting the ability of the plant to withstand drought. A crop might have four copies of this gene, but it may be that only one copy is expressed in adult leaf cells; that specific gene copy might be the most promising target for breeders.  
[email protected]
"
https://www.turing.ac.uk/research/research-projects/design-change-digital-twins,"
Engineers and managers find it difficult to understand the impacts of late design changes in complex systems, where changes in one sub-system may have implications for other sub-systems. Digital twins offer new opportunities, and research is starting to develop new decision-support methods, identifying key connections from design product and process data. This research will develop new methods to realise the benefit of the digital twin in data-driven design by providing engineering designers with leading indicators, helping them to trace and handle interdependencies and uncertainties in design.
This work combines both data science and systems engineering to develop new methods for engineering design. Multidisciplinary design data based on criteria (e.g. energy consumption, cost and constructability) is analysed to improve collaborative design processes (in both early design and mature design). The project builds on new trajectories of research on: a) matrices to identify and track interdependencies and change propagation in the design of complex systems; b) techniques for automating the generation and interpretation of these matrices; and c) Building Information Modelling (BIM) techniques. 
The first step in the research is to review existing approaches to automating generation of the Design Structure Matrix (DSM) from Building Information Modelling (BIM) data. The second step will then pilot and test novel approaches that extend this work to identify first and second order interdependencies. It is anticipated that this will use product and process generated in design and will extend the functionality available in commercial tools. It is anticipated that this research will require the development of new techniques for model integration, multi-criteria mapping and sensitivity analyses. The third step is to conduct global sensitivity analyses (GSA) to understand the uncertainties of outcomes associated with design changes. The fourth step involves simulating the performance of project outcomes to predict, and improve it across multiple aspects. In the fifth step, appropriate methods for developing optimal/near-optimal design solutions will be recommended from the analyses. 
This work is informed by configuration management approaches, which are important in design change in complex systems as they seek to maintain the integrity of multidisciplinary design; ensuring consistency across requirements, digital information and physical assets. 
 
The research aims to provide decision-support methods for designers evaluating late design changes by developing methods to identify interdependencies from the digital twin and analysing and visualising the associated uncertainty. Objectives are to:
The deliverables are beneficial to both academia and industry in understanding and addressing the interdependencies and uncertainties of design in complex systems, improving the efficiency and reliability of design decision-making. The proposed methods will enable the project stakeholders to complete collaborative design work within uncertain and complex engineering design contexts and generate reliable and robust design solutions.
A measure of success is that the project delivers both the tools and approaches for data-driven design change in complex systems; and it contributes to dealing with systems interdependencies and uncertainties of design decisions, and improving efficiency and reliability of design decision-making in practice. The contributions can be evidenced by publishable work in international peer-reviewed journals and the validation using real world data.
This project is part of the Data-centric engineering programme's Grand Challenge of 'Data-driven engineering design under uncertainty'.
The deliverables of research can be applied to design decision-making of infrastructure projects, and extended to other complex systems of which the complexity and interdependencies are growing faster than the design engineer's ability to address them. 
Both academia and industry (especially the construction industry) involved in design decision-making can benefit from the proposed tools and approaches, delivering reliable and robust design solutions. 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-glasgow,"
"
https://www.turing.ac.uk/people/researchers/kristian-lehre,"
Dr Lehre is Professor in the School of Computer Science at the University of Birmingham. Before joining Birmingham, he was since 2011 Assistant Professor with the University of Nottingham. He is a Turing AI Acceleration Fellow with the project ""Rigorous Time-Complexity Analysis of co-Evolutionary Algorithms"". He coordinated the 2M euro EU-funded project SAGE which brought together theory of evolutionary computation and population genetics.
He is member of the editorial board of Evolutionary Computation and associate editor of IEEE Transactions of Evolutionary Computation. Dr Lehre has guest-edited special issues of Theoretical Computer Science and IEEE Transaction on Evolutionary Computation on theoretical foundations of evolutionary computation.
"
https://www.turing.ac.uk/people/external-researchers/katy-warr,"
"
https://www.turing.ac.uk/people/guest-speakers/jian-dai,"
Jian is an Expert Data Scientist at Genentech. His current role is the R&D Lead of the AI-based Ophthalmology Imaging Products in the Personalized Healthcare (PHC) Department of Roche Pharma. He got his PhD in Theoretical High Energy Physics from Peking University, and has 14 years industry experience in Medical Device, Pharma & Biotech
"
https://www.turing.ac.uk/people/guest-speakers/rebecca-anselmetti,"
Rebecca Anselmetti is a Senior Policy Advisor in the UK government's Department for Digital, Culture, Media and Sport (DCMS), where she leads the government's work on AI standards policy and the delivery of the objectives set out in the National AI Strategy. This includes the government's pilot of the AI Standards Hub, alongside the Alan Turing Institute, the British Standards Institution (BSI) and the National Physical Laboratory (NPL). Rebecca is also representing the UK in international standards bodies, including ETSI OCG AI and BSI ART/1. In her role, she is also responsible for contributing to the development of pro-innovation AI governance and regulation. She holds an MPhil in Technology Policy from the University of Cambridge, and a First-class honours BA in Digital Culture from King's College London. 
"
https://www.turing.ac.uk/research/research-projects/nocell-probabilistic-programming-spreadsheet-experts,"
Despite advances in machine learning and probabilistic methods and tools, the majority of analysts working in government and industry use spreadsheets (such as Excel) for their day-to-day work and hence do not have these advances readily available. Most analysts, broadly construed, do not use machine learning ideas.
The reasons for the widespread use of spreadsheets are clear. Models are built for one-off tasks but become established models; they are a convenient vehicle for communication; and they often form the glue between operational systems. Often a spreadsheet is the only tool available to an analyst working in a particular domain.
In addition, there are significant, well-known problems that arise when spreadsheets are one’s only tool. Two of the most powerful tools in the programmer’s repertoire, abstraction and modularity, are very difficult to implement in spreadsheets, resulting in repetitive, ‘low-level’ code. Lack of version control makes it difficult to trace back the changes to a model. Re-use of parts of existing spreadsheets in new models is essentially impossible and updates to one's models are error-prone and time-consuming.
In addition, and critically for this project, use of traditional spreadsheets means that the analyst cannot easily compute with uncertainty, and is locked out of some of the most promising advances in probabilistic reasoning.
This project's long term aim is to develop a simple, high-level programming language for creating the sort of models that are typically built in a spreadsheet in industry and government. The language will be extended with the capability for probabilistic modelling and inference. Models built using this language will then be ‘compiled’ to a well-structured spreadsheet, complete with formatting.
The hope is to bring the advantages of software re-use, version control, abstraction, and machine learning to real-world modelling.
[email protected]
"
https://www.turing.ac.uk/research/research-programmes/finance-and-economics/economic-data-science,"
Economic data science is about decoding and managing the digital transformation. The interplay between new technologies and human behaviour, both at the individual and the collective level, produces complex feedback loops that offer tremendous socio-economic opportunities but at the same time expose society to unprecedented potential risks.
New kinds of data, behavioural and ecological in the broadest sense, allow us to measure economic activity – outputs, employment, wages, spending, mobility, trade – faster, more precisely and at a more dis-aggregated level than it has previously been possible. This means that individual, business and government decisions can be based on a much more complete and up-to-date picture of what is going on in the economy.
New data also allow us to tackle the full complexity of the socio-technical systems we create and inhabit, and require new methods to support traditional approaches. The interdisciplinary approach in this theme combines machine learning and AI with concepts and tools developed in complex systems science and network science.
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-digital-health,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of Digital Health. We'll be exploring how the increasing amount of collected 'footprint' data can be used to develop healthcare research and products and considerations around this. 
We'll be hearing from Elizabeth Dolan, PhD Student at University of Nottingham’s Digital Economy Horizon Centre of Doctoral Training and N/LAB research centre for the use of big data and machine learning, Jian Dai, Expert Data Scientist and Ophthalmology Imaging R&D Lead, Data Science Capability Lead and Yajing Zhu, Principal Data Scientist and Neuroscience Network Lead, both at Roche.                                               
Elizabeth will be discussing AI in population health surveillance through digital footprint data and the Turing Interest Group Novel Data Linkages for Health and Wellbeing.
Jian and Yajing will be giving an overview of the Data, Analytics, and Imaging Data Science Capabilities in the personalized healthcare department at Roche.
The event will conclude with a Q&A, we also hope to continue discussions pre and post the event via our Turing-Roche Slack Workspace which you can join.
You can watch a recording of this event here.

"
https://www.turing.ac.uk/people/guest-speakers/miguel-bernabeu,"
Miguel is Senior Lecturer at the University of Edinburgh and his research is on vascular structure and function. His approach is primarily theoretical through mathematical modelling and machine learning in close collaboration with vascular/cancer biologists and clinicians.
His research interests concern: a) the development of automated methods for eye and systemic disease diagnosis in retinal scans, b) the study of the tumour microvascular environment and its impact on treatment, c) the mechanistic investigation of vascular remodelling during angiogenesis.
"
https://www.turing.ac.uk/people/enrichment-students/abdulrahman-abdullatif-alsayed,"
Abdulrahman holds his degree in general linguistics from Britain’s oldest university the University of Oxford where he gained interest in studying natural language syntax, information structure, and corpus linguistics. He worked as a lecturer in King Faisal University, KSA before returning to the UK to commence doctorate research at Scotland’s oldest university the University of St Andrews. In addition to his research work, Abdulrahman's work while in St Andrews includes teaching at the School of Modern Languages, coordinating training for the Centre for Educational Enhancement and Development (CEED), and serving as the Linguistics Catalyst representative within the Doctoral Researcher Committee of the Scottish Graduate School for Arts & Humanities (SGSAH).
In his most recent research project conducted at the University of St Andrews’ School of Modern Languages, Abdulrahman has created a pilot for the first annotated corpus of spoken language mediated by extended reality technology (XR). As part of the next stage of his study, he is currently working on expanding this XR linguistic corpus to document the language used by multiple speakers placed within replicable virtual environments. Analysis and modelling the information structure and the context-sensitive syntactic constructions used in the corpus will follow upon completion of this expansion. This work is expected to provide a data-driven extension to theories concerned with the interaction between syntax and information structure in addition to presenting novel experimental tools for the empirical analysis of general linguistic phenomena. Further expected applications of this work extend to inter-disciplinary research related to communication in virtual worlds, Natural Language Generation, and Artificial Intelligence. Abdulrahman welcomes all enquiries and collaboration invites from researchers interested in inter-disciplinary work related to his research.
"
https://www.turing.ac.uk/people/researchers/felix-anselm-van-lier,"
Felix-Anselm van Lier is a public law scholar focusing on constitution-making processes and a Research Fellow at the Max Planck Institute for Social Anthropology. He holds degrees in law and anthropology from the universities of Bremen, Turin and the London School of Economics and Political Science. He completed his doctorate at the University of Oxford in 2018. Amongst others, he has worked as a consultant for the Bingham Centre for the Rule of Law, Democracy Reporting International, the Berghof Foundation, and Lawyers for Justice in Libya.
Felix current work focuses on finding productive ways of harnessing the democratic and deliberative potential of digital technologies to redefine the relationship between state institutions and citizens. He is particularly interested in the impact of such technology on public participation in constitution making and on the institutional and procedural frameworks through which constitutions are created.
"
https://www.turing.ac.uk/news/private-information-retrieval-sheep-homomorphic-encryption-evaluation-platform,"
In this post we describe the SHEEP project and platform by means of an example. SHEEP is a recursive acronym for “SHEEP is a Homomorphic Encryption Evaluation Platform,” and is under development by researchers at the Turing Institute.
Consider the following problem: I want to host a database remotely, but the data within it are sensitive and must be encrypted. In addition, I want clients who access the database not to reveal which records were accessed. This last requirement especially sounds impossible, short of transmitting the entire database. This is in fact a classical problem in cryptography known as Private Information Retrieval (PIR) and there are several ways to solve it. It turns out that we can construct a protocol for private information retrieval based on a technology known as Homomorphic Encryption.
Homomorphic Encryption (HE) allows us to compute on encrypted data without decrypting it. Let's give an example. You can encrypt the number 23 and obtain an encryption ? ? ?, and encrypt 19 and obtain an encryption ? ? ?. Homomorphic encrytion schemes then provide a way to add the two encrypted values to obtain a new encryption, say ? ? ? + ? ? ? = ? ? ?, that when decrypted equals 23 + 19 = 42. Crucially, whoever operated on the encrypted values (the fruit) did not have to be able to decrypt them or otherwise make sense of them in order to perform the addition correctly.
This enables secure outsourced computations, where Alice can encrypt her data as ? ? ? ? ? ? ?, upload to a private computing platform (let's call it BlindFruit™), and obtain the result of a computation again encrypted. This differs from existing cloud computing frameworks in that BlindFruit™ never learnt anything about Alice's data despite computing on it. This is as awesome as it sounds. Of course, in practice it is also way more costly than computing in the clear. Helping to easily determine how much more costly is one of the goals of the SHEEP project.
In the latest HE schemes, ciphertexts are not strings of fruit but polynomials, but we don't have cute emojis for those...
We will use the property of Homomorphic Encryption to compute on private data to construct a Private Information Retrieval protocol. Schematically, our PIR protocol with Homomorphic Encryption will look something like this:

At the Alan Turing Institute we have been experimenting with Homomorphic Encryption for almost a year now and contributing to its more practical engineering aspects. The outcome is SHEEP: a platform for practitioners to evaluate the state-of-the-art of (fully) homomorphic encryption technology in the context of their own concrete application.
Encryption schemes with homomorphic properties have been around for a while. Some schemes are additive homomorphic (meaning we can operate on encrypted values to obtain the encryption of their sum, e.g. Paillier), and others are multiplicative homomorphic. Some encryption schemes are both additive and multiplicative, and we call those Fully Homomorphic Encryption (FHE) schemes. Before FHE schemes were known, several Somewhat Homomorphic Encryption schemes were described, which can support only a limited fixed number of multiplications (see this presentation and article). SHEEP supports a number of fully homomorphic encryption libraries (currently HElib, SEAL and TFHE) as well as libpallier.
Returning to the Private Information Retrieval problem, the version of PIR in SHEEP's native benchmarks works like this: a server holds a dataset $d$ of $n$ integers encrypted under the client's key. For the server to select an element obliviously, the client sends an encrypted selection binary vector $s$ of length $n$ containing $0$s everywhere except for a $1$ in the position $i$ of the element to be selected. The server then computes the dot product of $d$ and $s$ using the homomorphism, with the encrypted result $r = \sum_{j = 0}^n d_j\cdot s_j$ and sends it to the client, who can decrypt it.
Note that this protocol requires an amount of communication linear in the size of the database (since the selection vector must have as many elements as the database has records). To achieve sublinear communication at the cost of additional computation on the server one can use a nice trick, which we will describe a little later.
We can express this simple dot-product protocol for a database of a specific size (here, eight records) in an assembly-like language which can be run by the SHEEP platform:
INPUTS d_0 d_1 d_2 d_3 d_4 d_5 d_6 d_7 s_0 s_1 s_2 s_3 s_4 s_5 s_6 s_7
OUTPUTS r
s_0 d_0 MULTIPLY c_0
s_1 d_1 MULTIPLY c_1
s_2 d_2 MULTIPLY c_2
s_3 d_3 MULTIPLY c_3
s_4 d_4 MULTIPLY c_4
s_5 d_5 MULTIPLY c_5
s_6 d_6 MULTIPLY c_6
s_7 d_7 MULTIPLY c_7
c_0 c_1 ADD p_1
p_1 c_2 ADD p_2
p_2 c_3 ADD p_3
p_3 c_4 ADD p_4
p_4 c_5 ADD p_5
p_5 c_6 ADD p_6
p_6 c_7 ADD r
This results in a single value (r) being returned that holds the selected element from the database.
The meaning of this program should be fairly self-explanatory: after a list of the inputs and outputs of the program are a list of statements of the form a1 a2 ... aN OP b, which defines b to be the result of evaluating the operation OP with a1 a2 ... aN as arguments. Variables are required to be defined before they are used and never redefined (requirements that mean it is in SSA form). One usually wants to generate such code programatically.
First, the server organizes the database $d$ into an $\alpha$-dimensional $\lambda_1 \times \cdots \times \lambda_\alpha$ hyperrectangle $d'$. Think of $d'$ as the result of folding the database $d$ repeatedly $\alpha$ times, where the $j$th folding arranges the database into $\lambda_j$ parts of equal size. In this way, every entry in the database $d$ is addressed in $d'$ by an array $q$ of length $\alpha$, where $q_i\in\{0, \ldots, \lambda_i - 1\}$. Note that if $\lambda_1 = \cdots = \lambda_\alpha = 2$ then $q$ is a binary array of length $\log(|d|)$.
Hence, a query $q = (q_1, \ldots, q_\alpha)$ on the database $d$ can be recursively answered as a query $q_{rec} = (q_2, \ldots, q_\alpha)$ on a database $d_{rec}$ defined as follows:
$$d_{rec} = \left(\sum_{i=0}^{\lambda_1} d_{i\cdot c + j}\cdot s_i\right)_{j\in\{1, \ldots, c\}}$$
Here, the selection vector $s\in\{0,1\}^{\lambda_1}$ is the one-hot encoding of the number $q_1$, i.e. a binary array of length $\lambda_1$ with $0$s everywhere except for the $q_1$th position, and $c = \frac{|d|}{\lambda_0}$. In the base case, i.e. when $|q| = 0$, the answer to the query is simply the whole database.
The general idea behind the recursive approach to PIR is to have the client encode its query $q$ as selection vectors $s_1, \ldots, s_\alpha$: one-hot encodings of $q_1, \ldots, q_\alpha$. The client sends the $s_i$s encrypted using an encryption scheme $E$ for which only the client knows the private key. Then, the server runs the whole recursive evaluation described in the equation above using the encrypted $s_i$s, and returns the encrypted answer to the query to the client, who can decrypt it. This approach gives sublinear communication, and in fact logarithmic communication in the case where $\lambda_1 = \cdots = \lambda_\alpha = 2$.
Finally, note that this approach imposes some constraints on $E$. In particular, an encryption scheme supporting homomorphic addition and $\alpha$ homomorphic multiplications is required.
As an example, we show the SHEEP code for the case $|d| = 8, \vec{\lambda} = (2, 2, 2)$:
INPUTS d_0 d_1 d_2 d_3 d_4 d_5 d_6 d_7 s_00 s_01 s_10 s_11 s_20 s_21
OUTPUTS r
s_00 d_0 MULTIPLY c_00
s_00 d_1 MULTIPLY c_01
s_00 d_2 MULTIPLY c_02
s_00 d_3 MULTIPLY c_03
s_01 d_4 MULTIPLY c_04
s_01 d_5 MULTIPLY c_05
s_01 d_6 MULTIPLY c_06
s_01 d_7 MULTIPLY c_07
c_00 c_04 ADD e_00
c_01 c_05 ADD e_01
c_02 c_06 ADD e_02
c_03 c_07 ADD e_03
s_10 e_00 MULTIPLY c_10
s_10 e_01 MULTIPLY c_11
s_11 e_02 MULTIPLY c_12
s_11 e_03 MULTIPLY c_13
c_10 c_12 ADD e_10
c_11 c_13 ADD e_11
s_20 e_10 MULTIPLY c_20
s_21 e_11 MULTIPLY c_21
c_20 c_21 ADD r
where as before d is the database and s is the selection vector and the result is returned in r. Note that there are now only six selection elements (the ss) instead of eight in the simpler scheme, although there are more arithmetic operations.
In this article we have given some simple examples of using the SHEEP language to describe privacy-preserving computation. In the next article in this series, we will describe the various components of the SHEEP platform and how they fit together, and how they can be used to evaluate several built-in benchmarks.
This work was supported by the Defence and Security, and the Data Science at Scale research programmes.
Cover photo by Sam Carter on Unsplash.
 
"
https://www.turing.ac.uk/news/alan-turing-institute-work-finnish-centre-artificial-intelligence-data-science-research,"
The Alan Turing Institute and The Finnish Centre for Artificial Intelligence (FCAI) have signed a memorandum of understanding (MOU), formally creating an ambitious agreement centred around the Turing’s data-centric engineering programme, a major research programme funded by the Lloyd's Register Foundation.
The FCAI – which conducts fundamental research on AI in cooperation with businesses and public sector organisations - is funded by the Academy of Finland. The MOU will enable both institutions to embark on shared research and translation projects. This will include the development of AI methods to improve the diagnosis of Diabetic Retinopathy - a project which is establishing one of the largest data collections of retinal images and optical coherence tomography (OCT) scans in the world. Diabetic retinopathy is a complication of diabetes caused by high blood sugar levels damaging the back of the eye (retina). It can cause blindness if left undiagnosed and untreated. 
Adrian Smith, Institute Director, The Alan Turing Institute, said: “This is a significant international collaboration and I am delighted the Turing is now formally linked to one of the most dynamic research institutions in Europe. Together, we share a common goal of shaping the world we live in for the better and this collaboration will enable us to combine world-class expertise and apply data science and AI approaches to tackle real world problems.”
Prof M. A. Girolami the Turing’s Director of Data Centric Engineering programme (and Sir Kirby Laing Professor of Civil Engineering at the University of Cambridge and the Lloyds Register Foundation-Royal Academy of Engineering Research Chair in Data Centric Engineering) has been appointed as Adjunct Professor of Machine Learning at Aalto University, which will help develop the partnership.  
In addition, Professor Kimmo Kaski (former Academy Professor and Dean of the School of Science at Aalto University) is a Turing Rutherford Fellow and will continue to work as the Turing-FCAI Liaison Director. Professor Kimmo Kaski said: “I am excited about this strategic partnership between FCAI and The Alan Turing Institute – the world’s foremost data science and AI research set-up, as it gives us the opportunity to jointly further Turing’s unique and world-changing legacy in finding solutions to challenging problems around us by applying data science and AI to the most valuable resource, data, for common good and better services to us all.”
A number of other projects are currently being developed between FCAI and the Turing. Professor Samuel Kaski, director of FCAI said “We are looking forward to continuing the already existing collaboration with a number of Turing partners, and working on the new initiatives we identified based on our complementary strengths.”
 
Further Information About FCAI
FCAI is a nationwide competence Centre for Artificial Intelligence in Finland, initiated by Aalto University, University of Helsinki, and VTT Technical Research Centre of Finland. It aims to create Real AI for Real People in the Real World — a new type of AI, which can operate with humans in the complex world - and to renew the Finnish industry with the new AI - http://fcai.fi
 
Media Enquiries
[email protected]   
 
 
"
https://www.turing.ac.uk/events/dangers-stochastic-parrots,"
Professor Emily M. Bender will present her recent (co-authored) paper On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜
In this paper, Bender and her co-authors take stock of the recent trend towards ever larger language models (especially for English), which the field of natural language processing has been using to extend the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks. In the paper, they take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks?
The presentation will be followed by a panel discussion.
Time: 16:00-17:15 BST / 8:00-9:15 PDT
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? - Emily M. Bender, Timnit Gebru, Angelina McMillan-Major and Shmargaret Shmitchell
Ideal Words: A Vector-Based Formalisation of Semantic Competence - Aurélie Herbelot and Ann Copestake 
Alignment of Language Agents - Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik and Geoffrey Irving
Improving Language Model Behavior by Training on a Curated Dataset - Irene Solaiman and Christy Dennison

"
https://www.turing.ac.uk/people/external-researchers/jonathan-davies,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/university-college-london-hospitals,"
www.uclh.nhs.uk
"
https://www.turing.ac.uk/research/interest-groups/synthetic-data,"
Gaining access to, and sharing data, is fundamental in all AI research. But doing so can prove extremely difficult due to concerns around privacy. Synthetically generated data offers a potential solution.
There are challenges to this though. Synthetic data (SD) is not inherently private; methods need to actively enforce privacy in order to achieve it. Enforcing privacy often comes with a trade-off of utility. These trade-offs can only be understood in context; what is acceptable in one field may be actively harmful in another. The interest group aims to provide a platform for exchanging knowledge across several active projects on synthetic data generation at the Turing, to enable discussions about what various fields with an interest in synthetic data find acceptable (and unacceptable).
Beyond privacy, SD has the potential to allow for corrections to bias in data, the ability to create larger datasets from smaller sample sizes, and to simulate plausible scenarios beyond historical data which are needed to validate and increase robustness of machine learning pipelines.
This group aims to bring together groups interested in the development and science of synthetic data, to develop a framework of understanding and to share learnings amongst practitioners.
As a leading privacy technology, differential privacy is of particular interest to the group. This includes alternatives to it, and potential criticisms of it. Efforts have been made to incorporate differential privacy into synthetic data generators such as GANs (generative adversarial networks), VAEs (variational auto-encoders), and Bayesian networks. The interest group will explore the variety of methods out there.
Moreover, privacy has been shown to have some unexpected/undesirable side effects, such as a disproportionate impact on outliers and minority groups. The interest group will explore these types of drawbacks to existing privacy approaches.
The objectives of the interest group are as follows:
Click here to join us and request sign-up
Please contact Tony Zemaitis at [email protected]
"
https://www.turing.ac.uk/people/business-team/achintya-rao,"
Achintya Rao is the Community Manager for the AI for Science and Government research programme at The Alan Turing Institute. He is pursuing a PhD in science communication from UWE Bristol. He was previously a science communicator at CERN, the European Organization for Nuclear Research, and holds a BSc in physics and an MA in science journalism.
"
https://www.turing.ac.uk/people/former-enrichment-students/elena-kochkina,"
Elena Kochkina is a Postdoctoral Researcher at Queen Mary University of London and The Alan Turing Institute, working on tackling misinformation using Natural Language Processing. Her current research is funded by UKRI trough the PANACEA project.
Elena have completed a PhD in Computer Science supervised by Dr Maria Liakata and Professor Rob Procter with the Warwick Institute for the Science of Cities (WISC) CDT, funded by the Leverhulme Trust via the Bridges Programme. She was an enrichment and visiting student at The Alan Turing Institute in London. Her background is Applied Mathematics (BSc, MSc, Lobachevsky State University of Nizhny Novgorod) and Complexity Science (MSc, University of Warwick, Chalmers University).
The main focus of Elena's research is on Tackling Misinformation using Natural Language Processing. 
In her PhD she focused on rumour stance and veracity classification in social media conversations. Veracity classification means a task of identifying whether a given conversation discusses a True, False or Unverified rumour. Stance classification implies determining the attitude of responses discussing a rumour towards its veracity as either Supporting, Denying, Questioning or Commenting. In her work she studies  the relations between these tasks, as patterns of support and denial can be indicative of the final veracity label. As the input data is in the form of conversations discussing rumours, she utilises the conversation structure to enhance predictive models. She works with deep learning models as this approach allows flexible architectures and has benefits of representation learning. Recurrent and recursive neural networks allow to model time sequences and/or conversation tree-like structures.
Currently she is working on the “PANACEA: PANdemic Ai Claim vEracity Assessment” project, which aims to create an AI-enabled evidence-driven framework for claim veracity assessment during pandemics. Within the project her focus is on (1) collecting COVID-19 related data from social media platforms and authoritative resources and (2) developing novel unsupervised/supervised approaches for veracity assessment by incorporating evidence from external sources.
She is also interested in general area of online harms, and tasks like propaganda detection and multimodal hate speech detection. 
"
https://www.turing.ac.uk/people/former-doctoral-students/nikolas-kuhlen,"
Nikolas is a PhD student at the University of Cambridge. His research in economic data science is sponsored by HSBC.  Before joining the Turing, he obtained a BSc in Economics from the University of Muenster, an MSc in Economics from the University of Bonn and an MSc in Statistics from the London School of Economics and Political Science (LSE). During his studies, he spent one semester at Aix-Marseille University.
Nikolas’ research interests lie at the intersection of Computational Statistics, Econometrics, Machine Learning, and Economics. Currently, he is working on working on large-scale probabilistic modelling for econometrics.
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/multi-agent-systems,"
"
https://www.turing.ac.uk/people/hari-sood,"
Hari is a Research Application Manager at The Alan Turing Institute, working primarily on the Data Safe Haven. Previously he has founded a free-to-access social justice learning platform (Pyxium), and worked in an operational consultancy.
"
https://www.turing.ac.uk/people/guest-speakers/gordon-wai,"
"
https://www.turing.ac.uk/news/alan-turing-institute-responds-governments-new-foundational-model-taskforce,"
The Alan Turing Institute welcomes the Government’s announcement today (Monday 24 April) of an initial £100 million for an expert taskforce to help the UK to build and adopt safe artificial intelligence technology.  
 
The investment will fund a new government-industry taskforce to ensure sovereign capabilities and broad adoption of safe and reliable foundation models in the UK. 
 
The Government hopes that this investment will build the UK’s sovereign national capabilities so the UK’s public services can benefit from the impact of this type of AI.  
 
Over the past year, the Turing has been working with UK government partners and others to raise awareness of this important new technology and its challenges. 
 Sir Adrian Smith, Institute Director at The Alan Turing Institute, said: ""Large Language Models and Foundational Models can be immensely powerful and have the potential for great benefit, but as with all AI technologies it is vital to understand their limitations and the very real risks associated with them. Sovereign AI capability of the kind supported by the Turing and to be taken forward by the Taskforce, open to proper scrutiny and developed to the highest ethical standards, stand to have an enormous benefit for public services, society and the economy."" 
The Turing hosted the first ever conference on foundation models, held at the IET headquarters in February 2023 with 300 delegates, and leading speakers from UK academia and the UK’s vibrant AI startup scene.  
 
The Turing has also begun a programme of work around “benchmarking” foundation models – developing the core science to map out their capabilities in depth. It plans to significantly extend this programme of work with a range of public and private partner organisations in the year ahead.  
"
https://www.turing.ac.uk/people/researchers/florence-enock,"
Florence is a Research Associate in the Online Safety Team within the public policy programme. She is an experimental psychologist with expertise in social cognition and her research has mainly focussed on the study of intergroup biases. Before joining the Turing, Florence was a postdoctoral researcher at the University of York, where she examined the role of dehumanization in social attitudes and behaviours. Florence completed her DPhil in Experimental Psychology at the University of Oxford.
"
https://www.turing.ac.uk/research/research-projects/ground-truth-mental-health-data-science,"
As we go about our daily lives, each of us lays down digital footprints, such as through our interactions on social media, our search behaviour, and the things we buy. This vast repository of data on real life behaviour has huge potential to inform research into the causes of poor health. However, we very rarely have reference or “ground truth” data available to tell us whether our interpretation is accurate. Fortunately, the UK is home to worldleading birth cohorts: samples of thousands of people recruited at birth, who have been donating their data to research their whole lives. Linking digital footprint data in these cohorts could allow researchers to validate their algorithms against existing gold standard measures, improving the accuracy and reliability of interpreting these digital data, and helping to realise its potential in improving our understanding of mental health and wellbeing.
Although it is straightforward to collect social media data from consenting study participants via the application programming interfaces (APIs) that social media sites make available, there are challenges specific to linking these data in birth cohorts. For example, cohorts have a responsibility to protect the identity of their participants. To do this, they maintain data safe havens certified by the International Organization for Standardisation (ISO) and the International Electrotechnical Commission (IEC), and ensure that participants’ personal and identifiable data remain inside these havens. This means that instead of providing a centralised service to cohorts, it is easier for cohorts to run social media linking software themselves, within their own data safe havens. This introduces the challenge of developing software that is not only secure, but straightforward to run in heterogenous computing environments, and robust enough to run long-term without too much intervention.
For researchers to be able to validate algorithms for making inferences from digital footprint data, it is necessary to run these algorithms on the original social media data held by the cohorts and compare the results to more traditional measures. Because the original data cannot leave the data safe haven, this project is developing a framework for researchers to submit algorithms for the cohort to run themselves, returning anonymised summary results on the performance of the algorithm to the researchers. To supplement this approach of bringing the analysis to the data, the project has also been developing machine learning models to generate synthetic data sets that can be shared directly with digital footprint researchers, allowing them to develop and train better algorithms for later validation on the real cohort data.
Building on previous work with cohort participants and leaders to develop the processes and software frameworks to allow studies to easily and securely collect social media data, this Turing project is making it possible for digital footprint researchers to submit mental health coding algorithms for validation against the gold standard ground truth data that already exists in the cohorts, while keeping personal data secure. This will allow researchers to use the cohorts as a platform for developing better algorithms for inferring mental health and wellbeing from digital footprints, helping the data to realise their potential for improving our understanding of mental health and wellbeing. The project is also considering the ethical and social implications of improving our ability to infer mental health and wellbeing from digital data, and incorporating this into the design of the framework.
Alongside these technical challenges there are important social challenges. Cohorts rely on long-term trusting relationships with their participants, so it is crucial that cohorts link and use digital footprint data in a way that is acceptable to them, such as by maintaining copies of the full social media records within the data safe havens while making anonymised information derived from these data available for health and social research. This project is working closely with cohort leaders and participants from the Avon Longitudinal Study of Parents and Children (ALSPAC) and others from the CLOSER group of 19 UK birth cohorts to co-produce these approaches, making sure that the outputs are both useful to the cohorts and acceptable to their contributors. With these collaborations, and the ongoing dedication of tens of thousands of study participants across the country, the project will provide both unique data on how mental health changes across time, and the opportunity to improve how the whole field of research interprets the digital footprints we leave behind.
"
https://www.turing.ac.uk/people/researchers/sarah-morgan,"
Sarah Morgan is an Accelerate Science Research Fellow at the Cambridge Department of Computer Science and Technology, where she leads the AI for Brain Sciences Group. She completed her PhD at the Cambridge Physics department, in the Theory of Condensed Matter group (TCM). She then spent time as a postdoc in the Cambridge Psychiatry department, and as a Henslow Research Fellow at Lucy Cavendish College, Cambridge. In 2019, Sarah was awarded a Turing Fellowship to investigate whether speech can be used to detect psychotic disorders.

Sarah is a co-organiser of the Cambridge Networks Network, which brings together researchers from in and around Cambridge with an interest in complex networks (http://www.cnn.group.cam.ac.uk/). She is also passionate about making STEM more diverse, and co-founded the Cambridge group for women and non-binary people in physics (https://www.cavendishinspiringwomxn.co.uk/).
 
Sarah's research applies data science and AI approaches- including network science, machine learning and bioinformatics- to better understand and predict brain development, cognition and mental health.

One key focus is using MRI to study brain connectivity across the lifespan, in health and disease. To do this, Sarah works with brain networks derived from MRI data, in which nodes represent large scale brain regions and edges represent connectivity between brain regions.

Sarah is also interested in using data science to investigate other aspects of cognition and mental health, for example using network science and natural language processing methods to study speech from patients with psychotic disorders.
 
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/communications,"
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/safe-and-ethical,"
A world-leading centre for the technical underpinnings of safe and ethical AI which is trustworthy to encourage responsible innovation and cutting-edge research breakthroughs. Our work connects broadly with inter-disciplinary experts, industry, government, regulators, civil society and other stakeholders to ensure that we build the right tools for society.
The key aim of this strategic challenge of the Turing’s AI programme is to establish a centre of excellence for the study of technical aspects of safe and ethical AI, in line with the government’s Industrial Strategy and in step with global demand for research and guidance in this domain.  This will be achieved by conducting deep theoretical research, looking for rigorous, quantifiable and verifiable guarantees, and pushing the state-of-the-art frontiers, to enable trustworthy deployment. Key themes include: 
To achieve this, it will be vital to interface well with other disciplines, policy makers, industry and the public.
 
AI systems are rapidly being developed and deployed across society. This creates tremendous opportunities, but also raises a pressing need to ensure that these systems are safe and ethical in order to function properly, grow public trust and avoid a potential backlash.
What is missing in the UK landscape is a major effort to build the necessary technical foundations for this endeavour. Knowing how to regulate in a way that fosters innovation, while providing optimal guard rails for society, will not be possible without this effort. We propose to establish a centre of excellence for the study of technical aspects of safe and ethical AI at the Turing, to build the technical underpinnings for trustworthy deployment, responsible innovation and appropriate governance.
This is not a one-off endeavour but will require continual upgrading as the technology evolves. There are trade-offs between desirable goals for society (eg privacy and transparency, individual vs societal benefit). Our aim is to enable the best possible frontier across these goals and communicate with policy makers and the public to help ensure that the right point on the frontier is enforced.
Watch this video to learn about the Turing AI programme, its background and purpose as well as its activities. If you would like to know anymore about the programme or discuss collaboration opportunities, please get in touch with the team [email protected]   

From November 2021 to January 2022, The Alan Turing Institute and the Information Commissioner’s Office (ICO) hosted a series of workshops about fairness in AI. The aim of the series was to assist the ICO in the development of an update to the fairness component of its existing non-statutory guidance on AI and data protection, by convening key stakeholders from industry, policy, and academia to identify the key issues and challenges around fairness in AI, and to discuss possible solutions. The guidance is part of ICO25’s Action Plan for 2022-2023. The update to the guidance has now been published, and the ICO will ensure any further updates will reflect upcoming changes in relation to AI regulation and data protection.
Read more here
The paper published by Sandra Wachter, Brent Mittelstadt and Chris Russell in March 2020, 'Why fairness cannot be automated: Bridging the gap between EU non-discrimination law and AI', explains which parts of AI fairness can, cannot and should not be automated and suggests ideas for legally compliant algorithmic bias audits.
Read the full paper
A workshop between UK, France and Canada, 'CIFAR-UKRI-CNRS AI and Society: From Principles to Practice' at the Turing, provided a forum for AI ethics researchers in the UK, France and Canada to meet and exchange notes on their recent work through a series of brief presentations. This utilised the Turing’s convening power to bolster the creation of cross-country research teams to apply for collaborative funding and at the same time raised the profile of the Institute in AI ethics across the UK, France and Canada.
Find out more about the workshop
Google's 'What-If Tool' as part of their TensorFlow™ machine learning framework, is an open source software library for high performance computation and a widely used deep learning framework. The tool enables non-technical individuals to easily understand what their machine learning is doing and allows the user to edit examples from datasets and show how the model’s predictions change as any single feature is changed. 
Read the Google blog to find out more
View the code and references to the Turing researcher's work
Turing researchers from diverse fields have produced a new way of approaching fairness in algorithm-led decisions, by looking at the causes of certain factors that can often result in biased decision-making.
Read the impact story
Advice from Turing researchers, urging the need for individuals to have a legally binding right to have automated decisions made about them explained, is helping shape how the new EU data protection regulations will be implemented.
Read the impact story
Turing researchers have also been developing methods to train machine learning models which do not discriminate against gender or race – published in 'Blind Justice: Fairness with Encrypted Sensitive Attributes'.
Read the full paper
Yarin Gal, Group Leader on AI Programme and Turing AI Fellow, University of Oxford, has been named on MIT Technology Review's Innovators Under 35, Europe 2019 list in the 'pioneers' category. Professor Sandra Wachter, Turing Fellow and AI Programme Project Lead based at University of Oxford, has been featured in the Financial Times, reporting on her recent paper regarding online discrimination by association.
Read 'Algorithms drive online discrimination, academic warns'
[email protected]
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-edinburgh,"
"
https://www.turing.ac.uk/events/real-people-behind-artificial-intelligence,"
This is a public discussion about finding the real people behind artificial intelligence.
#RealPeopleBehindAI
Artificial intelligence surrounds us, some automation we’re aware of: assembly line workers replaced by machines, or drones delivering packages ordered online. Other automation is more hidden: algorithms which decide who should serve prison sentences and for how long, or which CVs are sent in response to job recruitment adverts.
These differences in how we interact with our world are not new. From the plough to the PC, changes in technology have always been accompanied by changes in the nature of work. The growing use of artificial intelligence raises new questions across the sciences, arts and humanities, and for public policy.
This panel-led debate will focus on opening up and addressing questions about whose work underpins automation, whether this is work at all, and if so what kind? It will be a lively and multidisciplinary conversation, with as much audience participation as possible. Please bring your opinions and be ready to hear from others on what might we need to do to ensure diverse and fair contributions to automation which are appropriately recognised and rewarded.
This public event follows on from the one day workshop, where a group of researchers, practitioners and policy makers will discuss Automating the crowd. 
Event timings: 18:00 - 19:30, registration from 17:30The Alan Turing Institute is located on the first floor of the British Library and can be accessed via the main doors. Please allow 10 minutes for queuing when planning your journey.
 
 

"
https://www.turing.ac.uk/blog/camdens-data-charter-why-good-data-governance-being-parent,"
Navigating parenthood is a bewildering experience for the newly initiated. Everything changes… almost daily. Just as you begin to feel as though you’ve learned the rhythm of your little one’s routine, they start to crawl! And this all has to be navigated while suffering from ever-increasing fatigue. Despite being a father for nine months now, I’m still shocked every day around midnight to find out it’s only 6pm!
Perhaps it’s because I work in the Turing’s public policy programme, but I think parenting is a lot like trying to keep up with the social impacts of technological innovation. Similar to the hard-to-fathom rhythm of a child’s development, technology changes almost daily.
So, when Camden Council decided to develop a Data Charter to guide its use of emerging data-driven technologies, it did the same thing that all new parents do when faced with a new challenge: it reached out to its community for help.
The saying “it takes a village to raise a child” is an acknowledgement of the myriad challenges faced by all parents and the diverse forms of help that are required. Making decisions about how data ought to be used in the delivery of public services also requires a village. Well, a London borough to be precise.
In late 2021, over the course of three weekends, Camden Council brought together 20 of its residents to co-develop a Data Charter, giving the residents a say in how their data will be collected, stored and used. This representative panel of Camden’s residents was asked about its views on issues such as:
Questions such as these become more complex the more you explore them, so our project team wanted to equip the residents with the platform and technical knowledge necessary to engage, debate and reach a consensus. To provide the residents with background information on the ethical, social and legal issues around data, I put together a short guide, which I am now in the process of revising for a broader audience.
In planning these sessions, we aimed to strike an appropriate balance between three components: the educational component (overseen by the Turing), which ensured that residents were equipped to deliberate on an equally informed basis; the deliberative component (overseen by public participation charity Involve), which enabled residents to reflect and deliberate through a series of structured exercises; and the practical component, i.e. the logistical processes put in place by Camden Council to ensure that this could all happen.
The resulting Data Charter, which was published by Camden Council last week, emphasises principles such as greater transparency, improved accessibility of information, and clear mechanisms of accountability, to ensure that all residents can be involved in decisions about the delivery of services that affect them. But alongside these principles, which are reinforced by data protection and privacy laws, is a positive vision for how data ought to be used. Far from being an afterthought, ethical values such as fairness and equity are deeply woven into the charter, expressing a clear commitment from the residents that data should be treated not solely as an economic commodity, but also as a co-developed resource that promotes public good.
Just as raising a child works best with support, I believe that this collaboration with residents has produced a more inclusive, forward-looking and mutually-beneficial Data Charter than we ever could have achieved alone.
Media theorist Marshall McLuhan coined the term ‘global village’ in the 1960s to describe how technology extends our communicative abilities across the globe, creating an interconnected world that can feel more like a village.
However, technology clearly doesn’t affect everyone in the global village equally. The positive impacts continue to accrue to wealthier nations and those individuals who have the greatest capacity to use and understand technology, while the negative impacts exacerbate existing social injustices that disproportionately affect marginalised or vulnerable populations. Whether intentional or not, these consequences are always the result of a series of choices to design, develop and deploy data-driven technologies in a specific social context.
As our lives become increasingly influenced by data-driven technology, the only way to ensure a more equitable society is to create accessible, public spaces for debate, which are supportive and inclusive to diverse voices. When faced with the enormity of this challenge, it can be easy to get overwhelmed. Data flows across the boundaries of boroughs, cities, countries and continents. And, therefore, its governance can extend beyond the regulatory remit of any one local council, regional body, or even national government.
Despite making up only ~3% of the Capital’s nine-million-plus population, Camden has taken an important, and in many ways unprecedented, step towards a more positive vision for how data can be used for the public good. Camden Council’s decisions about how it collects data to inform policy decisions, or which data is shared with local healthcare providers, will help inform similar decisions made by other local authorities.
Here in our Camden-based offices in the British Library, the Turing will continue to work closely with the council and its residents to support its vision. However, as a national institute, it is also vital that we build on the lessons of this project and explore how the initiative might be applied in other councils around the UK.
After all, no two children are the same!
Find out more and connect with the researchers:Developing a data charter with Camden Council
 
Top image: Zck_ / Unsplash
"
https://www.turing.ac.uk/people/turing-fellows/sam-cohen,"
Sam Cohen is an Associate Professor in the mathematical Institute in Oxford, senior research fellow at New College, Oxford, and an associate member of the Oxford Man Institute for Quantitative Finance. Before Oxford he was a student in mathematics at the University of Adelaide. Sam’s research interests are at the interaction between data science, statistics, probability theory and stochastic analysis, economics and finance, and optimal decision making. He is particularly interested in understanding the role of statistical information in decisions, and how to address the inconsistent behaviours that this entails.
"
https://www.turing.ac.uk/people/enrichment-students/andrea-luppi,"
Coming from a background in philosophy and cognitive science, Andrea’s PhD at the University of Cambridge aims to characterise how the capacity for cognition and consciousness arises from the complex interactions between brain systems. To this end, Andrea’s work combines tools from information theory, network science and whole-brain computational modelling to study information-sharing in the brain across pharmacological and pathological states of consciousness, with the ultimate goal of promoting recovery of consciousness in brain-injured patients.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/rescue-global,"
www.rescueglobal.org
"
https://www.turing.ac.uk/research/research-projects/analysing-humanities-books-and-newspapers-data,"
The goal of the collaboration was to port text analysis codes which were developed in order to exercise the data access, transfer and analysis services of the Turing's deployment of a Cray Urika-GX analytics system. Two data sets of interest to the University of Edinburgh's College of Arts Humanities and Social Sciences (CAHSS) and hosted within the University of Edinburgh were used: British Library digitised newspapers from the 18th to the early 20th century; and British Library digitised books data from the same period. Once ported, the text analysis codes were extended to support new queries across the datasets.
Two text analysis codes were used, one for querying each each dataset, which were initially developed by UCL with the British Library in 2015-2016. UCL's codes are written in Python and runs queries via Apache Spark. The code was originally designed to run queries on a user’s local machine or on UCL’s high performance computing (HPC) services.
To run the codes within Urika, both codes were modified so that they could run without any dependence UCL’s local environment, and instead access data located within Urika. An older version of UCL's code - pre-dating Apache Spark and which uses the message-passing interface (MPI) for parallel programming - was also run on Urika to generate sample query results for queries that were then migrated to Spark.
New queries were implemented at CAHSS's request to search for occurrences of keywords (e.g. ""Krakatoa"" or ""Krakatua"") and their concordances (the text within which the words are found) co-located words (e.g. ""stranger"" and ""danger""). Support for an additional dataset - New Zealand newspapers - was also added.
Visualisations were also developed in Jupyter notebooks to present query results as N-grams, graphs of occurrences by year, and word clouds.
Analysing historical newspapers and books using Apache Spark and Cray Urika-GX, EPCC blog post, 16 August 2019
Analysing humanities data using Cray Urika-GX, EPCC blog post, 11 December 2018.
Rosa Filgueira
[email protected]
"
https://www.turing.ac.uk/people/guest-speakers/daniel-wolpert,"
Daniel Wolpert read medicine at Cambridge before completing an Oxford Physiology DPhil and a postdoctoral fellowship at MIT. He joined the faculty at the Institute of Neurology, UCL in 1995 and moved to Cambridge University in 2005 where he was Professor of Engineering and a Royal Society Research Professor.
In 2018 he joined the Zuckerman Mind Brain Behavior Institute at Columbia University as Professor of Neuroscience. He was elected a Fellow of the Royal Society (2012) and has been awarded the Royal Society Francis Crick Prize Lecture (2005), the Minerva Foundation Golden Brain Award (2010) and the Royal Society Ferrier medal (2020).
His research interests are computational and experimental approaches to human movement (www.wolpertlab.com).
"
https://www.turing.ac.uk/people/guest-speakers/sarah-sharples,"
Professor Sarah Sharples is Chief Scientific Adviser for the Department for Transport. She is a Professor of Human Factors in the Faculty of Engineering at the University of Nottingham and from 2018-2021 was Pro-Vice-Chancellor for Equality, Diversity & Inclusion and People. She has led research in transport, manufacturing and healthcare. Sarah currently leads the EPSRC (Engineering and Physical Sciences Research Council) Connected Everything Network Plus.
She founded and was co-director of the EPSRC Horizon Centre for Doctoral Training, and has led research programmes examining implementation of new technologies in rail, highways and aviation. She was President of the Chartered Institute of Ergonomics and Human Factors from 2015-16. 
"
https://www.turing.ac.uk/research/research-projects/hate-speech-measures-and-counter-measures,"
Hateful content online is a growing problem in the UK. It can pollute civic discourse, inflict harm on targeted victims, create and exacerbate social divisions, and erode trust in the host platforms. The Hate speech: measures and counter-measures project is developing and applying advanced computational methods to systematically measure, analyse and counter hate speech across different online domains, including social media and news platforms.
This project is funded by the UKRI Strategic Priorities Fund (ASG).
This research is using advanced computational methods, including supervised machine learning, stochastic modelling and natural language processing, to detect and analyse hate speech. Initial research is aimed at developing supervised machine learning classifiers to detect and categorise different strengths and targets of hate speech.
The aim is to understand the scale and scope of online hateful content, taking into account its different forms, from ‘everyday’ subtle actions to overt acts of aggression and criminality, and the different targets, such as ethnic minorities and women. The project also aims to understand the dynamics and drivers of hate, providing granular insight into when, where and why it manifests.
The tools being developed for automatically identifying and categorising hateful content will be of interest to government, policymakers, companies, and other researchers. The project researchers will work across stakeholders to establish best practices, share findings and offer insight. The code and data involved our being made as accessible as possible and the researchers will be blogging about their work as the project progresses.
Researchers from the Universities of Oxford, Surrey, Sheffield and the George Washington University, led by The Alan Turing Institute’s Hate Speech: Measures & Counter-measures project, have developed a tool that uses deep learning to detect East Asian prejudice on social media. The tool is available open source, along with the training dataset and annotation codebook. It can be used immediately for research into the prevalence, causes and dynamics of East Asian prejudice online and could help with moderating such content. You can find the paper describing the methodology and results on arXiv.
Read more about it here.
'Challenges and frontiers in abusive content detection' presented by Bertie Vidgen at ALW3: 3rd Workshop on Abusive Language Online.
Turing article: The Turing’s Public Policy Programme responds to the Online Harms White Paper
Helen Margetts and Bertie Vidgen presented at CogX on 11th June. Watch the recording of the talk.

Blog post by Bertie Vidgen, 'Four ways social media platforms could stop the spread of hateful content in aftermath of terror attacks'
Blog post reporting on the project's first classification work.
Bertie Vidgen [email protected]
"
https://www.turing.ac.uk/people/external-researchers/cynthia-rudin,"
Cynthia Rudin is a Professor of Computer Science, Electrical and Computer Engineering, Statistical Science, and Biostatistics & Bioinformatics at Duke University, and directs the Interpretable Machine Learning Lab. Previously, Prof. Rudin held positions at MIT, Columbia, and NYU.
She holds an undergraduate degree from the University at Buffalo, and a PhD from Princeton University. She is a three-time winner of the INFORMS Innovative Applications in Analytics Award, was named as one of the ""Top 40 Under 40"" by Poets and Quants in 2015, and was named by Businessinsider.com as one of the 12 most impressive professors at MIT in 2015. She is a fellow of the American Statistical Association and a fellow of the Institute of Mathematical Statistics.
"
https://www.turing.ac.uk/news/publications/decomposing-feature-level-variation-covariate-gaussian-process-latent-variable,"
The interpretation of complex high-dimensional data typically requires the use of dimensionality reduction techniques to extract explanatory low-dimensional representations. However, in many real-world problems these representations may not be sufficient to aid interpretation on their own, and it would be desirable to interpret the model in terms of the original features themselves. Our goal is to characterise how feature-level variation depends on latent low-dimensional representations, external covariates, and non-linear interactions between the two. In this paper, we propose to achieve this through a structured kernel decomposition in a hybrid Gaussian Process model which we call the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We demonstrate the utility of our model on simulated examples and applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes. In each setting we show how the c-GPLVM can extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not present in other commonly used dimensionality reduction approaches.
Märtens, K., Campbell, K. R., Yau, C. (2019) Proceedings of the 36th International Conference on Machine Learning, PMLR 97:4372-4381.
"
https://www.turing.ac.uk/news/publications/data-justice-practice-guide-policymakers,"
The Advancing Data Justice Research and Practice project aims to broaden understanding of the social, historical, cultural, political, and economic forces that contribute to discrimination and inequity in contemporary ecologies of data collection, governance, and use. This is the consultation draft of a guide for policymakers. It provides actionable information for policymakers who wish to implement the principles and priorities of data justice in their policymaking activities. In the first section, we introduce the nascent field of data justice, from its early discussions to more recent proposals to relocate understandings of what data justice means. This section includes an account of the outreach we conducted with stakeholders throughout the world in developing a nuanced and pluralistic conception of data justice and concludes with a description of the six pillars of data justice around which this guidance revolves. 
Depending on their contexts, potential impacts, and scale, data policymaking activities may be carried out in a way that involves stakeholder engagement. To facilitate this process, the next section provides an explainer of the Stakeholder Engagement Process and the steps it includes—preliminary horizon scanning, policy scoping and stakeholder analysis, positionality reflection, and establishing stakeholder engagement objectives and methods. Finally, the last section presents the guiding questions that will help policymakers address issues of data, digital infrastructures, and affected areas of civic, public, and private life, throughout the policy lifecycle and in accordance with the six pillars of data justice.
Leslie, David, Katell, Michael, Aitken, Mhairi, Singh, Jatinder, Briggs, Morgan, Powell, Rosamund, Rincón, Cami, Perini, Antonella, & Jayadeva, Smera. (2022). Data Justice in Practice: A Guide for Policymakers. The Alan Turing Institute in collaboration with The Global Partnership on AI. https://doi.org/10.5281/zenodo.6429475
This report was commissioned by the International Centre of Expertise in Montréal in collaboration with GPAI's Data Governance Working Group, and produced by the Alan Turing Institute. The research was supported, in part, by a grant from ESRC (ES/T007354/1), Towards Turing 2.0 under the EPSRC Grant EP/W037211/1, and from the public funds that make the Turing's Public Policy Programme possible.
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-uk-dementia-research-institute-and-demon-network-0,"
All major neurodegenerative diseases are characterized by substantial heritability and while recent large-scale genetic efforts have identified variants associated with disease, these often lie in non-coding, regulatory regions and cannot be linked to any functional outcomes [11]. Recent work has repeatedly highlighted that the genetic risk for Alzheimer’s disease acts primarily via microglia - the resident macrophages of the central nervous system [14]. However, most of these variants do not directly affect protein function, instead they are suspected of influencing gene expression by altering genomic regulatory elements.
One of the main challenges in genome biology is the understanding of highly cell-type specific gene regulatory mechanisms. In the context of diseases, research is typically targeted towards cell-types relevant to disease phenotype, or which are presumed to have a causal downstream effect. Genetic variants which are associated with the function or activity of regulatory elements (known as quantitative trait loci or QTLs) will exert their effects in a cell-type specific manner [5]. Mapping out molecular and regulatory QTLs comprehensively in disease-relevant cell-types would enable us to interpret functional outcomes of genetic variants on gene expression and regulation.
While we are focused on understanding its impact on dementias, the impact of improving regulatory genomic predictions will affect research on all biological traits for which good genetic traits are available (approximately 2000 diseases/phenotypes). Specifically in the context of Alzheimers, it would enable us to test for genome-wide associations with particular regulatory factors (e.g. transcription factors) which are directionally involved in disease. If we become able to detect such an enrichment, then we would be well positioned to start looking into drug target development.
Data Study Group team. (2022, July 5). Data Study Group Final Report: UK Dementia Research Institute and DEMON Network. Zenodo. https://doi.org/10.5281/zenodo.6799048
PI: Mike Phuycharoen
3.3 MB
"
https://www.turing.ac.uk/research/research-projects/data-driven-design-assurance,"
Design assurance can reduce design errors, but it is a highly manual process that is often done poorly and late. The research will seek to extract features from design assurance datasets to identify patterns in the data, and to link these to key performance indicators on outcomes. The work is vital to developing methods for data-driven design that will assist decision makers involved in the design of complex systems, ensuring high-quality design that conforms to requirements.
Failures and errors in engineering practice are expensive and time-consuming to correct, especially in the late implementation phase of large engineering programs with safety requirements. A large percentage of failures that occur in the late implementation phase of engineering practice are attributed to the 'small' errors originating in earlier design process. Design assurance is the process of discovering, preventing, and correcting errors earlier in the design engineering life cycle when these problems are less expensive and possible to correct.
Discovering and eliminating errors earlier in design and engineering process is a non-trivial task, since it’s hard to make predictions on the outcomes of current design decisions without actually approaching and experimenting at a later implementation stage. However, fortunately, design engineering practice is a data-intensive process which generates huge amounts of data about behaviour, actions, and decisions, along with design requirements and definitions, drawings and specifications, design decisions, reviews and verifications, engineering change requests/logs, supporting documents, and even designers’ emails and chat logs. All these datasets may have interesting relationships with the corresponding performance and outcome of the design process.
Since most of this data is in text format, text mining and natural language processing can be applied as the key techniques to extract, build, and represent the hidden design process features from the unstructured texts. By linking these design process features with the key performance indicators, it is possible to use probabilistic approaches to quantify the correlation or dependency between the features and delivery performance in order to know what kinds of actions and decisions should be prevented or supported in the design process to ensure high-quality output. In addition, classification and regression models could be further built to provide a qualitative or even quantitative prediction on later performance based on early design process activities and decisions. 
The work seeks to improve design assurance processes. The ambition is to link an archive of structured design data, which involves a significant body of text data and associated event information, with a dataset of key performance indicators, in order to address one or more of the following questions:
The research seeks to develop results that can be generalised across industrial practices in engineering design, and can potentially yield advanced design assurance guidance in order to reduce process errors and improve delivery quality. Success is defined both in terms of the intellectual contribution to the development of methods and tools for data-driven design under uncertainty, and the practical impact on design assurance processes, the quality of design outputs, and the efficiency of their delivery. 
The contribution to machine learning and design engineering disciplines will be evidenced by outputs in international research journals; while the practical contribution will be monitored and discussed with industry partners. The work is of value as robust design assurance processes are key to unlocking the significant potential of automating aspects of design.
This project is part of the Data-centric engineering programme's Grand Challenge of 'Data-driven engineering design under uncertainty'.
This work is applied to and can benefit complex engineering design, such as construction and infrastructure projects, where small efficiencies through improved understanding of design assurance can have major benefits and impact.  
"
https://www.turing.ac.uk/research/transformation-healthcare,"
To transform healthcare and enable better outcomes for all.
The recent pandemic has brought starkly into view the scale of the transformation needed for health and social care systems around the world to be able to continue to ensure the future health of the population.  
Some of the main challenges include the demographic pressures of an ageing population, increasing levels of multiple long term conditions, the rising personal and societal costs of poor health and persistent inequalities in health outcomes across populations.  
There is a clear need for a more integrated approach across the spectrum of health, biology, social care, public policy and many other sectors addressing the underlying determinants of health.  
The opportunities arising from the revolution in data science and AI to enable a more proactive focus on the prevention, the earlier identification of disease, and earlier better targeted interventions to improve health for all are yet to be fully realised. 
The 'classic' machine learning challenge of mapping a large set of static and dynamic input data to a defined set of outcomes can be readily applied within a health context, and recent developments in this field are proving extraordinarily powerful in making progress in previously intractable areas such as computer vision, natural language parsing and object recognition. 
However, we have yet to see widespread or systematic applications of data science or AI techniques to address major health challenges and to harness the power of the UK’s health data assets. Constraints on progress have included the difficulty in attaining coherent long term health data sets, legitimate and ongoing concerns over the implications for patient privacy, and the sheer complexity of health data, which requires strong collaborations across expertise domains.  
There is the need for new approaches to data science and AI that appreciate inter-relations between drivers of health, and are informed by deep biomedical understanding, clinical expertise and wider societal and environmental context in which they relate.  
The revolution in data science and AI for health will enable a more proactive focus on the prevention, earlier identification, and better targeted treatments of disease, driving better health for all across the UK. 
As the national institute for data science and AI, Turing has been investing in, supporting and developing the UK’s research communities and building new partnerships necessary for success in addressing the challenge of realizing the power of AI to improve health for all.  
Our unique strengths include collaboration with the UK’s world-class data science, AI and health research communities and links with international partners, access to large datasets of individual and population health records, close links with the NHS and other UK and international organizations to ensure the innovation in AI is has real-world applications, and partnerships with institutions and major life sciences companies, that are capable of making significant multi-year funding commitments to support the necessary research.  
We believe the time is ripe for a concerted multi-disciplinary research effort to develop the concept of ‘Health AI’: data science and AI methodology co-developed with health practitioners and experts, leading to a practical working models and ultimately into health practice and policy. 
'We worked with Turing both before and during the pandemic, bringing together a community of data scientists to address important questions by building novel and innovative models. It is testament to the Institute’s ability and value as a national institute to convene independent groups around important scientific and policy questions.
It is rewarding to see this continued commitment to using data science and AI to support big national problems in their new strategy published today.'
Johanna Hutchinson, Director of Analytics and Data Science, UK Health Security Agency 
'Advances in AI and data science hold the potential to fuel transformational improvements in how we prevent, diagnose, and treat people living with devastating conditions, including many heart and circulatory diseases. Our own work with The Alan Turing Institute is already developing predictive tools to help patients and healthcare professionals make more informed decisions about their care. 
Transforming healthcare through AI and data science will depend upon meaningful collaboration between clinicians and data scientists, underpinned by scientific rigour and the highest ethical practices. The Institute acts as a catalyst for this type of collaboration, and we look forward to working with them on advances to improve the lives of people with heart disease in the years to come.’ 
Dr Charmaine Griffiths, Chief Executive, British Heart Foundation
In consultation with partners from across the health sectors, we are developing four interrelated missions, that extend from the cellular to the social determinants of health. These include cell and molecular mechanisms of disease, population health, disease risk prediction, early diagnosis, innovation in clinical trials and applications for clinical evidence evaluation. 
We need collaborators to help us to solve this Grand Challenge. Sign up to our newsletter to stay in touch with the latest news.

"
https://www.turing.ac.uk/research/research-projects/dynamic-forecasting,"
Forecasting is a common and crucial task that organisations rely on for planning and decision-making. In collaboration with British Airways, this project examines how modern machine learning techniques can be used to improve dynamic forecasting using large-scale business datasets.
The project investigates the use of Bayesian methods for forecasting and incorporating heterogeneous information. 
Bayesian statistics is a principled way of incorporating past experience, domain knowledge of the underlying system, recent trends and new data. Starting with a simple model you iteratively add information and complexity to make incremental improvements in forecasting accuracy. The result is an interpretable model that can be interrogated.
The project falls under the Turing's mission to develop and to promote applications of data science techniques to real-world problems. 
The unique challenges addressed by this project are modelling large datasets with complex time series structure and causal effects in a domain where forecast success metrics are commonly hard to define.
The complexity of forecasting problems in the airline industry make it a good case study for testing and developing data science methods. The main challenges in this project range from defining the research question in a complex domain and specifying a success metric to evaluating Bayesian models on large datasets. 
The methods and workflows developed in this project are applicable to other large scale data science projects at the Turing.
[email protected]
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/probability,"
"
https://www.turing.ac.uk/people/researchers/bastian-greshake-tzovaras,"
Bastian is a Senior Researcher in the Tools, Practices and Systems Programme where he works on implementing participatory citizen science approaches across different projects. As a great fan of open science and participatory approaches, he believes that including those affected into the research process not only improves data ethics and data justice aspects, but also leads to better and more applied research outcomes.
Bastian has a background in biology and received a PhD in Bioinformatics from the University of Frankfurt, Germany. Since then he has been working on different participatory data science efforts. He was a research fellow at the French National Institute of Health and Medical Research in Paris where he led a research group on facilitating participatory medicine approaches prior to joining the Turing.
 
Bastian is working on the open source Citizen Science platform.
"
https://www.turing.ac.uk/blog/what-data-scientific-report,"
With data science pervading all areas of society, the importance of technical data science work is increasing in organisations across sectors – and with it, the importance of technically sound data science reporting.
As data science continues to affect all parts of society, finding agreed and reliable ways of data science reporting will become crucial.
This article summarises some of the best practice advice to for anyone who plans to author, or critically read, professional data science reports, such as real-world practitioners or participants of the Turing Data Study Groups. I would also like to encourage professionals to participate in the ongoing open discussion around, or writing of, the Turing’s best practice guidelines.
Traditionally, professional reports occupy a grey area, in-between academic scientific discourse and scientific journalism. The field of science studies (i.e., research about research), categorises them as technical reports, part of so-called grey literature.
A stylised data scientific report is characterised by:
There are natural differences, as well as natural interfaces to academic research. In a report, an appealing narrative or academic novelty is less important than a correct, data-driven answer. Such an answer may be achieved by rote methodology, or may even take the form of a solid negative result – difficult to publish in a stylised academic outlet, but possibly still of high value to decision makers.
On the other hand, data science reports may highlight gaps in the state-of-art, and act as a motivator for domain application informed academic research.
There is much intersection of best reporting practice with best research practice, both being a matter of intellectual honesty and due diligence. I would like to highlight some common pitfalls that are specific to the data scientific reporting setting, and best practice principles to avoid them. An author may treat these as a mental checklist, while a reader may find them useful for spotting tell-tale signs of common bad practice.
While everyone will agree that fabrication of results is bad, it is rare. More common problems are wishful thinking or over-marketing.
How-not-to: the report is marketing material for a particular approach, algorithm, or method. It praises its advantages while avoiding talk of shortcomings.
How-to: before presenting a solution, the report describes what the data scientific problem is, and provides an unbiased overview of potential approaches.
To consider: Solid data scientific work is much more difficult than lying convincingly. Therefore, truthfulness is a competent data scientist’s unique selling point.
 
Any recommendation is just as good as the argumentation that supports it. In data science, the argumentative chain is a sequence of empirical, logical, and statistical arguments. Key parts are of empirical nature: e.g., how does the approach address the real world problem? How are the real world recommendations obtained from the analyses? Understanding these connections requires no deep methodological background knowledge, only careful bookkeeping. Moreover:
To consider: The only way to check correctness of a conclusion is checking correctness of the argumentative chain. An argument that is not made is just as good as an argument that is wrong.
 
If it can’t be checked, it can’t be trusted. Basic scientific quality control requires that the work can be fully scrutinised – the burden of proof lies with the originator of a claim. Even if the work is not open to the public, a reviewer should be able to check data, code for methods and analyses, and the full argumentative chain building on top of it.
Specialised technology may facilitate reproducibility, e.g., sub-version repositories and data management architecture, see The Turing Way. Of course, technology does not replace solid quality control processes.
To consider: It’s called quality control and not quality trust.
 
A requirement for the scientific method is that all claims can be put to test, and the scientific process requires to do so on a regular basis. Deviation from the method, or the process, are common signs of pseudo-data-science.
Of all principles this is perhaps the most subtle and difficult one: a report can be fully reproducible and argumentation complete, but may still be scientifically broken, e.g., at the level of success control. In general, ensuring quality here requires expertise in data science as well as in the application domain. Though the more common issues can be caught just by carefully working through the question “how would a negative finding look like”. Examples:
Regarding the scientific process: related fields need to be explicitly identified and referenced, in the domain (e.g. cancer treatment), and in data science (e.g. survival modelling). Common approaches there should be tried first. It is best to avoid untested experimental methodology until it has been separately subjected to (academic or internal) research quality control.
To consider: Any sufficiently untestable claim is indistinguishable from wishful thinking. An exciting real world application does not justify lack of scientific due diligence.
 
Ethical and technical appraisal of results go hand in hand: it is often the same mechanism that may cause damage to society and to the end user – recklessness and carelessness are more often the problem than malice. As a matter of professional integrity and technical accuracy, the report needs to spell out concerns and limitations very explicitly, e.g. in executive summary.
Common failure points involve:
In addition to appraisal by the author, most organisations will have a dedicated body for ethical checks – though currently only few have one for technical validity checks.
To consider: consequences need not be the same as intended consequences. Positive thinking does not replace careful ethical and technical appraisal.
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-predictive-modelling-healthcare,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
This event will be exploring the topic of predictive modelling in healthcare- how this branch of advanced analytics can potentially predict and help improve patient outcomes and beyond.
We'll be hearing from four speakers:
Dr Jasmina Bogojeska, Senior Principal Data Scientist at Roche Diagnostics – will be talking about Navify Kidney Companion, which focuses on the challenging problem of early detection of Chronic Kidney Disease patients with fast progression to end stage renal disease (ESRD). The tool allows for timely intervention and thus has the potential to delay or even prevent the development of ESRD.
Dr Matthew Sperrin, Senior Lecturer in Health Data Science at the University of Manchester – will be talking about the role of causal inference in predictive modelling
Dr Elif Ozkirimli, Head of Data Science and Advanced Analytics – Pharma International Data and Analytics Chapter at Roche- will be talking about using real world data for cardiac safety modeling
Dr Brieuc Lehmann, Assistant Professor at University College London (UCL) – will be talking to us about improving local prevalence estimates of SARS-CoV-2 infections using a casual debiasing framework.
As well as having a Q&A at the event, we hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace.
You can watch a recording of this event here. 

"
https://www.turing.ac.uk/people/elena-simperl,"
Elena is a professor of computer science at King’s College London in the UK. She is also a Fellow of the British Computer Society and a former Turing Fellow.
I have a doctoral degree in computer science (Dr. rer. nat.) from the Free University of Berlin and a diploma in computer science (Dipl. Inform.) from the Technical University of Munich. Before joining King’s in 2020, I held positions at Southampton, as well as in Germany and Austria.
"
https://www.turing.ac.uk/events/smart-charging,"
Join us for the next Smart Charging Webinar, where we'll discuss EEBUS Initiative EV and intelligent charging functions for the Audi e-tron. To watch previous Smart Charging webinars view the playlist here
Photo by israel palacio on Unsplash
15:00-15:05 – Introduction: Dr Myriam Neaimeh, Newcastle University and The Alan Turing Institute
15:05-15:50 – Presentation: Dr Maren Fiege, Standardization Manager, EEBUS Initiative e.V., and MSc Gerhard Paris, Function Owner Smart Charging, Audi AG
15:50-16:00 – Q&A session
• What is EEBUS and how does it work?
• How does it support EV charging?
• How has it been implemented at Audi?
• How is it standardized?
"
https://www.turing.ac.uk/research/research-projects/retrofit-design-built-environment,"
Retrofitting new interventions to existing buildings, rather than rebuilding from scratch, can help reduce disruption to local communities and help offset the 'embodied energy’ of a building – i.e. the energy consumed by all of the processes associated with its production. However, such new innovations may not fit current energy audit criteria, requiring changes in behavioural norms and modelling approaches. This project is exploring decision-making processes common in construction and building management, in order to evaluate the potential opportunities for retrofit innovations and solutions in the built environment.
The UK housing stock is of some of the poorest quality in the whole of Western Europe. This is due to a series of historical and policy reasons, including the aging housing associated with the extensive building programmes of the industrial revolution and post-World War periods, as well as one of Europe’s highest density populations. 
Poor levels of insulation and air tightness have contributed to soaring domestic energy bills. However, airtight houses suffer a number of issues with air quality unless measures are taken to ventilate the house. Bacteria, mould, VOCs (volatile organic compounds), and humidity can build up to unhealthy levels if internal air is not kept fresh through ventilation. 
Careful consideration and implementation of decision making and design is necessary in order to manage the multi-factor requirements of the technical, aesthetic, psychological, economic, and societal functions of our buildings and environment. This project will explore the interplay between function using problem structuring methods such as morphological analysis; a schema that explores cross consistency of factors along with likelihood or criteria based preference. 
A particular issue to be addressed in this research is interrelationships between implementations of specific retrofitting measures and overall building performance. 
This will be done by:
This project is part of the Data-centric engineering programme's Grand Challenge of 'Data-driven engineering design under uncertainty'.
This research will be of potential interest and benefit to the construction industry, registered social landlords, building managers, and homeowners, as well as political decision makers. 
"
https://www.turing.ac.uk/events/phd-student-presentation-and-networking-may-2023,"
Via the 'Turing Connections' initiative, The Alan Turing Institute creates and signposts opportunities which are relevant to PhD students connected with data science and artificial intelligence. The aims of the wider initiative are to:
Toward these aims, PhD students across the UK are invited to register to attend PhD student presentation and networking events.
These events are free to attend. If your plans change and you can no longer attend, please cancel your registration so that other students can benefit from the opportunity.
Registration includes a complementary sandwich lunch and use of venue facilities throughout the day.
Each event agenda will change depending on attendee presentations but each day includes a networking session, student presentations and lunch. 
We aim to confirm full registrations and release an agenda for this event by 7th May. This may impact the event closure time.
Register now
"
https://www.turing.ac.uk/research/research-areas/social-data-science/social-networks,"
"
https://www.turing.ac.uk/people/guest-speakers/damian-collins,"
"
https://www.turing.ac.uk/people/research-associates/marion-oswald,"
Marion Oswald is a lawyer with over 30 years’ experience spanning several contexts: law firms, international technology businesses, central government including national security, academia, and oversight functions.  She researches the interaction between law and digital technology, with a particular interest in the human rights, ethics and use of data analytics within policing and intelligence agencies. 
Marion works in the intersection between practice, policy and academia, and her research has made distinctive contributions in respect of privacy, fair decision-making and AI and the way that data is acted upon by the public sector.  She chairs the West Midlands Police & Crime Commissioner and West Midlands Police data ethics committee and is a member of the New Zealand Police independent advisory panel on emerging technologies. 
She is an Associate Fellow of the Royal United Services Institute, a member of the National Statistician's Data Ethics Advisory Committee, and from July 2021, Specialist Adviser to the House of Lords Justice and Home Affairs Committee advising the Committee on its inquiry into new technologies and the application of the law.  She has been appointed to the independent Advisory Board of the Centre for Data Ethics and Innovation, a government expert body focused upon the trustworthy use of data and AI.  She is Associate Professor in Law at Northumbria University.
"
https://www.turing.ac.uk/research/research-projects/human-action-recognition,"
The detection and recognition of human actions from real-time CCTV video data streams is a popular challenge, with the potential to aid in video surveillance and anomaly detection of, for example, potentially hazardous scenarios in factories. This project aims to efficiently and effectively address this challenge by developing a generalised framework for interpreting human actions, combining cutting-edge deep learning technologies with ‘path signatures’. The expected outcome is a high-performance, real-time human action recognition and detection system.
Video-based human action classification is one of the most challenging tasks in computer vision. In this project the ‘path signature’ technique is being used to represent streamed body pose data, as a time-evolving tree structure as seen in the diagram below.
See the related project ‘Capturing complex data streams’ for more information about path signatures.
The path signature provides an effective description of video data over a particular time interval, as it captures the trajectories of actions, and doesn’t vary with the speed at which a video plays. This invariance leads to a significant dimension reduction in representations of video data, so that when then path signature is incorporated into deep learning algorithms, actions can be classified more quickly and accurately.
Initial work has already contributed to state-of-the-art action recognition accuracy by combining the paths signature with shallow neural networks.

The ultimate goal of this project is to develop a generalised framework by incorporating ‘path signatures’ and deep learning to interpret complex multi-dimensional streamed data of human actions.
The underpinning challenges of the project are:
Incorporating path signatures into deep learning models is expected to extract rich prior knowledge and further boost system performance. The overall proposed framework will be flexible enough to extend to other applications related to motion analysis.
In a wide range of public or workplace scenarios, for example in subway stations, street crossings, supermarkets, factories and so on, anomaly detection based on human action recognition is crucial to reduce risk from both personal and property safety. Automatic human action detection and recognition systems can aid in real-time CCTV video surveillance and reduce reliance on costly, labour-intensive manual analysis.
In addition, human-machine interaction (HMI) could benefit greatly from human action recognition. Due to the fast development and popularity of various motion sensors, smart devices can capture a wealth of multi-modal streamed data such as colour values, infrared depth, motion acceleration, etc. Interpreting this informative data using the techniques from this project could greatly improve user experiences in HMI.
Furthermore, human action recognition also has the potential to assist in behaviour analysis and athletic rehabilitation. In the rehabilitation training process of injured people, automatic human action analysis can provide auxiliary guidance, evaluate rehabilitation progresses, and prevent secondary injury.
For more information, please contact The Alan Turing Institute
[email protected]
"
https://www.turing.ac.uk/people/researchers/jim-madge,"
Jim is a Senior Research Software Engineer at the Turing. He studied in Durham for his PhD where he used Metropolis Monte Carlo simulations to explore a generic model for patchy, colloidal building blocks and optimise their design for self-assembly.
Before joining the Turing, Jim worked at STFC's Daresbury Laboratory as a developer of the DL_POLY molecular dynamics software.
At the Turing Jim is eager to promote open-source and good software development practices.
He has particular interest in high-performance computing, infrastructure automation, and configuration management.
"
https://www.turing.ac.uk/people/external-researchers/tania-allard,"
Tania is an experienced data engineer and research engineer with both academic and industrial experience. She is involved in a number of projects spanning in machine learning, linguistics, and computational engineering, among others. She also consults research groups and SMEs on how to make the most of their data and to build robust data analysis pipelines. Her main focus is complex analysis workflows, scalable data science, and reproducibility and replicability in scientific computation. She is passionate about mentoring, open source, and its community and is involved in a number of initiatives aimed to build more diverse and inclusive communities.
Tania is a Research Engineer with vast experience in academic research and industrial environments. Her main areas of expertise are within data-intensive applications, scientific computing, and machine learning. One of her main areas of expertise is the improvement of processes, reproducibility and transparency in research, data science and artificial intelligence.
Over the last few years she has trained hundreds of people on scientific computing reproducible workflows and ML models testing, monitoring and scaling and delivered talks on the topic worldwide.
She is passionate about mentoring, open source, and its community and is involved in a number of initiatives aimed to build more diverse and inclusive communities. She is also a contributor, maintainer, and developer of a number of open source projects and the Founder of Pyladies NorthWest UK.
"
https://www.turing.ac.uk/news/publications/automatic-sdg-budget-tagging-building-public-financial-management-capacity,"
The “budgeting for SDGs” (B4SDGs) paradigm seeks to coordinate the budgeting process of the fiscal cycle with the Sustainable Development Goals (SDGs) set by the United Nations. Integrating the Goals into Public Financial Management systems is crucial for an effective alignment of national development priorities with the objectives set in the 2030 Agenda. Within the dynamic process defined in the B4SDGs framework, the step of SDG budget tagging represents a precondition for subsequent budget diagnostics. However, developing a national SDG taxonomy requires substantial investment in terms of time, human, and administrative resources. Such costs are exacerbated in least-developed countries, which are often characterised by a constrained institutional capacity. The automation of SDG budget tagging could represent a cost-effective solution. We employ well-established text analysis and machine-learning techniques to explore the scope and scalability of automatic labelling budget programs within the B4SDGs framework. The results show that, while our classifiers can achieve great accuracy, they face limitations when trained with data that is not representative of the institutional setting considered. These findings imply that a national government trying to integrate SDGs into its planning and budgeting practices cannot just rely solely on AI tools and off-the-shelf coding schemes. Our results are relevant to academics and the broader policymaker community, contributing to the debate around the strengths and weaknesses of adopting computer algorithms to assist decision-making processes.
Guariso, D. and Guerrero, O. and Castaneda, G., Automatic SDG Budget Tagging: Building Public Financial Management Capacity through Natural Language Processing (March 6, 2023). Available at SSRN: https://ssrn.com/abstract=4379856 or http://dx.doi.org/10.2139/ssrn.4379856
"
https://www.turing.ac.uk/work-turing/studentships/phd-at-turing,"

The Turing’s Doctoral programme recruited 67 students from 2016 to 2019. We are proud of our Doctoral students successes, some of whom have now finished their doctorate. We continue to actively support our existing Doctoral students until completion of their PhDs.
The Turing's Doctoral programme ran in conjunction with our network of 13 partner universities. 
The Institute will not run another call for Doctoral studentships, as we focus on a national offer for our flagship Enrichment scheme, as part of the Institute’s commitment to supporting a diverse range of students to engage with the Turing. 
You can see examples of the research our Doctoral students have conducted in yearbooks from:
“I’ve been able to set up an interest group on ‘Data and Inequality’ with my supervisor, opening a dialogue with industry and policy-makers.”
Sanna Ojanperä authored the Turing’s landscaping report, Data science, artificial intelligence and the futures of work and had a letter published in the Evening Standard on the topic. Sanna’s letter responded to comments from the Bank of England’s Chief Economist, who warned that the transformation caused by automation would be more intense than Britain’s first industrial revolution, and drew a reply from Editor, George Osborne. Read more
Daniel was appointed joint lead on a collaborative project with the British Library which aimed to train software to read historical handwritten Arabic manuscripts. With partners from the British Library and PRImA Research Lab, Daniel played an instrumental role in launching a competition as part of the 16th International Conference on Frontiers in Handwriting Recognition (ICFHR 2018) held in Niagara Falls, USA. The competition focused on finding an optimal solution for accurately and automatically transcribing historical Arabic scientific manuscripts.

"
https://www.turing.ac.uk/events/event-type/turing-for-the-public,"
"
https://www.turing.ac.uk/news/publications/human-rights-democracy-and-rule-law-assurance-framework-ai-systems,"
Following on from the publication of its Feasibility Study in December 2020, the Council of Europe’s Ad Hoc Committee on Artificial Intelligence (and its subgroups) initiated efforts to formulate and draft its Possible elements of a legal framework on artificial intelligence, based on the Council of Europe’s standards on human rights, democracy, and the rule of law. This document was ultimately adopted by the CAHAI plenary in December 2021. To support this effort, The Alan Turing Institute undertook a programme of research that explored the governance processes and practical tools needed to operationalise the integration of human right due diligence with the assurance of trustworthy AI innovation practices.
The resulting output, Human Rights, Democracy, and the Rule of Law Assurance Framework for AI Systems: A proposal, was completed and submitted to the Council of Europe in September 2021. It presents an end-to-end approach to the assurance of AI project lifecycles that integrates context-based risk analysis and appropriate stakeholder engagement with comprehensive impact assessment, and transparent risk management, impact mitigation, and innovation assurance practices. Taken together, these interlocking processes constitute a Human Rights, Democracy and the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the procedural requirements for principles-based human rights due diligence with the governance mechanisms needed to set up technical and socio-technical guardrails for responsible and trustworthy AI innovation practices. Its purpose is to provide an accessible and user-friendly set of mechanisms for facilitating compliance with a binding legal framework on artificial intelligence, based on the Council of Europe’s standards on human rights, democracy, and the rule of law, and to ensure that AI innovation projects are carried out with appropriate levels of public accountability, transparency, and democratic governance.
Leslie, David, Burr, Christopher, Aitken, Mhairi, Katell, Michael, Briggs, Morgan, & Rincon, Cami. (2022). Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal. Zenodo. https://doi.org/10.5281/zenodo.5981676
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/beis,"
www.gov.uk/government/organisations/department-for-business-energy-and-industrial-strategy
"
https://www.turing.ac.uk/people/researchers/walid-magdy,"
Walid Magdy is a lecturer at the School of Informatics, the University of Edinburgh (UoE), where he leads the social media analysis group. His main research interests include computational social science, data mining, and Arabic natural language processing. He holds his PhD from the School of Computing at Dublin City University (DCU), Ireland. He has an extensive industrial background from working earlier for IBM, Microsoft, and QCRI.
Walid has two main lines of research: 1) Computational social science, where he applies quantitative and qualitative analysis methods to study online users' behaviour. This includes social media analysis for political events, online racism, e-health, and privacy. 2) NLP for social data science, where he develops the required technology for analysing big social data, including sentiment analysis, stance detection, and demographic prediction, with additional focus on English and Arabic languages. Through The Alan Turing Institute, Walid is contributing to the research priority on technology and health & wellbeing through understanding human behaviour. He is currently working with the Turing defence & security programme on building better technologies for better analysis for social media.
"
https://www.turing.ac.uk/people/researchers/zack-xuereb-conti,"
Zack is a Postdoctoral Research Associate within the Data-Centric Engineering programme at the The Alan Turing Institute and a Visiting Research Fellow within the Department of Civil Engineering at the University of Cambridge. Prior to this, Zack was a Postdoctoral Research Fellow within the Architecture and Sustainable Design (ASD) pillar at the Singapore University of Technology and Design (SUTD). He completed his PhD from SUTD during which period he also conducted research at the Harvard Graduate School of Design.
Zack's PhD specialised in the application of Bayesian inference at the intersection of architectural design and structural engineering. Zack also holds an MPhil degree in Digital Architectonics from the University of Bath and a Bachelor’s degree in Architecture and Civil Engineering from the University of Malta. He is also a registered architect and civil engineer and has practiced with several architectural design offices.
Zack is passionate about multidisciplinary research that challenges traditional data-driven modelling approaches towards a more generalisable, interpretable and data-efficient machine learning that truly caters for dynamical engineering systems in real-world engineering applications. His current work seeks to improve the generalisability and interpretability of building energy forecasting models for assisting the decarbonisation of buildings, by combining well-known governing heat transfer equations together with unsupervised Reduced Order Modelling techniques in a subspace based- Domain Adaptation framework. Incorporating Physics into data-driven techniques leverages the governing structure of mechanistic knowledge while preserves mechanistic interpretability of the underlying dynamics. Zack is a co-founder of a popular online seminar series focusing on the advancement of Physics-enhanced Machine Learning with applications across Engineering fields, hosted at The Alan Turing Institute.
Previously, Zack's doctoral research focused on developing inference mechanisms to promote human intelligence at the interface of architecture and engineering in building design. More specifically, the work presented a novel bi-directional metamodeling approach using Bayesian networks to facilitate inverse prediction of the design space in parametric finite element analysis for structural engineering, and the interpretability of engineering responses from numerical analysis.
 
"
https://www.turing.ac.uk/events/turing-lectures-changing-role-national-statistics-offices-data-driven-future,"
We are joined by the UK's National Statistician, Professor Sir Ian Diamond, for this Turing Lecture hosted by Institute Director, Professor Sir Adrian Smith and Ethics Research Fellow, Dr Mhairi Aitken
The role of data, statistics and analysis has never been more pertinent in underpinning the most critical decisions being taken across the UK and globally. National Statistics Offices are central to this and there is an urgency for them to keep rising to the challenges placed on them by government and society who are increasingly hungry for more accurate and timely information to inform and develop social and economic interventions. In a world of many producers of analysis and statistics, it is critical that National Statistics Offices continue to maintain relevance and be the trusted source of information.
National Statistics Offices, therefore, need to continue to innovate at pace developing new methods and utilising new sources of data which are more relevant to the needs of its users. In this talk, the UK’s National Statistician, Ian Diamond, will set out his view of the role of National Statistics Offices in today’s world and how that contrasts with previous decades. He will explore how National Statistics Offices are responding and innovating, moving away from traditional methods to those enabling robust statistics and analysis in real time and based on data collected from a range of sources, both traditional and novel.
"
https://www.turing.ac.uk/events/rsf-seminar-series-challenge-defining-multiple-long-term-conditions,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility project. 
Speaker: Professor Bruce Guthrie, Professor of General Practice, University of Edinburgh
Event title: The challenge of defining multiple long-term conditions
Everyone is welcome but the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn about the challenges of defining multiple long term conditions and strategies that can be applied to their respective areas of work.
Here’s a description of the session from Bruce:
‘Research examining multiple long-term conditions (MLTCs) has been limited by wide variation in how MLTCs are chosen and defined. Studies are inconsistent in which conditions are included, how they are identified, and whether the measures are simple counts or were weighted in relation to a particular outcome.
In a systematic review of 566 studies examining multimorbidity - the clinical term for the measurement of MLTCs – the number of conditions included in measures varied from 2 to 285 (median 17, IQR 11-23), with an emphasis on cardiovascular, metabolic and respiratory conditions. Mental health conditions in particular are not well represented in these studies. A quarter of studies included no mental health conditions, and no single mental health condition was included in more than half of studies.
In this talk, I will present the Delphi consensus study that seeks to harmonise definitions of multimorbidity and explore how sharing measurement strategies and methods will improve the comparability and reproducibility of research examining MLTCs.’
"
https://www.turing.ac.uk/news/turing-launches-pioneering-new-digital-twins-initiative,"
The Alan Turing Institute, the national institute for data science and artificial intelligence (AI), has today (Tuesday, 21 March 2023) launched a ground-breaking Turing Research and Innovation Cluster (TRIC) focusing on digital twins.
The TRIC-DT was announced as a pivotal part of AI UK, the national AI showcase, and aims to democratise access to emerging digital twin technology by providing open and reproducible computational and social tools for digital twin development and deployment as a national service.
A digital twin is a virtual replica of a physical process or system that is dynamically updated using data collected from real-time monitoring of its physical counterpart. Globally, digital twin technology is proving to be a powerful tool in a range of areas, and their rapid development and deployment is helping to address real-world problems.
Since its inception in 2015, the Turing has established one of the largest concentrations of digital twins research and innovation in the UK, in areas from aerospace and civil engineering to urban modelling and agricultural monitoring. Working alongside a range of partners the Turing is developing twins that are informed by the latest thinking in AI, with the aim of addressing important societal challenges and generating tangible, real-world benefits.
Digital twins are a strategic priority for the Institute and an area of research and innovation strength. Through ongoing work within the Data-Centric Engineering, Urban Analytics and AI for Science and Government (ASG) programmes, and the Environment and Sustainability interest group, The Turing has established one of the most dynamic portfolios of DT research in the UK – supported by a total investment of more than £26m. 
The TRIC-DT will support the creation of specialised expertise and new computational digital twin infrastructure, unlocking barriers that previously hindered their use.
Speaking at AI UK, Mark Girolami, Chief Scientist at The Alan Turing Institute, said “The TRIC-DT will help to democratise access to digital twin technology by providing open and reproducible computational and social tools. These tools will be available for digital twin development and deployment as a national service – freely accessible to the UK research and innovation communities. Working alongside a range of partners, the creation our first TRIC will build on the wealth of digital twin activity and investment.”
''NATS is delighted to be working with the Turing on Project Bluebird, creating a digital twin of UK airspace and exploring the use of AI in air traffic control. The TRIC-DT is a great opportunity for us to share our learnings and working practices, and we look forward to seeing the output this creates for future UK Research and Innovation communities.'' Richard Cannon, Principal Research Engineer, NATS 
The TRIC-DT will pull on both the Turing’s existing multidisciplinary academic expertise, established research engineer and community science capability, as well as on development partners across the UK. 
The Turing intends to establish knowledge exchange between a central impact hub and a network of collaborators across the academic and private sectors. Research activity will be anchored on a set of societally-important challenges.
The TRIC-DT will work closely with partner organisations and will explicitly focus on solving significant societal challenges and generating tangible societal benefits in:    
These areas map onto those of the Institute more broadly, as well as aligning with the national priority areas. The TRIC-DT, has already successfully appointed an energetic team of some of the UK’s outstanding leaders in this field, including five co-directors:  
For further information visit www.turing.ac.uk/tricdigitaltwins
"
https://www.turing.ac.uk/research/research-areas/statistical-methodology/monte-carlo-methods,"
"
https://www.turing.ac.uk/research/project-status/finished,"
"
https://www.turing.ac.uk/opportunities/engage-turing-researcher,"

As the national institute for data science and artificial intelligence, we aim to engage with as many researchers as possible with an interest and expertise in this area. This page lists opportunities available to researchers from across the UK (and in some cases the globe), whether you’re early in your career or a senior researcher.
Keep in touch with the Turing and receive our newsletter and events guide
"
https://www.turing.ac.uk/people/researchers/ricardo-silva,"
Ricardo Silva got his PhD at Carnegie Mellon in 2005, in the newly formed Machine Learning Department. He moved to UCL as a Senior Research Fellow in the Gatsby Computational Neuroscience Unit. After a year at the Statistical Laboratory at Cambridge as a postdoc, Ricardo returned to UCL to join the Department of Statistical Science as a Lecturer in 2008.
His research focuses on: 1. Algorithms for probabilistic inference: approximations for likelihoods and posterior distributions based mostly of variational approximations and Markov chain Monte Carlo. 2. Latent variable models:  measurement error models and generalisations of probabilistic principal component analysis, as well as the modelling of network data. 3. Machine learning for causal inference: identification and discovery of models with unmeasured confounding and measurement error. He has also recently focused on applied work on human movement modelling, including partners such as TfL (for traffic data), UCL Institute of Behavioural Neuroscience (human navigation strategies) and Stratagem Ltd (sports modelling).
"
https://www.turing.ac.uk/people/researchers/serena-lambley,"
"
https://www.turing.ac.uk/people/researchers/alexander-dawid,"
"
https://www.turing.ac.uk/people/guest-speakers/rose-woolhouse,"
Rose Woolhouse is the Head of International in the UK Government's Office for AI - a joint unit between the Department for Business, Energy and Industrial Strategy (BEIS) and the Department for Digital, Culture, Media and Sport (DCMS).  Rose oversees engagement with key multilateral forums and bilateral partners to help support the delivery of the UK's National AI Strategy.  She has previously had roles in energy and research policy in Brussels, operational change management and international trade. She holds a Masters Degree in European Interdisciplinary Studies with a focus on digital policy from the College of Europe (Natolin) and a First-class honours MA in Psychology and Linguistics from the University of Edinburgh/McGill University (Montreal).
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/inmarsat-challenge-algorithm-predict-satellite-demand-and-supply,"

Inmarsat is a pioneer of mobile satellite communications and has been powering global connectivity for nearly four decades. They offer a broad portfolio of global satellite-communications solutions and their customers include governments, airlines, broadcast media, the oil and gas industry and humanitarian aid agencies among others.
Inmarsat is exploring the potential to roll out a new ‘dynamic booking’ system for its customers and Data Study Group researchers were challenged to develop an algorithm that would enable the company to predict demand and supply for its services within a short time horizon.
Siddhartha Ghosh, Head of Data Science, explains: “This is a commercially motivated challenge which will enable our networks to operate more efficiently. Internally it will allow better provisioning of capacity to match demand and supply. For example, media organisations often require large bandwidth for a short space of time. If you know the impact on total demand you might offer a good price based on that impact. A dynamic booking system would allow us to service customers requesting bandwidth on the fly.”
The team of 13 researchers with expertise in machine learning, applied statistics and applied mathematics came up with a range of potential forecasting algorithms.
Siddhartha says: “The researchers came up with a variety of ideas on how to approach the challenge. The problem was not solved, but it was definitely a good start. There are two proposed machine learning solutions that I intend to try internally and compare with the baseline. Within the allotted time it wasn’t possible for them to work on the extended datasets.”
""The best thing was the genesis of ideas and ideation – it was very enriching""
Siddhartha Ghosh, Head of Data Science, Inmarsat
After five intense days of hard work, Siddhartha is satisfied that it was time well spent. He says: “The experience definitely met my expectations – it was absolutely worthwhile. Everyone was really motivated and enthused and they came up with lots of good ideas. My interaction with the Institute was really positive and our facilitator was supportive throughout. The main highlights for me were the brainstorming sessions which I found excellent. The best thing was the genesis of ideas and ideation – it was very enriching. I liked discussing these and helping to shape them.”
Finally, he says: “We brought a data science problem to the Institute, so data science is definitely the answer. If the opportunity arises in future I would definitely do it again.”
In terms of advice to any partners or organisations considering a collaboration with Institute, Siddhartha advises: “To get the most out of the experience I would say that the clarity of the question is key – this needs to be well posed. Availability and high quality data is also essential.”
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/microsoft-research-cambridge,"
Microsoft Research Cambridge website
"
https://www.turing.ac.uk/news/publications/quantifying-technological-foundations-economic-complexity,"
The problem of inferring the technological structure of production processes and, hence, quantifying technological sophistication, remains largely unsolved; at least in a generalizable way. This reflects in empirical literature that either focuses on outputs instead of transformative processes, or preemptively assumes the nature of technology. Building on recent advances in information theory, we develop a method to quantify technological sophistication. Our approach explicitly addresses the transformative process where inputs interact to generate outputs; providing a more direct inference about the nature of technology. More specifically, we measure the degree to which an industry's inputs interact in a synergistic fashion. Synergies create novel outputs, so we conjecture that synergistic technologies are more sophisticated. We test this conjecture by estimating synergy scores for industries across nearly 150 countries using input-output datasets. We find that these scores predict popular export-based measures of economic complexity that are commonly used as proxies for economic sophistication. The method yields synergistic interaction networks that provide further insights on the structure of industrial processes. For example, they reveal that industries from the tertiary sector tend to be disassortative sector-wise. To the extent of our knowledge, our findings are the first data-driven inference of technological sophistication within production processes (on an industrial scale). Thus, they provide the technological foundations of economic complexity and represent an important step toward its empirical microfoundations.
"
https://www.turing.ac.uk/people/researchers/frederik-heber,"
Frederik was a visiting researcher at the University of Edinburgh collaborating with Ben Leimkuhler on adapting sampling methods to neural networks and exploring their loss manifolds. He received his PhD in Applied Mathematics at the Rheinische Friedrich-Wilhelms-Universität, Bonn in Germany, in 2014 under the supervision of Michael Griebel. There, he learned a lot about material science, ab-initio methods in quantum chemistry, C++ programming, third-party projects, and proper experimental practice in computational sciences.
He has held a post doc position in Thomas Schuster's group on inverse problems in Saarbrücken from 2013 till 2016, where he worked on matrix factorization and hyperspectral imaging.
Frederik's research interest are multitudinous but the common theme is the underlying problem of optimization of non-convex functions. He worked on extending the well-known Conjugate Gradient method to Banach spaces and applied the resulting method to low-rank matrix factorization in the context of hyperspectral images, see the library BASSO at https://github.com/FrederikHeber/BASSO.
He has a strong interest in material sciences and biomolecules. He helped to realise a software library of various fast solvers for the Coulomb problem (http://www.scafacos.de/). Moreover, he maintains a large software suite (http://www.molecuilder.de/) on constructing and equilibrating molecular systems and boot-strapping molecular simulations. There, the holy grail is to explore the potential energy surface whose knowledge would allow to assess material stability, devise chemical reactions, and ascertain reaction paths in biological processes. While local minima are found by optimization, global minima require enhanced sampling and other strategies.
At the moment, he is bringing these sampling methods to the realm of data science, where they will (hopefully) help to explore the loss manifold of (deep) neural networks and unravel the baffling mystery why neural networks work so well.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/tist,"
The International Small Group and Tree Planting Program website
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-mrc-clinical-trials-unit-ucl,"
Monitoring in clinical trials: Identifying poor performance at recruitment sites
This Data Study Group (DSG) Challenge aimed to build a human-in-the-loop machine learning model that could assist the Medical Research Council Clinical Trials Unit (MRC CTU) at University College London, in identifying clinical trial sites that are performing poorly and require monitoring intervention. When running clinical trials, patient safety is the greatest priority, with robust operation a secondary but nevertheless key feature in ensuring successful outcome of the trial. Poor performance therefore relates to any trial conduct that could compromise safety or lead to variation in how a trial is conducted between sites, making monitoring trials a large and complex challenge.
The CTU is responsible for ensuring that all clinical trial sites gather quality data, while also protecting patient rights and well-being. Currently risk-based monitoring is used to evaluate whether individual sites are following trial protocol. This approach relies on pre-defined metrics, such as those measuring the correct and timely completion of trial forms, as indicators of site performance and uses a trigger system based on those indicators to alert the CTU when a site visit is considered necessary. However, a recent study found that the intuitively developed pre-defined metrics were not sufficient at discriminating sites requiring a site visit, where relevant features may have been missed.
The purpose of this DSG challenge was therefore to undertake an initial investigation into whether machine learning approaches can improve identification of poorly performing sites.
Data Study Group team. (2022, September 13). Data Study Group Final Report: MRC Clinical Trials Unit, UCL. Zenodo. https://doi.org/10.5281/zenodo.7075637
PI: Louise Coutts
2.3 MB
"
https://www.turing.ac.uk/events/rsf-seminar-series-new-findings-age-dependent-topic-modelling-multiple-long-term-conditions,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility project. 
Event title: New findings from age-dependent topic modelling of Multiple Long-term Conditions in UK Biobank
Speakers: 
Chris Holmes, The Alan Turing Institute
Xilin Jiang, University of Cambridge
Longitudinal data from electronic health records (EHR) has potential to improve clinical diagnoses and personalised medicine (Abul-Husn et al. 2019 Cell). A key challenge is to extract biologically useful information from age-dependent patient disease histories spanning many distinct diseases.
In this session, we will introduce an age-dependent topic modelling (ATM) method that provides a low-rank representation of longitudinal records of hundreds of distinct diseases in large EHR data sets. The model learns, and then assigns to each individual, topic weights for several disease topics, each of which reflects a set of diseases that tend to co-occur as a function of age (quantified by age-dependent topic loadings for each disease). We use variational Bayesian methods to estimate the parameters of the model. Simulations show that ATM outperforms other approaches in distinguishing distinct age-dependent comorbidity profiles.
Results of the ATM applied to 282,957 UK Biobank samples, analysing 1,726,144 disease diagnoses spanning 349 diseases with ≥1,000 incidences identified 10 disease topics optimizing model fit. Topic loadings were highly age- dependent, implying differences in disease etiology for early-onset vs. late-onset disease. Further analyses reveal distinct polygenic risk score (PRS) subtypes across the topics.
"
https://www.turing.ac.uk/news/supercomputing-2018,"
I recently attended this year's Supercomputing conference, together with Tomas Lazauskas.
The conference itself saw just over 13000 attendees arrive in Dallas, TX, for part academic conference (on all aspects of high-performance computing) and part exhibition, in the huge Kay Bailey Hutchinson convention centre. The exhibits filled most of 500000 sq ft of exhibition hall, showcasing the latest HPC and networking hardware, alongside software providers, academic groups and national labs.
Aside from keynotes describing impressive US and Chinese systems, now approaching exaflop performance, there were presentations of machines based on a variety of emerging architectures, including some work from Intel related to a collaboration with Turing researchers, Arm-based HPC systems and dedicated hardware for neuromorphic computing.
There was a strong presence from cloud computing providers, and one session that particularly stood out for me was a panel discussion of on-premises HPC systems vs. the cloud.
The Turing Institute has a large donation from Microsoft in the form of Azure credits, but also has access to several UK HPC facilities nationally and through our partner universities, and some systems of our own (including one provided by Intel and a Cray Urika system), so this discussion was particularly relevant to us.
The overall conclusion of the panel seemed to be ""use the right tool for the job,"" but it is worth unpicking exactly what that means.
Some workloads are hard to move off-premises for security reasons, although there are some potential future technological solutions which could provide additional security models to address this in some cases.
Local HPC resources are often procured with a particular set of workloads in mind, traditionally dominated by large-scale physics codes, but increasingly being joined by machine learning tasks. High speed, low latency networking is the norm for these facilities, and there is invariably a local team of dedicated HPC administrators, and increasingly RSE teams who can work with users to extract the best performance from their simulation or analysis.
This isn't quite matched by current cloud offering. Azure batch is probably the closest to HPC practitioners' expectations in this regard. By contrast, cloud providers can offer high availability with no queuing, redundancy and always-current (and perhaps novel) hardware, detached from academic funding cycles, and with unmatched scalability for smaller jobs.
These smaller jobs are also used to backfill local HPC systems (and can therefore be priced competitively). Some HPC centres now seem to be aiming towards a model based on providing local resources targeted at the specific large-scale jobs for which they are ideally suited, but with a burst capability for offloading jobs to the cloud to match demand, perhaps even transparently through the same user interface.
New possiblilities and challenges are emerging in this space all the time. After all, cloud computing in the HPC world was relatively unknown as little as five years ago.
SC18 was a great event, and a fantastic opportunity to keep up to date with the developments in the field. I look forward to attending again in the future, as do colleagues from Turing Research Engineering.
Thanks to the Data Science at Scale Programme for funding this visit as part of my work on the Optimisation for Network Analysis project.
"
https://www.turing.ac.uk/people/doctoral-students/anujan-poologaindran,"
Anujan is a PhD student in the Brain Mapping Unit at the University of Cambridge and The Alan Turing Institute. Training with Professor and Physicist John Suckling, his doctoral project aims to map brain circuits governing cognition to guide neurosurgery and rehabilitation. Prior to this, Anujan trained at Vancouver Functional Neurosurgery under the guidance of Professor and Neurosurgeon Christopher Honey where he developed and refined circuit-based therapeutics for neurological and psychiatric illnesses. Outside academia, Anujan is actively involved in classical Indian music and youth outreach.
Anujan’s scientific work intersects at functional neurosurgery, behavioural neuroscience, and brain mapping. His main work focuses on the frontal lobes of the brain and their connections with other regions. To date, modern medicine is dependent on clustering clinically observed symptoms and their evolution over time. This observer-driven approach is not biologically-driven and does not naturally align with the brain’s physiological dynamics. To address this problem, Anujan uses connectomics ('brain cartography') coupled with surgery to establish causal relationships between brain structure/function and resultant behaviour. This work aims to guide the development of circuit-based therapeutics for drug-resistant neurological and psychiatric illnesses. 
At the Turing, Anujan will apply AI and machine learning tools to advance our understanding of neuroscience and the practice of medicine. 
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence,"
"
https://www.turing.ac.uk/people/alicia-cork,"
Alicia is a third year PhD student in the Psychology Department at the University of Exeter. She holds a BSc in Psychology from the University of Bath. Alicia's research uses computational methodologies to test social psychological hypotheses using naturally occurring online data. More specifically, she uses natural language processing techniques to ascertain whether it is possible to determine social group characteristics. On a more general level, she wishes to encourage other social scientists to branch into more novel methodologies for studying psychological processes online. She is excited for how the collaboration between computer scientists, mathematicians and social scientists will enable the advancement of computational social science as a discipline. 
Social psychologists have developed and tested theories of group and individual psychology for decades, however few theorists have sought to test these well-established theories using naturally occurring online data. Whilst much social psychological research takes place in laboratory conditions or using self-report survey data, Alicia wishes to combine these traditional offline approaches with more computational online approaches to better understand how we can test theories online and better understand online human behaviour.
At present, Alicia is studying patterns in the linguistic style of online forum posts in order to understand how influential an individual is in relation to their group. Moreover, using theories of social psychology, she wishes to understand whether it will be possible to predict which individuals are likely to become influential in the future through the study of language. Her research uses basic natural language processing techniques, machine learning and social network analysis techniques in order to compare and contrast her linguistic approach to the current approaches used to study social influence online. She is also working alongside the National Crime Agency in order to apply her novel methodology within the criminal sphere. 
Whilst at the Turing, Alicia wishes to develop her understanding of current approaches to social influence from a network theory perspective. In parallel to this goal, she wishes to gain a better understanding of the more complex approaches to natural language processing. 
"
https://www.turing.ac.uk/research/research-projects/climate-security-index,"
Climate variability is widely impacting natural and human systems. One of the many impacts is its potential threat to human security. The role of climate variability as a possible cause of violent conflict has been coming to the forefront of public and scientific debates. In this so-called Climate Security Nexus, climate variability acts as a multidirectional threat multiplier, aggravating existing vulnerabilities of people and communities. We collaborate with CGIAR Focus Climate Security to devise a quantitative framework to develop a Climate Security Index (CSI), for measuring and monitoring climate security vulnerability. Primarily meant for long-term planning and decision making for resilience building, it incorporates a broad range of drivers of the Climate Security Nexus, including those from climatic, conflict, socioeconomic, agricultural, and contextual (i.e., mitigating factors, adaptive capacity) dimensions, to indicate climate-security risks at subnational level. Emphasizing the role of water, land and food systems, a systemic approach, based on innovative modelling tools is envisioned, to account for the underlying relationships within the Climate Security Nexus.
"
https://www.turing.ac.uk/sessions/ai-governance-in-the-uk-and-around-the-world-international-perspectives-on-regulation-and-standardisation/,"


"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/multi-agent-systems,"
The software agent paradigm emerged in the late 1980s, when it was envisaged as a new mode of user interface. Instead of software acting as a dumb, passive recipient of our instructions, the dream was to build software applications that would instead act as pro-active assistants, working with us on our everyday tasks. This approach is now standard: we have software agents on our phones and in our homes – Siri, Alexa, and Cortana are all manifestations of the software agent dream. A natural development of the software agent paradigm is the idea that these agents will interact not just with humans, but with each other, and it is this idea that gives rise to the paradigm of multi-agent systems.
Traditionally, AI focussed on attributes of intelligent behaviour such as learning, planning, and problem solving. Multi-agent systems (MAS) emphasise a different set of skills. Specifically, to make the dream of multi-agent systems a reality, then we will need to build AI systems that have social skills – the ability to cooperate, coordinate, and negotiate with other software agents and with humans in order to autonomously achieve delegated goals. Considerable emphasis has recently been given to issues of AI safety, and multi-agent systems bring with them their own challenges – in particular, the dynamics of multi-agent systems can be unpredictable and difficult to understand. To make safe multi-agent systems a reality, we need to address these challenges head on.
Multi-agent systems are now a standard technique in many areas, for example in agent-based financial modelling, agent-based epidemiological modelling, automated trading, business process modelling, security resource allocation, and multi-robot systems. The work of this programme will have impacts across this spread of applications.
The MAS interest group at The Alan Turing Institute provides a central meeting point for UK-based researchers in universities, industry, and defence who are interested in MAS research. We maintain a UK-wide MAS mailing list through which list members can communicate relevant opportunities and events. We organise regular events (spotlight presentations, UK-MAS symposia) and maintain a virtual MAS labs map. Find out more about the interest group.
Mike Wooldridge [email protected]
You can find out more information about the multi-agent systems interest group by joining the mailing list.

"
https://www.turing.ac.uk/people/researchers/kathyrn-r-fair,"
Kathyrn R. Fair is a postdoctoral research associate with the computational social science group in the public policy programme, working on the Shocks and Resilience project. Prior to this, she was a postdoctoral research fellow in the School of Environmental Sciences at the University of Guelph. Kathyrn holds a PhD in Applied Mathematics from the University of Waterloo.
 
Her current work focuses on labour market dynamics and aims to move beyond classical economic models and methodologies to improve our understanding of how shocks to the economy impact labour mobility.
Kathyrn has previously developed models describing the coupled dynamics of human behaviour and SARS-CoV-2 transmission, climate change impacts on forest-grassland mosaic ecosystems, the growth of international food trade networks, and the interconnectedness of human population growth, food production, and trade. She also collaborated on projects exploring the gender wage gap within Canadian academia, definitions of resilience, and the mechanisms by which scientific consensus can be achieved.
"
https://www.turing.ac.uk/about-us/equality-diversity-and-inclusion/women-data-science-and-ai,"
"
https://www.turing.ac.uk/news/publications/source-code-disclosure-primer-trade-negotiators,"
Software is embedded into more and more products and services. Lines of code lie behind devices in our homes and pockets. Businesses are also increasingly relying on software-based tools to improve their operations, design products, set prices, and advertise goods and services. As software's presence in our lives and businesses grows, policy-makers have the difficult task of striking the right balance between encouraging innovation and minimising the risks that software-based solutions bring. Software’s expanding presence in internationally traded goods and services means that trade negotiators are also being tasked nowadays with finding this fine balance.
Recent trade agreements incorporate specific provisions related to source code -- namely the lines of code that lie behind every software product and service. These provisions prohibit governments and their agencies from requiring the transfer of, or access to, source code. This general prohibition matters. On the one hand, it encourages international trade by reassuring foreign software developers that they will not have to disclose the source code underlying their products and services. On the other hand, this general prohibition, even when accompanied by extensive exemptions, places limitations on the powers of governments and their agencies to examine source code.
The aim of this paper is to provide a primer to trade negotiators on source code disclosure. It is the first comprehensive paper to cover what source code is, provide a landscape mapping of the possible motivations for government-mandated source code disclosure requirements, and examine how recent trade agreements have handled source code disclosure. In the trade agreements that we examined, we find that the exceptions to the general prohibition on requiring access to source code are comparatively narrow and do not cover the multitude of reasons why public authorities might want access to source code. We also note a tendency in recent agreements to expand the scope of the general prohibition on source code access to include algorithms. We recommend that trade negotiators give thorough consideration to all legitimate reasons why public authorities might need access to source code, and exercise caution in expanding the scope of the general prohibition on source code access to include algorithms.
Dorobantu, C., Ostmann, F., & Hitrova, C. (2021). Source code disclosure: A primer for trade negotiators. In I. Borchert & L. A. Winters (Eds.), Addressing Impediments to Digital Trade (pp. 105-140). London: CEPR Press.
447.7 KB
"
https://www.turing.ac.uk/people/external-researchers/nicholas-fuggle,"
Nicholas Fuggle is a Dunhill Medical Trust research fellow and rheumatology regsitrar at the MRC Lifecourse Epidemiology Unit (LEU), University of Southampton. After completing his medical degree and intercalated BSc at Imperial College and a clinical research fellowship at St George’s University London he was awarded an NIHR academic clinical fellowship in rheumatology at the MRC LEU.
His current research centers on the epidemiology of musculoskeletal diseases within the Hertfordshire Cohort (a group of community-dwelling older adults) and focuses on the epigenetic determinants of accelerated ageing and using imaging processing to analyse musculoskeletal imaging modalities.
"
https://www.turing.ac.uk/people/guest-speakers/karen-croxson,"
"
https://www.turing.ac.uk/people/business-team/jennifer-ding,"
Jennifer Ding is a Senior Researcher for Research Applications at The Alan Turing Institute. Previously, she was a startup founder and Data Scientist at several public interest tech companies where she worked with Computer Vision and Natural Language Processing to develop research and product deliverables for Fortune 50 and government customers.
She holds a B.S./B.A. in Electrical Engineering and Policy Studies from Rice University, as well as an M.Eng in Computer Science from Cornell University.
Her research interests lie in algorithmic ethics and privacy by design, and she has a personal interest in small data projects and data visualisation.
"
https://www.turing.ac.uk/news/publications/product-form-estimators,"
We introduce a class of Monte Carlo estimators that aim to overcome the rapid growth of variance with dimension often observed for standard estimators by exploiting the target’s independence structure. We identify the most basic incarnations of these estimators with a class of generalized U-statistics and thus establish their unbiasedness, consistency, and asymptotic normality. Moreover, we show that they obtain the minimum possible variance amongst a broad class of estimators, and we investigate their computational cost and delineate the settings in which they are most efficient. We exemplify the merger of these estimators with other well known Monte Carlo estimators so as to better adapt the latter to the target’s independence structure and improve their performance. We do this via three simple mergers: one with importance sampling, another with importance sampling squared, and a final one with pseudo-marginal Metropolis–Hastings. In all cases, we show that the resulting estimators are well founded and achieve lower variances than their standard counterparts. Lastly, we illustrate the various variance reductions through several examples.
J. Kuntz, F. R. Crucinio, and A. M. Johansen. Product-form estimators: exploiting independence to scale up Monte Carlo. Statistics and Computing, 32(12):1–22, 2022.
1.3 MB
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/refinitiv,"
refinitiv.com
"
https://www.turing.ac.uk/events/structured-missing-data-grand-challenges-learning-multi-modal-data-scale,"
Missing data are an unavoidable complication in many machine learning tasks. When data are ‘missing at random’ there exist a range of tools and techniques to deal with the issue. However, as machine learning studies become more ambitious, and seek to learn from ever-larger volumes of heterogeneous data, an increasingly encountered problem arises in which missing values exhibit an association or structure, either explicitly or implicitly. Such ‘structured missingness’ raises a range of challenges that have not yet been systematically addressed, and presents a fundamental hindrance to machine learning at scale. 
During this event you will hear from Dr Robin Mitra, Associate Professor of Statistics at UCL and Dr Sarah McGough, Senior Data Scientist at Roche. They will outline the current literature in missing data and introduce the concept of ‘structured missingness’ using both theory and real-world examples in large healthcare datasets. Inspired by these, they will present some of the biggest challenges and opportunities to advance the field of missing data and spotlight ongoing research led by the Turing-Roche partnership.
Register in advance for this meeting
By registering for the event, you are agreeing to the events code of conduct.
"
https://www.turing.ac.uk/people/guest-speakers/charlotte-deane-0,"
Professor Charlotte Deane is the Deputy Executive Chair of the EPSRC, on secondment from the University of Oxford where she is currently Professor of Structural Bioinformatics, and was Head of the Department of Statistics. Professor Deane is the UKRI director for the COVID-19 response
"
https://www.turing.ac.uk/people/guest-speakers/daniel-sabanes-bove,"
Daniel Sabanes Bove studied statistics and obtained his PhD in 2013 for his research work on Bayesian model selection. He started his career in Roche as a biostatistician and then worked at Google as a Data Scientist before rejoining Roche, where he is currently leading the Statistical Engineering team that works on productionisng R packages, Shiny modules and how-to templates for data scientists.
Daniel is (co-)author of multiple R packages published on CRAN and Bioconductor, as well as the book Likelihood and Bayesian Inference: With Applications in Biology and Medicine. He is currently chairing the ASA BIOP working group on Software Engineering (SWE WG).
"
https://www.turing.ac.uk/research/impact-stories/a-right-to-explanation,"
​
Concerns are growing about how our personal data is used, particular in algorithms making life-altering decisions; from loan and health insurance applications to recruitment and criminal justice systems. The EU’s General Data Protection Regulation (GDPR), which came into effect on 25 May 2018, aims to give individuals more control over the usage of personal data.
However, whilst the GDPR and the UK’s related Data Protection Bill have been produced to make these processes more transparent, researchers at the Turing identified significant gaps in the legislations. They found that a legally binding ‘right to explanation’ for decisions made by algorithms was missing from the proposed GDPR, which could have harmful ramifications for people affected by such algorithmic processes.
Through publishing multiple papers and providing evidence to UK government committees, the researchers provided extensive recommendations for how meaningful, legally binding explanations could be implemented. Such explanations would enable someone to understand a decision, and possibly contest it, without the need for technical, trade-secret information about the process to be disclosed.
These recommendations have been cited by both the House of Lords and the official EU body that is providing expert guidance on how the GDPR should be implemented.
In 2016 Dr Sandra Wachter, now a Turing Fellow, was working as a research assistant at the Turing. “I was helping basically everybody who was working in data ethics”, she says. This included setting up the Data Ethics Group with Turing Fellow Luciano Floridi. The group leads the Institutes’ research into understanding the ethical and societal implications of data.
Dr Wachter, a trained lawyer, became interested in potential loopholes in the GDPR concerning individuals’ rights to explanations of algorithmic decision-making. “It wasn’t supposed to be my actual project”, she confesses, “I worked on it at nights and on weekends because I felt so passionately about it.” Along with Professor Floridi and Turing Research Fellow Brent Mittelstadt, she published the paper ‘Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation’ in January 2017.
The paper showed that the GDPR as it was presented would only require data controllers – everyone who handles personal data – to merely inform individuals about the usage, and basic design, of algorithmic decisions made about them. It wouldn’t require them to give individuals proper explanations of the details of, or steps taken in, the algorithm. The paper garnered international attention and led to Wachter and her colleagues being at the forefront of the subsequent discussion of the issue.
Despite the Brexit referendum result, the UK announced the Data Protection Bill in August 2017, which aims to transfer the GDPR into UK law to ensure the regulations hold in the UK too. This meant that issues within the GDPR identified by Turing researchers were also present in the Data Protection Bill. Triggered in part by the concerns raised in the Turing researchers’ paper, the House of Commons’ Science and Technology Committee subsequently launched a public inquiry into algorithms in automated decision-making.
Dr Wachter contributed to written evidence provided by the Turing in response to this inquiry, and also provided oral evidence to the committee in November 2017, summarising the problems found in the legal framework. She also provided oral evidence for the related House of Lords Committee on AI. “There is so much resistance to making a right to explanation legally binding”, Wachter explains, “so we felt the need to show decision makers that it is feasible.”
There have been major concerns voiced about providing transparency in algorithms. Reasons such as feasibility (how to go about explaining an algorithm), understandability (algorithms are complex and hard to understand for non-experts), IP infringement (companies are reluctant to give away their data or code), and exploitability (too much transparency could allow people to manipulate or game the system.)
What Dr Wachter and her colleagues, Brent Mittelstadt and Turing Fellow Chris Russell, showed in their paper was that all of these concerns could be mitigated by the use of so-called ‘counterfactual explanations’. These essentially take a decision-making algorithm and show what the outcome would have been for an individual if they had had certain different attributes, without disclosing too much technical, trade-secret information about the process itself. “You only need to give very limited information about what’s happening,” Wachter explains, “to enable someone to understand the decision, to possibly contest the decision, and, if a decision proves to be fair, to know what they need to change in order to alter future decisions in their favour.”
She gives the example of being denied a loan, and this having a significant effect on an individual. The person would go to the financial institution involved and request an explanation. A counterfactual analysis and explanation of the algorithm used might tell the person that the only factor that affected the decision was salary, and if this were higher by a certain amount they would get the loan.
See our Impact Story ‘Fairer algorithm-led decisions’ to see how Turing researchers are applying this thinking to ensure discrimination, based on factors such as race, is eliminated from automated systems.
“Engaging and explaining what is going on with algorithmic decision-making [may help avoid] repercussions for the UK economy”
Sofia Olhede, Director of the UCL Centre for Data Science
The value of this work is being recognised by experts across academia and technology. Hetan Shah, Executive Director of the Royal Statistical Society, says, “The work has been a welcome contribution to the current debate. There is considerable discussion about how far we can peer into ‘black box’ algorithms, but this may well prove to be a cul-de-sac. Approaches like counter-factual explanations are, in my view, likely to yield better results.”
Sofia Olhede, Director of the UCL Centre for Data Science, adds that, “Dr Wachter’s work is very important in tying our everyday understanding to mathematical concepts such as counterfactual fairness. Unless we make the effort in engaging and explaining what is going on with algorithmic decision-making, society may not adopt it, which would have unfortunate repercussions for the UK economy.”
The Article 29 Working Party is the official body that gives guidance to EU member states on how to interpret the GDPR and makes recommendations to the public on matters relating to the processing of personal data. In their final guidelines on automated individual decision-making and profiling they cite the work of Dr Wachter and her colleagues when giving their recommendations on explaining algorithms.
In addition, the House of Lords has proposed amendments to the Data Protection Bill which include many of the recommendations proposed by Turing researchers. See parliament’s dedicated page for the current progress of the bill.
These citations show the significant impact that the work of Turing researchers is having on legislation and subsequently the rights of individuals throughout Europe. This work also has the potential to have wider impact on trust and accountability in AI systems in general. Dr Wachter believes that, “because AI is so agile, every area of life that deeply affects people is potentially linked to the need for explanations.”
“We aim to provide a clear path forward for how to achieve meaningful accountability and transparency”
Sandra Wachter, Turing Fellow
Examples of this include in robotics and in healthcare – doctors using AI systems to predict diseases or propose treatment plans need a trusting relationship with AI systems, and to know why a certain course of action has been identified or chosen. There is a need for precise regulation and better methods to certify, explain, and audit inscrutable systems.
Dr Wachter and her Turing colleagues are committed to ensuring this happens. “An open dialogue with industry, government, academia and civil society is needed on how to proceed, and we aim to provide a clear path forward for how to achieve meaningful accountability and transparency in algorithmic and AI systems.”

In September 2018 Google announced that they have included this work's contributions to counterfactual explanations into a 'What-If Tool' for their TensorFlow™ machine learning framework. The tool is designed to let non-programmers understand what their machine learning is doing, with the ability to manually edit examples from datasets and show how the model’s predictions change as any single feature is changed. As detailed above this functionality will help ensure that algorithms produced by software using TensorFlow™ will be easily explainable.
TensorFlow™ is an open source software library for high performance numerical computation and is one of the most widely used deep learning frameworks in the world, powering, for example, all the machine learning in Android phones. Therefore the work of these Turing researchers will be implemented in a huge number of machines around the world as well.
Read more on the Google blog post, and see the code and references to the Turing researcher's work.

"
https://www.turing.ac.uk/people/researchers/victoria-nockles,"
Dr Victoria Nockles is the Dstl Radio Frequency Fellow at The Alan Turing Institute. She is organiser of the Institute’s machine learning for radio frequency applications special interest group. In previous roles at DSTL she worked as a SAR scientist on novel techniques for geospatial analysis and SAR capability development, and as a data science researcher. Victoria holds a DPhil from the University of Oxford in Earth Sciences specialising in studying intercontinental deformation using InSAR and a degree in geophysics from the University of Edinburgh. Prior to her PhD she worked in the oil and gas industry as a research geophysicist at PGS on the production and development of algorithms for the processing and inversion of electromagnetic data. Her research interests include machine learning, co-design of hardware and software, SAR, and the synthetic modelling of complex physical processes.
"
https://www.turing.ac.uk/research/interest-groups/turing-trustworthy-ai-forum,"
The benefits of AI are relatively well-understood, including driving revenue, reducing costs, and improving the efficiency and delivery of products and services. However, when it comes to deploying AI systems in the real-world, there are significant barriers to progress and obstacles to AI adoption. Some barriers are cultural – there are still too few executives and board members of organisations who understand how to deliver AI-driven systems in a safe and ethical way by managing their risks. Some are skills related - there is still not sufficient AI talent around, it can be hard to recruit and retain people with diverse perspectives and upskilling for existing staff in the desired areas can also be challenging. Another barrier is the data readiness level of organisations due to poor data quality or legacy systems hindering data usage.
Beyond all of these barriers, one of the main reasons for organisations not innovating using AI or deploying AI systems is the lack of understanding of how to manage the risks and ethical considerations of AI, especially given the uncertainty around regulatory thinking. Indeed whilst there are hundreds of high-level ethical AI guidelines available, operationalising these ethical principles remains hard to do.
 
The Turing, as the national institute for data science and AI, is well positioned to convene players from across sectors, together with technical experts and leading researchers in ethics and the humanities, so as to provide a safe space for dialogue on the subject of trustworthy AI design, development, and deployment. 
The purpose of this forum is to engender active dialogues between academics and organisational leaders, with real-life practice informing our researchers and our research informing best practices. Through the Turing trustworthy AI forum, Institute-affiliated researchers and staff are uniquely positioned to present on the most recent advances in related fields in an accessible way, whilst organisations from across domains can describe specific use cases, and share best practice and challenges they face in managing the risks of AI. A wide range of organisations stand to gain by hearing about latest developments across academia and industry and by sharing their thinking and questions where appropriate – this could help translate ideas into best practice and work towards meaningful and implementable action items for AI/ML that businesses can adopt with clarity.
In the longer-term, there is potential for the Turing to leverage its independent research expertise, and that of universities across its network, to help to develop guidance for implementing AI systems safely and ethically, and potentially also actually to set appropriate AI standards – providing a common understanding and clear articulation on the development and deployment of trustworthy AI. This could include, and is by no means limited to, AI governance, bias removal, best-practice development processes as well as other topics such as AI privacy and AI safety.
The Turing is very well positioned to help the UK to be a leader in developing and implementing trustworthy AI across sectors. Many private, public and third sector organisations we speak to say that they would like to have more guidance on AI and greater understanding of key issues as well as best practice sharing. They are also typically in search of independent scientific advice to help them understand AI better and they would like to see better standards and accountability in AI.
There is a great deal of enthusiasm for this across industries concerned with safety (for example, manufacturing and pharmaceuticals), privacy (for example, e-commerce) and conduct (such as financial services). Convening and bridging research-industry discussions on trustworthy AI by acting as a convener of independent AI experts is a natural extension of the Turing’s activities.
The benefits to UK Plc include, in the short-term:
In the longer-term:
The aims of the trustworthy AI forum are to:
The trustworthy AI forum differs from traditional interest groups because it goes beyond research discussions, providing a platform for practitioners and executives who currently are not able to share use cases easily between one another or have facile access to the cutting-edge in AI and/or independent scientific advice. This forum supports the sharing of ideas and best practice for participants. It does not have an advisory function for the Institute.
The first meeting of the Turing trustworthy AI forum, on the subject of human-machine teaming took place on Friday 29 October. You can read this blog post summarising the discussions.
Click here to request sign up and join
Ruth Drysdale [email protected]  
Nicolas Guernion [email protected]
Interest groups [email protected]  
"
https://www.turing.ac.uk/people/researchers/aureo-de-paula,"
Dr. Aureo de Paula received his B.A. and M.Sc. in Economics from Pontificia Universidade Catolica—RJ (Brazil) in 1996 and 2000 respectively, he then went on to complete his M.A. and Ph.D. in Economics at Princeton University in 2002 and 2006. Prior to joining University College London (UCL), he was an associate professor (with tenure) at the University of Pennsylvania and has been a visiting faculty scholar at Northwestern and Harvard universities. Aureo is an elected Fellow of the Econometric Society and the International Association for Applied Econometrics and a Turing Fellow in 2021/22. He is an elected member of the Econometric Society Council from 2022 and a director for the Review of Economic Studies. Professor de Paula is affiliated with the Centre for Microdata Methods and Practice (UK), the Institute for Fiscal Studies (UK), CEPR and several other research groups worldwide.
Professor de Paula’s work has been featured in various academic publications and he has been associate editor for various academic journals (The Review of Economic Studies, Journal of Business and Economic Statistics, Econometrics Journal, Econometric Reviews) and was a managing editor for The Review of Economics Studies. He has won the Irving B. Kravis Award for Excellence in Undergraduate Teaching at the University of Pennsylvania and the Faculty Education Award at UCL (joint with Dunli Li).
"
https://www.turing.ac.uk/people/former-enrichment-students/guillem-mosquera,"
Born and bread in the coast of Catalonia, Guillem graduated in physics at Barcelona and is now doing a PhD at Warwick University. With broad research interests in statistical physics and network theory, his current focus is on the study of human geography from the perspective of complexity science.
With ever growing computing and data-mining power available, complex networks cast light on key properties of many artificial and natural systems. Social systems are not an exception, and in an increasingly urbanized world complex networks provide a global perspective on collective human interactions. In this context, Guillem’s research focuses on networks where nodes are real cities from around the globe, whose sustained interactions give rise to trade, immigration, cultural exchange or war, amongst other phenomena. My aim is to clarify this contact patterns and investigate their relation to the geographical and economical environment of cities.
"
https://www.turing.ac.uk/news/artificial-intelligence-dominates-headlines,"
Artificial intelligence (AI) has dominated the headlines over the past six months, since the launch of ChatGPT and other Large Language Models.  
Researchers from The Alan Turing Institute have taken part in dozens of media and speaking engagements to support understanding around these rapidly emerging technologies. This post shares just a few highlights from our contributions to reporting during this time.  
Following the release of ChatGPT at the end of 2022, The Alan Turing Institute was featured in the Telegraph, the Guardian on BBC Radio 5Live and Tonight with Andrew Marr on LBC to set the scene for how this technology could work. 
Professor Mike Wooldridge, director of Foundational AI Research at the Alan Turing Institute told the Telegraph at that time that the technology is seeing “an unimaginable amount of data”. But he warned that, “because [the training data] is behind closed doors, we don’t know exactly what it’s picked up"". 

And Dr Mhairi Aitken, Ethics Research Fellow at the Turing told Andrew Marr that there are concerns that the technology could create “convincing and well-structured essays that seem like they’ve been created by a human”. The potential uses of ChatGPT in education has been a major talking point, with Mhairi speaking about this on BBC Radio 5Live and BBC Two’s Politics Live.  
Mhairi also spoke to Glamour about whether AI could advance gender equality and Cosmopolitan about using ChatGPT to in daily tasks, such as responding to emails or booking a dentist appointment.  
Mhairi was interviewed by the Daily Mail, the Today Programme and Woman’s Hour on BBC Radio 4 and New Scientist about the competition from Google’s rival chatbot, Bard, which launched in February 2023. And Professor David Leslie, Director of Ethics and Responsible Innovation at the Turing, did an interview with Bloomberg on the topic. 
Shortly after, we held a national press briefing in partnership with the Science Media Centre about ChatGPT and Large Language Models with the aim of providing journalists with a good understanding of the technology to support accurate reporting. Mike, Mhairi and Turing AI Fellow Professor Maria Liakata shared their expert views on the panel. This led to a piece in the Times highlighting the potential difficulties that historians may have years from now telling the difference between texts that were written by humans and by artificial intelligence unless a “digital watermark” is added to all computer-generated material.  
This press briefing was preceded a one-day symposium hosted by the Turing and Mike Wooldridge exploring the state-of-the-art in foundation models, how they work, what they are and will be capable of, how they are being and will be used, and how to address the many challenges – both technical and ethical – that they raise. 

Dame Wendy Hall, a web pioneer who also sits on the Government’s Artificial Intelligence Council, told MPs on the Science and Technology Committee: “We absolutely need to develop a sovereign large language model capability. The UK government needs to get behind this.”, reported the Telegraph.  
Mike told the Times: “We have absolutely world class researchers in UK academia. What they don’t have is the facility to build the systems. [A sovereign AI] will generate immensely good research, which is then going to feed back into the UK economy by way of spin off companies.” 
The Times reported that Britain has fallen behind Russia, Italy and Finland in the world league table for computing power, according to the Independent Review of the Future of Compute which the Turing contributed to. Mike told the Times: “All of science is computer science today. The problem this report is tackling is that our national capability in this space has not grown at the same speed, which to be fair is not surprising given the speed at which science and technology have progressed.” He also told the Telegraph that growth in demand for large-scale computer facilities “has been incredibly rapid, and the UK got left behind.” 
Following the publication of the Government’s AI regulation white paper in April, Institute Director Sir Adrian Smith said: “This white paper is good news for how AI is used and regulated in the UK.” The Turing has responded to many other stories about ChatGPT and Large Language Models, including an appearance from Adrian Weller on BBC PM’s programme and an interview on Bloomberg.  
Other recent and notable broadcast appearances include the Turing’s Drew Hemment on BBC Radio 4’s Moral Maze, Mike on the 30-minute Radio 4 programme, The Briefing Room, Mhairi on LBC speaking about the impact of AI on jobs and featuring on BBC Radio 4’s Today Programme in April discussing the importance of regulating against existing AI risks.  
The Government’s announcement of the £100m for a Foundation Model Taskforce is vital to help the UK to build safe and ethical AI technology. Large Language Models and Foundational Models can be immensely powerful and have the potential for great benefit, but as with all AI technologies it is crucial to understand their limitations and the very real risks associated with them. 
The Turing has also begun a programme of work around benchmarking foundation models – developing the core science to map out their capabilities in depth. It plans to significantly extend this programme of work with a range of public and private partner organisations in the year ahead.
"
https://www.turing.ac.uk/people/former-doctoral-students/michael-murray,"
Michael started his DPhil in Mathematics at Oxford in October 2017, supervised by Professor Jared Tanner. Prior to joining Oxford and the Turing he worked as a strategy consultant and before that as an engineer for a startup building products in the smart home space. Michael completed his Masters in Electrical and Electronic Engineering at Imperial College London in 2014.
Michael’s research is concerned with the mathematical analysis of algorithms used in machine learning. His general approach is to make certain structural assumptions on the data, e.g., sparse under a certain transform, arising as a result of a particular generative model or lying on some smooth manifold and analyse conditions under which various algorithms, at least with high probability, are successful. To do this he leverages a range of tools, techniques and ideas from a number of areas, including applied probability, numerical linear algebra, optimisation and discrete mathematics. He is particularly interested in how and why deep learning performs so well.
"
https://www.turing.ac.uk/people/researchers/cosmina-dorobantu,"
Cosmina joined The Alan Turing Institute in January 2018 to set up the public policy research programme. Starting from scratch in 2018, the programme now has 40+ full-time employees and 70+ affiliated researchers. Within the UK, more than public sector organisations reached out for help, and the programme's researchers conducted 100,000s of hours of work to support these organisations. Outside the UK, the programme has gained international recognition. The European Commission highlighted the value of the Turing’s public policy programme in its flagship policy report on AI, while the OECD identified the programme as a highly successful model in its primer on AI for the public sector.
Cosmina sits on the Financial Conduct Authority and the Bank of England's Financial Services Artificial Intelligence Public-Private Forum, as well as the Department for International Trade's Trade and Economy Panel. She is a regular participant in various working groups, such as the Regulators and AI working group and the London Emerging Tech Charter working group. She also sits on the Advisory Board of Patterns, a data science journal published by Cell Press. In the past, she was a member of the UK Government's Digital and Telecoms Expert Trade Advisory Group. 
Prior to joining the Institute, Cosmina was an Executive Director and co-founder of Aurora Energy Research, a company that now has 200 employees and is the UK’s leading energy market modelling and analytics firms. She also spent five years at Google, defining the company’s investment and business strategy in Eastern Europe, the Middle East, and Africa. 
Cosmina has an MPhil (obtained with Distinction) and a DPhil (PhD) in Economics from the University of Oxford. She has been a Research Associate at the University's Oxford Internet Institute since 2018.
Since joining the Turing, Cosmina’s invited lectures, talks, and public appearances include:
Cosmina has a deep interest in working with unconventional data and new modelling approaches to improve policy makers’ understanding of the world. At the Turing, she is co-leading a large-scale, 3-year research project on measuring policy impact in the Covid-19 crisis and building resilience against future shocks. The project's primary aim is to produce data, methodologies, and tools that help policy-makers make better informed choices. 
In the past, Cosmina worked with large datasets from online platforms like Google to study international e-commerce patterns as well as online advertising auctions. She led research collaborations with organisations like the European Commission, aimed at quantifying the impediments to cross-border e-commerce within the digital single market.
"
https://www.turing.ac.uk/news/publications/antilizer-run-time-self-healing-security-wireless-sensor-networks,"
Wireless Sensor Network (WSN) applications range from domestic Internet of Things systems like temperature monitoring of homes to the monitoring and control of large-scale critical infrastructures. The greatest risk with the use of WSNs in critical infrastructure is their vulnerability to malicious network level attacks. Their radio communication network can be disrupted, causing them to lose or delay data which will compromise system functionality. This paper presents Antilizer, a lightweight, fully-distributed solution to enable WSNs to detect and recover from common network level attack scenarios. In Antilizer each sensor node builds a self-referenced trust model of its neighbourhood using network overhearing. The node uses the trust model to autonomously adapt its communication decisions. In the case of a network attack, a node can make neighbour collaboration routing decisions to avoid affected regions of the network. Mobile agents further bound the damage caused by attacks. These agents enable a simple notification scheme which propagates collaborative decisions from the nodes to the base station. A filtering mechanism at the base station further validates the authenticity of the information shared by mobile agents. We evaluate Antilizer in simulation against several routing attacks. Our results show that Antilizer reduces data loss down to 1% (4% on average), with operational overheads of less than 1% and provides fast network-wide convergence.
Ivana Tomic, Po-Yu Chen, Michael Breza, Julie McCann, ‘Antilizer: Run Time Self-Healing Security for Wireless Sensor Networks’, accepted to Proc. MobiQuitous 2018.
"
https://www.turing.ac.uk/research/research-areas/social-data-science/social-media,"
"
https://www.turing.ac.uk/research/research-projects/mean-field-games-and-interacting-particle-systems,"
Large systems of interacting individuals are central to countless areas of science; the individuals may be people, computers, animals, or particles, and the large systems may be financial markets, networks, flocks, or fluids. This project is looking at the theory of mean field games (MFG) and how its study of strategic decision making in very large populations of small interacting individuals can be used to improve smart grids and statistical sampling.
The theory of mean field games (MFGs) was developed independently by mathematicians Jean Michel Lasry and Pierre-Louis Lions and engineers Minyi Huang, Roland P. Malhame, and Peter Caines in 2006.
It deals with the study of strategic decision-making in very large populations of small, randomly interacting agents or ‘players’. It is inspired by mean field theory in physics which looks at the behaviour of systems of large numbers of particles, such as in gases, where individual particles have negligible impact upon the system as a whole, but affect each other.
In MFGs this equates to the decisions each small agent makes depending on the statistical properties of the states and strategies of other players in the ‘game’ they interact with, but having a very small impact on the game’s overall outcome.
This project is developing the application of mean field game theory in different application areas.
Smart Grids
The key feature of the smart grids paradigm is that end customers actively participate in the energy market and consequently their strategic behaviour needs to be analysed (see e.g. Bagagiolo and Bauso, 2014).
Statistical sampling
Diffusion processes are at the core of many algorithms used in statistics to sample with so-called 'intractable distributions.'
[email protected]
"
https://www.turing.ac.uk/blog/just-add-time-dizzying-potential-dynamic-graphs,"
We have never been more connected. The gadgets we carry in our pockets mean that we’re only a button-press away from people on the other side of the planet. At the same time, the rise of big data has resulted in a tsunami of information about the people we chat to, the places we visit, the purchases we make. Tools are increasingly needed to make sense of these networks, but current software lacks one crucial aspect: the ability to track how connections change over time.
A new analysis tool called Raphtory, developed with funding from The Alan Turing Institute, is the first to be built specifically for this purpose. By plotting how networks evolve (known as ‘dynamic graphs’), it promises to take data science into unmapped territory. The list of potential applications is mind-boggling, from tracking changes in the meanings of words, to spotting cryptocurrency fraud, monitoring toxic online communities and understanding urban transport patterns.
Mathematicians have been interested in networks, known in the community as ‘graphs’, since the 1700s, drawing up diagrams composed of points (‘nodes’) connected by lines (‘links’). The vast majority of research has so far focused on static graphs – graphs captured at a particular moment in time, or averaged over a certain timescale.
But while static graphs can provide useful information, they ignore an important dimension: time.
“In the real world, graphs are rarely static,” says Richard Clegg, a mathematician at the Turing’s partner university Queen Mary University of London (QMUL), who is co-leader of the Raphtory project.
Dynamic graphs incorporate information about the nodes and links at different points in time, allowing researchers to find out how the graph has changed over the past month or year, for instance, or after a new node or link was added.
Current systems for analysing graphs were built with static graphs in mind. This makes them cumbersome when dealing with the time element. If you want to analyse the graph at a new point in time, you have to reload the graph at that time and reanalyse the data. Similarly, if you want to update the data, you have to reload the entire dataset, not just the data of interest. This is slow and inefficient, especially with the huge datasets that are becoming the norm in data science.
Raphtory (its name is a play on the words ‘graph’ and ‘story’) addresses these problems by automatically updating the dynamic graph as new data comes in. Crucially, the software keeps a rolling record of any changes to the nodes and links, giving the user instant access to the graph’s entire history.
Raphtory is a project that’s been brewing for some time. “I’d been wanting to build this kind of software for years,” says Clegg. In 2016, Clegg moved to QMUL and made contact with Felix Cuadrado, a computer scientist there who researches analysis methods for large-scale dynamic graphs. Fortuitously, Cuadrado (who is now a Turing Fellow) had just been awarded a grant to develop exactly the kind of software that Clegg had in mind, so the two began a collaboration, working with PhD student Ben Steer to develop a prototype. In 2019, they obtained additional funding from the Turing’s ‘AI for science and government’ (ASG) programme and hired computer scientist and programmer Imane Hafnaoui to help them sculpt the software into an open-source resource that can be used by other researchers. (Building open-source infrastructure is a key goal of the Turing’s ‘Tools, practices and systems’ programme, which the project is also part of). Meanwhile, Steer has founded a spin-out company, Pometry.
Raphtory works by splitting dynamic graphs over multiple networked computers, increasing the amount of memory for data storage and processing. When data is fed into Raphtory (this can either be previously stored data or a real-time stream), the algorithm analyses the data to figure out which section (‘partition’) of the graph needs to be updated, sending the updates to the computer that’s hosting the relevant node or link. This is trickier than it sounds.
“If you have two nodes on different computers, then any link between them travels between the two computers,” says Clegg. “You need to make sure that this link is never broken and is updated on both computers. That one thing took months to code.”
Once the data is in the system, the user is free to analyse the graph. And this is where the really exciting stuff begins: using Raphtory to solve real-world problems.
The internet offers the perfect testing ground for Raphtory. Take social networks. Every social network is essentially a dynamic graph, with users as the nodes, and interactions between them as the links.
Recent work at QMUL has focused on a social network called Gab that is similar in format to Twitter. Gab is known to attract users from the alt-right, and previous research has shown that it hosts a relatively high amount of hate speech, and users sometimes arrive after being banned from other social networks.
Gab presents a unique opportunity for studying how social networks grow, because it is possible to obtain datasets going back to the site’s launch in August 2016. Naomi Arnold, a PhD student at QMUL, is using Raphtory to analyse all of the interactions between users up until May 2018 (interactions are defined as a user replying to another user’s post, or reposting it).
Arnold’s analysis shows that the network experienced bursts of activity on certain days, coinciding with events of potential interest to the alt-right community, such as Donald Trump’s US election win in November 2016, and the ‘Unite the Right’ white supremacist rally in Charlottesville, Virginia in August 2017. Her analysis also reveals that Gab consists mostly of a small ‘elite’ of influencers writing to strangers, rather than interactions between friends.
One of the ultimate aims of this research is to understand how toxic communities might evolve within social networks. “We could use Raphtory on any social network to look for communities that are becoming more insular and interacting less with the rest of the network,” says Arnold. “This might be a sign of the community becoming more extreme in its views.” Combining this with analysis of the content being posted could provide an automatic way of flagging up any communities that are potentially problematic.
Staying in the darker realm of cyberspace, Raphtory also offers promise for tracking illegal cryptocurrency transactions. Here, the nodes of the dynamic graph are the cryptocurrency buyers and sellers, while the links are their transactions. By looking for unusual changes in the graph’s structure, it might be possible to spot fraud.
“Cryptocurrency criminals use a number of tactics to make it more difficult to trace them,” says Cuadrado, “such as using a ‘mixer’ node to combine different transactions into one pot, muddling up the data. We’ll be able to use graph analysis to look for these kinds of patterns.”
The Raphtory team has now started to work with the blockchain analysis company Chainalysis, who in November 2020 used similar techniques to recover more than US$1 billion in stolen bitcoin tied to the online black market Silk Road.
It’s not just in the virtual world that Raphtory could have impact. Transport networks, for example, are dynamic graphs. As we move around our towns and cities, we create a graph in which the locations are the nodes, and our journeys between them are the links.
To explore how Raphtory might be applied to transport data, Clegg and Cuadrado have teamed up with Susan Grant-Muller, a Turing Fellow and statistician at the Institute for Transport Studies, University of Leeds. They’ve begun to look at the effectiveness of campaigns designed to subtly influence citizen behaviour – a field of research known as ‘nudge theory’. One example is a six-month campaign run in Newcastle upon Tyne during 2018, which encouraged residents to download an app that rewarded them with points and prizes if they made greener transport choices by walking, cycling, or using public transport or electric cars instead of fossil-fuelled cars.
“Over 1,400 people downloaded the app,” says Clegg. “They agreed to have their location tracked as they moved around the city, and we’re using dynamic graph analysis to find out how the app incentives affected their behaviour.” The team is able to infer a user’s mode of transport from the time it takes them to travel between locations. Finding out what kinds of incentives led to more people walking or cycling, for instance, could help authorities to develop more effective public health initiatives.
There’s also another aspect of our day-to-day lives that is ready-made for dynamic graph analysis: language. In this case, the words are the nodes and the connections between them are the links. Two words might be linked in a graph if they appear close together in the same sentence, for instance. Visualising language in this way allows researchers to look for changes in the connections between words, and hence their meanings, over time.
To this end, the Raphtory team is working with Barbara McGillivray, a computational linguist and Turing Research Fellow based at the University of Cambridge, to analyse a dataset containing all of the words published on UK internet pages between 1996 and 2013. “One change we’re looking for is where words go from negative to positive meaning,” says Clegg, “such as with ‘bad’, ‘sick’ and ‘wicked’.”
This is a wonderful example of the relevance of dynamic graphs to real life. Now the team is hoping to foster a wider community of Raphtory users, with the aim of dreaming up even more inventive uses for the software.
“The more time I spend working on dynamic graphs, the more excited I am by their potential,” says Cuadrado. “We’re hoping that Raphtory will become the go-to tool for their analysis.”
 
This piece was edited on 23 August 2021 to reflect a change in the spin-out company’s name.
Main image: Hanson Lu / Unsplash
"
https://www.turing.ac.uk/research/research-areas/machine-learning/pattern-recognition,"
"
https://www.turing.ac.uk/people/guest-speakers/fanny-sie,"
Fanny Sie has over 20 yrs of experience as a clinician, researcher and business development professional in the area of medical devices, AI and analytics, and digital health assets. Fanny joined Roche in 2019 and progressed from the Head of Artificial Intelligence and Digital Health for Roche Canada to Head of AI and Emerging Technology External Collaborations and is the founder of the One Roche AI Centre of Excellence. A few career highlights include treating patients bedside as a Radiation Therapist, implementing emerging precision radiotherapy technologies in cancer centers globally, and scaling digital health solutions nationally to increase equitable healthcare access for Canadians through startup company creation and investment. She holds degrees in human biology, radiation sciences and a Masters in Innovation Management.
"
https://www.turing.ac.uk/people/guest-speakers/deborah-yates,"
Deborah is Programme Lead - Data Assurance at the Open Data Institute. She is responsible for setting the strategic direction of the ODI’s work to help organisations to assess, build and demonstrate trustworthiness of data and data practices, leading the delivery of related projects and building partnerships with other organisations. Deborah joined the ODI in May 2018 and has a background implementing data governance and leading data projects in the public sector. She has experience working with a range of public and private sector clients across different sectors and delivering advice to multinational organisations to improve collection, use and sharing of data. Recently her work has included overseeing work on ethical data practices which includes methods to provide assurance that ethics is being properly considered in organisations’ data practices.
"
https://www.turing.ac.uk/research/research-projects/time-sensitive-sensing-language-and-user-generated-content,"
Understanding how individuals' behaviour changes over time is very important, particularly in monitoring mental health conditions, which affect one in four people globally. This project will create AI models and natural language processing methods that are able to track the progress of an individual over time, based on their language use and other interactions through the use of digital media, called 'user-generated content' (UGC).  Major outcomes include software implementing time-sensitive sensors from heterogeneous UGC, and new tools for diagnosis and monitoring of mental health conditions, in keeping with good clinical practice. 
This project involves working with sensitive user-generated linguistic and heterogeneous content, the protection of which is a top priority of the work. The research protects the users first by anonymising all of their data that are used during the project and then by storing them in a secure environment that ensures their safety, non-transferability and allowing only certified access.
The major goal of this project is to develop personalised longitudinal sensors from individuals' language use and user-generated content (UGC) to better understand changes in behaviour over time, with applications in mental health. 
To achieve this goal, the project has the following objectives:
Finally, while the major focus of this work is around mental health, the aim is to develop models that can be incorporated in other domains leveraging UGC within the healthcare domain and beyond.
The project has a range of different stakeholders:
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/imperial-college-london,"
Imperial College London is a world top ten university with an international reputation for excellence in teaching and research.
Consistently rated amongst the world's best universities, Imperial is committed to developing  the next generation of researchers, scientists and academics through collaboration across disciplines.
Located in the heart of London,  Imperial is a multidisciplinary space for education, research, translation and commercialisation, harnessing science and innovation to tackle global challenges.
Imperial College London website
"
https://www.turing.ac.uk/people/researchers/ganna-pogrebna,"
Ganna is a decision theorist and a behavioral scientist. Before joining The Alan Turing Institute, she worked at the University of Innsbruck (Austria), the University of Bonn (Germany), Humboldt- Universität zu Berlin (Germany), University of Sheffield (UK), and Columbia University in New York (USA). She is currently a Professor of Behavioral Science at the University of Birmingham and a Research Fellow at Warwick Manufacturing Group (WMG) at the University of Warwick.
 
Blending behavioral science, computer science, data analytics, engineering, and business model innovation, Ganna helps cities, businesses, charities, and individuals to better understand why they make decisions they make and how they can optimize their behavior to achieve higher profit, better social outcomes, as well as flourish and bolster their well-being. Her recent projects focus on smart technological and social systems, cybersecurity, human-computer and human-data interactions and business models.
"
https://www.turing.ac.uk/news/publications,"

"
https://www.turing.ac.uk/research/interest-groups/computer-vision-digital-heritage,"
This group unites international researchers and heritage professionals who have an interest in using digitised image collections (maps, photographs, newspapers, books) in computer vision tasks. Applying CV to historical datasets raises issues of provenance and bias as well as processing challenges distinct from the recent or born digital images used in most computer vision work. We offer researchers at the Turing and beyond an opportunity to establish connections and build this new interdisciplinary field together. We focus on shared practices in data science around computer vision, providing a much needed center of gravity for a growing, otherwise disparate community.
Computer vision refers to a range of tasks and methods aimed at allowing computers to work effectively with images. Common computer vision tasks include the classification of images into different categories and segmenting images into smaller parts based on image content, for example, detecting which parts of an image contains a person. Computer vision methods have been applied across a broad range of scientific and engineering domains including application in self-driving vehicles, processing medical images and to process digitised documents to recognise text content (optical-character-recognition). Increasingly there is a growing interest in applying and expanding these techniques to work with heritage materials including maps, books, newspapers and paintings, which have been digitised. This raises questions about how well computer vision methods developed for other types of images perform on digitised heritage content as well as questions about how we effectively and responsibility work with collections at scale through automated methods.
Challenges: Many libraries, archives, museums, and galleries have restrictions on sharing images of their collections that have been scanned in the last few decades.
Example output: A white paper co-authored by researchers, curators, funders, and third-party digitisation partners about the future of open data access.
Challenges: Computer vision literature currently focuses on problems related to certain kinds of relatively contemporary images (web content, photographs, remote sensing data, text with standard fonts).
Example output: Case studies demonstrating open challenges for working with older materials (printed and manuscript maps, woodcuts, newspapers with historical fonts and layouts).
Click here to join us and request sign-up
The way we access information in the virtual space is changing. Discovery and exploration are no longer constrained by a keyword entered into a blank search bar. Instead, museums, libraries, archives, and galleries worldwide are welcoming a shift to 'generous interfaces' – presenting their collections online in browsable and linkable networks of information that allow users to explore and discover new ideas through meaningful and contextualised relationships.
A key component in this emerging virtual browsing landscape is 'visual search', an AI-based method for matching similar images based on their visual characteristics (colour, pattern, shape), rather than a keyword description. The Deep Discoveries project aims to create a computer vision search platform that can identify and match images across digitised collections on a national scale. The research will focus on botanically themed content, allowing us to test how far we can stretch the recognition capabilities of the technology (can it recognise a rose in a textile pattern and the same flower in a herbarium specimen? How about on a ceramic vase?).
One of the Towards a National Collection Foundation Projects, Deep Discoveries is a collaboration between The National Archives, the University of Surrey, Royal Botanic Gardens Edinburgh and the V&A. We started with ~30,000 botanical images from these institutions and our project partners Gainsborough Weaving Company, the Sanderson Design Archive, and the Museum of Domestic Design and Architecture. The wide range of partners allowed us to explore the necessary criteria for UK image collections to be linked, and to survey the searching needs of diverse audiences.
Lora Angelova (Project PI) is Head of Conservation Research and Audience Development at The National Archives, UK, where she previously worked as a Conservation Scientist. Lora’s background is in chemistry and surface cleaning of cultural and heritage materials, and current focus lies in the intersection of heritage science, conservation research, and archival practice.
John Collomosse (Project CI) is Professor of Computer Vision at the University of Surrey where he is a member of the Centre for Vision Speech and Signal Processing (CVSSP), one the UK's largest academic research groups for Artificial Intelligence with over 150 researchers. John is a Visiting Professor at Adobe Research, Creative Intelligence Lab (San Jose, CA).  John joined CVSSP in 2009.  Previously he was an Assistant Professor at the Department of Computer Science, University of Bath where he completed his PhD in 2004 on the topic of AI for Image Stylization.
Dipu Manandhar (Project Researcher) is a Postdoctoral Researcher at the University of Surrey. Dipu obtained a PhD from Nanyang Technological University, Singapore where his work was related to visual search. Currently, he is a postdoctoral researcher working with Professor John Collomosse at CVSSP Lab, University of Surrey, where he works with graphic UI design layout modelling and visual search tasks.
A talk by Stephen Law (UCL and The Alan Turing Institute) on 'Discussing the opportunities linking computer vision, urban analytics and digital humanities'. 
We are living in an age where data is ubiquitous and data science methods are pervasive. A fundamental question that arises in interdisciplinary research is how do we adapt these novel data-driven methods across multiple domains. Stephen’s talk explored these opportunities on how different data science approaches including computer vision ones can be used to measure and map our cities in urban planning but also with potential implications for the digital humanities.
Stephen Law is a Lecturer in UCL Geography and a Turing Fellow at The Alan Turing Institute. Prior to his appointment, he received a Research Fellowship at the Turing and was a senior research fellow at the UCL Bartlett. He completed a PhD in UCL Space Syntax Lab, studying the economic value of spatial network accessibility.
The CVDH SIG welcomed Keith Burghardt (University of Southern California), who spoke about 'Reconstructing Settlements Reveals Varying Costs To Urbanisation' on March 4, 2022, from 15:00-16:30. 
The structure of cities can strongly shape our lives, from improving food access and driving economies, to creating traffic jams and pollution. I will discuss my work on analysing and reconstructing how cities have developed since 1900, and what it shows about urbanisation in the US. We find that both small towns and large cities have created less-walkable environments, and as these regions have grown, they have become less dense and need more roads to connect people. That said, we also find that these problems are not universal, with larger cities consistently utilising their space more efficiently (an emergent property known as city scaling), and some regions developing more compact and walkable cities. I will discuss the reasons for this diversity, including how the data we created shows strong agreement with some theoretical models.
Keith Burghardt is a Computer Scientist at the USC Information Sciences Institute who specializes in understanding human behavior with physics-inspired models. Burghardt received a 2021 ISI Meritorious Service Award and 2016 ISI Director’s Intern Award, and has papers in several high-tier venues, such as Communications Physics. Burghardt received a PhD in Physics and B.S. in Physics (Magna Cum Laude with High Honors) at the University of Maryland in 2016 and 2012, respectively. 
On February 4, 2022 from 15:00-16:30 the CVDH SIG welcomed Leonardo Impett (Cambridge) to speak about 'Computer Vision as Symbolic Form: art history and bias in AI.'
This talk will address the specifically visual component of bias in computer vision. We will consider the consequences for the use of computer vision in digital art history, digital art, and society in the ‘age of image-machines’ more generally. Reflecting on the incompleteness and inadequacy of technical solutions for identifying and mitigating bias in computer vision, I will attempt to highlight some fundamental contributions that art history (and art historians) can bring to the table. This opens the door to digital art history projects that, by engaging critically with computer vision, enable new ways of thinking about visual culture as inscribed in pictures and algorithms. 
Leonardo Impett is assistant professor of digital humanities at Cambridge University. He was previously based at Durham University; the Bibliotheca Hertziana - Max Planck Institute for Art History; Villa I Tatti; and the École Polytechnique Fédérale de Lausanne. He has a background in computer vision, and is a PI of the collaborative “AI Forensics” project on bias in computer vision financed by the Vokswagen Stiftung.
On November 19, 2021, we were happy to announce a talk about the Scivision project based at The Alan Turing Institute. We welcome Evangeline Corcoran, Sebastian Ahnert, and Alejandro Coca-Castro to discuss the project with the CVDH SIG. 
Scivision aims to be a well-documented and generalisable python framework for applying computer vision methods to a wide range of scientific imagery.
This tool aims to foster collaboration between data owners and developers by:
We will welcome Dr Tarin Clanuwat and Professor Asanobu Kitamoto (both from the ROIS-DS Center for Open Data in the Humanities, Japan).
Professor Asanobu Kitamoto will present a talk on “Visual and Spatial Digital Humanities Research for Japanese Culture”. Dr Clanuwat will be presenting her work on “miwo: Kuzushiji recognition smartphone application with AI”.
Kitamoto Abstract: The talk introduces various research projects at the ROIS-DS Center for Open Data in the Humanities (CODH) with emphasis on visual and spatial data. In contrast to textual data, which is typical in digital humanities research, visual and spatial data requires different approaches and expertise, such as computer vision, machine learning, geographic information science, and linked data. The talk will discuss information technology to solve research questions in the humanities and data platforms such as IIIF (International Image Interoperability Framework) to create big structured data of the past.
Project links (some in Japanese only):
IIIF Curation Platform http://codh.rois.ac.jp/icp/
Collection of Facial Expressions (KaoKore) http://codh.rois.ac.jp/face/
Bukan Complete Collection http://codh.rois.ac.jp/bukan/
Edo Maps http://codh.rois.ac.jp/edo-maps/
GeoLOD https://geolod.ex.nii.ac.jp/
Asanobu Kitamoto earned his Ph.D. in electronic engineering from the University of Tokyo in 1997. He is now Director of Center for Open Data in the Humanities (CODH), Joint Support-Center for Data Science Research (DS), Research Organization of Information and Systems (ROIS), Professor of National Institute of Informatics, and SOKENDAI (The Graduate University for Advanced Studies). His main technical interest is in developing data-driven science in a wide range of disciplines such as humanities, earth science and environment, and disaster reduction. He received Japan Media Arts Festival Jury Recommended Works, IPSJ Yamashita Award, and others. He is also interested in trans-disciplinary collaboration for the promotion of open science.
Clanuwat Abstract: Reading kuzushiji (cursive script) is an essential skill for the study of premodern Japan, but gaining kuzushiji proficiency can be a challenge. This talk will offer a brief introduction to approaches to learning how to decipher kuzushiji. Dr. Clanuwat will demonstrate how artificial intelligence based kuzushiji recognition system works to transcribe premodern Japanese documents. Finally she will introduce the Kuzushiji recognition smartphone app “miwo”. The name “miwo” comes from the fourteenth chapter of The Tale of Genji, “Miwotsukushi”, referring to waterway markers. Just as the Miwotsukushi is a guide for boats in the sea, the miwo app aims to act as a guide for reading kuzushiji materials. https://tkasasagi.github.io/
Dr Clanuwat is a project assistant professor at ROIS-DS Center for Open Data in the Humanities. She received her PhD from the Graduate school of Letters Arts and Sciences at Waseda University, where she specialized in Kamakura-era Tale of Genji commentaries. In 2018, she developed an AI-based kuzushiji recognition model called KuroNet. In 2019, she hosted a Kaggle machine learning competition for kuzushiji recognition which attracted over 300 machine learning researchers and engineers from around the world. Her AI and kuzushiji research won the Information Processing Society of Japan Yamashita Memorial Research Award. Her Kuzushiji recognition smartphone application won the ACT-X AI Powered Innovation and Creation research grant from Japan Science and Technology Agency.
Visit our GitHub page.
Katherine McDonough[email protected]
The Colouring Cities repository provides open code for open data platforms on building stocks, co-developed as part of its international research programme, which currently operates across nine countries.
A Generic Method for Segmented Map Realignment, using Local Features and Graph Neural Network. JADIS SuperMapRealigner offers automatic tools and models to semantically segment the road network of historical city maps and realign the maps on geographical coordinates. Created by Rémi Petitpierre, Maxime Jan, & Paul Guhennec (EPFL). 
"
https://www.turing.ac.uk/events/data-debates-ai-and-fake-news,"
Fake news is fuelling rising public mistrust of the media, politics and big business, with even health services subject to conspiracy around life-saving vaccines and other treatments. What role does AI play in creating and disseminating fake news and how can we harness the same technologies to counteract them?
Alex Krasodomski-Jones Director of the Centre for the Analysis of Social Media, leading work on digital political extremism, information environments, disinformation and machine-enabled decision-making. He manages CASM’s visual analytics capability, and provides written and televised comment for the BBC, CNN, the Spectator, the Huffington Post and other outlets.
Yulan He is a Professor of Computer Science at the University of Warwick. Her research centres on the exploration of statistical models in representing uncertainty and the benefit they bring over earlier work in a wide range of application areas, particularly the integration of machine learning and natural language processing for text understanding. Some of her interested research topics include sentiment analysis and opinion mining, topic/event extraction from text, combining language and vision for multimodal analysis, clinical text mining, conversational agents and social media analytic
Bertie Vidgen is a researcher on two of The Alan Turing Institute's projects, The (Mis)Informed Citizen and Hate Speech: Measure and Counter-measures. His main research is focused on detecting, analysing, and countering online hate speech, examining it in the context of both news and social media. In his work, he primarily uses computational social science methods, including machine learning, natural language processing, and statistical modelling.
Our panel of experts will debate all these issues and more. The discussions are encouraged to continue more informally, over a glass of wine (included), once the debate finishes. 
Chaired by writer and broadcaster Timandra Harkness. Timandra presents BBC Radio 4 series, FutureProofing and has presented the documentaries, Data, Data Everywhere, Personality Politics & The Singularity.
Enjoy food and drink purchased from the Knowledge Centre Bar from 18.30 and after the event until the Bar closes at 22.00
Data Debates is a collaboration between The Alan Turing Institute and the British Library and aims to stimulate discussion on issues surrounding big data, its potential uses, and its implications for society. 
Join the conversation #TheDataDebates
Visit the main event page
"
https://www.turing.ac.uk/events/new-agendas-for-data-justice,"
The Alan Turing Institute invites you to join us as we bring together 12 partners from across the world to share their research and to celebrate the many voices of data justice.
We will host two panel sessions featuring experts on the intersection of data and social justice from organisations spanning Africa, Asia, Oceania and South America. These panels will explore the 12 reports on data justice they have produced in close collaboration with impacted stakeholders as part of a research project undertaken in partnership with The Alan Turing Institute and The Global Partnership on AI.  This event will also feature the launch of episode 1 of a documentary-style series, Advancing Data Justice, and will mark the commencement of a consultation period where we invite public feedback on our data justice guides for policymakers, developers, and impacted communities.  
In 2021, the Global Partnership on AI chose The Alan Turing Institute to lead a project they had begun on data justice. With the help of additional funding from the UK government, the Global Partnership on AI and the Turing team were able to expand this project.
As a result, the scale of this research increased massively to incorporate the work of twelve Policy Pilot Partner organisations from Asia, Oceania, Africa, and South America.
Each of these organisations have been working to understand what data justice might look like in their distinct contexts. They have conducted extensive interview and workshop engagements with policymakers, developers, and impacted communities across the world.
This event presents the first opportunity for all 12 partners to come together with the Turing team to discuss their research findings with one another and to share them publicly.
In addition to these panel discussions, this event will feature the launch of Introducing Data Justice. This is the first of four films which track the work of our project partners in order to uncover how they understand data injustice, past and present, and how we might work towards improved data justice in the future.
Finally, this event will mark the start of a consultation period where we, at The Alan Turing Institute, begin to seek public feedback on three sets of data justice guidelines. Through this consultation, we hope to give communities around the world the opportunity to shape new agendas for data justice.

"
https://www.turing.ac.uk/news/publications/using-message-logs-and-resource-use-data-cluster-failure-diagnosis,"
Failure diagnosis for large compute clusters using only message logs is known to be incomplete. Recent availability of resource use data provides another potentially useful source of data for failure detection and diagnosis. Early work combining message logs and resource use data for failure diagnosis has shown promising results. This paper describes the CRUMEL framework which implements a new approach to combining rationalized message logs and resource use data for failure diagnosis. CRUMEL identifies patterns of errors and resource use and correlates these patterns by time with system failures. Application of CRUMEL to data from the Ranger supercomputer has yielded improved diagnoses over previous research.
CRUMEL has: (i) showed that more events correlated with system failures can only be identified by applying different correlation algorithms, (ii) confirmed six groups of errors, (iii) identified Lustre I/O resource use counters which are correlated with occurrence of Lustre faults which are potential flags for online detection of failures, (iv) matched the dates of correlated error events and correlated resource use with the dates of compute node hang-ups and (v) identified two more error groups associated with compute node hang-ups. The pre-processed data will be put on the public domain in September, 2016.
E. Chuah, A. Jhumka, J. C. Browne, N. Gurumdimma, S. Narasimhamurthy and B. Barth, ""Using Message Logs and Resource Use Data for Cluster Failure Diagnosis,"" 2016 IEEE 23rd International Conference on High Performance Computing (HiPC), Hyderabad, 2016, pp. 232-241. doi: 10.1109/HiPC.2016.035
"
https://www.turing.ac.uk/news/turing-work-university-texas-austins-oden-institute-advance-data-centric-engineering-research,"
A Memorandum of Understanding (MoU) has been signed between The Alan Turing Institute and the Oden Institute for Computational Engineering and Sciences at The University of Texas (UT) at Austin. This formally creates an ambitious agreement led by the Turing’s data-centric engineering programme, a major research initiative funded by the Lloyd's Register Foundation.
This new collaboration aims to deliver research advances in a number of key impact areas: artificial intelligence for science and engineering, computational science and engineering (CSE), scientific machine learning, and data-centric engineering.
The agreement, signed by UT Austin President Greg Fenves and the Turing’s Institute Director and Chief Executive, Adrian Smith, is expected to continue for five years and establishes plans to develop joint research projects and exchange academic materials and publications, as well as to jointly host meetings, research seminars, workshops and symposia. There are also plans for new opportunities for UT Austin faculty and students to conduct research in London and vice versa.
The Oden Institute is a world-leading interdisciplinary institute dedicated to CSE, which brings together faculty from across 23 different departments in engineering, medicine, the natural sciences, the geosciences, and business. The Oden Institute plays a leadership role in Texas Computing, a tight collaboration among the units that contribute to UT Austin's global leadership in computing research, education and technological innovation. It is so named to honor Dr J. Tinsley Oden, widely considered to be one of the founding fathers of the field of computational science and engineering.
“Forging closer ties with The Alan Turing Institute is the first step in what I expect to be a long and rewarding journey for UT Austin and our new UK partners,” said Oden Institute director, Karen Willcox.  ""Both organizations share a deep commitment to research with real impact, research that can help bring about solutions to the toughest challenges we currently face as a global community. And, given the scope of priority areas identified, the benefits of working more closely with The Alan Turing Institute will be felt, not only by the Oden Institute community, but across UT campus.   
Mark Girolami, Programme Director of Data-Centric Engineering said: “This is an important and significant new collaboration for The Alan Turing Institute.  The unique combination of the computational and data centric engineering expertise of the Oden and Turing will create exciting opportunities for innovative and impactful research outcomes. Indeed working with the Oden Institute will allow our research teams to effectively address a range of pressing global challenges, such as achieving net zero greenhouse gas emissions, and improving the quality of air we breathe in our major cities.""
Cover image credit: Marsha Miller, The University of Texas at Austin  
"
https://www.turing.ac.uk/people/alessandro-ragano,"
Alessandro Ragano is a Ph.D. student in the School of Computer Science at University College Dublin, a member of the Insight Centre for Data Analytics, a visiting researcher at the Centre for Digital Music at Queen Mary University of London, and an enrichment student at The Alan Turing Institute. He is supervised by Dr. Andrew Hines and Dr. Emmanouil Benetos. He holds a B.Sc. in Computer Engineering from the Università degli Studi di Salerno and an M.Sc. in Computer Science and Engineering with a specialization in Sound and Music Computing from the Politecnico di Milano. During his master's degree, he worked as a research assistant in the Audio and Multimedia Division at the Fraunhofer IIS.
His current interest lies in machine learning applied to audio quality assessment. The topics of interest include how sound recordings aside from the content factor, affect people’s listening experience and what factors are involved in the sound quality control. He is interested in understanding how machines can be used to assess sound quality in order to help to preserve audio archives. His research interest leads him to the Turing where one of its core research areas focuses on data science applied in digital humanities.
His research investigates how to develop audio quality metrics that measure end-users’ perceived quality of audio signals with a focus on objective reference-free models that can evaluate the quality of experience (QoE) of digitised and restored audio archives. He is interested in perceived quality of historical audio material that is subject to digitisation and restoration which is typically evaluated by individual judgements or using inappropriate objective quality models. His goal is to develop new quality models which can be used for assessing quality in audio archive scenarios.
He is interested in deep learning models for quality prediction, especially in the techniques that can be used to compensate for the absence of the reference signal, a typical characteristic of audio archives. His interests concern musical signals and audio enhancement/restoration techniques applied to musical signals.

 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/autistica,"
www.autistica.org.uk
"
https://www.turing.ac.uk/news/publications/deriving-equations-sensor-data-using-dimensional-function-synthesis,"
We present a new method for deriving functions that model the relationship between multiple signals in a physical system. The method, which we call dimensional function synthesis, applies to data streams where the dimensions of the signals are known. The method comprises two phases: a compile-time synthesis phase and a subsequent calibration using sensor data. We implement dimensional function synthesis and use the implementation to demonstrate efficiently summarising multi-modal sensor data for two physical systems using 90 laboratory experiments and 10 000 synthetic idealised measurements. We evaluate the performance of the compile-time phase of dimensional function synthesis as well as the calibration phase overhead, inference latency, and accuracy of the models our method generates. The results show that our technique can generate models in less than 300 ms on average across all the physical systems we evaluated. When calibrated with sensor data, our models outperform traditional regression and neural network models in inference accuracy in all the cases we evaluated. In addition, our models perform better in training latency (over 8660× improvement) and required arithmetic operations in inference (over 34× improvement). These significant gains are largely the result of exploiting information on the physics of signals that has hitherto been ignored.
Youchao Wang, Sam Willis, Vasileios Tsoutsouras, and Phillip Stanley-Marbell. 2019. Deriving Equations from Sensor Data Using Dimensional Function Synthesis. ACM Trans. Embed. Comput. Syst. 18, 5s, Article 84 (October 2019), 22 pages. DOI:https://doi.org/10.1145/3358218
NAN
"
https://www.turing.ac.uk/people/anthony-tuckwell,"
Anthony is a PhD student at the Economics department of the University of Warwick and an enrichment student at the Turing.
His research uses methods from behavioural and experimental economics, applied econometrics, machine learning and natural language processing to understand important questions for society and the economy and to develop new tools for future research. It has been supported by a scholarship from the Economic & Social Research Council and grants awarded by the Mind & Life Institute and the CAGE Research Centre at the University of Warwick.
Prior to his PhD, Anthony completed an MRes and MSc at Warwick and a BSc at the University of Bath.
Anthony's research focuses mainly on topics from cultural and behavioural economics. In particular he has interests in religious and political values, identity, mindfulness and, most recently, music. At the Turing he is working on one project that uses natural language processing to explore the close relationship between religion and politics in the US, and on another that applies emotion-classification algorithms to music to investigate the link between musical expression and its social context.
"
https://www.turing.ac.uk/people/external-researchers/xiaoxuan-liu,"
Dr Xiaoxuan Liu is an ophthalmology doctor and clinician scientist at University Hospitals Birmingham NHS Foundation Trust and the University of Birmingham. The goal of her work is to ensure AI and digital health technologies are safe, effective and promote health equity.
She completed her PhD in 2020, which focused on the validation of a computational image analysis technique for measuring inflammation in the eye using optical coherence tomography. She co-led the SPIRIT-AI & CONSORT-AI initiative, which resulted in internationally adopted reporting standards for AI clinical trials. She leads STANDING Together, an initiative seeking to improve inclusiveness of healthcare datasets to ensure they promote health equity. Dr Liu advises on a number of health policy and regulatory bodies on their approach to evaluating AI in healthcare, including the MHRA, NICE, WHO, BSI and the NHS AI Lab.
"
https://www.turing.ac.uk/news/publications/data-study-group-final-report-centre-environment-fisheries-and-aquaculture,"
Cefas (The Centre for Environment, Fisheries, and Aquaculture Science) is an agency of Defra (the Government’s Department of Environment, Food and Rural Affairs) and world leading experts in marine and freshwater science. Research at Cefas aims to tackle the serious global problems of climate change, marine litter, overfishing, and pollution to secure a sustainable future for marine ecosystems.
The Cefas Endeavour, a multi-disciplinary research vessel, collects millions of plankton images during its surveys through the Plankton Imager (PI) system: a high-speed imaging instrument which continuously pumps water, takes images of the passing particles, and attempts to identifies the zooplankton organisms present (Figure 1). Images have varying shapes and sizes with a highly-skewed distribution towards smaller particles/images. Of these, over 80 percent can be classified as detritus (e.g., sand, seaweed fragments, microplastics) which are traditionally removed by-eye before any analysis, leaving the remaining plankton images to be manually labelled.
The challenge dataset consisted of 58,791 TIF (Tag Image File Format) images of individual objects detected and segmented in imagery collected on the RV Cefas Endeavour research vessel using the PI system.
Approximately 17,000 of these images are of individual zooplankton. The plankton images had previously been manually classified by experts into two main categories: Copepods, small or microscopic aquatic crustacean of the large taxonomic class Copepoda (see Figures 25 and 26), and Non- Copepods (see Figures 23, 24, 27, 28), for all other plankton not belonging to the Copepoda class. The experts also categorised these images further into 38 species classes. This expert manual classification allowed challenge participants to verify the accuracy of the automated classification methods explored.
The number of images varied greatly between the 38 classes, ranging from 4000 images to 10 images per class. Challenge participants therefore had to decide how to address this imbalance in order to produce a model that could be useful and accurate classifications of plankton.
The remaining 40,000 images consisted of individual pieces of detritus (see Figures 29 and 30). These images were of other objects collected by the RV Cefas Endeavour PI system such as sand, seaweed, or microplastics. Manual removal of these images has been shown to be a significant bottleneck in the analysis of imagery collected using the PI. Therefore as an additional challenge, participants had the opportunity to explore automated sorting of images into plankton and detritus in order to facilitate application of plankton classification models to imagery collected from the PI in real time without pre-processing to remove these erroneous objects.
Data Study Group team. (2022, July 5). Data Study Group Final Report: Centre for Environment, Fisheries and Aquaculture Science. Zenodo. https://doi.org/10.5281/zenodo.6799166
PIs: Alejandro Coca-Castro, Evangeline Corcoran and Beatriz Costa-Gomes
4.3 MB
"
https://www.turing.ac.uk/people/doctoral-students/cecile-de-bezenac,"
Cécile de Bézenac is currently part of the doctoral training program based in Leeds Institute for Data Analytics under the supervision of Professor Alison Heppenstall. She has a civil engineering background, graduating from Ecole Centrale Paris before completing a data analytics and demography Masters in Aix Marseille University. She has previously worked in the statistic and modelling laboratory (SAMM) in Paris Pantheon Sorbonne exploring new spatial segregation measures in an urban context.
Cécile’s research, rooted in quantitative geography, is more specifically linked to urban studies. She is interested in developing prospective urban models that can later serve as medium for policy makers when addressing particular urban issues. Understanding the inner workings of agent-based models, that describe individual social interactions and help predict emerging global phenomenon, is at the core of her research project. In particular, exploring descriptive model calibration and validation methods represents a first step to approaching more realistic designs.
"
https://www.turing.ac.uk/people/researchers/jeff-dalton,"
Dr Dalton completed his PhD in 2014 from the University of Massachusetts Amherst at the Center for Intelligent Information Retrieval (CIIR), a leading international research group in search. After completing his PhD, he joined Google Research studying information extraction with the Knowledge Discovery team (led by Kevin Murphy and Evgeniy Gabrilovich). His experience at Google also includes contributing to the launch of Sympton Search and as a developer in the Google Assistant NLU team. 
"
https://www.turing.ac.uk/research/interest-groups/environment-and-sustainability,"
The explosion in the availability of large and complex data sets from diverse sources (e.g. environmental monitoring; remote sensing; bio-logging, climate modelling; social media; and contributions from citizen science), together with advances in methodology and computation presents an exceptional opportunity to transform our knowledge of both the effects of environmental change and to develop solutions to environmentally related challenges.
Unlocking the full power of data will have far reaching impact across a wide spectrum of environmental areas; it will raise the bar in data-driven environment research and provide the information and insight required by sectors strongly impacted by the environment including agri-food, fisheries, water, waste, transport, health, finance and energy. It will enhance our resilience to natural hazards ability through enhanced prediction, forecasting and early warning systems, and the development of next generation, data-led, resilience planning, resilient infrastructure and financial instruments.
Environmental challenges are inherently global, and the sheer volume of data that need to be incorporated into decision making at this scale means that data science and AI will be crucial in providing the robust, decision-quality information that is required. This is especially true in the case of the Sustainable Development Goals (SDGs) for which quality, accessible, timely and reliable disaggregated data is essential for understanding the scale of the challenges and in measuring progress.
The global environmental research community has developed a plethora of observational and modelling platforms designed to better understand the evolution of our environment and our role within in. As we further push the boundaries on detailed observations from molecular through to global processes, we gather more evidence deemed important to affect change. However, we have reached a crossroad of exploration that hinges on utilising this information to develop sustainable solutions to key problems, such as pollution and resource management. Likewise, implementing change requires a deep understanding of future impact, unintended consequences, behavioural change and effective policies. Numerical models of environmental systems used to predict and quantify change are traditionally containerised collections of stiff coupled ODEs with restrictions placed around computational complexity. New data driven approaches could exploit the wealth of global observations to build new modelling frameworks that cover local to global scales whilst also exploiting emerging compute hardware.
In many cases, a mechanistic framework that couples process to environmental impact does not exist. Machine learning representations could offer a potential route to develop robust tools for a range of environmental stake-holders and would allow us to make truly effective use of the rapid increase in the data from a variety of sources including:
Environmental data science naturally sits at the boundaries of multiple disciplines. It is the aim of this interest group to:
In the long-term, the Environmental sustainability interest group aims to discuss mechanisms for implementing change through policy and behaviour change. This will require engagement with local, regional and national bodies to understand the key data and technology driven challenges they face to affect change.
The opportunity to join this group is open to academics across all universities, businesses and public and third sector organisations.
Click here to join us and request sign-up
"
https://www.turing.ac.uk/people/doctoral-students/albese-demjaha,"
Albese Demjaha was a former PhD student at UCL supervised by David Pym and Simon Parkin. Her research interests are in information security management,  human-centred security, and behavioural economics. She specialises in the human aspects of information security and focuses on cultural and behavioural aspects of security in her work.

During her doctoral experience, she was a teaching assistant for the module “People and Security”, both at UCL and University of Oxford. In addition, she was first the teaching assistant for the module ""Information Security Management"" at UCL and later became the lecturer for that module.

She did her MSc in Information Security at UCL where her thesis explored the possibility for new metaphors for end-to-end encryption. Albesa also has a BSc in Information Systems and Management from the South East European University in Skopje.

 
Albesa's PhD work is in the field of human-centred information security in an organisational context. More specifically, it focuses on the cultural and behavioural aspects of security and it is about properly and rigorously defining what a security culture is in such a context. Within this field, a parallel interest is to create a robust and repeatable methodology for co-design and modelling of security. Albesa is also keen to link her work to security policy.
"
https://www.turing.ac.uk/people/researchers/evelina-gabasova,"
Evelina is Principal Research Data Scientist at The Alan Turing Institute, focusing on open source activities in the research engineering group and the wider institute. She is passionate about making data science understandable and accessible to everyone. She is part of the Leadership Team of the Research Engineering Group and currently a co-investigator on two major projects in the Turing:
She is a member of the Volunteer Leadership Team of the Tools, Practices and Systems Turing programme, focusing on open source strategy for open infrastructure.
In 2022, Evelina became Vice President of the Society of Research Software Engineering.
She originally started as a programmer but got interested in machine learning early on and did a mathematics PhD at the University of Cambridge. During her PhD, she worked on Bayesian models for unsupervised learning that integrate heterogeneous biomedical datasets. After that she worked in cancer research at the MRC Cancer unit in Cambridge, where she focused on infrastructure to helping biologists analyse genomic data.
Outside of academia, she is also an avid speaker at developer conferences and she was awarded the Microsoft MVP award for her work in the F# community.
"
https://www.turing.ac.uk/people/programme-leadership/george-balston,"
George Balston is Programme Co-Director, Defence and Security at The Alan Turing Institute. He leads a portfolio of work applying data science and artificial intelligence to national security, cyber security, and defence challenges, working closely with national and international partners. Prior to joining the Turing, George worked within UK Government as a Researcher, Data Scientist, and Technical Lead, focused on researching, developing, and deploying new analytical capabilities.
George is an Associate Fellow at the Royal United Services Institute (RUSI), and an Honorary Research Fellow at UCL. He is a member of the Royal Society working group on Privacy Enhancing Technologies, and advises the UK Government on the implications of emerging technologies to national security.
George’s research interests include human language technology, privacy technology, and Government innovation, and he has presented novel research in such areas at international government conferences. He holds a first class degree in Music.
"
https://www.turing.ac.uk/research/research-areas/optimisation/convex-programming,"
"
https://www.turing.ac.uk/people/spotlights/tim-hillel,"

I work as a Lecturer in Analytics of Infrastructure Systems at UCL’s Faculty of Engineering Science. I’m also currently heading up a research project focused on developing dynamic digital twins of interconnected energy and transport networks, as part of an ongoing collaboration between the Turing and Accenture.
So, the Turing has launched a five-year strategic partnership with Accenture. The aim is to collaborate on research in, and real-world application of, data science, analytics and AI, with a focus on delivering tangible business and societal value. A key pillar of the partnership involves conducting joint research. As part of this, the Turing issued an open call for new research projects in mid-2021. I applied and was lucky enough to have my project chosen for sponsorship.
We’ll be looking at how new technologies (specifically electric vehicles [EVs]) and lifestyle patterns (including a rise in homeworking) will impact energy and transport demand in cities. To do this, we plan to develop dynamic digital twins of existing and planned energy and transport infrastructure. The digital twins will also include real-time monitoring of transport and energy network conditions and demand, plus agent-based simulations of transportation and domestic energy usage.   
A conventional digital twin is an accurate digital representation of something that exists in the real world: in this case, transport and energy infrastructure networks. Our dynamic digital twins add three new dimensions to that: 1) the behaviour of the individuals who interact with infrastructure networks; 2) the interactions between flows (of people, goods, energy, data, etc.) in a system; and 3) the physical response of infrastructure to loads and hazards. 
Incorporating these dynamic dimensions makes it possible to investigate complex counterfactual scenarios with our digital twins. So, we can make more human-centric predictions about how infrastructure networks will respond to both planned and unplanned changes. And, because of this, we can use the dynamic digital twins as a policy planning tool rather than just an infrastructure management tool.
The key application is to help energy and transport network operators understand how EV penetration rates and flexible working policies impact demand for their services. With this, they can perform better long-term strategic planning, network management, and sustainability and resilience analysis.
Beyond that, there are all kinds of potential planning and analytical applications. Our research could help EV charging station operators decide where to deploy new charging points. Government could use it to test different scenarios around their plans to transition to an emissions-free vehicle fleet by 2050. It could also help inform future public transportation investments, flexible working policies, and electricity pricing strategies.
I think my proudest moment was when one of my fellow researchers on this project, Gael Lederrey, was awarded his PhD earlier this year. I was a co-director of his thesis and he passed his viva just after I started my current position as a lecturer at UCL, which makes this transition extra special. I'm really excited to enter this new phase of being able to lead and supervise research.
I moved to London a few months ago from Lausanne in Switzerland (where I was working as a postdoc at EPFL) so a lot of my free time has been taken up by the move. I enjoy keeping active – I did a lot of outdoor activities in Switzerland and am hoping to get back into playing rugby now I am back in London. I’ve also started high diving at my local pool, which is a good adrenaline rush!
"
https://www.turing.ac.uk/people/business-team/joanna-dungate,"
"
https://www.turing.ac.uk/people/researchers/barbara-mcgillivray,"
Hi! I grew up in the beautiful city of Florence in Italy, and I have Scottish origins. I am Turing Research Fellow at The Alan Turing Institute, senior research associate the University of Cambridge and Editor in Chief of the Journal of Open Humanities Data. In September 2021 I will start as lecturer in Digital Humanities and Cultural Computation at the Department of Digital Humanities of King's College London.
I have always been passionate about how the sciences and the humanities can meet. I completed a PhD in Computational Linguistics from the University of Pisa in 2010 after a degree in Mathematics and one in Classics from the University of Florence (Italy). Before joining the Turing, I was language technologist in the Dictionary division of Oxford University Press and data scientist in the Open Research Group of Springer Nature.
Check out my Turing research spotlight interview here and my interview for Cambridge Language Sciences.
You can follow me on Twitter here.
Language changes all the time in fascinating ways. My research at the Turing is on how words change meaning over time and how to model this change in computational ways. I work on machine-learning models for the change in meaning of words in historical times (Ancient Greek, Latin, eighteen-century English) and in contemporary texts (Twitter, web archives, emoji). My interdisciplinary contribution covers Data Science, Natural Language Processing, Historical Linguistics and other humanistic fields, to push the boundaries of what academic disciplines separately have achieved so far on this topic. 
My research was featured in the Christmas 2020 edition of the Economist ""How data analysis can enrich the liberal arts"". My research on emoji semantic change was featured in the New Scientist and BBC.
Check out my interview for the Economist's podcast in December 2020 and my interview with Dennis Relojo-Howell (January 2021).
I have founded and convene the Data Science and Digital Humanities Special Interest Group at the Turing. 
"
https://www.turing.ac.uk/events/tmcf-workshop-series-prediction-algorithms-causal-interpretation-online-event,"
38.7 KB
This event will present the outcomes of the Turing's Theory and Methods Challenge Fortnights in Data Science and AI event, Prediction algorithms with a causal interpretation, held in Manchester in February 2020. A challenge team, comprising twelve academics in predictive modelling, machine learning, and causal inference, will share the findings from the challenge - including perspectives on 'counterfactual prediction' and pilot work to address some of the methodological challenges involved. This is now topical work, as such methods can be used for decision support in the COVID-19 pandemic. 
For participants less familiar with causal inference and prediction, we are holding a companion introductory session in the morning of the same day.
This event will take place via Zoom webinar
 
Prediction algorithms in AI use machine learning and statistics to make predictions about an event, given what we know so far. Examples include whether a COVID-19 patient will require ventilation, or whether a person seeking insurance will make a claim. These predictions can be used for planning and decision making. However, a limitation of these approaches is they cannot (and should not) be used to ask ‘what if’ questions. For example, ‘what if we give a patient CPAP rather than ventilation?’ or ‘what if the person seeking insurance had a different ethnicity?’ The first example is important for decision making, while the second has implications for fairness because it could be considered discriminatory to charge a different insurance premium based on ethnicity. In both cases, causal inference can help to enrich prediction algorithms with ‘what if’ capabilities.
For further questions, contact [email protected].
1 MB
2.3 MB
7 MB
1.8 MB
8.5 MB
3.6 MB
"
https://www.turing.ac.uk/people/research-staff,"
"
https://www.turing.ac.uk/news/first-pitchfest-winner-announced-ai-uk-2023,"
We’re delighted to announce that Maria Delgado-Ortet, a Turing Enrichment student, is the winner of the inaugural PitchFest, hosted at AI UK 2023. Maria presented her work on using AI-powered image analysis to improve detection and prediction of ovarian cancer.
Each of the six finalists had just 90 seconds to present their research to a panel of industry judges and an enthusiastic AI UK audience. The winner was then voted for live by the audience.
Maria is based at the University of Cambridge, where her PhD is focusing on the application of AI to detect ovarian cancer in computerised tomography (CT) scans. Around 7,500 women are diagnosed with this form of cancer in the UK each year, and catching it early can greatly improve prognosis. Ultimately, the aim is to be able to use this technology to not only detect tumours but also predict their response to treatment – allowing doctors to design the best tailored therapy for each patient.
“Taking part in PitchFest has been such a memorable experience,” says Maria. “I feel so humbled, thankful and honoured to have received this prize. Myself and the other finalists received nothing but positive feedback from the audience, and we realised that our research has real interests beyond the walls of our laboratories.”
Contestants had to make sure that their pitch was accessible to the wide range of attendees at AI UK, and that it highlighted the real-world applications and future directions of their research. As well as competing for the coveted PitchFest trophy, all finalists received a mentoring session from the Turing’s Research Application Management team, who organised PitchFest and play a crucial role at the Turing by helping to adapt research outputs for real-world use while ensuring openness and reproducibility.
Alongside Maria, this year’s finalists were:
The five judges for the event were Simon Reeve (Director of Innovation, The Alan Turing Institute), Esra Kasapoglu (Director of AI and Data Economy, UKRI), Jan Przydatek (Director of Technologies, Lloyd’s Register Foundation), Gavin Stephenson (Managing Director, Accenture Analytics) and Anna Brailsford (CEO, Code First Girls).
“All the finalists did a fantastic job with their pitches,” said Simon Reeve. “They communicated their technical ideas in clear and engaging language, demonstrating the real-world applications of their work. I look forward to seeing what they do next. Well done to all!”
Stay tuned to the Turing’s YouTube channel for a full recording of the session.
"
https://www.turing.ac.uk/news/publications/tackling-threats-informed-decision-making-democratic-societies,"
Access to reliable information is crucial to the ability of a democratic society to coordinate effective collective action when responding to a crisis, like a global pandemic, or complex challenge like climate change. Through a series of workshops we developed and analysed a set of hypothetical yet plausible crisis scenarios to explore how technologically exacerbated external threats and internal vulnerabilities to a society’s epistemic security – its ability to reliably avert threats to the processes by which reliable information is produced, distributed, and assessed within the society – can be mitigated in order to facilitate timely decision-making and collective action in democratic societies.
Overall we observed that preserving a democratic society’s epistemic security is a complex effort that sits at the interface of many knowledge domains, theoretical perspectives, value systems, and institutional responsibilities, and we developed a series of recommendations to highlight areas where additional research and resources will likely have a significant impact on improving epistemic security in democratic societies
26 MB
"
https://www.turing.ac.uk/people/researchers/maria-liakata,"
Maria is a Professor in Natural Language Processing (NLP) at Queen Mary, University of London. She is in receipt of an EPSRC/UKRI Turing AI fellowship award on Creating Time Sensitive Sensors from Language & Heterogeneous User-Generated Content (2019-2025) https://www.turing.ac.uk/research/research-projects/time-sensitive-sensing-language-and-user-generated-content . 
At the Alan Turing Institute she co-leads the NLP and data science for mental health interest groups and supervises PhD students. She is co-leading projects on Language sensing for dementia monitoring & diagnosis (https://www.dcs.warwick.ac.uk/langsensing/), Opinion summarisation from social media, an AI evidence based framework during pandemics (https://panacea2020.github.io/index.html). 
Maria has a DPhil from the University of Oxford on learning pragmatic knowledge from text.  Her work has contributed to advances in knowledge discovery from corpora, automation of scientific experimentation and automatic extraction of information from the scientific literature. She has published widely both in NLP and interdisciplinary venues. Past awards include an IBM Faculty Award for work on emotion sensing from heterogeneous mobile phone data, being a co-investigator on the EU Project PHEME, which studied the spread of rumours in social media (2014-2017) and an Early Career Fellowship from the Leverhulme Trust (2010-2013) on reasoning with scientific articles.  
Natural language processing (NLP), NLP for social and biomedical applications, analysis of multi-modal and heterogeneous data and especially the development of personalised longitudinal language processing, opinion mining and summarisation, rumour verification, biological text mining, computational semantics, scientific discourse analysis.
"
https://www.turing.ac.uk/people/researchers/roy-ruddle,"
Roy Ruddle is a Professor of Computing at the University of Leeds, and Deputy Director (Research Technology) of the Leeds Institute for Data Analytics (LIDA). He has worked in both academia and industry, and researches visualization, visual analytics and human-computer interaction in spaces that range from high-dimensional data to virtual reality. In a 12-year collaboration with pathologists at the Leeds Teaching Hospitals NHS Trust (LTHT), he developed the Leeds Virtual Microscope (LVM) for visualizing tera-pixel image collections on Powerwall and ultra-high definition displays, leading to its use for pathology training in NHS hospitals and commercialisation by Roche.
The LVM has already won awards for its research (ACM ToCHI Best Paper 2016) and application (Yorkshire & Humber NHS Innovation Award for Medical Devices and Diagnostics, 2014). His industry-sponsored petrophysics research led to the PETMiner software, which integrates objective and subjective data in visualizations for interactive data analysis, and has been commercialised by the spin-out Petriva Ltd. He currently researches novel visualization methods for data profiling, investigating data quality, and analysing complex coded data such as electronic health records and retail transactions. That research involves collaborations with NHS Digital, Leeds City Council, LTHT, Bradford Institute for Health Research, Sainsbury's, and other organisations.
Professor Roy Ruddle designs, develops and evaluates visual analytic methods that address two neglected but challenging topics: (a) data quality, and (b) broken workflows. Both are essential if data analysis pipelines and models are to be rigorously designed, and hence are fundamental to data science. Visual analytics places users in the driving seat during the analysis of complex data, by combining the unique power of humans for detecting and reasoning about patterns (via interactive visualisation tools) and of machines for handling scale (via sophisticated models and powerful on-the-fly computation).
Data quality spans everything from data that is missing ('completeness') to data that is invalid, inconsistent, incompatible, implausible or clearly an error (these are all types of 'correctness'). Broken workflow occurs because it is rare that users rigorously investigate the knock-on consequences of choices made in one analysis step on the output of the next.
"
https://www.turing.ac.uk/research/research-areas/statistical-methods-theory/information-theory-statistical-methods-theory,"
"
https://www.turing.ac.uk/news/seven-ten-premier-league-footballers-face-twitter-abuse,"
The Alan Turing Institute and Ofcom have worked together to analyse more than 2.3 million tweets directed at Premier League footballers over the first five months of the 2021/22 season.
The study found that nearly 60,000 abusive posts were sent in the period, affecting seven in ten Premier League players. It also found that 68% of players (418 out of 618) received at least one abusive tweet, and one in fourteen (7%) received abuse every day.
Researchers discovered that half of all abuse towards Premier League footballers is directed at twelve particular players. These players each received an average of 15 abusive tweets every day.
The team developed a machine learning model that was trained on thousands of tweets until it could automatically identify whether or not a tweet was abusive. It was trained using two cutting-edge techniques: ‘active learning’ (in which the model essentially chose what data it needed to learn from) and ‘adversarial data generation’ (in which the model learnt from data that has been created in order to trick it).
To provide a benchmark for the model and a more in-depth breakdown of the tweet content, the team also hand-labelled 3,000 tweets, categorising them as either ‘abuse’, ‘positive’, ‘critical’ or ‘neutral’.
The model used to identify the abusive tweets was developed as part of The Alan Turing Institute’s Online Harms Observatory, led by their Online Safety Team. 
Ofcom, the UK's communications regulator, is holding an event today (Tuesday 2 August) to discuss these findings. Hosted by broadcast journalist and BT Sport presenter, Jules Breach, the event will hear from presenter and former England player Gary Lineker; Manchester United player Aoife Mannion; Professional Footballers' Association Chief Executive Maheta Molango; and Kick It Out Chair Sanjay Bhandari.
Dr Bertie Vidgen, lead author of the report and Head of Online Safety at The Alan Turing Institute said: “These stark findings uncover the extent to which footballers are subjected to vile abuse across social media. Prominent players receive messages from thousands of accounts daily on some platforms, and it wouldn’t have been possible to find all the abuse without the innovative AI techniques developed at the Turing.
“While tackling online abuse is difficult, we can’t leave it unchallenged. More must be done to stop the worst forms of content to ensure that players can do their job without being subjected to abuse.”
Read the full story, view the report or find out more about The Online Harms Observatory at The Alan Turing Institute.
"
https://www.turing.ac.uk/people/external-researchers/peter-diggle,"
Professor Peter Diggle is a Technical Director of the Turing-RSS Laboratory at The Alan Turing Institute, which supports statistical and machine learning capabilities for the UK’s COVID-19 response, delivered in partnership with the Royal Statistical Society and the Joint Biosecurity Centre. He is a Distinguished University Professor in CHICAS (a teaching and research group within the Lancaster Medical School at Lancaster University), a Trustee for Biometrika, and founding joint editor of Biostatistics. He holds honorary adjunct professorships with Johns Hopkins, Columbia and Yale. He is a former President (2014-2016) of the Royal Statistical Society, former chair of the UK Medical Research Council's Skills Development Fellowships Panel, and current member of the Royal Statistical Society’s COVID-19 Task Force.
Before Peter joined Lancaster University he held positions at the University of Newcastle upon Tyne, and at the Commonwealth Scientific and Industrial Research Organisation in Australia.
Peter’s research is motivated by applications in the biomedical, clinical or health sciences, but especially in real-time health surveillance and in tropical disease epidemiology. His main methodological research interests are in spatial statistics and longitudinal data analysis.
"
https://www.turing.ac.uk/people/external-researchers/lina-dencik,"
"
https://www.turing.ac.uk/events/towards-autonomous-robotic-systems-conference-taros,"
The 20th TAROS conference will be hosted by the Centre for Advanced Robotics at Queen Mary University of London from 3 to 5 July 2019.
Find out more on the main conference website.
TAROS is the longest running UK-hosted international conference on Robotics and Autonomous Systems (RAS), which is aimed at the presentation and discussion of the latest results and methods in autonomous robotics research and applications.
TAROS offers a friendly environment for robotics researchers and industry to take stock and plan future progress. It welcomes senior researchers and research students alike, and specifically provides opportunities for research students and young research scientists to present their work to the scientific community.
 
Keynotes:
Machine Learning for Robots to Think Fast in the Face of Unexpected Events - Professor Aude Billard (Professor of Robotics, Swiss Institute of Technology Lausanne, EPFL)
An Overview of Research and Robotics at Google DeepMind - Dr Francesco Nori (Head of Robotics, Google DeepMind)
Robotic Dynamic Manipulation: Perception, Planning and Control - Professor Bruno Siciliano (Professor of Robotic Surgery, University of Naples Federico II)
Title TBA - Professor Véronique Perdereau (Professor of Robot Manipulation, Sorbonne University)
 
 
This conference is organised by Queen Mary University of London and is supported by:

 
Graduate Centre, Queen Mary University of London 
Mile End Campus 
Mile End Road
London            
E1 4NS
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/reading-university,"
"
https://www.turing.ac.uk/events/data-study-group-event-pages,"
"
https://www.turing.ac.uk/people/research-associates/adrian-m-stetco,"
Adrian Stetco obtained a BSc in Computer Science from West University of Timisoara (Romania) in 2009, an MSc in Machine Learning from Johannes Kepler University (Austria) in 2012 and a PhD in Unsupervised Machine Learning from the University of Manchester in 2016. During his PhD he became interested in applications of data science to finance and attended summer schools in Switzerland and Spain. As a postdoc he researched machine learning applications for wind energy and healthcare, published extensively at major conferences, and won the Researcher2Innovator grant offered by the Masood Enterprise Centre for young entrepreneurs. His current research includes recommender systems leveraging graph theory, with applications to public procurement.
Adrian’s current research focuses on novel architectures for Recommender Systems in public procurement. Given its ex-post nature, an important limitation of observed procurement data is the absence of bidding participation knowledge. To solve this survivorship bias problem, he developed a generator of tripartite networks (modelling suppliers, contracts and buyers) for benchmarking.
"
https://www.turing.ac.uk/research/research-projects/computational-statistical-inference-engineering-and-security-cosines,"
There are tremendous demands for advanced statistical methodology to make scientific sense of the recent deluge of emerging data. Huge challenges in modelling, computation, and statistical algorithms have been created by diverse and important questions in virtually every area of human activity. This project aims to create a step change in the use of statistical methodology, motivated by challenges in modelling, computation, and statistical algorithms, for use in engineering and security.
The problems this project will tackle will be characterised by complex large datasets indexed in space and often continuously in time. New models will require not only spatial components, but also large network and time-series structure often requiring the solutions to stochastic (randomly distributed) differential equations. 
Inference - drawing conclusions from data - will require a completely new generation of algorithms. Such algorithms will need to be tailored for particular computer architecture, and will require underpinning theory to show they scale. New theory developed on the computational and statistical robustness of approaches will be required to ensure the usefulness of the procedures developed.
The main focus of the research will be in developing and studying generic statistical methods which have applicability in a wide-range of applications. 
The work will cover a range of different approaches:
Please see the dedicated website for the project at www.cosines.org
The research will focus on two major application domains which align with Turing research programmes: data-centric engineering, and defence and security. Partnership between the researchers and the Turing’s programmes will maximise the impact and speed of translation of the research being conducted.
For data-centric engineering, an example is the project ‘A digital twin of the world’s first 3D printed steel bridge’. The project is presenting enormous challenges to existing applied mathematical and statistical modelling of complex structures where even the bulk material properties are unknown and randomly distributed. A new generation of numerical inferential methods are therefore needed to support this progress.
For defence and security, there are many statistical challenges emerging from the need to process and communicate big and complex data sets, and to counter nefarious actors – from ‘bedroom hackers’ to state-sponsored terrorists – threatening cyber security. To counter such threats, it is necessary to produce a complete statistical representation of the virtual environment, in the presence of missing data, significant temporal change, and an adversary willing to manipulate systems in order to achieve their goals.
To counter the threat of global terrorism, it is necessary for law-enforcement agencies within the UK to share data, whilst rigorously applying data protection laws to maintain individuals’ privacy. It is therefore necessary to have mathematical guarantees over such data sharing arrangements, and to formulate statistical methodologies for the ‘penetration testing’ of anonymised data.
On November 2nd 2018, the project will be launched with an afternoon of presentations related to the project.
Anyone who would like to attend should register their interest by emailing Shital Desai ([email protected]). Lunch (from 12.15) will be provided and the event will conclude with a short drinks reception to celebrate the beginning of the project. The talks are scheduled to finish by roughly 16.30 and will take place in Warwick University Oculus lecture theatre OC0.02. More information can be found on the CoSInES website.
[email protected]
"
https://www.turing.ac.uk/people/researchers/rebecca-ward,"
Rebecca is a researcher in data-centric engineering with over 20 years of experience in industry and academia. Her research is broadly linked to energy efficient operation of buildings, with a specific emphasis on the impact of plants on internal environmental conditions. She has developed a bespoke Greenhouse Energy Simulation tool for the ornamental glasshouses at the Royal Botanic Gardens, Kew which has since been developed for the Growing Underground farm in Clapham, London. This tool forms part of the CROP digital twin developed by the Alan Turing Institute.
Since starting work at the Alan Turing Institute, Rebecca has been developing a methodology for sequential Bayesian calibration of computer models using particle filters. This methodology has been embedded within CROP and provides ongoing calibration of model parameters that facilitates model updating as parameters change in time.
Rebecca is a Visiting Researcher at the Energy Efficient Cities initiative at the University of Cambridge. A full list of publications is available on her personal website.
Rebecca is a Chartered Engineer, a member of the Institution of Mechanical Engineers and a member of the Institute of Physics.
"
https://www.turing.ac.uk/people/external-researchers/zeerak-waseem,"
Zeerak Waseem is a Ph.D. candidate at the University of Sheffield. He works on hate speech detection, abusive language detection, and fairness in machine learning. His areas of expertise are computational social science, hate speech detection, abusive language detection, and machine learning. He has organised the First Workshop on Abusive Language Online (co-located with ACL 2017), the Second Workshop on Abusive Language Online (co-located with EMNLP 2018), Widening NLP 2018 (co-located with NAACL 2018), Widening NLP 2019 (co-located with ACL 2019). He also sits on the research ethics committee for the University of Sheffield and on the Equality, Diversity and Inclusion Committee for the Department of Computer Science at the University of Sheffield.
"
https://www.turing.ac.uk/people/honorary-fellows,"

"
https://www.turing.ac.uk/events/turing-lectures-where-next-self-driving-vehicles,"
Sarah Sharples is Chief Scientific Adviser for the Department for Transport and Professor of Human Factors in the Faculty of Engineering at the University of Nottingham. She has led research in transport, manufacturing and healthcare, and currently leads the EPSRC (Engineering & Physical Sciences Research Council) Connected Everything Network Plus.
Hosted by Sethu Vijayakumar, Programme Co-Director for Artificial Intelligence (Robotics and autonomous systems) at The Alan Turing Institute, Professor of Robotics at the University of Edinburgh, UK and the Director of the Edinburgh Centre for Robotics.
This event is organised in collaboration with the Royal Institution of Great Britain.
 
The Turing Lectures series brings together leading thinkers to inform, entertain, and shape the national conversation on data science and AI.
In this Turing Lecture, Professor Sarah Sharples (Chief Scientific Adviser for the Department for Transport) will discuss the interface between policy and science for self-driving vehicles, and highlight some of the challenges and opportunities that we face in this area.
Automation has been present within our transport system for years, but we are now entering an age of autonomy where the nature of transport is rapidly evolving. As we continue on our path towards self-driving vehicles, what is the role for science, and how do we take research into the world of policy and legislation at pace? How can we make sure that self-driving vehicles are good for individuals, for society, and for the economy?

 
Hands-on experience
This year, with the help of the Royal Institution, The Turing Lectures will include experiential demonstrations, bringing the lectures to life for in-person attendees.
With partners of The Department for Transport exhibiting various interactive demonstrations and activities, join us from 18:00 to experience virtual reality driving simulators designed to study user behaviour with future automated vehicles. Obtain first-hand experience of how automated vehicles will interact with pedestrians in urban settings. Feed into ongoing safety studies into pedestrians’ road-crossing behaviour in the presence of automated vehicles, and take a ride in a virtual, automated taxi.
The event promises a dynamic experience for all, and the opportunity to be a part of the driverless revolution.
The Royal Institution
The Royal Institution is an organisation dedicated to scientific education and research. Located in Piccadilly, it was founded in 1799 to ""spread knowledge and facilitate general access to mechanical and useful inventions; to teach, through courses composed of philosophical conferences and experiments, the application of science to ordinary life.""
Please note that registration for this event is managed by the Royal Institution and tickets to attend this event in-person start from £10. An access fund is available for anyone who would otherwise be unable to attend. Please contact [email protected] to enquire.
You can attend virtually free of charge, please click the 'register now' button to attend.
This series of Turing Lectures is organised in collaboration with The Royal Institution of Great Britain.

 
 
 
18:00             - Doors open
18:00 - 19:00 - Demonstrations
19:00 - 20:00 - The Turing Lectures: Where next for self-driving vehicles? 
20:00 - 20:30 - Audience Q&A
20:30             - Event close
The Royal Institution of Great Britain
21 Albemarle Street
London
W1S 4BS
"
https://www.turing.ac.uk/research/research-projects/analysing-social-and-geographic-datasets,"
During the past decade, tools and techniques for social and geographic data have witnessed substantial changes. For this reason, there is a necessity for investigating methodological and computational aspects in this field. The mapping of tools and techniques for social and geographic data brings two main benefits: it provides an overview of this area, and it also allows researchers to identify potential research agendas for the future.
New sources of data, such as mobile data and social data, allow researchers to model and understand human behaviour in ways that were not possible just a decade ago. For this reason, in the past few years, there has been an explosion in the development of tools and techniques for the analysis of these 'digital traces' that we leave in our online and offline lives. The goal of this project is to provide an up-to-date overview of the technological landscape in this area in order to underpin future research agendas in this field.
The main purpose of this project is to provide a comprehensive description of the tools and techniques available for the analysis of social and geographic datasets.
During the past decade, there has been an increasing interest in this area from researchers from a variety of disciplines. Traditionally, data were collected through labour-intensive data gathering exercises (census, longitudinal studies, and so on). Instead, nowadays data are retrieved from social networks (user interactions and interests), mobile phones (location data and activity patterns) or IoT sensors (mobility patterns collected by means of card readers at stations, cameras, etc.).
The availability of a large amount of data from these new sources of data allow researchers to investigate different phenomena and processes such as sentiment and opinion analysis, mobility patterns, fake news detection, information spreading, and so on.
The goal of this project is a comprehensive survey that will be of interest for researchers and practitioners in any applied or theoretical area of data science related to the analysis, storage, modelling and visualisation of social and geographic datasets.
[email protected] 
"
https://www.turing.ac.uk/events/event-type/talk,"
"
https://www.turing.ac.uk/people/guest-speakers/dr-nina-klein,"
"
https://www.turing.ac.uk/people/former-enrichment-students/henry-louis-de-kergorlay,"
Henry-Louis is a PhD student in Mathematics at the University of Edinburgh. Prior to starting his PhD in 2015, he studied at Wesleyan University, Connecticut, USA, for his Bachelor, and at the University of Cambridge where he completed Part III of the mathematical tripos.
Henry-Louis is interested in Diffusion Maps, a way to perform non-linear dimension reduction on high-dimensional data sets. More generally, he is interested in developing techniques to extract geometric features of data sets. He is also working on Topological Data Analysis, and hopes to better understand how techniques from the former can be applied to the latter, and vice versa.
"
https://www.turing.ac.uk/people/guest-speakers/marc-lipsitch,"
Marc Lipsitch is Professor of Epidemiology and Director of the Center for Communicable Disease Dynamics at the Harvard T.H. Chan School of Public Health. He is an internationally-recognised expert in epidemiological methods and disease transmission modelling, and has been a leading scientific authority during the COVID-19 pandemic.
"
https://www.turing.ac.uk/people/external-researchers/praminda-caleb-solly,"
"
https://www.turing.ac.uk/opportunities-turing,"
​
There are many opportunities to work at the Turing, including Fellowships, research engineering, studentships and internships, and business team roles.
Find out more about employee benefits and our commitment to equality, diversity and inclusion.
"
https://www.turing.ac.uk/research/research-programmes/data-science-science-and-humanities,"
It is becoming easier and easier to collect large amounts of data across a broad range of research areas, and there is a growing need to understand how this can best be exploited to make new discoveries. The use of modern computational methods has already revolutionised research in physics and biology, and the stage is set for this approach to become a standard methodology in many different fields. However, we know it is not sufficient to just collect data, and hope that some generic algorithms will be able to help – the crucial step is in how to incorporate the deep knowledge that already exists about a system into the computational methods used.
For example, in particle physics, the search for the Higgs boson was guided by the well-developed theory of the Standard Model. In contrast, the search for particles of 'dark matter' has very little theory to guide it. In linguistics, there has been much progress in developing statistical models of language use, but it is not clear how to combine this with what is understood theoretically about how humans read, write and speak. In others cases, a computational model may be needed to make sense of data – for example, the need for an atomic-level model of a material when characterising its properties from high resolution images.
The main aim of the programme is to work with researchers from all disciplines across the Turing's university partner network, and with national research facilities, to make effective use of state of the art methods in artificial intelligence and data science.
The social sciences and arts provide particularly interesting challenges within the programme. Our understanding is often qualitative, and aligning this with what data sets are telling us can be difficult. There is also a considerable need to provide relevant training in these new methodologies, in a way that is acceptable and meaningful to researchers from non-numerate disciplines.
It is envisaged that, within a very few years, research across universities and national facilities will more and more come to be based on the computational data science and AI methods being developed at The Alan Turing Institute and its partners. It is therefore a key challenge that the programme's research remains at the vanguard of this movement nationally.

In April 2017, the Royal Society published the results of a major policy study on machine learning. This report considered the potential of machine learning in the next 5 – 10 years, and the actions required to build an environment of careful stewardship that can help realise its potential.  
Its publication set the direction for a wider programme of Royal Society policy and public engagement on artificial intelligence (AI), which seeks to create the conditions in which the benefits of these technologies can be brought into being safely and rapidly.
As part of this programme, in February 2019 the Society convened a workshop on the application of AI in science. By processing the large amounts of data now being generated in fields such as the life sciences, particle physics, astronomy, the social sciences, and more, machine learning could be a key enabler for a range of scientific fields, pushing forward the boundaries of science.  
Read the joint discussion paper by the Royal Society and The Alan Turing Institute, which summarises discussions at the workshop. It is not intended as a verbatim record and its contents do not necessarily represent the views of all participants at the event, or Fellows of the Royal Society or The Alan Turing Institute.
For more information about the programme, please contact [email protected]

"
https://www.turing.ac.uk/people/researchers/tomas-lazauskas,"
Tomas is Research Computing Team Lead at the Turing. His focus is to grow and lead a team developing and administering cloud and high-performance computing infrastructure, with a strategic responsibility for the Institute’s research computing needs. He works with researchers across the Institute, leads projects and makes individual contributions.
Tomas has a great interest in computer science and in a wide range of research domains, where the application of new technology can make a difference. He is particularly interested in helping researchers to take advantage of research computing, including cloud computing, high-performance computing and the internet of things, to facilitate cutting edge research in data science.
Tomas has a PhD in Mathematical Sciences from Loughborough University, which focused on developing and applying novel computer simulation techniques to model radiation effects in materials. Before joining the Turing, Tomas was a Research Associate at UCL, where he developed and maintained in-house software to perform complex simulations and turn terabytes of results' data into useful information. He also has experience working as a programmer in the commercial sector, as well as as an independent developer creating mobile applications and financial modelling tools.
"
https://www.turing.ac.uk/people/research-associates/chris-banerji,"
Chris is a Senior Research Associate with the Turing-Roche Partnership. As an interdisciplinary clinician-scientist Chris has a strong interest in mathematical and computational approaches to big biomedical data analysis, with the aim of understanding pathological heterogeneity at all levels (from molecular to clinical). His previous work has focused on biomarker and therapeutic discovery in muscular dystrophy, as well as cancer prognostics and stem cell biology. His academic work has been recognised by several international prizes, including the Muscular Dystrophy UK President’s Award for best Early Career Researcher.

Prior to joining the Turing, Chris studied Mathematics at Oxford before undertaking his PhD in computational and cell biology at UCL. After his PhD he studied clinical medicine as a graduate at Imperial College London and was subsequently employed as an academic clinician at Guy’s and St Thomas’ Hospitals, throughout the COVID pandemic. During his clinical training and employment, he has held a Visiting Postdoctoral position at King’s College London facilitating his ongoing research.
 
Chris is particularly interested in: stochastic modelling of gene expression, dynamical systems applied to cell-cell interactions, single cell/nuclear RNA-seq, spatial transcriptomics, network theory (applied to protein interactions), information theory (applied to biological signal transduction), transcriptomic biomarkers, cellular differentiation (in particular muscle regeneration and malignancy), clinical subtyping in heterogeneous pathology and classification approaches, dynamic image analysis, and importantly rare diseases. He would be delighted to discuss any of these topics and welcomes collaboration.
 
"
https://www.turing.ac.uk/people/external-researchers/albert-ko,"
Ko's research centers on the health problems that have emerged as a consequence of rapid urbanization and social inequity. He coordinates a research and training program on urban slum health in Brazil and is conducting prospective community-based studies on rat-borne leptospirosis, dengue, meningitis and respiratory infections. His research particularly focuses on understanding the transmission dynamics and natural history of leptospirosis, which is as a model for an infectious disease that has emerged in slum environments due to the interaction of climate, urban ecology and social marginalization. Current research combines multidisciplinary epidemiology, ecology and translational research-based approaches to identify prevention and control strategies that can be implemented in slum communities. Dr. Ko and his team has mobilized the public health research capacity at their site in the city of Salvador, Brazil to investigate the outbreak of Zika virus infection and microcephaly. He is also Program Director at Yale for the Fogarty Global Health Equity Scholars Program which provides research training opportunities for US and LMIC post and pre-doctoral fellows at collaborating international sites.
"
https://www.turing.ac.uk/people/researchers/adam-tsakalidis,"
Adam is a PhD candidate in Urban Science with the Warwick Institute for the Science of Cities, supervised by Dr. Alexandra I. Cristea and Dr. Maria Liakata. His main research interests are in the areas of Social Media Mining and Natural Language Processing, with a primary focus on sentiment analysis from social media and their implications in real-world events.
Over 70% of the global population is expected to live in big cities by the next 30 years. At the same time, the use of social media is growing at tremendous rates. Motivated by these basic facts, Adam's work focuses on opinion mining of social media users, as an attempt to model and predict urban-related indices (e.g., elections, wellbeing, etc.). The effective modelling of such domains is a challenging and important task, as it can provide additional ""sensors"" for informing citizens and policy makers on the current state of an urban environment, but also on possible consequences of their actions.
"
https://www.turing.ac.uk/research/research-areas/programming-languages/hardware-optimisation-fpgagpu,"
"
https://www.turing.ac.uk/policy-submissions,"

"
https://www.turing.ac.uk/people/enrichment-students/arnaud-dyevre,"
Arnaud is a PhD student in economics, with interests in macroeconomics and public economics. He is interested in the formation of production networks—the complex webs of trading relationships between firms—and how their evolution relates to trends in inequality, innovation and market power. To address these questions, he uses large-scale administrative and patent data, as well as models inspired by graph theory. During his time at the Turing, he is interested to collaborate with, and learn from, anyone interested in (1) non-conventional economic data, (2) network formation and (3) partial differential equations or functional analysis.
"
https://www.turing.ac.uk/people/spotlights/ibrahim-animashaun,"

I have a medical background and have always been interested in how technology could be used to improve healthcare. Around three years ago, I started my journey into data science by learning Python - I remember how excited I was when I wrote my first program ‘Hello world!’. 
My journey into data science led me to the exciting opportunity to be involved in the HDR UK’s second black internship programme. This is one of several steps they are taking to give black data scientists, a heavily under-represented group, the chance to be involved and flourish in the health data science sector. Interns are matched to various host organisations, and I was delighted to have been be welcomed and supported by The Alan Turing Institute. 
I had the opportunity to work as a research data scientist with the Research Engineering Team (REG) (a team of about 40 research data scientists and software engineers).  
I worked on a project exploring the use of synthetic data generators. This offers a way to generate realistic (but not real) patient data and associated health records in the OMOP (Observational Medical Outcomes Partnership) data models format using Synthea, an open-source patient generator. As a result of privacy and legal issues, healthcare data is difficult to access - synthetic data is useful to help enable data science experiments and machine learning applications.  
During the internship, with a lot of effort from myself, help from team members, my project team lead - May Yong, my line manager - Iain Stenson, my mentor - Markus Hauru, and Martin O’Reilly, Director of Research Engineering at the Turing. I developed technical and personal skills that will be extremely beneficial in my career
I had a very rewarding experience working at the Turing and it will certainly help my future in the field of health data science. I am grateful to have been given this opportunity to sharpen my job skills in this field and the chance to work for such a wonderful organisation.  
The most memorable experience was attending the RSECon 2022, the sixth annual conference for Research Software Engineering. There were interesting talks and enlightening workshops. I met people from different organisations, the dinner was amazing and the show at the dinner was just incredible.  
Attracting people from diverse backgrounds must be intentional and the Turing has made efforts in this regard with the Equality, Diversity and Inclusivity (EDI) strategy and action plan launched in September 2021.  
It is crucial to focus on growing the existing diverse talent in the Turing. If you can effectively nurture and promote diversity within an organisation, minorities will see it as an excellent place to work and will apply for open positions. Also, recruitment teams must actively seek out diverse candidates. One approach is to encourage diverse employees within the Turing to refer their connections. Creating a diverse candidate referral program is a great way to boost diversity. 
Finally, demonstrating the diversity already in the Turing is important to attract talents from diverse backgrounds,. This could be achieved by having a diverse interview panel for the interview process and displaying pictures that show the diversity at the Turing in places accessible to the public, for example the Turing’s website. A picture is still worth a thousand words. These could have far-reaching effects in encouraging people from a range of backgrounds to apply to the Turing.  
"
https://www.turing.ac.uk/events/london-data-week,"
From choosing the best Tube route to online shopping, data is part of our everyday lives as Londoners, and data can help us address big challenges in our city, like climate change and the cost of living.  
We want London to be the best city in the world; one where data is used to shape better, happier, healthier lives. We want data to be created, shared and used openly and fairly by everyone, from community groups and charities to businesses and the public sector. 
London is already a great place to use data, and we celebrate that: world-leading businesses using data or AI were created in London, by Londoners. There are 40 fantastic universities and research institutes across the city conducting transformative, interdisciplinary research in data science and AI. And at its heart are nine million smart and creative Londoners who make our city what it is. 
So, the Turing and the London Office of Technology and Innovation (LOTI) are delighted to present: London Data Week. From 3 to 9 July, there’ll be a great variety of fun and collaborative events tailor-made to bring together the people of London to inspire and empower them to be a part of shaping the city's future.  
Join us this July to imagine and create the future London. 
Visit the London Data Week website to find out more and see what events are on near you.
For more info and the week's agenda – including a brilliant variety of workshops, comedy events, cycling challenges and more - visit the London Data Week website.
Each day there’ll be something different. We want to make sure everyone can take part in something, so different events will take place at different times throughout the week, with some all day, some in the evenings and some in the mornings or afternoons.  
We welcome participation from all backgrounds. Visit the London Data Week website to find out more and register for individual events. 
"
https://www.turing.ac.uk/blog/future-work-will-soon-be-present,"
The future of work is a topic that is fuelling discussions and debates everywhere—from your neighbour’s dinner table to the break room of an IT outsourcing company in Bangalore and from an academic conference panel to the meeting room of an intergovernmental organisation. Everyone has a stake in the changing nature of work, yet many are confused about what is changing, why and how these changes take place, and whether anything can be done to shape the future.
There is an increasing amount of research focused on this domain. Many of the related themes such as the impact of new technology and role of automation tend to be presented as if they were novel phenomena disconnected from the past experiences of our societies and economies. However, careful research is able to analyse these changes while reflecting on the long history of the changing nature of work. After all, the first wave of automation already took place a few centuries ago during the Industrial Revolution. So what then, if anything, is different this time around, and what is at stake?
The overview that emerges from reviewing academic and policy literature on the topic suggests that multiple possible futures exist that depend on complex dynamics between context, choices and adaptability to new circumstances shaping the opportunities for individuals, firms, civil society organisations, governments and international organisations. Recent research on the topic has identified challenges, including rising inequality between and within countries, the undermining of collective power of workers, loss of jobs, hardship faced by younger generations and the disenfranchised and algorithmic control in the workplace.
However, the changing nature and organisation of jobs and the adoption of new technologies throughout the economy, also gives rise to new opportunities, such as new jobs in both technology-driven and traditional industries. Widening market access, better-suited models to govern and regulate the changing nature and organisation of work and improved workplace technology are other areas where changes may lead to positive outcomes. While these changes affect economies and societies differently around the world, the challenges and opportunities give rise to research opportunities and open up avenues for collaboration and learning. 
The changing nature and organisation of work and its diverse impacts on societies will need to be understood through not just best practice applied to new topics, but also emerging research approaches and themes rooted in data science and artificial intelligence, such as machine learning, robotics and network science. As the future of work quickly becomes the present, there is an urgent need for scholarship that attempts to understand how to make our new world of work sustainable, equitable and just.
The Alan Turing Institute is the UK’s national institute for data science and artificial intelligence, which aims to further data science and artificial intelligence research to address real-world problems. The institute commissioned our review of recent literature in order to understand what are some of the key questions related to the future of work, what approaches and empirical challenges exist to answering these questions and what are some of the results found. We’d encourage readers and those working in this area to take a look at the review and use it as an introduction to the topic.
Read the new landscaping report by the blog authors: Data Science, artificial intelligence and the futures of work commissioned by the Turing’s research programme in Economic Data Science. This week the Turing will publish a series of guest blogs exploring different angles of #futuresofwork as well as host a Data Debate: The AI will see you now, with the British Library the evening of Thursday 1 November. Later this autumn/winter the Turing will also be announcing a portfolio of research projects on 'The Changing Nature of Work' as part of the Turing-HSBC-ONS Economic Data Science Awards.
"
https://www.turing.ac.uk/people/guest-speakers/katie-collins,"
Katie is a PhD student in the Machine Learning Group supervised by Adrian Weller. She recently completed her MPhil in Machine Learning and Machine Intelligence in the same group. Her research centres around scalable trustworthy machine learning, data-efficient human-machine teaming and Bayesian probabilistic modelling.
She also has a deep interest in computational cognitive science, having received a BSc in Brain and Cognitive Science, with minors in Computer Science and Biomedical Engineering at the Massachusetts Institute of Technology (MIT).
She is also affiliated with the Human-Oriented Automated Theorem Proving Effort led by Sir Tim Gowers and is a Student Fellow at the Leverhulme Centre for Future Intelligence. Additionally, she is the founder of the MITxHarvard Women in AI Group, a Class of 2021 Marshall Scholar and a recipient of a Cambridge Trust Scholarship.
"
https://www.turing.ac.uk/research/research-programmes/public-policy/public-policy-people,"

"
https://www.turing.ac.uk/people/guest-speakers/tim-wright,"
Tim Wright has been at Leeds since 2006, initially as a Royal Society University Research Fellow and (since 2012) as Professor of Satellite Geodesy. His work has been at the forefront of developing the use of satellite radar interferometry (InSAR) for measuring tectonic and volcanic deformation. Major achievements include the first demonstration that inter-seismic strain can be measured using InSAR, in this case for the North Anatolian Fault; the investigation of a series of major earthquakes using geodesy, seismology and geomorphology, including Bam (Iran, 2003), Denali (Alaska, 2002), and Izmit (Turkey, 1999); the mapping and modelling of precursory inflation at a volcanic centre (Dabbahu, Ethiopia), and the subsequent discovery of a major rifting episode in Afar, Ethiopia. He has published more than 50 articles in major international journals, and his work is highly cited. He led the NERC-funded Afar Rift Consortium, a £3M project that is using a wide range of geophysical, geochemical and geologic techniques to investigate how the crust grows at divergent plate boundaries, and co-leads LICS, a NERC large grant to ""Look Inside the Continents from Space"". In 2006, he was awarded the William Smith Fund of the Geological Society, and a Philip Leverhulme Prize, in 2014 he received the AGU Geodesy Section Award, and in 2015 he was the British Geophysical Association's Bullerwell lecturer and received the Rosenstiel Award from the University of Miami. He was the Royal Astronomical Society's 2017 Harold Jeffreys Lecturer. He is director of the NERC Centre for the Observation and Modelling of Earthquakes, Volcanoes and Tectonics (COMET), and co-founder and director of SATSENSE Ltd.
"
https://www.turing.ac.uk/people/researchers/vaishak-belle,"
Dr Vaishak Belle is a Chancellor’s Fellow and Faculty at the School of Informatics, University of Edinburgh, an Alan Turing Institute Faculty Fellow, a Royal Society University Research Fellow, and a member of the RSE (Royal Society of Edinburgh) Young Academy of Scotland. At the University of Edinburgh, he directs a research lab on artificial intelligence, specialising in the unification of symbolic systems and machine learning, with a recent emphasis on explainability and ethics. He has given research seminars at numerous academic institutions, tutorials at AI conferences, and talks at venues such as Ars Electronica and the Samsung AI Forum. He has co-authored over 50 scientific articles on AI, at venues such as IJCAI, UAI, AAAI, MLJ, AIJ, JAIR, AAMAS, and along with his co-authors, he has won the Microsoft best paper award at UAI, the Machine learning journal best student paper award at ECML-PKDD, and the Machine learning journal best student paper award at ILP. In 2014, he received a silver medal by the Kurt Goedel Society. Recently, he has consulted with major banks on explainable AI and its impact in financial institutions. 
His research is at the intersection of machine learning and symbolic systems (logics, programs), in service of the science and technology of artificial intelligence. He is motivated by the need to augment learning and perception with high-level structured, commonsensical knowledge, to enable systems to learn faster and more accurate models of the world. He is most keen on computational frameworks that are able to explain their decisions, modular, re-usable, and robust to variations in problem description. Concretely, he works on themes such as probabilistic inference and learning, probabilistic programming, automated planning, statistical relational learning and epistemic reasoning.
"
https://www.turing.ac.uk/people/brieuc-lehmann,"
"
https://www.turing.ac.uk/node/3290,"

Researchers and practitioners face a common need for high quality tools, practices, methodologies, platforms and systems.
Many domains can benefit from the deployment of cutting-edge algorithms and approaches, but these cannot be effectively applied unless realised as usable software libraries, reproducible analyses and workflows, or high performance computational environments.
The Research Engineering Group (REG) contributes skills in research software engineering and data science in support of other programmes, as well as to its own projects. This model of working ensures that the tools they develop are useful and applicable to a wider range of areas. The team supports professional delivery of impactful research across the Turing's programmes, as well as its own research interests.
The team concentrate mainly on:
To keep up to date with the team, follow us on twitter at @turinghut23 or LinkedIn
Header image: Close up of a Bletchley Park Bombe, designed by Alan Turing and his colleagues to decipher German Enigma machine messages in WW2. Source: mendhak, Flickr, Creative Commons.

 
Read about some of our project highlights. See here for a complete list. 
Research Engineering is involved in many projects across the Turing, collaborating both with academics and strategic commercial partners.
Below is the list of our most recent projects, the full list can be found here.
For a full list of blogs and news associated with REG see here.
The most recent events involving REG are shown below. For a complete list see here. 
Below is the list of our most recent publications, the full list can be found here. 
"
https://www.turing.ac.uk/people/business-team/katy-thompson,"
"
https://www.turing.ac.uk/research/research-areas/machine-learning/reinforcement-learning,"
"
https://www.turing.ac.uk/news/publications/bayesian-statistics-and-modelling,"
Bayesian statistics is an approach to data analysis based on Bayes’ theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade.
van de Schoot, R., Depaoli, S., King, R. et al. Bayesian statistics and modelling. Nat Rev Methods Primers 1, 1 (2021). https://doi.org/10.1038/s43586-020-00001-2
"
https://www.turing.ac.uk/news/publications/repertoire-serous-ovarian-cancer-non-genetic-heterogeneity-revealed-single-cell,"
The inter-differentiation between cell states promotes cancer cell survival under stress and fosters non-genetic heterogeneity (NGH). NGH is, therefore, a surrogate of tumor resilience but its quantification is confounded by genetic heterogeneity. Here we show that NGH in serous ovarian cancer (SOC) can be accurately measured when informed by the molecular signatures of the normal fallopian tube epithelium (FTE) cells, the cells of origin of SOC. Surveying the transcriptomes of ∼6,000 FTE cells, predominantly from non-ovarian cancer patients, identified 6 FTE subtypes. We used subtype signatures to deconvolute SOC expression data and found substantial intra-tumor NGH. Importantly, NGH-based stratification of ∼1,700 tumors robustly correlated with survival. Our findings lay the foundation for accurate prognostic and therapeutic stratification of SOC.
Hu, Z., Artibani, M., Alsaadi, A., Wietek, N., et. al, The Repertoire of Serous Ovarian Cancer Non-genetic Heterogeneity Revealed by Single-Cell Sequencing of Normal Fallopian Tube Epithelial Cells, Cancer Cell, Volume 37, Issue 2, 2020, Pages 226-242.e7, ISSN 1535-6108,https://doi.org/10.1016/j.ccell.2020.01.003.
"
https://www.turing.ac.uk/news/statistical-oversight-could-explain-inconsistencies-nutritional-research,"
The standard and most common statistical approach to studying the relationship between food and health can give misleading and meaningless results, according to new research published today in the American Journal of Clinical Nutrition.
The researchers, from The Alan Turing Institute and the University of Leeds, found that the widespread practice of statistically controlling, or allowing for, someone’s total energy intake can lead to dramatic changes in the interpretation of the results. 
People often wonder why one nutritional study tells them that eating too many eggs, for instance, will lead to heart disease and another tells them the opposite. This report shows that the answer to this and other conflicting food studies may lie in the use of statistics.
Lead author Georgia Tomova, a PhD researcher in the University of Leeds’ Institute for Data Analytics and The Alan Turing Institute, said: ""These findings are relevant to everything we think we know about the effect of food on health. It is well known that different nutritional studies tend to find different results. One week a food is apparently harmful and the next week it is apparently good for you."" 
Controlling for other foods eaten can then further skew the results, so that a harmful food appears beneficial or vice versa. 
Ms Tomova added: ""Because of the big differences between individual studies, we tend to rely on review articles to provide an average estimate of whether, and to what extent, a particular food causes a particular health condition.  
“Unfortunately, because most studies have different approaches to controlling for the rest of the diet, it is likely that each study is estimating a very different quantity, making the 'average' rather meaningless.” 
The research, which was funded by The Alan Turing Institute, identified the problem by using new 'causal inference' methods, which were popularized by Judea Pearl, the author of “The Book of Why.” 
Senior author Dr Peter Tennant, Associate Professor of Health Data Science in Leeds’ School of Medicine and Turing Fellow, said: ""When you cannot run an experiment, it is very difficult to determine whether, and to what extent, something causes something else.  
“That is why people say, 'correlation does not equal causation.' These new 'causal inference' methods promise to help us to identify causal effects from correlations, but in doing so they have also highlighted quite a few areas which we did not fully understand."" 
The authors hope that this new research will help nutritional scientists to better understand the issues with inappropriately controlling for total energy intake and overall diet and gain a clearer understanding of the effects of the diet on health. 
Dr Tennant added: ""Different studies can provide different estimates for a range of reasons but we think that this one statistical issue may explain a lot of the disagreement. Fortunately, this can be easily avoided in the future.” 
Top image credit: Ella Olsson
"
https://www.turing.ac.uk/news/publications/design-choices-productive-secure-data-intensive-research-scale-cloud,"
We present a policy and process framework for secure environments for productive data science research projects at scale, by combining prevailing data security threat and risk profiles into five sensitivity tiers, and, at each tier, specifying recommended policies for data classification, data ingress, software ingress, data egress, user access, user device control, and analysis environments. By presenting design patterns for security choices for each tier, and using software defined infrastructure so that a different, independent, secure research environment can be instantiated for each project appropriate to its classification, we hope to maximise researcher productivity and minimise risk, allowing research organisations to operate with confidence.
Arenas, Diego & Atkins, Jon & Austin, Clare & Beavan, David & Cabrejas Egea, Alvaro & Carlysle-Davies, Stephen & Carter, Ian & Clarke, Rob & Cunningham, James & Doel, Tom & Forrest, Oliver & Gabasova, Evelina & Geddes, James & Hetherington, James & Jersakova, Radka & Kiraly, Franz & Lawrence, Catherine & Manser, Jules & O'Reilly, Martin & Whitaker, Kirstie. (2019). Design choices for productive, secure, data-intensive research at scale in the cloud. arXiv:1908.08737 [cs.CR]
"
https://www.turing.ac.uk/research/research-programmes/artificial-intelligence-ai/robotics,"
To create a world leading scientific programme of data driven AI research and innovation that addresses the unique challenges arising from and towards deployment of robotics and autonomous systems (RAS) technology for solving socially relevant problems across domains in a safe and ethical manner.
The key aim of this strategic challenge of the Turing's AI programme is to develop and support a world leading portfolio of activities that will lie at the intersection of data driven AI and machine learning, specifically targeted to the robotics and autonomous systems (RAS) domain. This will be achieved through:
The core research agenda will be focused through three strands (that are crucial but are missing or underrepresented elements in the current RAS and machine learning roadmap).
Each of these strands of research will be closely grounded in the RAS context and will have a grand challenge counterpart that it can enable, if successful. However, we will actively look for synergies where some of the results (for example in explainable and verifiable AI) can be applied to domains beyond RAS such as Internet of Things, medical diagnostics, etc.
Real time inference requirements, computational constraints of embedded, untethered and mobile platforms, hardware limits (torque, joint) for guaranteeing safety, approximate hierarchical inference for graceful performance degradation.
Intention detection and movement prediction, scalable multi-agent adversarial and collaborative policies, multi-modal sensor aggregation for decision making.
Enabling secure systems – communication, decision making; understanding and predicting failure modes, developing robustness and multiple failure recovery modes, fault and risk inference through probabilistic modelling.
The programme's engagement approach will aim to create a tangible pipeline that can take strong research and innovation to tangible deployable solutions. The solutions will deliver concrete benefits to society by closely working with various stakeholders – the government, local councils and industry.
The core research programme will identify and develop fundamental AI and machine learning underpinnings that need solving in the RAS domain. This will engage current EPSRC Centres for Doctoral Training (CDT) in Robotics, Data Science and Artificial Intelligence as well as their key scientific leads. Some of the Turing PDRA fellowships in this area are intended to act as a first opportunity for our brightest CDT graduates to play a leadership role.
In addition, we will leverage several world leading sites with substantial RAS assets in terms of cutting edge hardware platforms, to enable stakeholder testing of proof of concept deployments in living labs. We already have several UK wide national robotics hardware and field testing facilities in Edinburgh, Oxford, Bristol, London, Sheffield, and others. 
In conjunction with the living labs at the Bayes’ Centre in Edinburgh (the site of the Turing's robotics hub), we will work in domains ranging from oil and gas, mining, nuclear decommissioning, construction, smart mobility, high value manufacturing (e.g. aircraft), healthcare and smart assisted living space to galvanise industrial engagements through joint industry projects (JIPs) as well as proof-of-concept demonstrations for de-risking in realistic settings.
Finally, engagement with the government and funding agencies (including UKRI, BEIS and learned societies like RAEng, Royal Society, and Royal Society of Edinburgh) will help shape future research funding as well as policy making for enabling, de-risking and deploying RAS technology with the help of innovations in AI and data science.
[email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/cray,"
www.cray.com
"
https://www.turing.ac.uk/news/publications/decentralized-markets-and-emergence-housing-wealth-inequality,"
Recent studies suggest that the traditional determinants of housing wealth are insufficient to explain its current inequality levels. Thus, they argue that efforts should focus on understanding institutional factors. From the perspective of complex adaptive systems, institutions are more than the ‘the rules of the game’, they also consider the interaction protocols or the ‘algorithm’ through which agents engage in socioeconomic activities. By viewing markets as complex adaptive systems, I develop a model that allows estimating how much housing wealth inequality is attributable to the market institution. It combines virtues from two different modeling traditions: (1) the microeconomic foundations from overlapping-generation models and (2) the explicit interaction protocols of agent-based models. Overall, the model generates prices and housing inequality endogenously and from bottom-up; without needing to impose assumptions about the aggregate behavior of the market (such as market equilibrium). It accounts for economic and institutional factors that are important to housing consumption decisions (e.g., wages, consumption of goods, non-labor income, government transfers, taxes, etc.). I calibrate the model with the British Wealth and Assets Survey at the level of each individual household (i.e., ~25 million agents). By performing counter-factual simulations that control for data heterogeneity, I estimate that, in the United Kingdom, the decentralized protocol interaction of the housing market contributes with one to two thirds of the Gini coefficient. I perform policy experiments and compare the outcomes between an expansion in the housing stock, a sales tax, and an inheritance tax. The results raise concerns about the limitations of traditional policies and call for a careful re-examination of housing wealth inequality.
"
https://www.turing.ac.uk/opportunities/skills,"

Our current work focuses on delivering immersive opportunities that upskill the professional community and support research. This enables a two-way talent flow exchange between academia and industry, whilst facilitating the creation and delivery of open, free, and accessible technical training.
The skills programme remit spans the following four areas of work (more information below): 
Please see our current open opportunities that you can register for or apply to: 
Our academic programmes provide opportunities for early career researchers to engage with and become part of the Turing community. We offer immersive opportunities aimed at learning new methodologies, allowing space to develop research independence and engaging with the Turing community to establish new collaborations:
Past academic programmes include:   
The Turing aims to help our community apply their research to real-world problems. Our applied skills programme provides opportunities for working with industry, government and third sector on real-world data science questions alongside their academic research:​
Other two-way talent flow schemes are being developed to support junior and mid-career researchers, to help them gain experience in an industry setting. Additionally, to support industry practitioners engage closer with academia through placements and secondments.  
As a national institute we are involved in driving impact, shaping policy and convening groups of experts that can help develop strategies for data science and artificial intelligence skills across the UK through the following initiatives:
The Turing aims to support the development and delivery of high-quality and openly accessible courses for our research community and beyond:
Based within the Academic Engagement Directorate, the Skills Team is overseen by the Turing Skills Group (TSG) to develop the Turing's skills strategy. The TSG is chaired by Iain Styles and composed of subject matter experts from the Turing, Turing Fellows and industry and public sector practitioners.  
The TSG is responsible for the development and quality of the activities that fall under the remit of the skills programme, including operational and strategic matters both at the Institute and at the national level.
For enquiries, collaborations or anything else, please email us at [email protected]
To find out about upcoming Skills at the Turing opportunities, please enter your details below and sign up to our mailing list:

"
https://www.turing.ac.uk/events/clinical-ai-interest-group-aiuk-fringe-event,"
There is increasing excitement at the potential beneficial utility of artificial intelligence in clinical practice. Indeed, there are already examples of AI-computer vision applications assisting in clinical radiology, AI-enhanced sleep applications and AI-driven drug discovery being deployed in the health sphere.

The Alan Turing Institute Clinical AI Interest Group was launched in May 2022 and brings together over 250 health professionals and data scientists to connect, keep up-to-date with developments in the field of data science and AI, and enable research and application across health domains.
We welcome you to join our first hybrid event to meet, connect and formally launch the Supra-Clinical AI Groups. These groups are focused on specialist interests in the particular clinical fields of Medical imaging and Computer Vision, Neurology and Mental Health and Anaesthetics and Intensive Care.
You will hear from the leads of the Supra-interest groups, about our plans for the Clinical AI Summer School as well as it being a welcome opportunity to meet the wider community.
Our keynote will be given by Alastair Denniston, Consultant Ophthalmologist (Uveitis and Medical Retina), University Hospitals Birmingham NHSFT & Honorary Professor, University of Birmingham, UK.
His talk will be about 'Ensuring AI in health is safe, effective and equitable’.
This event is hybrid. The event location is The Alan Turing Institute, Enigma room, although you will also be able to join online using zoom link sent to you once you register.
Our agenda will be:
Please register your attendance at this event using this link: Register here
"
https://www.turing.ac.uk/people/researchers/carola-bibiane-schonlieb,"
Carola-Bibiane Schönlieb is a Reader in Applied and Computational Analysis at the Department of Applied Mathematics and Theoretical Physics (DAMTP), University of Cambridge since 2015. There, she is head of the Cambridge Image Analysis group, Director of the Cantab Capital Institute for Mathematics of Information, Co-Director of the EPSRC Centre for Mathematical and Statistical Analysis of Multimodal Clinical Imaging, and since 2011 a fellow of Jesus College Cambridge. Her current research interests focus on variational methods and partial differential equations for image analysis, image processing and inverse imaging problems.
Her research has been acknowledged by scientific prizes, among them the LMS Whitehead Prize 2016, and by invitations to give plenary lectures at several renowned applied mathematics conference, among them the SIAM conference on Imaging Science in 2014, the SIAM conference on Partial Differential Equations in 2015, the IMA Conference on Challenges of Big Data in 2016 and the SIAM annual meeting in 2017. Carola graduated from the Institute for Mathematics, University of Salzburg (Austria) in 2004. From 2004 to 2005 she held a teaching position in Salzburg. She received her PhD degree from the University of Cambridge in 2009. After one year of postdoctoral activity at the University of Göttingen (Germany), she became a Lecturer in at DAMTP in 2010, promoted to Reader in 2015.
She is interested in the interaction of mathematical sciences and imaging. She studies non-smooth and possibly non-convex variational methods and nonlinear partial differential equations for image analysis and inverse imaging problems, among them image reconstruction and restoration, object segmentation, and dynamic image reconstruction and analysis such as fast flow imaging, object tracking and motion analysis in videos.
Moreover, she works on computational methods for large-scale and high-dimensional problems appearing in, e.g. image classification and 3D and 4D imaging. Within this context she is interested in both the rigorous theoretical and computational analysis of the problems considered as well as their practical implementation and their use for real-world applications. Currently, her research focuses on customising variational image analysis and image reconstruction models to applications by learning their setup from real-world data training sets.
To this end she investigates so-called bilevel optimisation techniques in which the solution is typically constrained to a non-smooth variational problem or a nonlinear PDE. She has active interdisciplinary collaborations with clinicians, biologists and physicists on biomedical imaging topics, chemical engineers and plant scientists on image sensing, as well as collaborations with artists and art conservators on digital art restoration.
"
https://www.turing.ac.uk/research/interest-groups/bridging-machine-learning-and-behaviour-models,"
Over the years, the behavioural modelling and the machine learning communities have progressed in parallel streams in the pursuit of predicting outcomes in mobility, health, environment, marketing and other domains. The interdisciplinary interest group will work on developing behavioural theory-driven machine learning approaches that combine elements of traditional behaviour modelling and machine learning in different domains. It will thus help in getting the best of both worlds.
Historically, behaviour modellers have relied on mathematical models based on theories of psychology and economics to quantify how human decisions are affected by the attributes of the alternatives and characteristics of the decision-makers. These models are typically calibrated using small-scale survey data which are expensive to collect and prone to measurement errors and biases. On the other hand, the availability of large-scale passive data sources in recent years has led to wide-spread use of data-driven machine learning models which typically do not have a strong behavioural underpinning. This questions the applicability of the machine learning techniques in human behaviour prediction in the context of disruptive changes (e.g. radically new technologies, financial meltdown, etc.) where only the core behavioural principles are likely to hold, not the historic patterns of choices. 
O1: Identify the strengths and weaknesses of machine learning and traditional behaviour modelsO2: Contrast the methodologies used in machine learning and traditional behaviour modelling and formulate methods to combine themO3: Apply the methodologies in different domains like mobility and urban analytics, health, environment, marketing and finance, etc.
What can the machine learning research community ‘learn’ from the behaviour modellers and vice versa?
Click here to join us and request sign-up
Charisma Choudhury[email protected]
Ed Manley[email protected]
 
"
https://www.turing.ac.uk/people/guest-speakers/sam-cannicott,"
In this role he leads the Centre’s collaborative work with other organisations, supporting them to develop and deploy data driven technology responsibly. His project portfolio includes work with police forces, the Ministry of Defence, local authorities and the Department for Education. 
 
Before joining the Centre, Sam worked in the New Zealand civil service where he led a public engagement programme exploring citizens’ views of how data about them should be used and shared. He has previously worked in multiple policy roles with a particular focus on the education sector. 
"
https://www.turing.ac.uk/people/business-team/pauline-kinniburgh,"
"
https://www.turing.ac.uk/events/data-science-physics-and-astronomy-scoping-workshop,"
Fundamental research in physics and astronomy routinely produces some of the world's largest and most complex datasets, with huge discovery potential. Recent examples include the direct detection of gravitational waves, the discovery of the Higgs boson, and the identification of earth-like exoplanets. The success of many projects depends critically on the ability to extract information rapidly and/or with high precision and accuracy. These disciplines are therefore prime exploiters of modern data science concepts and algorithms, but they also push the boundaries of data science applications. Challenges include inference from large and low signal-to-noise datasets, mining for rare events/objects, decision/classification with extremely high data rates, machine-learning assisted modelling, extremely large simulations, and much more.
To tackle the data deluge, there is an urgent need for knowledge exchange between research fields, as well as with data science, statistics, and computer science practitioners. Great benefits can be expected from pooling ideas and resources and from efficient lines of communication between these fast-moving fields. Moreover, physics and astronomy researchers need to establish funding lines specifically for data science methodology and applications to remain internationally competitive.
This hands-on workshop will bring together physics and astronomy researchers, data science practitioners, as well as industry partners and public sector funders at The Alan-Turing Institute to discuss current challenges and opportunities in data science for physics and astronomy, launch new interdisciplinary collaborations, and work towards a more permanent forum for data science knowledge exchange.
 
Monday 2 December
10.30-11.00 - Registration, tea & coffee
11.00-11.30 - Welcome, practicalities, scope of meeting
11.30-13.00 - Problem pitches/self-intro
13.00-14.00 - Lunch
14.00-14.30 - Splinter group allocation
14.30-15.30 - Breakout session I
15.30-16.00 - Coffee break
16:00-17.00 - Breakout session I
17.00-18.00 - Closing
 
Tuesday 3 December
9.30-10.00 - Registration, tea & coffee
10.00-10.30 - Status reports from splinter groups; road blocks, mergers, splits, etc.
10.30-11.30 - Breakout session II
11.30-12.00 - Cofffee break
12.00-13.00 - Breakout session II
13.00-14.00 - Lunch
14.00-15.00 - Reports from splinter groups
15.00-15.30 - Coffee break
15.30-16.30 - Next steps, concluding remarks


"
https://www.turing.ac.uk/people/research-associates/kristina-ulicna,"
Kristina is a Research Associate in the Data Science for Science programme, where she works on developing cutting-edge AI-driven computer vision tools applied to bioimage analysis.
Kristina first became acquainted with The Turing through her PhD research work in the field of time-lapse microscopy image analysis and single-cell tracking. She is interested in multigenerational cell lineages and time series analysis of single-cell trajectories using computer vision approaches. In particular, Kristina puts emphasis on training deep neural network models which learn meaningful and explainable latent representations of the image data to drive biological discovery.
Prior to her PhD with the London Interdisciplinary Doctoral (LIDo) Programme at UCL, Kristina completed an integrated BSc in Biomedical Science at King’s College London with a year in industry, where she gained practical experience in a biopharmaceutical company in Cambridge, UK. As an undergraduate, she built a diverse repertoire of internship experience in world-renowned biomedical research laboratories, including MIT and Cancer Research UK.
Fascinated by the area of computational biology, Kristina then transitioned into a fully computational bioscience PhD research project and strengthened her skills over her 6-month placement as an AI Research Intern at Microsoft Research Cambridge. Her thesis work yielded multiple outputs, including scientific publications and contributions to collaborative codebases, as well as frequent research talks and poster presentations at national and international conferences, which goes well in line with her passion to travel and explore new regions of the world.
Beyond her research activities, Kristina is passionate about bridging the gap between biomedicine and computer science as well as bringing the science outcomes to the lay audience. She often tweets about her work and the recent advances in her field, and in the past actively contributed to improve the communication of scientific research as a National Delegate at the United Nations’ Youth Assembly and at the European Commission’s international conference series.
Kristina speaks multiple languages (including python), continues to deepen her expertise in other branches of AI, and grows her involvement in various mentoring schemes sharing expertise to navigate the pitfalls of research education.
Away from the computer screen, she is a leisure dancer and enjoys long walks, runs, and taking good care of her body and mind.
Data Science for Science and Humanities
Molecular Structures under Constraints
Machine Learning for CryoEM
"
https://www.turing.ac.uk/research/research-projects/improving-tracking-iceberg-populations-southern-ocean,"
The shape and distribution of icebergs and their trajectory through the oceans from source, to where they break up and melt, can be diagnostic of ice-sheet dynamics, and ocean and atmospheric conditions. Existing methods to detect icebergs in satellite imagery are optimised for open ocean and work less well for icebergs within the sea ice pack. This project seeks to use machine learning techniques to identify, track and follow the disintegration of icebergs in the Amundsen Sea near Antarctica, using radar satellite images. The study may reveal temporal trends in icebergs populations and their trajectories through the coastal seas.
The Antarctic ice sheet loses ice in roughly equal parts through direct melting and the formation of icebergs. Icebergs range from the massive tabular icebergs, that cover many hundreds or square kilometres, to small fragments the size of a car. In the Amundsen Sea - an arm of the Southern Ocean off Marie Byrd Land in western Antarctica - there is an expectation the current retreat of the ice sheet, which is now making a significant contribution to sea-level rise, is causing a transition in the dominant processes of iceberg formation, from the larger tabular icebergs to small narrow icebergs which topple over. This project is using machine learning techniques to identify, track and follow the disintegration of icebergs using all-weather radar satellite images from ESA satellites.
There are established methods for identification of icebergs in radar satellite imagery mainly based on CFAR (Constant False Alarm Rate) detection methods. However there are a number of limitations to these approaches, e.g. they work well in open ocean, but are less effective for icebergs within the sea ice pack or tabular icebergs in close proximity. New approaches will aim to identify icebergs in both the open ocean and in sea ice. Further improvements will consider information about iceberg size and shape, and tracking icebergs trajectories.
"
https://www.turing.ac.uk/research/research-areas/theoretical-mathematics/geometry-topology,"
"
https://www.turing.ac.uk/events/rsf-seminar-series-everyone-has-say,"
A regular series of open invitation talks from leading voices in data science, AI, healthcare and those with lived experience dealing with multiple long-term conditions (MLTC). This seminar series is part of the AI for multiple long-term conditions: Research Support Facility (RSF) project.
Event title: Everyone has a say: Embedding human-centred design (HCD) in the research setting
Speakers: Meag Doherty, Director, Research Experience, National Institutes of Health, All of Us Research Program
 
Human-Centred Design is the intentional process of building products and experiences in direct collaboration with the intended audience and can be a useful approach when designing research that focuses on public and patient experience.
In this talk, Meag Doherty will first explain the key elements of Human-Centred Design and how its frameworks are useful for Patient and Public Involvement and Engagement research projects. She will then describe how to get started with applying these frameworks at a programme level, and how to advocate for the inclusion of patients throughout a project. Finally, she will provide an example of applying Human-Centred Design specifically in a health programme, by discussing All of Us, a Research Programme of the National Institutes for Health aiming to recruit one million people across the US ""to build one of the most diverse health databases in history"".
This talk is part of a series of events that the RSF is hosting in September, all centred around Patient and Public Involvement and Engagement (PPIE). In this seminar session, we hope that attendees will learn about human-centred design and how this learning can be applied to their respective areas of research.
Everyone is welcome at this session, however, the target audience is clinicians, policymakers and health data science researchers. We hope that attendees will learn more about this HCD method and how this learning can be applied to multiple long-term conditions research.
By registering for the event, you are agreeing to the events code of conduct: Events code of conduct | The Alan Turing Institute
"
https://www.turing.ac.uk/people/researchers/peter-tennant,"
Peter Tennant is an Associate Professor of Health Data Science at the University of Leeds and Fellow of the Alan Turing Institute. Initially trained as an epidemiologist, his research is now primarily focussed on adapting and translating contemporary causal inference methods into health and social science research. He is also an experienced educator and renowned public speaker, who is regularly invited to speak to a diverse range of audiences. He leads the Causal Inference Interest Group within the Alan Turing Institute and the Introduction to Causal Inference Course for Health and Social Scientists.    
Peter’s research is primarily focussed on adapting and translating contemporary causal inference methods into health and social science research. This takes place across four overlapping themes: 
Methods and theory - increasing fundamental understanding of the analysis and interpretation of observational data using causal inference methods. Examples include his recent work on analyses of change scores and analyses of compositional data. 
Meta-science and guidelines – increasing the use and improving the quality of causal inference research. Examples include his reviews on the use of directed acyclic graphs and on the use of causal and association language. 
Applied domain implications - using causal inference theory to understand and explain methodological challenges in specific applied research domains. Examples include his recent work on adjustment for energy intake in nutritional research and the choice of outcomes in COVID-19 research. 
Applied causal research – using causal inference methods to estimate causal effects in health and social science settings. Examples include his recent work on the effect of gestational diabetes on the risk of stillbirth and the effect of fasting plasma glucose on the risk of large-for-gestational age 
"
https://www.turing.ac.uk/privacy-policy,"
​
The Alan Turing Institute and its subsidiaries (“we” or “the Turing”) are committed to ensuring transparency in our use of personal information.
This notice explains how we collect and use your personal information when:
Due to the nature of our work as a research institute, we may also process personal information for research projects. This will not be the case for the typical site user, and is more likely to apply to the data we collect through our platforms. However, because it is not always possible for us to proactively provide a privacy notice to those individuals, we also explain below how we process information when we carry out research.
This notice is not intended to cover those who are engaged or employed by us in their engaged/employed capacity and a separate privacy notice will apply to them.
The Alan Turing Institute is a registered charity in England and Wales (charity number 1162533) and a company limited by guarantee, incorporated in England (number 9512457) which is based at the British Library, 96 Euston Road, London NW1 2DB. We also have a subsidiary, Turing Innovation Limited.
If you have any questions about this notice or how we handle your personal information, please contact [email protected].
 
When we use the phrase ‘personal information’ we mean any information that relates to you. At its simplest, this could be your name or email address. It could also include the content of any communications you have with us.
 
We may collect personal information:
 
We may collect, store and use the following categories of personal information about you:
Certain categories of information are sensitive and need more protection (for example, information about race, ethnicity, or health, as well as information about criminal convictions). We do not usually collect these categories of information for a typical site visitor. Where we collect this information we will explain our purposes for using this data and we will ask for your consent. You have the right to withdraw this consent at any time (see below for more information). If you withhold or withdraw consent this will not affect your use of the services.
We might separately collect these categories of information for a research project, but only if we have a valid reason for doing so and only if the law allows us to. For example, the law may allow us to use this information where it is substantially in the public interest and where we have protections in place to safeguard your information and your rights.
 
We will only use your personal information when the law allows us to.  Most commonly, we will use your personal information:
We may (less commonly) use your personal information:
Where we rely on legitimate interests, our legitimate interests include:
 
We use your personal information to:
Or for another reason if that reason is compatible with the original purpose.
 
As a research institute, we may use personal information in ways which involve ‘automated decision making’ and which might constitute ‘profiling’; however, we do not typically do this in ways which will have a significant legal effect on you – for example, to assess your creditworthiness.
We do not make automated decisions based on any particularly sensitive personal information, unless it is justified in the public interest, in which case we will put in place appropriate measures to safeguard your rights.
When you use our sites, we may use the information collected to profile you. This is done to help us monitor the demographics of individuals who are engaging with certain materials where such monitoring is considered to be essential to the core aims of the relevant site - for example to meet our aim of engaging with a diverse cross section of our community in steering projects.
 
We sometimes share information with other organisations, including other entities in our group, partner universities, founding members, research partners, and service providers.
We will share your information with third parties where required by law or where we have a legitimate interest in doing so.
Where we share information with other ‘controllers’ (organisations who will use the information for their own purposes, rather than to provide services to us) they are responsible to you for their use of your information and compliance with the law. We share information with other controllers such as:  founding members; research partners; project sponsors; HMRC; Charity Commission; and our professional advisers. We share the information in a secure way and take appropriate steps to ensure the recipient protects it.
We may also need to share your personal information in the context of the possible merger, restructuring or expansion of the Institute or our subsidiaries.
Where permitted by law, we may share the personal information collected through our sites with carefully selected partners and parties who collaborate with us on projects.
We do this so that: we can demonstrate our understanding of the diversity of our sites’ communities; to improve our sites;  to run events that will be of relevance to the interests of the communities that use our sites; and so that we can be transparent and held accountable in relation to the commitments we have made to promote diversity and inclusion in our projects.
We will always limit the data we share to what is necessary to achieve these purposes and we will apply measures with the aim of anonymising the data we share so that it will be difficult to trace back to you, such as removing direct identifiers and aggregating data before sharing.
For example, for the AI Standards Hub our partners would include: The British Standards Institution (“BSI”); the National Physical Laboratory (“NPL”) and the Department for Digital, Culture, Media & Sport (“DCMS”). For more information on how they will handle your data and how to contact them, please see their privacy notices that can be found by clicking the links below:
 
We are based in the UK. Generally, we hold personal information on servers located within the UK. However, we do use suppliers that operate around the world, which means it is possible that personal information we collect from you may be stored outside of the UK or the EU. Some of our sites may have servers outside of the UK or EU.
Certain countries outside the UK have a lower standard of protection for personal information, including lower security protections and fewer rights for individuals. Where your personal information is transferred outside the UK to a country which does not offer similar protection, we put in place appropriate measures to ensure that your personal information is treated by those third parties in a way that is consistent with and respects UK data protection laws (such as requiring the recipients to enter into contracts which have been approved by the relevant UK authorities for this purpose).
 
We have put in place suitable technical and organisational measures to protect the security of your information. Details of these measures are available from [email protected]. These measures include limiting access to your personal information to those individuals who have a business need to know.
 
We keep your personal information for as long as we reasonably need to use it for the reasons set out in this notice (see section 6 above). Typically, this will include the period for which the personal information is actively in use, plus up to 7 years, which is the time that the information may be relevant for the establishment or defence of legal claims.
Where a minimum retention period is required by law (such as retaining records for HMRC purposes) we comply with that minimum period plus up to 12 months to allow time for us to anonymise or delete information in accordance with our internal data management processes.
We may also keep personal information for longer where we are processing it for scientific or historical research purposes.
In some circumstances we may anonymise your personal information so that it can no longer be associated with you.
 
Under certain circumstances, by law you have the right to:
To exercise any of the above rights, please contact [email protected].
These are legal rights, so they only apply in certain circumstances and are subject to exemptions.
We may also need to take steps to confirm your identity. This is another security measure to ensure that personal information is not disclosed to any person who has no right to receive it.
If you have concerns about the way we are handling your personal information, you also have the right to complain to the Information Commissioner’s Office (ICO) at ico.org.uk/make-a-complaint/.
 
We may update this notice. If we make substantial changes, we will bring these to your attention where reasonably possible. Otherwise, you can access the latest version of this notice on our main website.
 
If you have any questions about this privacy notice, please contact [email protected].
"
https://www.turing.ac.uk/research/research-projects/ai-multiple-long-term-conditions-research-support-facility,"
The Alan Turing Institute, in conjunction with Swansea University and the University of Edinburgh, is developing data standards, disseminating best practices, and building community around researchers, patients and the public involved in AI for multiple long-term conditions research.
The innovative Research Support Facility is part of a £23 million investment by the NIHR in AI, and will connect researchers across the consortia, to ensure the investment delivers long-term, real-world impact for the programme and beyond.
The new Research Support Facility (RSF), based at the Turing in conjunction with Swansea University and University of Edinburgh, will offer AI and advanced data science support to the research teams funded by AIM and foster collaboration. The facility, led by Dr Kirstie Whitaker and Professor Chris Holmes, will embed best practices in data security and standards, reproducibility, and public and patient engagement across the research collaborations funded by the programme, ensuring effective knowledge sharing and reinforcing the Turing’s role as a national convenor and capacity builder in data science and artificial intelligence.
The Research Support Facility work is split across five, interconnected themes:
Theme 1: Reproducible, secure and interoperable infrastructure
This theme will bring different research collaborations across the UK together within a trusted research environment to facilitate data, software and analysis sharing. This will support a progressive move from reproducible, to reproduced, to reused research artefacts and outputs, maximizing the return on the NIHR’s investment in the overall research programme.
Theme 2: Accessible, research ready data
Data wranglers, experts in data curation and quality control, will work with the Research Collaborations to align datasets and standards. This will enable a broad range of data to be incorporated into extended analyses within AIM and beyond.Theme 3: Community building and training
This theme aims to build connections between early career researchers across the AIM programme so their existing expertise can be shared across AIM and into the wider network. The other core aspect of the theme will be training and mentorship in the digital skills researchers need to deliver open source outputs from their Research Collaborations.Theme 4: Patient and public involvement and engagement
Enhancing existing patient and public involvement networks across the AIM programme, this theme will support and empower people with lived experience of MLTC to co-create the research with the teams. Online engagement activities such as talks and seminars that are accessible to all will set the standard for patient involvement in other healthcare areas in the future.
Theme 5: Sustainability and legacy
This theme will work with researchers to embed outputs in existing communities, both clinical and academic, and engage with policy makers to coordinate a long term investment in MLTC research. It is a key part of ensuring that AIM research continues beyond the end of this specific investment and the impact of the work conducted benefits as many people as possible. 
An estimated 14 million people in England are living with two or more long-term conditions, with two-thirds of adults aged over 65 expected to be living with multiple long-term conditions by 2035.
People who develop multiple long-term conditions (MLTC) often do not have a random assortment of diseases but rather a largely predictable cluster of conditions. Developing a better understanding of these disease clusters, including how they develop over the course of a person’s life and are influenced by wider determinants of health, requires novel research and analytical tools that can operate across complex datasets.
The Artificial Intelligence for Multiple Long-Term Conditions (AIM) call, from the National Institute for Health and Care Research (NIHR), in partnership with NHSX, funds research that combines data science and AI methods with health, care and social science expertise to identify new clusters of disease and understand how multiple long-term conditions develop over the life course.
 
To support this work, the AIM RSF will host a series of ‘open invitation talks’ from thought-leaders and experts on aspects of data science and MLTC research. The talks will be open to everyone across the AIM programme and the broader multiple long-term conditions research community. Sessions will be made available on YouTube afterwards.
The series is scheduled for the second week of each month on Tuesdays at 1:30-2:30, starting on 8th March 2022.
For more information about the RSF’s programme of events, and to register for future sessions, please go to this page.

The NIHR (National Institute for Health and Care Research) has awarded almost £12 million to new research that will use advanced data science and AI methods to identify and understand clusters of multiple long-term conditions and develop ways to prevent and treat them. The Alan Turing Institute, alongside Swansea University, University of Edinburgh and MRC Harwell, has been awarded £3 million to establish a new Research Support Facility as part of the programme. Find out more.

[email protected]
"
https://www.turing.ac.uk/people/researchers/alejandro-frangi,"
Alejandro (Alex) was born in La Plata, Argentina. In 1991, he moved to Barcelona, Spain, where he obtained his undergraduate degree in Telecommunications Engineering from the Technical University of Catalonia (Barcelona) in 1996. Then he researched electrical impedance tomography for image reconstruction and noise characterisation at the same institution under a CIRIT grant. In 1997, he obtained a grant from the Dutch Ministry of Economic Affairs to pursue his PhD in Medicine at the Image Sciences Institute of the University Medical Centre Utrecht on model-based cardiovascular image analysis. During this period, he was visiting researcher at the Imperial College in London, UK, and in Philips Medical Systems BV, The Netherlands.
Prof Frangi is Diamond Jubilee Chair in Computational Medicine at the University of Leeds, UK, with joint appointments at the School of Computing and the School of Medicine. He is also Royal Academy of Engineering Chair in Emerging Technologies, focusing on Precision Computational Medicine for In Silico Trials of Medical Devices. He is the Scientific Director of the Leeds Centre for HealthTech Innovation, a joint initiative from the University of Leeds and the Leeds Teaching Hospitals NHS Trust, and the Director of Research and Innovation of the Leeds Institute for Data Analytics. During 2001-2020, he founded and directed the Centre for Computational Imaging and Simulation Technologies in Biomedicine. Prof Frangi has visiting positions at KU Leuven, the Cixi Institute of Biomedical Engineering, CNITECH Chinese Academy of Science (Ningbo), the Zhejiang University of Technology (Hangzhou), Shenzhen University (Shenzhen), and Beijing Institute of Technology (Beijing).
Professor Frangi's main research interests lie at the crossroad of medical image analysis and modelling, emphasising machine learning (phenomenological models) and computational physiology (mechanistic models). He has a particular interest in statistical methods applied to population imaging and in silico clinical trials. He is also interested in physics informed deep learning and, more generally, integrating prior knowledge in otherwise data-driven approaches. His highly interdisciplinary work has been translated to the areas of cardiovascular, musculoskeletal and neurosciences. He has been the principal investigator or scientific coordinator of over 25 national and European projects funded by public and private bodies totalling over £50m.
Prof Frangi edited several books and editorial articles and over 260 journal papers in key international journals of his research field. He has been times Guest Editor of special issues of IEEE Trans Med Imaging, IEEE Trans Biomed Eng, and Medical Image Analysis journals. He was chair of the 3rd International Conference on Functional Imaging and Modelling of the Heart (FIMH05) held in Barcelona in June 2005, Publications Chair of the IEEE International Symposium in Biomedical Imaging (ISBI 2006), Programme Committee Member of various editions of the Intl Conf on Medical Image Computing and Computer-Assisted Interventions (MICCAI) (Brisbane, AU, 2007; Beijing CN, 2010; Toronto CA 2011; Nice FR 2012; Nagoya JP 2013), International Liaison of ISBI 2009, Tutorials Co-Chair of MICCAI 2010, and Programme Co-chair of MICCAI 2015. He was also General Chair for ISBI 2012 held in Barcelona and General Chair of MICCAI 2018 held in Granada, Spain. He will be General Chair of Information Processing in Medical Imaging (IPMI) in 2023 in Bariloche, Argentina.
"
https://www.turing.ac.uk/news/publications/significance-hyperparameter-optimization-metastasis-detection-breast-histology,"
Breast cancer (BC) is the second most leading cause of cancer deaths in women and BC metastasis accounts for the majority of deaths. Early detection of breast cancer metastasis in sentinel lymph nodes is of high importance for prediction and management of breast cancer progression. In this paper, we propose a novel deep learning framework for automatic detection of micro- and macro- metastasis in multi-gigapixel whole-slide images (WSIs) of sentinel lymph nodes. One of our main contributions is to incorporate a Bayesian solution for the optimization of network’s hyperparameters on one of the largest histology dataset, which leads to 5% gain in overall patch-based accuracy. Furthermore, we present an ensemble of two multi-resolution deep learning networks, one captures the cell level information and the other incorporates the contextual information to make the final prediction. Finally, we propose a two-step thresholding method to post-process the output of ensemble network. We evaluate our proposed method on the CAMELYON16 dataset, where we outperformed “human experts” and achieved the second best performance compared to 32 other competing methods.
Koohbanani N.A., Qaisar T., Shaban M., Gamper J., Rajpoot N. (2018) Significance of Hyperparameter Optimization for Metastasis Detection in Breast Histology Images. In: Stoyanov D. et al. (eds) Computational Pathology and Ophthalmic Medical Image Analysis. OMIA 2018, COMPAY 2018. Lecture Notes in Computer Science, vol 11039. Springer, Cham
"
https://www.turing.ac.uk/news/publications/measuring-national-mood-music,"
We propose a new measure of national valence based on the emotional content of a country’s most popular songs. We first trained a machine learning model using 191 different audio features embedded within music and use this model to construct a long-run valence index for the UK. This index correlates strongly and significantly with survey-based life satisfaction and outperforms an equivalent text-based measure. Our methods have the potential to be applied widely and to provide a solution to the severe lack of historical time-series data on psychological well-being.
E. Benetos, A. Ragano, D. Sgroi, and A. Tuckwell, “Measuring National Mood with Music: Using Machine Learning to Construct a Measure of National Valence from Audio Data”, Behavior Research Methods, Feb. 2022. https://doi.org/10.3758/s13428-021-01747-7
636.2 KB
"
https://www.turing.ac.uk/people/researchers/niels-peek,"
Niels Peek is Professor of Health Informatics and Strategic Research Domain Director for Digital Health at the University of Manchester. He has a background in Computer Science and Artificial Intelligence, and his research focuses on data-driven methods for health research, healthcare quality improvement, and computerised decision support. He leads the Greater Manchester Connected Health City, which is part of the £20M ""Health North"" investment to establish a learning health system in the North of England. Professor Peek has co-authored more than 150 peer-reviewed scientific publications. From 2013 to 2017 he was the President of the Society for Artificial Intelligence in Medicine.
He is a member of the editorial boards of the Journal of the American Medical Informatics Association and the Artificial Intelligence in Medicine journal. In April 2017, he organised the Informatics for Health 2017 conference in Manchester which was attended by more than 800 people from 30 countries. He also co-chaired the Scientific Programme Committee of MEDINFO-2017, the 16th World Congress on Health and Biomedical Informatics, which was held in Hangzhou, China, in August 2017. In 2018 he was elected to become a fellow of the American Collecege of Medical Informaticians and a fellow of the Alan Turing Institute.
Health data, for example from electronic health records or other sources such as wearables and the Internet, are subject to complex data generation processes, which are not addressed with existing methods for risk prediction. Professor Peek's research for the Turing focuses on two key issues. First, routine health data is subject to ‘informative presence’ whereby presence of a particular observation (e.g. a blood test) is informative independent of the actual result of the test. For example, it gives information about an individual’s tendency to engage with the healthcare system, and/or information about a clinician’s prior beliefs about a patient’s condition that drives them to run particular tests. This is challenging to model because it essentially corresponds to a ‘missing not at random’ scenario. Second, existing risk prediction models model prognostic risk only – and do not allow consideration of ‘what-if’ scenarios.
potential outcomes (causal) framework would allow us to infer risk under different intervention scenarios – both at individual patient level and at policy level. Initial work has explored the use of marginal structural models to infer potential outcomes for patients at risk of cardiovascular events – where the intervention is a statin prescription. Currently more advanced models that allow ‘what-if’ prediction modelling using messy observational data are being developed.
"
https://www.turing.ac.uk/research/research-projects/instrumented-infrastructure,"
Instrumentation of infrastructure is revolutionising how it is delivered, maintained and controlled. For example, engineers are starting to instrument railway bridges with fibre optic sensors, in order to understand more about the structural health of networks of bridges, and manage their operation, resilience, and reliability. In this project, new statistical methods are being developed to improve productivity of networks through better understanding of the vast amounts of data produced by instrumented infrastructure.

In this project, we are currently investigating a prototype railway bridge instrumented with ~130 fibre-optic sensors that record changes in wavelength over time. These time series of wavelength readings are converted to strain measurements that are used to monitor how the load of a train is distributed across the entire bridge. The sensor readings are correlated and exhibit time-lagged and dampened features due to the different locations of the sensors and the force exerted by an incoming train.
Structural engineers are interested in characterising these correlations as a train passes over the bridge in order to understand the strength and stiffness at different locations and also detect any changes in bridges structure. We are currently investigating various statistical methods to model the wavelength readings, that encapsulate the correlation structure between sensors. In order to detect degradation of the bridge, we shall monitor this correlation signature of the bridge and signal when there is a departure. This may involve using statistical techniques used in the change-point detection literature.
Further, this model will be used to predict the strain of the bridge at non-sensor locations and at future times. These models need to be able to produce predictions along with a measure of uncertainty i.e. a confidence interval. This is of key importance to structural engineers, who always prefer conservative predictions.
Watch a short presentation from project leader Dr Liam Butler about the instrumented infrastructure work:

The investigation described in 'Explaining the science' is establishing statistical methods for modelling sensor readings and detecting changes in the structure. Once these methods have been established, the project will focus on how to deploy such methods across multiple instrumented bridges. This will allow for monitoring the structural health of similar bridges over time and perhaps indicate which bridges at most at risk.
The project work is being done alongside engineers in the Cambridge Centre for Smart Infrastructure and Construction that are monitoring railway bridges.
This work may lead to a more accurate assessment of the capacity of bridges and their degradation could avoid heavy-handed and costly interventions. The results of this project will benefit both engineering and statistics researchers and industry by improving the understanding of instrumented infrastructure and by taking the first step toward adaptive sequential control of systems. Industry partners i.e. railway companies who own and maintain such bridges, will also be interested in using these data-driven techniques to guide inspection policies.
[email protected]
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/health-foundation,"
www.health.org.uk
"
https://www.turing.ac.uk/research/interest-groups/data-science-education,"
A breadth of organisations, spanning academic, industry, learned societies and professional bodies contribute to addressing the skills challenges in data science and AI, and to identify good practices for education and skills provision. The data science education interest group brings these groups together to identify and address key challenges in research and innovation in data science and AI pedagogy and its implications for policy.
The principal activities of the Data Science Education group are the following:
The group will encourage members to:
Lack of alignment between training offerings and the needs of the workforce is a key inhibiting factor in both education and industry. Translation of academic innovation into industry is a key barrier to AI adoption; meanwhile, taught programmes rarely expose learners to the scale of problems faced in industry and research practice, nor provide the opportunity to develop business and sector awareness. This leads to challenges not only in supporting the translation of AI into practice, but the translation of leading industry practice back into the national training curriculum. How can we bring together industry and academic communities to share best practice around data science and AI education?
Diversity is essential to developing a robust data science skills base; therefore, we need to draw on the country’s entire intellectual capital. The inequitable impact of COVID-19 on women and BAME communities is likely to have slowed progress on diversity and inclusion in all sectors. When we consider the Office for National Statistics’ estimate that over 70% of 1.5 million roles at risk of automation are held by women, it is clear that urgent intervention is required. How can we democratise and widen access to training which is responsive and “at scale” and supports the creation of a culture and environment that retains talent, and a diverse workforce?
Click here to join us and request sign-up
[email protected]
Lead image courtesy of WOCinTech.
"
https://www.turing.ac.uk/news/expansion-ai-alan-turing-institute,"
Professor Mark Girolami, Chief Scientist at The Alan Turing Institute has announced a significant expansion of its artificial intelligence (AI) work.
The expansion will help achieve the aims of the National AI Strategy, and also marks a significant step in the Science and Innovation strand of the Institute’s Strategy. 
This new overarching work strand will be headed up by Professor Mike Wooldridge, who has been a member of the Turing community since 2018. As  Professor of Computer Science at the University of Oxford, he was Head of Department from 2014-21. He currently holds a UKRI Turing AI World-Leading Researcher Fellowship. 
Under Professor Wooldridge’s leadership, the Turing’s AI work will focus on a range of foundational areas, including fundamental AI science that complements and adds value to the existing work of the UK AI community.
It is expected that Professor Wooldridge will begin to lead the expansion of Foundational AI in the upcoming months.
Chief Scientist Professor Mark Girolami, said: “I’m delighted that the AI Foundations programme will be headed up and developed by world-renowned AI researcher Professor Mike Wooldridge, who is already working with the Turing.”
Professor Wooldridge said: “The UK has always led the way in understanding the foundations of AI, and I am delighted to have the opportunity to build on these strengths in the UK’s national AI centre.”
"
https://www.turing.ac.uk/news/turings-finance-and-economics-programme-responds-smart-data-consultation,"
In markets such as financial services, energy and communications, consumers often find it difficult to identify the best-suited service providers, switch to new services, or receive support and advice on the options available to them. These markets have been the focus of a recent UK Government review aimed to improve consumer outcomes by enabling businesses to securely share consumer data with third-party providers.
In 2018, the Department for Business, Energy and Industrial Strategy (BEIS) published its Modernising consumer markets: green paper announcing the launch of a Smart Data Review. The resulting review sets the vision to foster Smart Data initiative developments and is building on the success of Open Banking, originally set up by the Competition and Markets Authority on behalf of the UK Government and now gathering pace in various markets around the world. Open Banking empowers fintech innovators to develop new, more frictionless solutions for individual and business customers. This is achieved by enabling the primary data holder, such as a retail bank, to share the relevant financial data with third-party providers (TPPs) in a safe and secure way.
The vision for Smart Data in other markets is very similar, but the wider scope necessitates a more comprehensive analysis of potential risks and benefits, as well as technical and regulatory models for implementation.
As part of the Consultation process (closed as of 6 August 2019), The Alan Turing Institute’s Finance and Economics Programme released a publicly available response that highlights key considerations in Smart Data initiatives.
Below we describe the top three themes, highlighted by our contributing researchers.
Regardless of the market in question, all Smart Data initiatives need to consider how people’s data will be treated on an ongoing basis, not just at a fixed point in time.
The review highlights the importance of sharing data securely, with consent from data subjects (individual users)— which is an important first consideration. However, it can be difficult to track the movement and the metamorphosis of data over time, for example when it is anonymised or used to train machine learning models.
Consumer-generated data often informs business decision-making and impacts data subjects in indirect ways. It is therefore not enough to consider the technical and regulatory ways that would enable secure and ethical sharing of data. It is also important to think of new ways to encourage organisations to engender trust in their business models and offer more transparency about the provenance of the data they hold.
While the General Data Protection Regulation (GDPR) is an important first step in giving data subjects control over personal data, we would welcome further clarity on the legal and economic rights of data subjects — especially in situations where data moves beyond GDPR jurisdictions or gets separated from the user’s identity.
""All Smart Data initiatives need to consider how people’s data will be treated on an ongoing basis, not just at a fixed point in time.""
We welcome the Government’s motivation to put in place specific measures to protect consumers in vulnerable circumstances, and we encourage further clarity on what constitutes a “vulnerable consumer.” For example, financial vulnerability is related to low financial resilience, and can include experiences such as low financial capability, a divorce, a bereavement, physical or mental health conditions that affect one’s daily activities in a significant manner.
The different types of vulnerability may warrant different measures; otherwise whole societal groups will be at increased risk of exploitation, or their vulnerabilities will be exacerbated in indirect ways. Our response proposes a few high-level examples of potential interventions, such as introducing proxy services that can act on behalf of those with low technical or financial literacy.
More importantly, we believe an in-depth analysis will be required for each market—and potentially across markets segments—to provide guidance for businesses and improve consumer outcomes. The ongoing work by the Financial Conduct Authority (FCA) on the fair treatment of vulnerable consumers can serve as an important foundation for determining a more consistent and measurable approach to tackling vulnerability.
Being part of the national institute for data science and AI, we believe that the safe and secure sharing of data – if done right – will be the essential pre-requisite for innovation across all industry sectors.
It is also true that Smart Data initiatives can involve considerable reputational and societal risk. It is important to start in the areas that are least likely to cause public backlash and inhibit further market development.
We encourage the Government to lead the charge in establishing best practice by becoming a Smart Data facilitator for citizen data. The Government can set good examples of being transparent about the reasons certain types of data can or cannot be made available, such as tax, education, healthcare, or Border Force records.
Our response also cites a few high-potential, lower-risk examples of data sharing in public services and academic research, for example the use of people’s mobility data to inform transportation initiatives or to improve the electric vehicle infrastructure.
We are excited about the vision to enable a safer and more secure flow of data across organisational borders. While there can be inherent tensions between the goals of consumer control over data and enabling innovation, we remain optimistic that the right balance can be struck between regulation and ethical innovation. Building on the achievements of Open Banking in the UK, Smart Data initiatives can be successful in many industry sectors if we don’t lose the sharp focus on consumer trust and nurturing innovation.
3.3 MB
"
https://www.turing.ac.uk/research/interest-groups/data-centric-biological-design-and-engineering,"
Our growing ability to engineer biological systems is poised to radically disrupt how we make chemicals, grow food, treat disease, and ultimately support a more sustainable future. Modern engineering biology techniques can be used to reprogram cells, but major challenges remain in handling the complexity, scale and diversity of the data sets underpinning these efforts. New approaches are required to use biological data effectively for engineering. This interest group aims to act as a point of focus to develop and explore data-centric approaches that can unlock the practical engineering of life and overcome these current hurdles to harnessing engineered biology. The group will bring together computer scientists, mathematicians, engineers, biologists, chemists, and social scientists to address interdisciplinary research challenges in this area, build a diverse and inclusive community, and establish a national vision for how data science and AI can support engineering biology.
Engineering Biology is the application of engineering principles to the design and construction of new biological systems such as microbes, plants and tissues. These systems can be employed for many applications in therapy, diagnostics, new materials, and sustainable manufacture of chemicals that are indispensable in our everyday lives. In 2021, Engineering Biology was listed by the Government as one of the seven key technologies of UK strength and development. This interest group aims at nucleating the world-leading UK knowhow in AI and Engineering Biology into a unified programme that supports the future growth of the field.
Advancing our ability to engineer biology is a strategic priority for the UK. Data-centric approaches are playing an increasingly important role in enabling the development of highly predictive models to accelerate biological design and engineering. However, many of the existing research programs in this area are disjoint, lacking a clear long-term vision and a community to support future growth and development. This interest group will position the Alan Turing Institute front and centre of this discipline and leverage its national role to support its future growth and development. The Institute will provide a physical location and access to deep expertise in data science and AI to develop the necessary theory and computational tools for engineering biology towards positive real-world impact. The core aims are:
Click here to request sign-up and join
[email protected]
"
https://www.turing.ac.uk/people/external-researchers/alfredo-carpineti,"
Dr Alfredo Carpineti is an astrophysicist, journalist (IFLScience), science communicator, and social activist. He's the chair and founder of Pride in STEM, an award-nominated charity working to raise the profile and showcase the work of LGBTQ+ people working in STEM. He was one of the organisers of LGBTSTEM Day
"
https://www.turing.ac.uk/people/researchers/tapabrata-rohan-chakraborty,"
Tapabrata (Rohan) is a Senior Research Associate with the Turing-Roche Partnership. His main area of research is clinical machine learning and health data science, with a particular focus on cancer image analysis. Before joining The Alan Turing Institute, he was a postdoctoral researcher at the Big Data Institute, Oxford University, where he collaborated with Roche on the UKRI DART lung cancer programme. He continues to be affiliated with the University of Oxford through a non-stipendiary research fellowship with Linacre College, Oxford under the Oxford University collegiate system. He completed his PhD in Computer Science in New Zealand, his doctoral research won the Diane Campbell Hunt Memorial Award.
Tapabrata (Rohan) is also a passionate educator. He is the Lead Tutor in Information Engineering at the Dept of Engineering Science, University of Oxford. He leads a team of 8 tutors for this highly popular course and teaches image/signal analysis and bayesian estimation/inference. He is a Tutor at the Oxford Centre for Doctoral Training in Health Data Science where he teaches the modules on deep learning and medical image analysis. He is a Senior Member of IEEE, Associate Fellow of the Higher Education Academy (UK), Associate Editor for Springer Nature Computer Science, and an Invited Expert with the Global Partnership on AI for their international project on responsible AI for drug discovery and social good.
As a rambler, he likes to go on long nature walks and hikes. He likes to play pool and attend Oxford college dinners with his friends.
As the first senior research associate with the Turing-Roche strategic partnership, Tapabrata (Rohan) is leading the effort in scoping out relevant research pieces aligned with the north star of the programme. He will also be responsible in proof of concept research on those projects and publish the results through leading scientific outlets. Besides these primary objectives, he also contributes to other initiatives of the partnership like the planning of Turing-Roche workshops, science communication and collaboration with the expert panel.
"
https://www.turing.ac.uk/people/guest-speakers/sue-daley,"
Sue leads techUK's Technology and Innovation work. 
This includes work programmes on cloud, data protection, data analytics, AI, digital ethics, Digital Identity and Internet of Things as well as emerging and transformative technologies and innovation policy. She has been recognised as one of the most influential people in UK tech by Computer Weekly's UKtech50 Longlist and in 2021 was inducted into the Computer Weekly Most Influential Women in UK Tech Hall of Fame. A key influencer in driving forward the data agenda in the UK Sue is co-chair of the UK government's National Data Strategy Forum. As well as being recognised in the UK's Big Data 100 and the Global Top 100 Data Visionaries for 2020 Sue has also been shortlisted for the Milton Keynes Women Leaders Awards and was a judge for the Loebner Prize in AI. In addition to being a regular industry speaker on issues including AI ethics, data protection and cyber security, Sue was recently a judge for the UK Tech 50 and is a regular judge of the annual UK Cloud Awards. 
Prior to joining techUK in January 2015 Sue was responsible for Symantec's Government Relations in the UK and Ireland. She has spoken at events including the UK-China Internet Forum in Beijing, UN IGF and European RSA on issues ranging from data usage and privacy, cloud computing and online child safety. Before joining Symantec, Sue was senior policy advisor at the Confederation of British Industry (CBI). Sue has an BA degree on History and American Studies from Leeds University and a Masters Degree on International Relations and Diplomacy from the University of Birmingham. Sue is a keen sportswoman and in 2016 achieved a lifelong ambition to swim the English Channel. 
"
https://www.turing.ac.uk/about-us/impact/dynamic-forecasting-british-airways,"
Major airlines generate colossal amounts of information across a spectrum of domains: logistics, flight information, asset movements, pricing, fuelling, catering – you name it. In 2017, British Airways approached the Turing to tap into the Institute’s data science expertise to co-develop  techniques to help wrangle their massive amounts of data. In January 2018, an initial year-long joint project kicked off with the Turing’s Research Engineering Group, led by Evelina Gabasova, a Senior Research Data Scientist, and James Geddes, a Principal Research Data Scientist.
Like all airlines, BA constantly updates its forecast ticket sales for each flight prior to take-off. It’s a vital piece of its pricing strategy. Typically, the first tranche of seats for a flight becomes available to book a year or so before departure. By tactically making different classes of seat available at different prices at different times, an airline can optimise its revenue without leaving empty seats. It's a balancing game of supply, demand and pricing, and the key to the game is making accurate, dynamic forecasts of passenger demand for each flight.
The aim of the collaboration with the Turing was to develop bespoke machine learning techniques to improve BA’s dynamic demand forecasting. It’s an aim that speaks to one of the key missions of the Institute – developing cutting-edge data science approaches and championing their application to real-world problems. This is a mission that can sometimes be challenging due to the frequent inaccessibility of real-world data.
The collaboration with BA, however, showed the value of giving talented researchers secure access to commercially sensitive data. BA entrusted the Turing with daily ticket sales for approximately one million flights from the previous three years. That’s a total of about six billion rows of data, with a complex time-series structure. Just the sort of juicy challenge that the Turing’s Research Engineering Group relishes.

“In the collaborative process, BA was very open, and spent a lot of time explaining to us what was in the data, how it worked and so on,” says Jersakova, a team member in the Turing’s Research Engineering Group. “We shaped the research goals together. It was fantastic.”
Then the researchers got to work using machine learning to create and test novel predictive models. “Our added value is that we used the Bayesian modelling approach, which not only provides estimated forecasts, but also the level of confidence in those predictions,” says Jersakova. Using such techniques on large, real-world data sets was traditionally hard, both mathematically and computationally, but recent developments in probabilistic programming is changing the game, providing the researchers with the tools they need.  
Bayesian statistics allows models to incorporate historical data, existing knowledge, recent trends and new data when available. Such models start relatively simple, and grow iteratively as further data and information are added. Crucially, such models are not ‘black box’ algorithms; they make explicit assumptions about how the world works, so that the way they arrive at their forecasts is interpretable.
It was clear that the data pertaining to a single flight was insufficient to provide an accurate prediction of future demand, and that occurrences that affect the sales of one flight – anything from external events to pricing decisions made by BA – have effects on other flights. What makes the data set even more challenging to model is that the “counterfactual” is missing: what would happen if a particular event didn’t occur, or BA made a different pricing decision, or took no action? This is where Bayesian techniques come into their own. “Bayesian methods allow the model to ‘borrow information’ across similar flights and use the data more efficiently,” says Gabasova.
The collaborators’ approach to the project was to define, early on, an agreed measure of model performance and then to automate the process of testing each iteration of their model. This provided a clear, ongoing evaluation of how the modelling was progressing and, ultimately, the new predictive algorithms were ready to go head-to-head with BA’s more traditional forecasting methods.
BA was pleased with how the collaboration evolved. “Working with the Turing was hassle-free and had none of the tensions of working with commercial consultants,” says Jack Bovey, Revenue Optimisation Manager at British Airways. “There was just a genuine desire to understand our challenges and help us think about new ways to tackle them. We valued that honest approach: some things will be possible, some things won’t, and it’s difficult to know how anything will go until we try it. It sounds basic but it makes us feel like we’re genuinely trying to find the best possible solution to our problems as opposed to feeling like we’re being pushed towards a pre-packaged solution. Everyone involved learned a lot from the Turing team – we trusted them.”
“Everyone involved learned a lot from the Turing team – we trusted them.”
Jack Bovey, Revenue Optimisation Manager at British Airways
That trust is important, because working with sensitive commercial data requires high security. “The Turing has many collaborations with industry,” says Jersakova, “and the challenge from day one is always: how do you handle that data in a secure way, but flexibly enough that researchers can work with it effectively?”
This is where the work with BA fed into a major ongoing project at the Turing; the creation of “data safe havens” in the cloud. This work, led by James Hetherington, the Turing’s Director of Research Engineering, is combining the development of secure environments for the analysis of sensitive data sets with high-performance computing capability. The idea is to provide secure research environments that do not hinder research, nor limit the tools a data scientist can bring to bear on their data. The project makes use of the Microsoft Azure cloud platform, following Microsoft’s sponsorship of the Turing with $5 million in Azure credits in 2016.
The collaboration with BA has added breadth to the Turing’s ability to create the tools that data-rich industries will need if they are going to effectively exploit the ongoing revolution in data science. The cross-organisational nature of the Institute’s Research Engineering group means that the ideas, techniques and expertise developed in this collaboration can be built upon or adapted to address related challenges in other parts of the Turing’s diverse research portfolio, and in other parts of industry.
From BA’s perspective, the airline’s dynamic forecasting capability was bolstered. “For us, it was the first step towards what will hopefully become an important part of how we forecast, and hence price, flights,” says Bovey. “It showed us that there are different ways to approach forecasting to how we have tried before; it showed us that this new approach can give promising-looking results; and that it could help us address lots of the challenges we’ve been thinking about.” He now hopes to see a “wider collaboration between BA and the Turing”.

"
https://www.turing.ac.uk/people/guest-speakers/shmyla-khan,"
Shmyla Khan is the Research and Policy Director at Digital Rights Foundation. She is a trained lawyer who also works intermittently as a lecturer for law, gender and privacy.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/department-digital-culture-media-sport,"
"
https://www.turing.ac.uk/people/enrichment-students/meredith-leston,"
Meredith Leston is an accomplished public health consultant and researcher who is passionate about using her skillset to address modern health challenges and to improve public services and patient outcomes in the UK and abroad. She has established a strong track record for bringing scientific and technical insight into real-world health and social care problems at the national and international level. With past experience pursuing positive outcomes in these areas across commercial, clinical, academic and philanthropic settings, she is a passionate believer in the value of cross-sector collaboration and interdisciplinary approaches when tackling intractable social and medical issues. She is currently pursuing her doctorate in the Nuffield Department of Primary Care Health Sciences at the University of Oxford. 
Meredith's research is focused upon developing a real-time vaccine benefit risk platform capable of monitoring and differentiating seasonal vaccine effectiveness, uptake and adverse effects amongst immunocompromised patient groups - the first of its kind. Her research utilises a combination of routinely collected primary care medical record data (courtesy of the Oxford-Royal College of General Practitioners' Research and Surveillance Centre) and biological specimen data (virology and serology) and aims to inform vaccine scheduling and dosing amongst this vulnerable group. By leveraging the technological capabilities of her commercial partner, EMIS Health, and the expertise of the Turing Fellowship, Meredith intends to unpick which immunocompromised sub-categories, patient demographics and concurrent medications are most susceptible to vaccine failure or accelerated waning and will establish a mechanism for immunocompromised patients to report adverse events of interest immediately post-vaccination. This work is especially important considering that immunocompromised patients are rarely eligible for the large-scale clinical trials that validate the safety of vaccine candidates. It is her hope that, by presenting immunocompromised patients with more personalised information about vaccine benefit-risk in this way, she might contribute to efforts to reduce vaccine hesitancy and advance precision medicine.  
"
https://www.turing.ac.uk/news/publications/bayesian-statistical-learning-big-data-biology,"
Bayesian statistical learning provides a coherent probabilistic framework for modelling uncertainty in systems. This review describes the theoretical foundations underlying Bayesian statistics and outlines the computational frameworks for implementing Bayesian inference in practice. We then describe the use of Bayesian learning in single-cell biology for the analysis of high-dimensional, large data sets.
Yau, C. & Campbell, K. Biophys Rev (2019). https://doi.org/10.1007/s12551-019-00499-1
"
https://www.turing.ac.uk/research/research-projects/input-output-analytics-urban-systems,"
Understanding inter-dependency in urban systems is often hindered by a lack of suitable data. This project combines research methods from geography, urban planning, economics, and history to provide a new approach to this issue. By creating a time series of input-output accounts for British cities in both the modern day and the nineteenth century, the project is able to model inter-city trade flows between economic centres.
This work supports the Urban Analytics programme and Living with Machines project at the Turing.
 
The foundational methods upon which this project builds are Leontief’s input-output analysis and Wilson’s family of spatial interaction models. It is the combination of the two which facilitates the creation of a mathematical model of trade flows between urban centres. The model can then be employed as an aid to economic analysis of the dynamics of city systems.
Input-output analysis – made famous by Wassily Leontief in the mid-twentieth century – provides a framework for examining inter-dependencies between sectors in an economy. By examining the technical relationships, the method provides a way of considering not just the final output of an economic sector, but also the goods and services used to create other goods and/or services before they are sold to consumers.
The Wilsonian family of spatial interaction models are ways of predicting movements between sets of origins and destinations within a system. They not only account for the distances between origin-destination pairs, but also incorporate the factors which might make one destination more attractive than another.
By combining these two techniques, this project is able to estimate trade between every pair of British cities, for each economic sector.
The project will create two mathematical models of inter-city trade, built from the same core specification and accompanied by custom visualisation tools. The first will model trade flows in the second half of the nineteenth century, and concentrate on historical development of the British economy. The second will focus on Britain in the modern day, and include forecasting of future scenarios.
The project therefore seeks to provide insight regarding the evolution of cities over the nineteenth century, and the future challenges faced by modern cities. Both the historical and contemporary strands are motivated by a desire to understand the so-called ‘north-south’ divide: the former in terms of its origins; and the latter in relation to the ‘levelling-up’ agenda.
The findings of this work will therefore have a significant impact on our understanding of city growth, development, and trade, and provide a framework for the analysis of future economic policy.
Understanding the (past and future) evolution of cities has considerable importance for policymakers, urban planners, and third sector organisations. By creating dynamic tools – capable of modelling a variety of scenarios – the project will equip stakeholders with the necessary information to understand the long-term implications of current policy and urban development.
Guy Solomon - [email protected]
"
https://www.turing.ac.uk/news/publications/optimizing-equity-sensor-coverage-networks-and-responsive-city,"
Decisions about sensor placement in cities are inherently complex, balancing social-technical, digital, and structural inequalities with the differential needs of populations, local stakeholder priorities, and the technical specificities of the sensors themselves. Rapid developments in urban data collection and geographic data science have the potential to support these decision-making processes. Focusing on a case study of air-quality sensors in Newcastle-upon-Tyne, UK, we employ spatial optimization algorithms as a descriptive tool to illustrate the complex trade-offs that produce sensor networks that miss important groups—even when the explicit coverage goal is one of equity. We show that the problem is not technical; rather, it is demographic, structural, and financial. Despite the considerable constraints that emerge from our analysis, we argue the data collected via sensor networks are of continued importance when evidencing core urban injustices (e.g., air pollution or climate-related heat). We therefore make the case for a clearer distinction to be made between sensors for monitoring and sensors for surveillance, arguing that a wider presumption of bad intent for all sensors potentially limits the visibility of positive types of sensing. For the purpose of monitoring, we also propose that basic spatial optimization tools can help to elucidate and remediate spatial injustices in sensor networks.
Caitlin Robinson, Rachel S. Franklin & Jack Roberts (2022) Optimizing for Equity: Sensor Coverage, Networks, and the Responsive City, Annals of the American Association of Geographers, 112:8, 2152-2173, DOI: 10.1080/24694452.2022.2077169
 
4.6 MB
"
https://www.turing.ac.uk/people/external-researchers/karla-diaz-ordaz,"
"
https://www.turing.ac.uk/people/researchers/chris-oates,"
Chris is a Professor of Statistics at Newcastle University and Group Leader for the data-centric engineering programme at the Turing.
Chris’ research aims to develop probabilistic numerical methods for simulation-based risk management. The use of computers to simulate physical governing equations introduces numerical and discretisation errors; his research aims to account for these with statistical methods, to provide a proper quantification of risk for decisions that are based on simulation output.
"
https://www.turing.ac.uk/news/ai-uk-2023-national-showcase-returns-its-third-year-over-50-demonstrations,"
AI UK 2023, the national showcase of data science and AI, will feature over 50 innovative demonstrations from world-leading institutions across the UK. These will showcase research applications for the environment, healthcare, digital twins and smart cities – as well as many others.  
This year’s event is themed around the Office for Science and Technology’s key priorities which include: the sustainable environment; health and life sciences; national security and defence, including space; and a digitally and data driven economy and an open call for the showcase resulted in a huge response. 
A digital twin of the vessel RRS Sir David Attenborough showing how AI can reduce carbon in polar navigation, a tool monitoring and predicting sea ice forecasts, and new AI automated monitoring tool for moths are just some of the demonstrations that will explore environmental challenges. 
Demonstrations on the theme of healthcare include a tool that can automatically locate the outlines of bones in X-rays, a prototype of a new companion robot and a live showing of AI-tool that can help radiologists detect ovarian cancer. 
A tool for designing low-traffic neighbourhoods that can be used to suggest changes to our streets and a project aiming to improve cycling safety on the roads will use VR headsets to help users experience scenarios through the lens of the rider, both consider how to improve the cities we live in. 
Other topical issues that will be explored include a data-film created using analysis of 5 million NFTs to explore the NFT phenomenon, a 3D shoe sketcher that will revolutionise how we shop online and the launch of ‘The Blame Game’ which aims to engage the public in a discussion on how blame should be attributed in a hybrid world of humans and machines. 
Delegates attending the showcase will also hear the latest thinking from leading academics, policymakers and industry leaders in fields ranging from digital twins, the environment and AI ethics. 
The event will feature three main stages (conversation, research and impact), lightning talks, a workshop area and the exhibition space, which will feature interactive demonstrations. 
Secure your place online now for the two-day event which will take place on 21-22 March at the Queen Elizabeth II Centre in Westminster, London. Early bird tickets are available until Monday 23 January. 
Chief Scientist of The Alan Turing Institute, Mark Girolami said: “We’re pleased to start 2023 with the launch of our national showcase, AI UK. We look forward to building on the success of last year’s event by welcoming some of the UK’s leading voices to help us show the innovative AI, machine learning and data science research and innovation taking place across the UK.  
""With lively, interactive content, world-leading experts, and immersive demonstrations, the event will explore how AI can help us face some of the most important issues facing society today.”  
"
https://www.turing.ac.uk/blog/how-are-academic-disciplines-changing-over-time,"
When will the next pandemic appear? How can we create sustainable energy? Can we build intelligent robots? All of the big societal questions require collaboration between researchers across multiple disciplines. Interdisciplinary research is something that is increasingly encouraged and embraced.
But scientific disciplines are not rigid, well-defined entities. They are fluid and context-dependent. They are made up of interactions between academics in workplaces and conferences, via papers, presentations and informal conversations. They emerge as conventions from accumulated individual and collective behaviours. Various classifications exist, but these struggle to keep up with the changing research landscape.
To better understand how individual disciplines evolve, we recently developed a new (interdisciplinary!) methodology, published in Humanities and Social Sciences Communications, which draws on techniques from computational linguistics and natural language processing (particularly algorithms that help us to determine how words change their meaning over time).
The idea behind our linguistic line of research is that we can divide a corpus of texts into separate portions corresponding to different time periods (months, years, decades or centuries). We can then trace the evolution of a word’s meaning across the time periods by analysing how its context changes (we do this by building geometric representations of words, called word embeddings, from the data). For example, we can identify the evolution of the word ‘tweet’ to mean both ‘a message posted on Twitter’ and ‘a sound made by a small bird’.
We applied this approach to scientific disciplines, analysing the discipline labels attached to over 21 million scientific articles published in 8,400 academic journals between 1990 and 2019 (this data all came from the Dimensions database). By creating geometric representations of these labels from the data and measuring the closeness between them, we tracked their development over time, detecting those that had changed the most.
For example, computer hardware, a foundational part of the broader computer science field, has drifted closer to other computational disciplines over time, and away from specific application areas. In 1990-1992, the most similar disciplines to computer hardware (its ‘neighbours’) were in the biological sciences (plant biology, physiology, zoology, genetics and horticultural production), which points to a focus on biology-specific applications. If we fast-forward to 2017-2019, we find that its neighbours are more computational disciplines (distributed computing, computer software, data format, information systems, and computation theory and mathematics).
The profile of communication and media studies has also changed significantly. In 1990-1992, its neighbours were medical biotechnology, physiology, building, veterinary sciences, and transportation and freight services. These neighbours fit well with the early focus of communication studies on health and risk management communication. From around 2002-2004, communication and media studies began to surround itself with neighbours including journalism and professional writing, religion and religious studies, and anthropology. This is compatible with the development of communication studies from a focus on applications towards a more theoretically informed field that studies multiple aspects of communication.
Our research demonstrates how scientific disciplines are always in flux. Analysing the evolution of disciplines and their relationships can help us to better understand the broader research landscape of academics, research organisations, and funders. In academic publishing, our techniques could be used, for example, to help decide the scope and profile of new journal launches so that they best capture current disciplinary trends. Our work is also relevant to those who produce and use research strategies and policies, such as funding agencies and university research strategy offices.
In the future, we plan to study the development of a larger number of disciplines to see if we can find confirmation of known shifts in discipline profiles, and even discover unknown ones. We’ll also be looking into the broader trends that our work has started to reveal.
As we tackle the big societal problems, different disciplines are going to need to work together more efficiently and effectively. A clearer picture of the research landscape can only help with this endeavour.
Read the paper:Investigating patterns of change, stability, and interaction among scientific disciplines using embeddings
 
Top image: Pawel Czerwinski / Unsplash
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-oxford,"
"
https://www.turing.ac.uk/research/publications/trustworthy-assurance-digital-mental-healthcare,"
There is a culture of distrust surrounding the development and use of digital mental health technologies.
As many organisations continue to grapple with the long-term impacts on mental health and well-being from the COVID-19 pandemic, a growing number are turning to digital technologies to increase their capacity and try to meet the growing need for mental health services.
In this report, we argue that clearer assurance for how ethical principles have been considered and implemented in the design, development, and deployment of digital mental health technologies is necessary to help build a more trustworthy and responsible ecosystem. To help address this need, we set out a positive proposal for a framework and methodology we call 'Trustworthy Assurance'.
To support the development and evaluation of Trustworthy Assurance, we conducted a series of participatory stakeholder engagement events with students, University administrators, regulators and policy-makers, developers, researchers, and users of digital mental health technologies. Our objectives were a) to identify and explore how stakeholders understood and interpreted relevant ethical objectives for digital mental health technologies, b) to evaluate and co-design the trustworthy assurance framework and methodology, and c) solicit feedback on the possible reasons for distrust in digital mental health.
PDF:  Burr, C. and Powell, R., (2022) Trustworthy Assurance of Digital Mental Healthcare. The Alan Turing Institute. https://doi.org/10.5281/zenodo.7107200
Web Version: Burr, C., & Powell, R. (2022). Trustworthy Assurance of Digital Mental Healthcare. The Alan Turing Institute. https://alan-turing-institute.github.io/trustworthy-assurance/
4.2 MB
"
https://www.turing.ac.uk/news/world-first-3d-printed-steel-smart-bridge-opens-pedestrians-amsterdam,"
AMSTERDAM, 15 JULY 2021 – The world’s first 3D printed steel bridge opens to the public today: a  pedestrian structure located in the city centre of Amsterdam. The bridge is a result of a multi-disciplinary team of experts collaborating on the future design of public spaces.
The Alan Turing Institute’s data-centric engineering (DCE) programme has been integral to the bridge’s development. They have teamed with Dutch scale up MX3D and other global collaborators to install the bridge over Oudezijds Achterburgwal canal.
The bridge was not only printed by robotic arms, but is also equipped with an innovative sensor network, linked to a ‘digital twin’ computer model that monitors its performance in real-time. Everyone that walks, runs, or cycles over the bridge will generate data, which will help to monitor the bridge’s structure and provide information about how it’s being used.
The bridge has the potential to revolutionise how urban infrastructure is designed, built and maintained. 3D printed steel is a new material, which may have the power to radically change conventional construction and overhaul the building industry.
As a new material, safety is crucial. After extensive pre-installation testing, the bridge team has also created an inbuilt ‘digital twin’ of the physical structure. This means that sensors built into the bridge will constantly be gathering data on strain, displacement, vibration, air quality and temperature.
This data will be used to create a digital version – or twin – of the bridge. This digital twin can then predict how the structure will behave as the bridge is used. So, not only can maintenance needs be highlighted at the earliest opportunity, but the digital twin will also help engineers understand how 3D printed steel might be used for larger scale and more complex building projects.
DCE Programme Director Mark Girolami said, “We are incredibly excited about the 3D printed bridge project. Not only is it a stunning design, but with the sensors inbuilt into the fabric of the bridge it is also pioneering ground-breaking digital twin technologies. It is a significant step towards making huge changes to how we think about and improve the engineered world and our built environment. It is fantastic to see the project coming to life and the public engaging with it.”
The bridge is an international collaborative effort. It was designed and built in the Netherlands by Joris Laarman Labs and 3D printing company MX3D. Researchers at The Alan Turing Institute in the UK undertook the materials testing and are designing the digital twin, incorporating San Francisco company Autodesk’s software.
It is living science. Simply by walking or cycling over the bridge the public is contributing to a potentially world-changing scientific endeavour.
Cover photo credit/caption: Her Majesty the Queen of the Netherlands, Máxima, opens the first 3D printed steel bridge in the oldest neighbourhood of Amsterdam. Photo by Adriaan de Groot.
Turing researcher Eric Daub explains why we created a ‘digital twin’ for the world’s first 3D printed steel bridge.
A detailed look at the science behind the bridge.
Location of the bridge

"
https://www.turing.ac.uk/events/network-science-financial-services,"
74.6 KB
During the last decade, the emerging area of network science has witnessed tremendous growth, with virtually thousands of papers written on the topic. Networks arise ubiquitously in physical, financial, biological, engineering, and social sciences, and pose the crucial problem of understanding how the node interactions affect the functionality of the system, and how they can be exploited for forecasting various processes on the network and designing robust systems. The recent astonishing growth of network analysis was also stimulated by the increasing availability of massive data sets that most often can be naturally modelled as graphs, and the demonstrated successes of network-based theories for the analysis of “big data”.
This workshop will showcase potential applicability of network analysis tools to problems of interest within the realm of finance and economics, discussing topics that include community detection and clustering, temporal multilayer networks, analysis of the interbank market, network models of systemic risk, portfolio construction, analysis of large-scale supply chains, and anomaly detection in transaction networks.
The workshop will provide an overview of relevant research in the field from a number of academics and industry practitioners.
This is a technical event, suitable for individuals with a background and interest in network analysis and its applicability to problems arising in finance and economics.
Full agenda (TBC) will include an overview of relevant research in the field from a number of academics, followed by short talks by industry practitioners.
Confirmed talk titles and speakers are as follows:
Fabio Caccioli (University College London)
Title: Network models of systemic risk
Marya Bazzi (The Alan Turing Institute & University of Warwick)
Title: Community structure in temporal multilayer networks, and its application to financial correlation networks
Paolo Barucca (University College London)
Title: Dynamic network models in finance:
Xiaowen Dong (University of Oxford)
Title: Network-based learning for understanding collective human behaviour
Ginestra Bianconi (Queen Mary University of London)
Title: Network theory for finance
Felix Reed-Tsochas (University of Oxford)
Title: An ecosystem perspective on financial and industrial networks
Jeremy Turiel (University College London)
Title: Sector Neutral Portfolios: Long memory motifs persistence in market structure dynamics
Andrew Elliott (The Alan Turing Institute)
Title: Anomaly detection and core-periphery structure in networks
Peter Mitic (Santander UK)
Title:  Money Laundering Networks
Kimmo Soramäki (FNA - Financial Network Analytics)
Title: Applications of financial networks in finance - and challenges faced

"
https://www.turing.ac.uk/events/cogx-2019,"
The Alan Turing Institute is pleased to host the research stage at this years' CogX: showcasing our cutting edge research in data science and AI, applied to our ambitious challenges, which represent areas that will have a game-changing impact for science, society and the economy.
Visit the conference website for more information. 
 
"
https://www.turing.ac.uk/people/research-engineering/luke-hare,"
Luke is a Junior Research Data Scientist in the Research Engineering Group at the Turing. He is interested in ethical applications of machine learning methods. He has an MSc in Data Science from the University of Edinburgh, and has experience working as a data scientist in industry.
Luke is currently working on trustworthy digital infrastructure for identity systems, and benchmarking for AI for Science at exascale.
"
https://www.turing.ac.uk/research/research-projects/distributed-training-machine-translation,"
Neural networks have huge potential for a wide range of applications, amongst them machine-led translation of various different languages. Training these networks takes significant time and resources, which can be reduced by distributing the training across multiple machines. In collaboration with Intel, this work is aiming to make this training process, and the related hardware, better at translating millions of words of online text.
Neural networks are multi-layered systems inspired by the way the human brain is wired, that are trained to learn the mapping between inputs and a response. Each individual ‘neuron’ or node in the network is given a set of parameters (or weightings and biases) which are iteratively adjusted. These adjustments help to form smart ‘neural pathways’ that optimise the network’s ability to fulfil certain tasks, e.g. accurately translating a word from one language to another.
A training method known as ‘stochastic gradient descent’, or SGD, is often used. The ‘gradient descent’ part of this method describes the changes (or gradient) in parameters that happen with each iteration of data passing through the network. These changes aim to minimise (descend) a ‘cost function’, which indicates how accurate the network is at fulfilling its purpose – i.e. the smaller the value of the cost function, the better the network. ‘Stochastic’ (i.e. random) relates to how it is more efficient to take small, random samples of the training data to compute each tiny step change in the parameters, than it is to use the entire dataset every time.
Despite recent advances in training methods, as well as in GPU (graphical processing unit) hardware and network architectures, training these neural networks with data can take an impractically long time on a single machine. However, distributed training across multiple machines allows for significantly more efficient development of neural networks.
The most common form of distributed training is data parallelism, in which each machine or ‘worker’ gets a different portion of the input data, but a complete copy of the network, and then each machine’s results are subsequently combined. It’s therefore important to ensure the same level of quality on all machines as one would have on one machine.
When utilising distributed training, it’s often possible to run out of bandwidth and run into increased overheads. This project is looking at ways to improve the efficiency of the training process, within the specific application of machine translation of text.
The machine translation group at the University of Edinburgh have collected 236TB of text, partly from trawling the internet, partly from EU parliamentary transcripts, looking at the following languages: French, Spanish, Portuguese, Italian, Danish, Polish, Czech, and Mandarin. Each word in this text is encoded as 1024 numbers or bits, naturally leading to very large datasets.
Training efficiency is increased by reducing the size of the data ‘vectors’ used, by acknowledging that most words don’t appear in most sentences. By looking at the probability of words being used and discarding 99% of the data related to unused words, it’s possible to produce a ‘sparse vector’ that requires 50 times less bandwidth. Efficiency can also be increased by ensuring each distributed machine or ‘worker’ completes in as similar an amount of time as possible, by giving each worker sentences of the same, or similar, length to work with.
First, a low quality, first pass translation to English is run. Information retrieval is then used to identify if words are a good matching pair, and if they are they’re added to the training data, to iteratively improve the network. The work has already required in excess of 400,000 GPU hours.
The main goals of the project have been to translate as much text as possible, improve the quality of translation, reduce overheads, and make the neural networks produced as useful for as many different cases as possible.
The main aim of the collaboration with Intel is to run neural networks faster on Intel hardware. Advances in hardware also lead to more advanced models and make training more efficient. By applying new distributed training techniques to the challenge of machine translation, Turing researchers have been able to inform Intel what specific aspects of their hardware would make them better at dealing with neural network analysis.
September 2017: The Edinburgh group ranked tied first for eleven out of twelve translation directions they participated in at the 2017 Conference on Machine Translation.
August 2016: The machine translation group at Edinburgh University won first place in seven of twelve language pairs in the 2016 Conference on Machine Translation, in particular beating Google Translate in Mandarin and Czech.
[email protected]
"
https://www.turing.ac.uk/research/research-areas/optimisation/nonlinear-programming,"
"
https://www.turing.ac.uk/research/research-programmes/public-policy,"
The public policy programme works alongside policy makers to explore how data-driven public service provision and policy innovation might solve long running ‘wicked’ policy problems and to develop the ethical foundations for the use of data science and artificial intelligence in policy-making.
Our aim is to contribute to the Institute's mission – to make great leaps in data science and artificial intelligence research in order to change the world for the better – by developing research, tools, and techniques that have a positive impact on the lives of as many people as possible.
The public policy programme has four challenges.
In a world of changing and interlinked policy measures, data science and AI can provide policy makers with unprecedented insight: from identifying policy priorities by modelling complex systems and scenarios, to evaluating hard-to-measure policy outcomes. Our aim is to equip policy makers across all levels of government with the tools they need to not only design effective public policy, but also to track and measure policy impacts.
 
Governments today are major holders of data which data science and AI can harness to improve the design and provision of public services. The public policy programme brings researchers and policy makers together in order to develop innovative ways to provide public services. Our aim is to change everyday life for the better: from allocating resources in the fairest and most transparent way, to designing personalised public services that are tailored to people's individual needs and situations.
 
Understanding the ethical and societal implications of data science is one of The Alan Turing Institute's key research priorities. Our Data Ethics Group leads research in this area. The public policy programme will work with the Data Ethics Group to develop the ethical foundations for the use of data science and AI in policy-making, with the aim of securing the benefits and addressing the risks these technologies pose.
 
The effects of data science and AI on society are already being felt, and their impact will only grow in the years to come. Our aim is to ensure that this impact is as beneficial and equitable as possible. The public policy programme will work with policy makers to develop well-crafted laws and sensible regulation, using the ethical principles and norms developed with the Data Ethics Group that clarify the socially acceptable uses of these powerful technologies.
AI and machine learning technologies are helping people do remarkable things. From assisting doctors in the early detection of diseases and supporting scientists who are wrestling with climate change to bringing together diverse groups from around the globe through real-time speech-to-speech translation, AI systems are enabling humans to successfully confront an ever-widening range of societal challenges.
This progress has, however, brought with it a new set of difficulties. Many machine learning applications, such as those in natural language processing and computer vision, identify subtle patterns in large datasets. These patterns allow them to complete their assigned tasks by linking together many hundreds, thousands—or sometimes even millions—of data points at a time. Humans don’t think this way and because of this have difficulty understanding and explaining how these sorts of AI systems reach their results.     
This gap in AI explainability becomes crucial when the outcomes of AI-assisted decisions have a significant impact on affected individuals and their communities. If an AI system is opaque then there is no way to ensure that its data processing is robust, reliable and safe. Similarly, in cases where social or demographic data are being used as inputs in AI decision-support systems—for instance, in domains such as criminal justice, social care, or job recruitment—the employment of ‘black box’ models leaves designers and deployers no way to properly safeguard against possibilities of lurking biases that may produce inequitable or discriminatory results.
Over the last year, The Alan Turing Institute and the Information Commissioner’s Office (ICO) have been working together to discover ways to tackle these difficult issues. The ultimate product of this joint endeavour - the most comprehensive practical guidance on AI explanation produced anywhere to date - has now been released for consultation. The consultation runs until 24 January 2020, with the final guidance due to be released later in the year.
Read more.
 
In order to manage the impacts of AI responsibly and to direct the development of AI systems toward optimal public benefit, The Alan Turing Institute's public policy programme partnered with the Office for Artificial Intelligence and the Government Digital Service to produce guidance on the responsible design and implementation of AI systems in the public sector. 
The guide, Understanding Artificial Intelligence Ethics and Safety, is the most comprehensive guidance on the topic of AI ethics and safety in the public sector to date. It identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. The guide stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems.
The guidance is relevant to everyone involved in the design, production, and deployment of a public sector AI project: from data scientists and data engineers to domain experts, delivery managers and departmental leads. Our aim -- and hope -- in writing the guide is to encourage civil servants interested in conducting AI projects to make considerations of AI ethics and safety a first priority.
Read more
 
 
Consultation
The creation of the Centre for Data Ethics and Innovation was announced in late 2017. The Government launched its Consultation on the Centre in June 2018, seeking the views of interested groups from across society regarding how the Centre should operate, and which work it should prioritise.
Summary of the Turing’s submission
The Institute’s response addresses each question posed by the Consultation, and focuses on two areas that we feel we are particularly suited to advise on: how the new Centre should work, and which issues in data ethics it should focus on.
On how the Centre should work, we raise questions around the overall resources that the Centre will have at its disposal, how it is structured, and to what extent research will be undertaken by the Centre itself versus by outside groups on the Centre’s behalf. We argue that amongst the Centre’s proposed functions, its mandate to “publish recommendations to Government” and to “provide expert advice and support to regulators” will allow it to differentiate itself in an increasingly crowded field of digital ethics.
On which issues it should work on, we agree that the Centre’s identification of six areas of interest represents a good overview of the current state of the field. We also highlight other areas, including accountability, proportionality, and consent, which we feel are relatively underemphasised in the Consultation, and argue that some of the existing themes require more conceptual clarity.
Our response also includes comments on the legal and political basis of the new Centre, the ways in which it might engage with stakeholders, and how it should deliver its recommendations to Government.
Read more
For more information, please contact the public policy programme's team at:
[email protected]
"
https://www.turing.ac.uk/people/external-researchers/gerhard-paris,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/dso-national-laboratories,"
"
https://www.turing.ac.uk/news/alan-turing-institute-and-singapores-dso-national-laboratories-sign-new-agreement,"
A new Memorandum of Understanding has been signed between The Alan Turing Institute and DSO National Laboratories in Singapore. The agreement will drive research advances in three critical areas affecting society: Countering Mis- and Dis-information; Counter Terrorism; and Humanitarian Aid and Disaster Relief.
Researchers from DSO, Singapore's largest national defence research and development organisation will work in tandem with leading experts from the Turing’s thriving Defence and Security Programme. Together, the Turing and DSO will explore new research methods designed to help understand complex datasets, covering different modalities such as image, text, and audio. Researchers will work to develop novel methods to help with the analysis of these fascinating and complex data sets.
The agreement will also lead to new, joint research projects and an exchange of academic materials and publications. Collaboration will be at the heart of the research programme, pulling in academic experts and other academic institutions in the UK.
Speaking at a virtual signing ceremony, Adrian Smith, Chief Executive and Director of The Alan Turing Institute, said, “Misinformation, terrorism and humanitarian aid are three of the most critical challenges facing governments across the world. This agreement with DSO will allow us to build our shared capabilities in these areas and explore these issues through a symbiotic international collaboration.”
CEO of DSO National Laboratories, Mr Cheong Chee Hoo, added ""Partnership with The Alan Turing Institute will provide our researchers the opportunity to work side-by-side with like-minded AI experts. We look forward to pushing the boundaries and tackling the challenges in data science and AI, and these would be useful in developing applications to enhance our national security.""
Kara Owen, British High Commissioner to Singapore, said ""The UK and Singapore’s partnership on science, research and innovation is one of the most exciting parts of the relationship, where we work now on things that will define our world in future years. This MOU between The Alan Turing Institute and DSO National Laboratories is yet another milestone. Through the MOU, top minds from the Turing and DSO will research advanced Artificial Intelligence (AI) technologies, with the goal of supporting the evolving needs of digital communities. I look forward to witnessing the opportunities this collaboration will bring and have no doubt that the results will help shape our futures.""
Further information about the themes:
New techniques in analysing social media content will enable the identification of harmful online material such as malign influence, extremist rhetoric, and the encouragement of serious violence, whether it is spread as text, image, or video. This will help to curb the speed at which such material can be shared and ensure that citizens continue to benefit from safe and trusted online spaces. 
In a period of unprecedented global disasters fuelled by a changing climate, the ability to effectively provide responses to these disasters becomes more important than ever. A growing ability to collect data about such disasters, driven by both more prolific sensors and autonomous vehicles, provide a significant opportunity to first responders in tackling these disasters. AI techniques can help in areas such as analysing imagery, locating survivors, and providing important cultural, contextual, and linguistic support to first responders, especially in typically low-resource languages. Such techniques are of much importance in multilateral humanitarian relief efforts.
Data analysis plays a large and substantial part in countering extremism. Analysts have to sift through large amounts of data to find threats, looking for the needle in the haystack that could result in a foiled extremist plot. Novel AI techniques can help analysts undertake this in a more efficient and expedient way.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/zooniverse,"
www.zooniverse.org
"
https://www.turing.ac.uk/node/7396,"
"
https://www.turing.ac.uk/research-engineering,"

Researchers and practitioners face a common need for high quality tools, practices, methodologies, platforms and systems.
Many domains can benefit from the deployment of cutting-edge algorithms and approaches, but these cannot be effectively applied unless realised as usable software libraries, reproducible analyses and workflows, or high performance computational environments.
The Research Engineering Group (REG) contributes skills in research software engineering and data science in support of other programmes, as well as to its own projects. This model of working ensures that the tools they develop are useful and applicable to a wider range of areas. The team supports professional delivery of impactful research across the Turing's programmes, as well as its own research interests.
The team concentrate mainly on:
To keep up to date with the team, follow us on twitter at @turinghut23 or LinkedIn
Header image: Close up of a Bletchley Park Bombe, designed by Alan Turing and his colleagues to decipher German Enigma machine messages in WW2. Source: mendhak, Flickr, Creative Commons.

 
Read about some of our project highlights. See here for a complete list. 
Research Engineering is involved in many projects across the Turing, collaborating both with academics and strategic commercial partners.
Below is the list of our most recent projects, the full list can be found here.
For a full list of blogs and news associated with REG see here.
The most recent events involving REG are shown below. For a complete list see here. 
Below is the list of our most recent publications, the full list can be found here. 
"
https://www.turing.ac.uk/people/researchers/mehrdad-dianati,"
Professor Mehrdad Dianati is the Head of Intelligent Vehicles Research Department and the Technical Research Lead in the area of Networked Intelligent Systems at the Warwick Manufacturing Group (WMG), the University of Warwick, UK.
The focus of his research is on the application of Digital Technologies (Information and Communication Technologies and Artificial Intelligent) for the development of future mobility and transport systems. He has over 30 years of combined industrial and academic experience, with 20 years in various leadership roles in multi-disciplinary collaborative R&D projects. He works closely with the Automotive and ICT industries as the primary application domains of his research. He is also the Director of Warwick's Centre for Doctoral Training on Future Mobility Technologies, training doctoral researchers in the areas of intelligent and electrified mobility systems in collaboration with the experts in the field of electrification from the Department of Engineering of the University of Warwick. In the past, he has served as an Editor for the IEEE Transactions on Vehicular Technology and several other international journals, including IET Communications. Currently, he is the Field Chief Editor of Frontiers in Future Transportation. 
"
https://www.turing.ac.uk/events/nature-reviews-physics-machine-learning-fluid-dynamics-and-climate-physics,"
Researchers in field of fluid dynamics have been experimenting with machine learning since the 1990s, having driven many advances in the use of these methods in modelling and simulation. The combination of real and simulated data together with physics-informed machine learning is now used in climate modelling. The extensive experience in benchmarking and validating fluid dynamics simulations can inform climate modelling and other fields.   
Chair - Dr. Gabriel Weymouth 

In this event, we will hear from Dr. Steven Brunton and Professor Laure Zanna. 
Dr. Steven Brunton - Machine learning for scientific discovery with examples in fluid mechanics 
I will describe how machine learning may be used to develop accurate and efficient nonlinear dynamical systems models for complex natural and engineered systems. I explore the sparse identification of nonlinear dynamics (SINDy) algorithm, which identifies a minimal dynamical system model that balances model complexity with accuracy, avoiding overfitting. This approach tends to promote models that are interpretable and generalisable, capturing the essential “physics” of the system. Additionally, I discuss the importance of learning effective coordinate systems in which the dynamics may be expected to be sparse. This sparse modeling approach will be demonstrated on a range of challenging modeling problems in fluid dynamics and I will discuss how to incorporate these models into existing model-based control efforts. As fluid dynamics is central to transportation, health and defence systems, I will emphasise the importance of machine learning solutions that are interpretable, explainable, generalisable and that respect known physics.
Professor Laure Zanna - Leveraging interpretable machine learning for climate physics 
In this presentation, I will describe the complex and multiscale nature of the climate system and how machine learning can be leveraged to deepen our understanding of key physical climate processes. I will focus on advances in interpretable and physics-aware machine learning methods that have the potential to accelerate scientific discovery in climate physics and modeling. In particular, I will discuss examples of interpretable and generalisable machine learning models that capture ocean turbulence processes (horizontal scale of 10 km-100 km) and how these turbulent features can impact large-scale ocean currents (1000’s of kms). The machine-learned models of turbulent processes are shown to improve coarse-resolution climate simulations by faithfully capturing the complex multiscale dynamical properties in the climate system. 
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/strategic-priorities-fund-lwm,"
Research funded by the Strategic Priorities Fund, EPSRC Grant P/T001569/1
Read more about the Strategic Priorities Fund here
"
https://www.turing.ac.uk/people/researchers/pica-johansson,"
Pica is a Research Associate on the Online Safety Team at The Alan Turing Institute. She also has experience working with policymakers in UK Parliament on the forthcoming Online Safety Bill. Pica holds a Master’s Degree in Media, Data and Society from the London School of Economics and Political Science (LSE), where she graduated with an excellence award for achieving the highest mark in her program cohort. At LSE her research focused on white supremacist discourse. Prior to joining the Turing, Pica worked as a communications analyst at the UN Refugee Agency.
At the the Turing, Pica is helping develop the Online Harms Survey and the Online Harms Observatory. In August 2022 she co-authored the Turing's report ""Tracking abuse on Twitter against football players in the 2021-22 Premier League Season"", commissioned by Ofcom and supported by the Department for Digital, Media, Culture and Sport. She is also responsible for the team's stakeholder engagement with key policymakers, civil society and other experts in the online safety space.
"
https://www.turing.ac.uk/research/research-areas/machine-learning/speech-recognition,"
"
https://www.turing.ac.uk/people/guest-speakers/jennifer-quint,"
Jennifer Quint is a Professor of Respiratory Epidemiology in the School of Public Health, National Heart and Lung Institute, at Imperial College London. She is an Honorary Consultant Physician in Respiratory Medicine at both the Royal Brompton and Imperial College London NHS Foundation Trust.
She is a Fellow of the Higher Education Academy, the Royal College of Physicians and of the Faculty of Clinical Informatics. Most recently she has taken over as joint Editor-and-Chief of Thorax. 
"
https://www.turing.ac.uk/news/publications/skill-based-shared-control,"
Performing a number of motion patterns – referred to as skills – (e.g., wave, spiral, sweeping motions) during teleoperation is an integral part of many industrial processes such as spraying, welding, and wiping (cleaning, polishing). Maintaining these motions whilst simultaneously avoiding obstacles and traversing complex terrain requires expert operators. In this work, we propose a novel skill-based shared control framework for incorporating the notion of skill assistance to aid novice operators to sustain these motion patterns whilst adhering to environmental constraints. Our shared control method uses streaming joystick data to estimate the model parameters that provide a description of the operator’s intention. We introduce a novel parametrization for state and control that combines skill and underlying trajectory models, leveraging a special type of curve known as Clothoids. This new parameterization allows for efficient computation of skill-based short term horizon plans, enabling the use of a Model Predictive Control (MPC) loop. We perform experiments on a hardware mock-up, validating the effectiveness of our method to recognize a switch of intended skill, and showing an improved quality of output motion, even under dynamically changing obstacles.
Mower, CE, Moura, J & Vijayakumar, S 2021, Skill-based Shared Control. in Robotics: Science and Systems 2021. Robotics: Science and Systems 2021, 12/07/21.
2.3 MB
"
https://www.turing.ac.uk/people/researchers/julie-mccann,"
Julie McCann is Professor of Computer Systems at Imperial College London and Strategic Leader for the 'Resilient and robust infrastructure' Grand Challenge of the Turing's programme in Data-centric Engineering.
McCann has been mainly researching and publishing work that centres on highly decentralised algorithms, protocols, cross-layered solutions to wireless sensor networks. Here, there is a particular focus on low-powered sensing nodes with some work on hybrid and hierarchical topologies. She also has some interests in Ubiquitous and Mobile computing (Internet of Things, if you will), specifically in context awareness, as well as Autonomic Computing. She leads Adaptive Emergent Systems Engineering (AESE) group, is Imperial PI for the Intel Collaborative Research Institute on Sustainable Connected Cities and is Co-PI on both the NEC Smart Water and Cisco Future Cities projects.
Her past, in computer performance, dynamic operating systems, database machines and text retrieval systems, has informed this work. For her earlier research in text retrieval, she was co-awarded Emerald Literati Network “Highly Commended”. All her projects are interdisciplinary, applied to the arts or engineering and mainly funded by EPSRC/NERC/TSB national programmes, collaborating with; RCA; The University of the Arts; Interactive Institute Stockholm etc.; as well as many other industrial partnerships such as with Sun Microsystems, Thames and Severn-Trent Water, BT, Arup Engineering and BBC to name but a few. She has also been a consultant for TV and Film on occasion.
"
https://www.turing.ac.uk/events/pint-science-0,"
Pint of Science is a worldwide science festival which brings researchers to your local pub to share their scientific discoveries with you - no prior knowledge required. Tickets cost £5; all proceeds go towards sustaining the events.
This is an in-person event. Doors open at 18:30, talks start at 19:00
From social media algorithms to autonomous vehicles, the possibilities of AI seem endless. It can do incredible things, but there are potential harms and drawbacks too. How can we harness its power for the betterment of society, and who can we trust to ensure that our data is being handled responsibly?
Join researchers from The Alan Turing Institute as we delve into some of the ways we are using AI and data science for good. We'll be exploring topics such as AI for the natural world, and the trade-off between privacy and accuracy when it comes to finding love online. 
Can AI save the planet? - Evangeline Corcoran 
The climate crisis is already having profound effects on our everyday lives. From longer droughts to more severe storms, the future of our planet is uncertain. Finding solutions requires reliable and effective methods for measuring and predicting how animals, plants, and ecosystems will respond to future conditions.
AI may provide hope, with the promise of more accurate and rapid methods to monitor and predict environmental responses to possible futures.
But the question is, how best to apply these methods to make sense of our ever-changing, unpredictable natural world?
Swipe right for privacy-preserving, fair, and robust machine learning models - Aras Selvi
The last five years have seen a massive shift in the way we view and understand our rights when it comes to data privacy. With countless social media giants hitting the headlines for questionable practices over the uses and misuses of our personal data, we’ve had to wise up, and quickly. Enter dating apps, and the promise of love with the perfect match throws all our good sense out the window, making us vulnerable to bad actors.
Is it too much to expect private, fair and robust algorithms in the world of online dating? We say no!
The Big Fat Quiz of AI - Kristina Ulicna, Martin O'Reilly, Samuel Tonks, hosted by Hari Sood
Are headlines about AI spamming your newsfeed? Can you tell the difference between a human and a bot? Well, buckle up for a quiz on all things AI(ish), guided by a panel of experts.
 
Hosted by Bea Costa Gomes
The Gallery at Tileyard
5 Tileyard Road
King's Cross
London, N7 9AH
"
https://www.turing.ac.uk/people/external-researchers/marta-batlle,"
Marta is currently a data scientist in Pharma Informatics and the technical lead of the Synthetic Data Squad at Roche, which is building a synthetic data service within the company to break barriers to patient data sharing. She has also worked with different Machine Learning and NLP techniques to improve our understanding of patient journeys and speed up clinical trials. Prior to joining Roche, she completed an MSc in Health Data Science and an MRes in Neuroscience at University College London. 
"
https://www.turing.ac.uk/user/login,"
"
https://www.turing.ac.uk/research/research-areas/machine-learning/unsupervised-learning,"
"
https://www.turing.ac.uk/people/research-fellows/peter-boyle,"
Peter received his PhD in Particle Physics from the University of Edinburgh and has been a reader in the Theoretical Particle Physics research group at Edinburgh since 2004. Peter is a member of the RBC-UKQCD collaboration, and collaborates with Columbia University, Southampton, Brookhaven National Laboratory, RIKEN Brookhaven Research Center, University of Virginia, University of Connecticut.
 
Low energy Quantum chromodynamics using computer simulation, and particularly matrix elements of hadrons that are required to constrain fundamental parameters of the standard model and search for physics beyond the standard model. Peter believes his publications represent the best constraints on Vus and BK which enter the famous unitarity triangle. Subtopics: Kaon matrix elements - BK, Kl3 , K->pi pi. Non-perturbative renormalisation of Lattice operators. Chiral lagrangian. Hadron spectrum and decay constants. Electromagnetic effects.
"
https://www.turing.ac.uk/news/alan-turing-institute-launches-new-turing-university-network,"
The Alan Turing Institute has launched the Turing University Network today (Monday 3 April) which provides all UK universities with an interest in data science and artificial intelligence (AI) the opportunity to engage and collaborate both with the Institute and its broader networks.
Thirty six universities from the previous Turing university partners and Turing Network Development Award recipients have joined the pilot network and a call, available to all UK universities, will open in the Spring.
The Turing University Network is a key component of the new Institute Strategy aimed at changing the world for the better with data science and AI. The network supports the Institute in achieving its three ambitious goals: advance world-class research and apply it to national and global challenges, build skills for the future, and drive an informed public conversation. 
Through the Turing University Network, the Institute will build a structured and inclusive university network representative of the wider data science and AI community in the UK which will facilitate better connections across the data science and AI landscape. The network will be open, with minimal barriers to both entry and bureaucracy, and will provide enablers such as mechanisms to connect across the ecosystem, interesting problems and routes to impact.
Donna Brown, Director of Academic Engagement at The Alan Turing Institute, said: “We’re delighted to launch a new UK-wide university network of data science and AI expertise via the Turing University Network. The network is crucial to successful delivery of the Institute Strategy, enables our role as a national convenor in the data science and AI landscape, and creates opportunity for meaningful collaboration where interests align across research and innovation, skills, and engagement.”
"
https://www.turing.ac.uk/events/human-centric-artificialcomputational-intelligence-and-applications,"
Human-Centred artificial/computational intelligence (HCAI/CI) is an emerging discipline intent on creating AI/CI systems that amplify and augment rather than displace human abilities. Instead, HCAI/CI seeks to preserve human control in a way that ensures AI meets our needs while also operating in a reliable, robust, and transparent manner, delivering equitable outcomes, and respecting privacy.
This workshop is designed to bring key data scientists and AI experts from both academia and industry to share up to date research and innovation developments themed on HCAI/CI, but also help identify the key challenges for stakeholders.
We anticipate this workshop playing as the springboard to explore potential collaboration between academics and industrial stakeholders. It will take place as part of the outreach activities of the Alan Turing Institute and the Institute for Data Science and Artificial Intelligence (AI) at the University of Exeter, partnering with the University of Oxford and the University of Birmingham. 
The event will take place in Exeter, from lunchtime on 26th April, to lunchtime on 28th April.  The event is open to all working in the area, both academia and industry.  Travel funds are available, which are prioritised for early career researchers.  Please see the Eventbrite registration for more information.
Register now
"
https://www.turing.ac.uk/news/publications/hermitian-matrices-clustering-directed-graphs-insights-and-applications,"
Graph clustering is a basic technique in machine learning, and has widespread applications in different domains. While spectral techniques have been successfully applied for clustering undirected graphs, the performance of spectral clustering algorithms for directed graphs (digraphs) is not in general satisfactory: these algorithms usually require symmetrising the matrix representing a digraph, and typical objective functions for undirected graph clustering do not capture cluster-structures in which the information given by the direction of the edges is crucial. To overcome these downsides, we propose a spectral clustering algorithm based on a complex-valued matrix representation of digraphs. We analyse its theoretical performance on a Stochastic Block Model for digraphs in which the cluster-structure is given not only by variations in edge densities, but also by the direction of the edges.
The significance of our work is highlighted on a data set pertaining to internal migration in the United States: while previous spectral clustering algorithms for digraphs can only reveal that people are more likely to move between counties that are geographically close, our approach is able to cluster together counties with a similar socio-economical profile even when they are geographically distant, and illustrates how people tend to move from rural to more urbanised areas.
Cucuringu, Mihai, Huan Li, He Sun, and Luca Zanetti. ""Hermitian matrices for clustering directed graphs: insights and applications."" arXiv preprint arXiv:1908.02096 (2019)
10.3 MB
"
https://www.turing.ac.uk/events/machine-reflections-visual-generative-ai-workshop,"
Join this event in Glasgow for a networking workshop addressing the challenges raised by new visual generative AI (VGAI) tools. This interdisciplinary encounter seeks to explore the interplay between VGAI tools and various creative ecosystems, taking into account the structure and practices of different industries, drawing on CREATe’s expertise on digital copyright and content moderation issues.
The workshop is structured as a series of roundtables anchored by expert presentations from CREATe or invited speakers, addressing the following four themes:
Register now
"
https://www.turing.ac.uk/people/enrichment-students/anna-george,"
Anna George is a Social Data Science PhD student at the University of Oxford who uses computational approaches to study online harms. Her research focuses on the message transmission of harmful online communities. Before joining the doctoral program at the Oxford Internet Institute (OII), Anna graduated with distinction from the OII’s MSc in Social Data Science. Anna holds an M.S. in Industrial/Organizational-Social Psychology and a B.S. in Psychology, Second Major in Sociology, and minor in Statistics.
The goal of Anna’s research is to make the internet a safer space for all by studying those who make the internet unsafe. Anna is continuing this mission at The Alan Turing Institute so that she can increase her use of high-quality social data science methods in her work. During her time at the Alan Turing Institute, Anna will be using Natural Language Processing techniques to quantitatively capture harmful narratives to see where these narratives are present online. Anna hopes her work will inform algorithmic approaches to tracking and tackling hateful narratives.
"
https://www.turing.ac.uk/collaborate-turing/data-study-groups/accenture-challenge-fairness-algorithmic-decision-making,"

As the development and implementation of artificial intelligence continues to gather pace conversations on ethics and how to make the technology fair and beneficial for all are increasingly taking place across industry circles. As a major global company that aims to help business transform in the digital world, Accenture’s challenge to Turing DSG researchers was to develop a tool that can promote fairness in algorithmic decision-making across the financial services sector.
Dr Rumman Chowdhury, Accenture’s Lead on Responsible AI explains, “Fairness is currently a hot topic at conferences and our challenge is inspired by Dr Arvind Narayan’s tutorial at the FATML conference where he discussed 21 different types of fairness. Clients are increasingly aware of the importance of responsible deployment of AI and our challenge for Turing researchers was to develop a tool that can be applied to real world contexts and that is compliant with GDPR.”
Before work on the challenge began Rumman - a data scientist with a background in political science and quantitative social science - convened a space where the team of 11 researchers with backgrounds in statistics, ethics, applied statistics, and machine learning had a chance to review the literature on fairness and discuss its conceptual meaning.
Explaining her rationale for preparing the researchers in this way, she says: “At the moment a global conversation on fairness is taking place, but developing a real-world tool that is explainable, transparent, and accountable appears to be an insurmountable challenge. The idea of fairness is perceived in different ways across disciplines such as statistics and social science and I wanted to give the group the opportunity to consider a range of academic definitions outside of their own disciplines to help inform their approach to the challenge.”
Using a publicly available dataset on credit risk, the researchers were tasked with developing a quantifiable metric-based tool to map out and identify where bias and unfairness can creep in and fix it. Rumman says: “We envisage this tool being used by our clients in a broad range of industries. They would use the work flow assistant or software to assess the algorithms that they are currently using.”
""An excellent forum to solve hybrid questions that are experimental and underdefined""
Dr Rumman Chowdhury, Lead on Responsible AI at Accenture
After five long days of dedicated hard work, Rumman is highly satisfied with the outcome and believes it was definitely time well spent. She says: “This experience has been absolutely worthwhile. The team were fabulous and have come up with code that Accenture will build into a prototype. We plan to demo this tool at Accenture’s pavilion on responsible AI at the CogX exhibition in June [2018].”
Reflecting on the team’s dynamics and the DSG experience overall, she adds: “The researchers were able to work collaboratively and come up with a coherent solution, but also demonstrate that there are multiple ways to address the problem. I think people were able to shine as individuals, learn something new, while also contributing at a macro level.
“It’s been an excellent forum to solve hybrid questions that are experimental and underdefined. I think it’s difficult to find the skills sets that they brought to this challenge and I think that as data scientists this is a great space to mentally breathe and try out new things and bring form and structure to what was a nebulous but fun challenge.”

Finally thinking back to the key highlight, Rumman concludes: “The main highlight was the code of conduct, which was understanding, overwhelmingly clear and went a long way in generating a safe, collaborative and zero tolerance environment. I think this really demonstrated the Turing’s willingness to address and rectify any issues. I think it was done right and given the due respect and time.”
"
https://www.turing.ac.uk/people/guest-speakers/emille-ishida,"
Emille Ishida is a Research Engineer at the French National Centre for Scientific Research and IS based at the Laboratoire de Physique de Clermont Ferrand, France. She is an expert in the development of recommendation systems for astronomical data, currently engaged in the Vera Rubin Observatory Dark Energy Science Collaboration (DESC).
Emille is the founder and manager of three interdisciplinary and international research networks (the Cosmostatistics Initiative -  COIN, the SNAD collaboration and the Fink broker), where she conducts experiments related to machine learning applications to astronomy as well as investigates the role of astronomy as a trigger for innovation in academia. 
She currently serves as Public Relations Officer for the International Astrostatistics Association (IAA). Her book Bayesian Models for Astrophysical Data won in the Astronomy and Cosmology category at the Proze Awards 2018. Her research interests include interpretability in machine learning, optimum experiment design, science of team science and studies on the future of the academic research environment.
"
https://www.turing.ac.uk/people/enrichment-students/russell-martin,"
Russell Martin is a PhD candidate at Leeds University Business School. His research interests are in people analytics, with a current focus on using machine learning with large workforce data sets to understand how careers unfold for underrepresented/disadvantaged groups.
Russell is researching the ways that occupational segregation in the NHS negatively impacts the careers of underrepresented/disadvantaged groups - and how this changing over time. Segregation is thought to arise from job and labour queues, which are explained to some extent by cultural capital and homophily theory. He is using an intersectional approach for analysis, which offers greater precision than single-axis analyses, such as looking at gender without ethnicity. The data set constitutes detailed information of Europe's largest internal labour market over the last 10 years and offers a unique opportunity to quantitatively examine some of the factors contributing to queues. He hopes to use machine learning techniques to uncover nonlinear relationships that would otherwise be missed.
"
https://www.turing.ac.uk/people/evangeline-corcoran,"
Evangeline Corcoran is a Research Associate at the Alan Turing Institute working on the Impact of Climate Change on UK Agriculture and Food Security Project as a part of the AI for Science and Government Program. Prior to joining the Turing, she completed her PhD on ‘Monitoring and Modelling Vulnerable Wildlife Populations Using Remotely Piloted Aircraft Systems and Machine Learning’ at Queensland University of Technology in 2021.
Evangeline is a quantitative ecologist, and her research interests include a focus on how advanced technologies such as machine learning and remote sensing can be applied to environmental data to address challenges to conservation and enhance our understanding of how ecosystems function. In her role on the UK agriculture project, she is focussed on data integration, tools, and scaling across the project. This is includes development and maintenance of a database of data and models related to crop yield, development of tools for automated plant phenotyping, and exploring use of machine learning to predict crop yield from satellite imagery at a coarse-grain UK-wide scale.
"
https://www.turing.ac.uk/research/research-projects/citizen-participation-and-machine-learning-better-democracy,"
The use of digital platforms for citizen participation enables the engagement of a large number of citizens, who contribute with proposals, comments and through their interactions with one another. However, it can be very difficult for citizens and local government officials to get an overview or summary of the many thousands of different proposals and comments and this 'information overload' makes it difficult to identify and achieve common objectives.
Machine learning and natural language processing (NLP) techniques have proven successful for facilitating access to large collections of information in situations of collective sense-making. Similar methods will be adapted to overcome this information overload problem in citizen participation platforms. This will, in turn, enable these platforms to have a more profound impact on institutions and on furthering direct democracy.
The first stage of the project will be to develop ways of using citizen-contributed tagging (i.e., 'folksonomy') for semantically tagging proposals in ways that will improve citizens' ability to discover proposals relevant to their interests.
The second stage focuses on the application of algorithms to cluster proposals into coherent topics and to generate summaries in ways that are easily understood by citizens. The approach will also allow topics to evolve over time as new proposals are submitted. The quality of the topic models and summaries will be evaluated using citizen feedback.
The third stage will cluster citizens in terms of their support for, and their comments on proposals. This will be used to improve the existing ways in which proposal communities (groups of like-minded citizens) are recommended to citizens.
The final stage will focus on grouping and summarising comments posted in response to proposals through, for example, analysis of the stance expressed in comments.
The project aims to address barriers to achieving effective direct democratic systems. If successful, it will allow citizens to directly contribute to the most important decisions in their communities.
The aim is to test the hypothesis that the use of machine learning and NLP techniques on a digital platform for citizen participation will significantly increase the capacity of citizens to participate in local democracy. The project will focus on one particular form of citizen engagement in which citizens submit proposals for policies they wish to see enacted by, e.g. local government administrators. Proposals must obtain a certain number of support votes as a condition for their proposals to be selected. With the use of machine learning and NLP techniques, it's hypothesised that a greater number of proposals will reach the support needed to be selected.
The Consul platform is designed in a generic way to support collective intelligence processes (how people propose ideas, connect them, improve them, select the most relevant ones, etc.), so that it can be used in many other settings not related with direct democracy where large-scale, collective intelligence may bring benefits to institutions.
Consul is currently in use by more than 100 institutions, ranging from city administrations such as New York, Buenos Aires and Paris, to national governments such as Colombia and Uruguay. It has been awarded the United Nations Public Service Award and is currently being used by the United Nations Development Programme and is also supported by the Inter-American Development Bank and the Open Government Partnership.
Combining Crowds and Machines: Experiments in collective intelligence design report published. This is the final report of Nesta's Collective Intelligence Grants programme. The Alan Turing Institute is one of the grantees.
Project received a Collective Intelligence Grant from Nesta to help fund the research.
[email protected]
"
https://www.turing.ac.uk/people/research-fellows/mhairi-aitken,"
Mhairi Aitken is an Ethics Fellow in the Public Policy Programme at The Alan Turing Institute, and an Honorary Senior Fellow at Australian Centre for Health Engagement, Evidence and Values (ACHEEV) at the University of Wollongong in Australia. She is a Sociologist whose research examines social and ethical dimensions of digital innovation particularly relating to uses of data and AI. Mhairi has a particular interest in the role of public engagement in informing ethical data practices.
Mhairi was included in the 2023 international list of “100 Brilliant Women in AI Ethics”.
Prior to joining the Alan Turing Institute, Mhairi was a Senior Research Associate at Newcastle University where she worked principally on an EPSRC-funded project exploring the role of machine learning in banking. Between 2009 and 2018 Mhairi was a Research Fellow at the University of Edinburgh where she undertook a programme of research and public engagement to explore social and ethical dimensions of data-intensive health research. She held roles as a Public Engagement Research Fellow in both the Farr Institute of Health Informatics Research and the Scottish Health Informatics Programme (SHIP).
Mhairi is passionate about finding creative ways of engaging members of the public in discussions around the roles of data and AI in society. She is a regular performer in the Cabaret of Dangerous Ideas at the Edinburgh Festival Fringe and has performed shows based on her research at the Edinburgh International Science Festival, Glasgow International Comedy Festival, Belladrum Tartan Heart Festival as well as in stand-up comedy clubs.
Mhairi is a frequent contributor to media discussions on AI and data, including on TV, radio and in print. She has featured on BBC 2 Politics Live, BBC Radio 4 Today Programme and Woman’s Hour, LBC with Andrew Marr, BBC Five Live and many more radio programmes across national and regional stations. She featured as an on screen expert in Shark Teeth Films’ six part documentary series “Secrets of Big Data”.
Mhairi’s recent invited lectures, talks, and public appearances include:
Mhairi’s research draws on her background in Sociology and Science and Technology Studies (STS) to examine social and ethical dimensions of innovation. Her current research includes a focus on developing child-centred approaches to AI and advancing data justice. Her past research has focussed in particular on the role of machine learning in finance; governance of data-intensive health research; ethical considerations around secondary uses of health data and; planning and development processes relating to renewable energy projects.
Mhairi has experience of using a range of qualitative and quantitative research methods and has a particular interest in deliberative engagement methods including citizens' juries, deliberative workshops, focus groups and public panels.
"
https://www.turing.ac.uk/research/research-areas/computer-systems-architectures/neural-evolutionary-computing,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/henry-royce-institute,"
www.royce.ac.uk
"
https://www.turing.ac.uk/people/guest-speakers/eun-ah-kim,"
Eun-Ah Kim is a Professor of Physics at Cornell University. Her research interests lie in the theoretical study of the collective phenomena condensed matter systems exhibit and in understanding how such phenomena emerge from microscopic physics with a particular focus on high-temperature superconductivity and topological phases. Eun-Ah is interested in the application of neural network-based machine learning to explore these systems.
She has received numerous prizes including the Excellence in Teaching Award, University of Illinois at Urbana-Champaign, the John Bardeen Award, the NSF CAREER Award and the DOE CAREER Award. 
"
https://www.turing.ac.uk/people/researchers/jennifer-whyte,"
Laing O’Rourke / Royal Academy of Engineering Professor of Systems Integration, Imperial College London and Strategic Leader for the 'Data-driven engineering design under uncertainty' Grand Challenge of the Turing's programme in Data-centric Engineering.
Jennifer’s research focuses on systems integration in civil infrastructure. Infrastructure projects are organisationally complex and deliver both physical assets and digital asset information. The vision is of developing next-generation tools for and approaches for systems integration. Research activities support the transformation to an advanced manufacturing industry with digital engineering capabilities, with decision-support tools for engineers and managers working on complex engineering projects.
"
https://www.turing.ac.uk/people/external-researchers/nijesh-upreti,"
Nijesh is an enthusiastic researcher currently engaged in doctoral studies at the University of Edinburgh, with a specific focus on Neuro-Symbolic Artificial Intelligence (AI). With a background in cognitive science, mathematics, and neuroscience, he brings a multidisciplinary approach to his research. His primary focus lies in developing computational frameworks and novel approaches that bridge the gap between neural learning and symbolic reasoning. By integrating these two domains, he aims to create AI systems that are verifiable and interpretable in applied settings. Nijesh is driven by the belief that transparent AI models are essential for building trust and enabling wider adoption of AI technologies. 
Nijesh’s research journey has been enriched by valuable experiences at various institutions. At the Picower Institute at MIT, he delved into the fascinating intersection of neuroscience and AI, gaining valuable insights into the inner workings of the brain and its connection to intelligent systems. Additionally, his time at the HCI lab at UW-Madison exposed him to the principles of human-centered design and interaction, highlighting the significance of creating AI systems that are intuitive and user-friendly. These experiences have fueled Nijesh’s curiosity and sparked his interest in exploring the potential of Neuro-Symbolic AI. By combining the power of neural networks with the reasoning capabilities of symbolic approaches, he aspires to unlock new possibilities and applications in the field of AI.
 
"
https://www.turing.ac.uk/opportunities/doctoral-student-opportunities,"

""The Alan Turing Institute has provided generous support to my work, not only in terms of a very prestigious studentship but also access to collaborators at Intel whose valuable feedback has led to the publication of my conference paper in 2017.""
Edward Chuah, Former doctoral student, 2016 cohort
The Turing has supported over 50 students to undertake PhDs in a range of topics related to data science and artificial intelligence.
Placements and online community awards for enhancing doctoral student research.
The Turing has supported a number of universities with successful bids to deliver CDTs.
"
https://www.turing.ac.uk/collaborate-turing/current-partnerships-and-collaborations/university-leeds,"
"
https://www.turing.ac.uk/people/business-team/daniel-lovelock,"
"
https://www.turing.ac.uk/events/turing-lecture-plan-ai-because-there-no-planet-b-online-event,"

To mark Earth Day, climate scientist Emily Shuckburgh joins us for our first virtual Turing Lecture on sustainability and the climate crisis. This event will be subtitled.

The last five years have seen the hottest temperatures ever on record and carbon dioxide has reached a level never before inhabited by humans. The conditions within which we are living today are unprecedented, and the effects are manifesting in extreme weather such as flooding and rising temperatures. Despite warnings from scientists and campaigners, world leaders have been slow to act, and we are now facing the biggest threat to our existence in the history of humanity. In this talk, climate scientist Emily Shuckburgh presents the facts and figures, based on world-class research conducted at Cambridge Zero, the University of Cambridge’s zero-carbon future initiative. She will discuss the scientific evidence around climate change, possible solutions being researched now to secure the future of our planet and the impact of COVID-19 on climate change.
This is an online event and will take place via Zoom webinar. You will have the opportunity to submit questions using the Q&A function once the event begins, or on Twitter using the hashtags: #TuringLecture #EarthDay and tagging @turinginst.
13:00-13:05 - Introduction - Scott Hosking (British Antarctic Survey and The Alan Turing Institute)
13:05-13:45 - Turing Lecture - Emily Shuckburgh (Director of Cambridge Zero, University of Cambridge)
13:45-14:00 - Q&A - Scott Hosking and Emily Shuckburgh
"
https://www.turing.ac.uk/research/research-areas/artificial-intelligence/symbolic-systems,"
"
https://www.turing.ac.uk/about-us/governance,"

The Alan Turing Institute was founded in March 2015 as a Registered Charity (1162533) and Company Limited by Guarantee (09512457).
Annual report 2021/22
Annual report 2020/21
Annual report 2019/20
Annual report 2018/19
Annual report 2017/18
Annual report 2016/17
Annual report 2015/16
 
The charitable object of the Institute is the furtherance of education for the public benefit particularly through research, knowledge exchange and public engagement in the fields of data sciences. In 2017, as a result of a government recommendation, we added artificial intelligence to our remit.
The Institute is governed by its Board of Trustees who are also its Directors. The Board of Trustees is made up of independent members and nominated Trustees from each of our joint venture partners.
The Institute’s founding partners are the Universities of Cambridge, Edinburgh, Oxford, University College London and Warwick and the Engineering and Physical Sciences Research Council.
Answering a national need for investment in data science research, they formed the Institute as a joint venture in 2015, following an open competition run by the EPSRC.
Each founding university has appointed a Turing University Lead who acts as an interface between the Institute and the founder university.
The Institute is governed by its Board of Trustees who are also its directors. The Board of Trustees is made up of independent members and nominated trustees from each of our joint venture partners. Douglas Gurr has been appointed as the new Chair of the Board of Trustees.
The Institute Director and Chief Executive is Professor Sir Adrian Smith. Research at the Institute is delivered and supported by the Chief Scientist, CCO, COO, and Programme Directors.
The Institute’s Research and Innovation Advisory Committee brings together the Institute Director, the Turing University Leads and the Institute’s Programme Directors to advise the Institute on its research and academic agenda. It is chaired by Professor Katharine Robson Brown.
The committee consists of the following four working groups:

The Institute will soon be opening an international call for new members of the Scientific Advisory Board, and will send out further communications in due course.
A team of Programme Directors oversee research programmes undertaken at the Institute with our strategic partners.
Representatives from the strategic partners are regularly convened in a Strategic Partners Board. The role of this Board is to advise the Board of Trustees on the content and translation of the research generated by the Turing, and work with the Institute and across the strategic partners to identify new opportunities for collaborative research and translation opportunities.
The Alan Turing Institute has an honorary fellowship scheme designed to recognise individuals who have substantially supported the Institute and its goals.
Anyone involved in the Turing community can nominate an Honorary Fellow. The Board of Trustees will review nominations, and expects to appoint a small number of Honorary Fellows each year.
Read our criteria for honorary fellowships and download the form to nominate an Honorary Fellow.
The long-term interests of the Institute’s University Partners are represented by the University Partners Board, which advises the Institute on strategy in the context of the UK higher education landscape:
Read more about our policies at the Institute
"
https://www.turing.ac.uk/research/research-areas/privacy-trust,"
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/information-theory-applied-mathematics,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/london-school-hygiene-tropical-medicine,"
www.lshtm.ac.uk
"
https://www.turing.ac.uk/people/researchers/chris-dent,"
Dr. Chris Dent is Reader (Associate Professor) in Industrial Mathematics in the School of Mathematics at the University of Edinburgh and held research and academic positions at Heriot-Watt, Marburg, Edinburgh and Durham Universities. He holds an MA in Mathematics (Cambridge University), PhD in Theoretical Physics (Loughborough University), and MSc in Operational Research (Edinburgh University). Since 2007 he has worked full time in energy systems analysis, and currently concentrates on security of supply risk analysis, and use of large scale computer models in decision making. He is a Senior Member of the IEEE, a Fellow of the Operational Research Society, and a Chartered Engineer.
Energy systems are growing in complexity and there is a need to manage uncertainty on a range of timescales (from short run uncertainty in renewable generation output, to uncertainty in planning background when taking major policy and capital planning decisions). Dr. Dent is broadly interested in the application of data science across energy system and planning, and has a particular interest in how approaches from data science can productively be taken to practical application in government and industry.
"
https://www.turing.ac.uk/people/guest-speakers/gabriel-weymouth,"
Dr. Gabriel Weymouth is the Professor of Ship Hydrodynamics at Delft University of Technology and Co-Lead of the Marine and Maritime Data-Centric Engineering Group at the Alan Turing Institute in London. He has held positions at University of Southampton, the Singapore MIT Alliance for Research and Technology, and MIT. 
Gabriel's research spans computational methods for ship hydromechanics, biologically inspired engineering and physics-informed machine learning. He develops world-leading methodologies for computational fluid mechanics and consults for leading software developers in that field, including in the reliable integration of machine learning into mechanics software. A list of open source research software and educational projects can be found at his website http://weymouth.github.io/. Gabriel and his students have received research and teaching awards, as well as a Guinness Book World Record in underwater robotics. 
"
https://www.turing.ac.uk/people/external-researchers/anna-noel-storr,"
Anna Noel-Storr has worked for Cochrane since 2008 as an Information Specialist for the Cochrane Dementia and Cognitive Improvement Group based at the University of Oxford. During that time she has played a leading role in the development and implementation of crowdsourcing in health evidence production. She currently leads an initiative called Cochrane Crowd. This work involves the development of a crowd platform offering willing contributors a range of micro-tasks to dive into, all of which are designed to enhance Cochrane’s content and speed up the review production process without any compromise on quality.
"
https://www.turing.ac.uk/events/launching-ai-standards-hub,"
Header image: Nacho Kamenov & Humans in the Loop / Better Images of AI / Data annotators discussing the correct labeling of a dataset / CC-BY 4.0
UPDATE: The AI Standards Hub website is live and can be accessed here: www.aistandardshub.org.
----
The Alan Turing Institute is pleased to host the official launch of the AI Standards Hub – the UK’s new initiative dedicated to knowledge sharing, community building, strategic research, and international engagement around standardisation for AI technologies.
The initiative is led by the Alan Turing Institute in partnership with the British Standards Institution (BSI), the National Physical Laboratory (NPL) and supported by the Department for Digital, Culture, Media and Sport (DCMS) and the Office for AI. 
We invite you to participate in the event online.

 
The event will take place on Wednesday 12 October, 11:00 - 15:45 and will be a hybrid event, with open online participation with Q&A. 
The sessions will feature presentations and interactive discussions with senior government representatives, the Hub’s partner organisations, and key stakeholders.
Session 1: Official launch of the Hub
Opening remarks
Ministerial remarks, DCMS
Damian Collins, MP, Minister for Tech and the Digital Economy, Department for Digital, Culture, Media & Sport
The AI Standards Hub: motivation, objectives and strategy
Introduction to the Hub's interactive online platform 
Panel 1: Stakeholder perspectives on AI standardisation
Session 2: Thematic kick-off: Standards for trustworthy AI
Governing AI effectively and the role of technical standards: a pro-innovation approach
Introduction to the Hub's work programme on trustworthy AI
Panel 2: Developing standards for trustworthy AI - priorities, challenges and opportunities
Round-up
Florian Ostmann
Livestream ends
Joining instructions for the live stream will be sent to registered attendees only via their registered email address the day before the event - on 11th October.
Register now to secure your place and be sent the joining instructions.
 
Live stream participation
"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-ai-clinical-trials,"
Photo by Testalize.me on Unsplash
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of AI in Clinical Trials. We will explore how AI and machine learning methods can improve the efficiency of various aspects of a clinical trial such as the design, operations and analyses.
We'll be hearing from Nandini Chitale and Michael DAndrea, Data Scientists at Roche (Genentech) and Louise Coutts, Senior Research Associate at The Alan Turing Institute.
Nandini and Michael will be discussing recent work on evaluating machine learning for forecasting key clinical trial operational metrics. They will share how data-driven forecasts based on trial features such as procedures, eligibility criteria, and endpoints can be potentially useful for trial design and planning, which in turn can improve key outcomes related to patient recruitment and trial duration. 
Louise will be discussing the collaboration between the Turing and the MRC Clinical Trials Unit, where they are exploring the impact of statistical machine learning on the design, conduct and analysis of randomised clinical trials.
As well as having a Q&A at the event, we also hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace which you can join.
You can watch a recording of this event here. 

"
https://www.turing.ac.uk/research/research-projects/turing-data-stories,"
A Turing Data Story is an interactive mix of narrative, code, and visuals that derives insight from real world open data. They are written as pedagogic Jupyter notebooks that aim to spark curiosity and motivate more people to play with data.
Our project lives in Github in this repo.
The stories are published online and you can check them out here.
 
Our aim is to help people understand the data driven world around us. We want to inspire an open community around a central platform. One that encourages people to harness the potential of open data by creating 'data stories'.
These 'data stories' mix computer code, narrative, visuals and real world data to document an insightful result. They should be educational and relate to society in a way that people care about. They must maintain a high standard of openness and reproducibility and be approved by the community in a peer review process. The stories will develop data literacy and critical thinking in the general readership.
This project is ongoing and everyone is encouraged to help us build something that will be useful to many. There are several ways you can help:Story ideas: Have an idea for an interesting story that could be told if you had the data, or knew how to analyse it? We can help.
Data: Stumbled across an interesting dataset, or perhaps mashed together several sources of data yourself? We want to hear about it.
Code: Are you an expert in Bayesian analysis? Do you have good data visualisation skills? Put that knowledge to work!
Peer review: Know a bit about data analysis? Good at communicating that knowledge? Interested in learning about how it can be applied to understanding society? We need reviews to make sure our stories are the best they can be.
Communication: Are you an amazing writer? Help us tell better stories.
Community: Don't fit in any of the above categories, but still want to hang out and be involved? We've got you, drop us a line.
Camila Rangel Smith[email protected]
"
https://www.turing.ac.uk/people/guest-speakers/mirella-lapata-0,"
Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh. Her research focuses on getting computers to understand, reason with, and generate natural language. She is the first recipient (2009) of the British Computer Society and Information Retrieval Specialist Group (BCS/IRSG) Karen Sparck Jones award and a Fellow of the Royal Society of Edinburgh, the ACL, and Academia Europaea.
Mirella has also received best paper awards in leading NLP conferences and has served on the editorial boards of the Journal of Artificial Intelligence Research, the Transactions of the ACL, and Computational Linguistics. She was president of SIGDAT (the group that organizes EMNLP) in 2018. She has been awarded an ERC consolidator grant, a Royal Society Wolfson Research Merit Award, and a UKRI Turing AI World-Leading Researcher Fellowship.
"
https://www.turing.ac.uk/people/ferran-gonzalez-hernandez,"
Ferran is an Enrichment PhD student working between The Alan Turing Institute and the CoMPLEX centre at University College London (UCL). His PhD research involves text mining and natural language processing (NLP) for biomedical information extraction. Previously, he obtained a BEng in Biosystems Engineering from the Universitat Politècnica de Catalunya where he joined the BIOCOMSC group and carried out research on agent-based modelling of anti-tuberculosis drugs. Following his graduation, he moved to London to pursue an MRes in computational biology at the CoMPLEX centre at UCL. His master's research ranged from molecular dynamics simulations of cell vesicles to machine learning approaches in cognitive neuroscience.
Outside his research, Ferran is highly involved in mentoring and teaching activities and has a keen interest in effective communication and education practices. Ferran is an avid chess player and has participated in numerous national and international competitions. In 2019, he won the blitz British Universities Chess Champion. As well as playing competitively, he currently coaches junior players ranging from beginner to advanced levels.
Ferran's current research focuses on centralising and standardising pharmacometric evidence found across the scientific literature by combining rule-based and machine learning approaches. His research is undertaken with support from BenevolentAI, and mainly involves document and sentence classification, biomedical named-entity recognition, entity linking and relation extraction from unstructured (full-text) and semi-structured (tables) sources. Ferran is particularly interested in efficiently dealing with the scarcity of annotated examples in the biomedical domain by applying techniques such as transfer learning, distant supervision and active learning.
 
"
https://www.turing.ac.uk/people/external-researchers/romin-pajouheshnia,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/newcastle-university,"
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/data-science-africa,"
datascienceafrica.org
"
https://www.turing.ac.uk/people/guest-speakers/maria-paz-hermosilla,"
Maria Paz Hermosilla is currently the Director of GobLab UAI – the public innovation lab at the Adolfo Ibanez University’s School of Government – whose mission is to help transform the public sector through data science. She has a Master of Public Administration from New York University. She is the Academic director of a certificate program in Data Science and Public Policy and researcher and professor of Data Ethics for graduate programs in the School of Engineering, School of Business and School of Government. She is leading several research projects related to the use of data science to improve public policy, including predictive models in the social sector, the integration of data ethics into the public procurement process, and algorithmic transparency. She was appointed by the Chilean Ministry of Science in the expert advisory committee to create the national strategy for Artificial Intelligence, and in the national committee for Public Interest Data. Before her current appointments, she was the Head of the Citizen Relations and Information Management Unit of the Chilean Ministry of Public Works, leading the redesign of citizen services and open data initiatives.
"
https://www.turing.ac.uk/people/guest-speakers/emily-m-bender,"
Emily M. Bender is an American linguist who works on multilingual grammar engineering, technology for endangered language documentation, computational semantics, and methodologies for supporting consideration of impacts language technology in NLP research, development, and education.  She is the Howard and Frances Nostrand Endowed Professor of Linguistics at the University of Washington.
Her work includes the LinGO Grammar Matrix, an open-source starter kit for the development of broad-coverage precision HPSG grammars; data statements for natural language processing, a set of practices for documenting essential information about the characteristics of datasets; and two books which make key linguistic principles accessible to NLP practitioners: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax (2013) and Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics (2019, with Alex Lascarides).
"
https://www.turing.ac.uk/contact-us/press-office,"

As the national institute for data science and artificial intelligence (AI), the Turing is a hub for over 400 national and international experts focusing on how data and AI shape and impact our lives today.   
We work in partnership with universities, industry, and public and third sector organisations, gathering insights and speaking with a unique and independent voice. We bring an interdisciplinary perspective to some of the most pressing challenges in science, society and the economy that require cross-boundary thinking. 
Our strengths lie in marrying technical expertise with an understanding of the wider ethical, legal and policy landscape.  
If you’re a journalist looking for a background briefing, commentary, an interview or to be added to our distribution list please contact [email protected]. Please note that contacting our press office is the most efficient way to book a spokesperson.
Fiona Dennehy (Senior Media Relations Manager), [email protected] and (0)20 3862 3598 
Mark Burey (Head of Communications and Marketing), [email protected] and (0)79 4770 6934 
Joanna Dungate (External Relations Officer), [email protected] and (0)20 3862 3541 
For general enquiries, please visit our Contact us page, use our contact form or call our main phone line on +44 (0)20 3862 3352. 
For more about The Alan Turing Institute, visit our about us page. Follow the Turing on Twitter and sign up to our monthly newsletter to keep up-to-date with our work. 
We provide commentary on a range of topics, including but not limited to:
And we apply our research to many real-life challenges and application areas affecting people today, with some examples being:
To find an expert, use our search function. You can also find people, events and projects by research area. 
Our research has been featured in publications such as Nature, Wired, the Financial Times, MIT Technology Review, and New Scientist and on programmes such as BBC local radio and Woman’s Hour, Talking Machines, Guardian science podcast, and the Economist’s Babbage podcast. 
Following the release of ChatGPT at the end of 2022, Turing researchers have taken part in dozens of media and speaking engagements to support understanding around these rapidly emerging technologies. Early interviews in the Telegraph, the Guardian on BBC Radio 5Live and Tonight with Andrew Marr on LBC set the scene for how this technology could work. 
Since then, Professor Mike Wooldridge, Director of Foundational AI Research, has been interviewed extensively. Highlights include a piece in The Times and BBC Futures and multiple radio interviews for the BBC World Service and BBC Radio 4.
Professor Wooldridge has also been interviewed on Britain’s computing power for articles in The Telegraph, The Times and Tech Monitor. Additionally, he provided comment in The Telegraph and The Times to the news that Britain needs its own ‘sovereign’ AI.
Dr Mhairi Aitken has also been interviewed by many media outlets including the Daily Mail, the Today Programme and Woman’s Hour on BBC Radio 4, New Scientist, Cosmopolitan, and BBC Two about other uses for this technology and the competition from Google’s rival chatbot, Bard which launched in February 2023.
Other recent and notable coverage includes the Turing’s Drew Hemment on BBC Radio 4’s Moral Maze, Mike Wooldridge featuring on the 30-minute Radio 4 programme, The Briefing Room, an interview with Mhairi on LBC about the impact of AI on jobs and on BBC Radio 4’s Today programme in April discussing the importance of regulating against existing AI risks, and Dr Adrian Weller on BBC PM’s programme and an interview on Bloomberg about prompt engineering.  
For a full round up of coverage, read our article.
"
https://www.turing.ac.uk/events/webinar-edge-computing-earth-observation,"
89 KB
After careful consideration, we have decided to host Edge computing for Earth observation workshop online only through Zoom video conferencing. You will not be able to attend in person at the Turing. 
Given public health concerns about the coronavirus we trust that you will understand we wish to prioritise the health and safety of the Turing community and our wider network. 
As the number of satellite launches increases and UAV technology continues to mature, the volume of data for Earth observation is growing at an ever-increasing rate. In order to process and analyse these massive volumes of data, advances have been made in remote sensing platforms and distributed processing. Synthetic Aperture Radar (SAR) forms the basis of Earth observation techniques due to its ability to sense at long range, through cloud cover, by day or by night, with the ability to generate 3D structure observations.
The drive towards multiple distributed, networked RF sensing nodes and automated manning of platforms with reduced human in the loop is pushing research towards developing solutions that can process data in near real time. Technologies enabling distributed processing with low power requirements, and suitable for bandwidth-limited and remote environments, are improving rapidly, particularly those based on FPGAs.
This workshop will look at the advances and challenges in onboard processing, machine learning and artificial intelligence for Earth observation.
 
This one-day interactive workshop aims to bring together a group of Earth scientists with interesting problem sets, data scientists with knowledge of machine learning algorithms and experts in computer architectures for power and bandwidth constrained environments.
The event will enable collaboration across related domains, accelerate our understanding of capabilities and limitations, and bring new functionality to bear on the continually evolving domain of edge computing for Earth observation. 
The event will be split into 5 key themes:
Draft agenda for ""Edge computing for Earth observation""
Abstracts for ""Edge computing for Earth observation""
Monitoring the resilience of patterned vegetation in the Sahel using Google Earth Engine, Nick Barlow and Camila Rangel Smith (The Alan Turing Institute)
Iterative principal component analysis in dual polarisation SAR images, A Faul and A Fleming (British Antarctic Survey)
Limited spaces are available for this event. Apply to attend is now closed.

"
https://www.turing.ac.uk/people/turing-liaison,"

Each university in the Turing University Network nominates a Turing liaison as the primary contact person at the university. Each university is also invited to nominate a Turing liaison (academic) as the primary research link between the Institute and the university.  
The primary contact link between the Institute and Turing University Network universities 
 
Cardiff University 
Niki Price 
City, University of London 
Claudia Kalay 
Durham University 
Dr Alan Real
Goldsmiths, University of London 
Emma Price 
Imperial College London 
Alex Gibbs 
Keele University 
Julie Street
King’s College London 
Dr Rose Hepworth
London School of Hygiene & Tropical Medicine 
[email protected] 
LSE 
Jill Beattie
Newcastle University  
Angela Craggs 
Northumbria University 
Lindsay Connolly 
Nottingham Trent University 
Padraig Mulcahy 
Queen Mary University of London 
Emma Grant  
Queen’s University Belfast 
Dr Tom Gibson 
Royal Holloway University of London  
Dr Marcela Acuna-Rivera 
Royal Veterinary College  
Dr Emma Tomlinson 
Teesside University 
Kirsty Metcalfe 
UCL  
Matt Davis 
University of Birmingham  
Jessica Mylchreest
University of Bristol 
John Newby 
University of Cambridge 
Arti Sheth Thorne 
University of Edinburgh 
Neil McGillivray 
University of Exeter 
Dr Emily Paremain 
University of Glasgow 
Neil Findlay
University of Leeds 
Kimberley Wright 
University of Liverpool 
Anthony Mtitimila 
University of Manchester 
Sarah Barton
University of Nottingham  
Dr David Hawker 
University of Oxford 
Kathleen Dolan 
University of Plymouth 
Susan Eick 
University of Reading  
Dr Bonhi Bhattacharya 
University of Sheffield 
Emma Barker 
University of Southampton 
Susan Davies
University of Strathclyde 
Elizabeth Ross 
University of Surrey 
Dr Andrew Rogoyski
University of Warwick 
Rebecca Silver
 
"
https://www.turing.ac.uk/news/turing-podcast,"

The Turing Podcast is your home for discussion on all things data science, AI and machine learning.
Now into our third series, hear directly from the researchers who are pushing boundaries in these areas, using data science and AI to tackle issues that are relevant to us all.
The podcast is produced by Luca Lane. Current hosts are Anneca York, Ed Chalstrey, Jo Dungate and Bea Costa Gomes. Past hosts include Christina Last, Effie Dennis, Tarek Allam, Ben Walden and Rachel Winstanley.
Music for the main podcast is written by Jamin Sun. Find him on Instagram or Bandcamp.
Music for the Coffee Pod podcast is written by Spiders Eat Vinyl. Listen to them on Spotify or Soundcloud.
Listen and subscribe via: Spotify, Apple Podcasts, Stitcher, Podbay, Podbean, iHeart Radio, Listen Notes and more.

Join Bea and Anneca as they discuss the potential and current AI tools being used around treating and diagnosing illnesses such as dementia, with Zoe Kourtzi.
Zoe is Professor of Computational Cognitive Neuroscience at the University of Cambridge. Her research aims to develop predictive models of neurodegenerative disease and mental health with translational impact in early diagnosis and personalised interventions.
In this week’s Turing Podcast, Ed chats with Tim Harford about the themes from his 2020 book “How to Make the World Add Up”, which include thinking about the ways statistics and numbers are (mis)used in the media and what we can do about it. Tim is an economist and presenter of the BBC radio show “More or less”.
Welcome to the Coffee Pod! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee.
In this episode, Bea is speaking to Hussein Rappel. Hussein has worked on the PDRA on Digital Twins in Infrastructure and Construction project in the Complex Systems Engineering theme.
Welcome to another Coffee Pod episode! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee.
Today Bea is speaking to Malvika Sharan, a senior researcher. Malvika works on Open Research in the Tools, Practices and Systems theme.
Welcome to another Coffee Pod episode! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee.
In this episode, Bea is joined with Fernando Benitez, a research associate. Fernando worked on PDRA - the Spatial Modelling project in the Shocks and the Resilience cross-theme project. 
Welcome to another episode of the Coffee Pod Podcast! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee. 
In this episode, Bea catches up with Tom Andersson, a data scientist.
Tom's worked on the Turing/British Antarctic Survey on Environmental models: Bridging the spatial scales, from surface sensors to satellite sensors in the Environment and Sustainability cross-theme project.
Welcome to another episode of the Coffee Pod podcast! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee. 
Joining Bea this week is Domenic DiFrancesco, a Turing Research Fellow.
Domenic's worked on the project PDRA Digital Twins for Fleets and Supply Chain Management in the Ecosystems of Digital Twins cross-theme.
Following Tom Mustill’s popular Turing Lecture at the Royal Institution, How to Speak Whale, Tom joins Bea and Jo to catch up on the latest advancements in communication between humans and animals.
This lively edition of the Turing podcast covers a great variety of subjects, including some of Tom’s favourite (and surprising) whale facts, and whether attempting to chat with aliens should be prioritised over speaking with animals.
A biologist and filmmaker, Tom has recently also turned writer. His debut book, How To Speak Whale: A Voyage into the Future of Animal Communication, was selected as one of Amazon's Best Books of The Year. 
Welcome to the first Coffee Pod episode! A short series about the Turing's AI for Science and Government fund, that gives you the chance to hear about the people behind the project. Each episode you’ll be listening to a different colleague, their background, career and most importantly… how they like their coffee. 
Joining podcast host Bea this week is Ruoyun Hui, a postdoctoral research associate at The Alan Turing Institute. 
This series is hosted by Bea Costa Gomes and produced by Luca Lane. The music has been produced by Spiders Eat Vinyl.
With Dr Adrian Weller (Programme Director and Turing Fellow) and Kate Platonova (Group Chief Data Analytics Officer at HSBC), Ed Chalstrey discusses how AI is being used in financial services and what data is useful in banking today.
Join Ed and David as they speak to Ellen Pasternack, a PHD student in evolutionary biology at the University of Oxford. 
Ellen is a science writer for UnHerd and Works in Progress. In this episode, we’re going to chat about one of her most recent articles, ""The Stats Gap"", which explores the issues with statistical education for university scientists.
Join Aoife and Sally as they chat to Dr Tamsin Edwards about how she uses AI to predict rising sea levels, following her Turing Lecture at the Royal Institution.
Tamsin is a climate scientist, specialising in the uncertainties of climate model predictions, particularly for ice sheets and glaciers. 
Within her research, she also uses information about past climates to improve predictions for the future.
In this podcast, we will be catching up with her as she answers some of the questions that the audience submitted at the Turing Lecture which did not get asked.
Watch Tamsin's full Turing lecture here: https://youtu.be/CbEKI_LfNWA
Join Aoife and Torty Sivill, Doctoral Student, University of Bristol as they chat to Professor Sarah Sharples about the future and current state of the technology and AI around driverless vehicles. Sarah, chief scientific advisor for the department for transport recently did a Turing lecture at the Royal Institution, discussing the topic. In this podcast, she’ll be answering a range of questions that weren’t answered at the Turing Lecture.
This week we are joined by Manchester United women's footballer Aoife Mannion, Author and CEO of Glitch Seyi Akiwowo and Turing Researcher Pica Johansson to discuss online abuse suffered by football players and other athletes online. The Turing recently partnered with OfCom, who comissioned a report in relation to its upcoming role as the UK’s Online Safety regulator tracking abuse on Twitter against football players in the 2021-22 Premier League Season. 
In this episode, hosts Bea and Anneca are joined by Robert Blackwell, from CEFAS (Centre for Environment, Fisheries and Aquaculture Science), who tells us, among many other things, how an algorithm to detect different species of plankton developed in two weeks during a Turing DSG ended up being deployed six months later on a ship.
This week we welcome Dr Miguel Arana-Catania and Professor Rob Proctor from the University of Warwick, along with Dr Felix-Anselm van Lier from Oxford University. The episode discusses their recent work in using machine learning to analyze large-scale peace dialogue transcripts from the war in Yemen, with the aim to assist conflict mediators.
This week the hosts are joined by David Beavan, a Senior Research Software Engineer and Dr Kasra Hosseini a Research Data Scientist, both of whom work in the Alan Turing Institute’s Research Engineering Group. The episode focusses on one of The Alan Turing Institute’s major research projects in the Digital Humanities known as “Living with machines”, which takes a fresh look at the history of the industrial revolution with data driven approaches. Find out more at https://livingwithmachines.ac.uk/
In this episode Christina catches up with two of her former collaborators, Prithviraj Pramanik and Dr Subhabrata Majumdar. The three of them worked as volunteers at Solve for Good (a platform to connect social good organisations with volunteer data scientists to solve socially beneficial challenges). The team discusses their work with UNICEF to build a post-pandemic global air pollution model to help map child exposure to harmful air pollutants.
This week Ed and Rachel speak with Geoff Goodell, Senior Research Associate in the Financial Computing and Analytics group at UCL, and associate of UCL’s Centre for Blockchain Technologies and the LSE systemic risk centre. Geoff is an advocate for privacy as a human right in the digital world, in particular with regard to digital identity systems. Recorded in mid-2021, this episode takes a deep dive into some of the important topics surrounding people’s identities in the online world, including both the technical and ethical considerations.
Ed and Rachel are joined by Dr Tim Hobson, Senior Research Software Engineer and resident Bitcoin enthusiast at The Alan Turing Institute. Tim offers his take on the phenomenon that is Bitcoin, the future of its adoption and how the underlying technology relates to his research interests.
The latest episode of The Turing Podcast features a special roundtable discussion with our strategic partner Accenture about career options in the data science sector. Our hosts Jo Dungate and Bea Costa Gomes were joined by three influential figures in AI and data science – Henrietta Ridley (Data Science Manager at Accenture), Alice Aspinall (Senior Manager at Mudano), and Kirstie Whitaker (the Turing’s Director of the tools, practices and systems programme).
Our guests brought their different experiences and perspectives to an insightful discussion on various aspects of the data science industry, from how they first got into their fields, their career motivations and lessons learned along the way. The episode concludes with each guest offering advice to anyone at the beginning of their career.
This week on The Turing Podcast, the hosts chat with Dr James Geddes, who is a Principial Research Data Scientist in the Research Engineering Group at The Alan Turing Institute. The discussion revolves around an all-important question: What actually is AI? James breaks down three categories of computer programs that could be considered AI: simulations, symbolic AI and machine learning, and the hosts debate which, if any of these, are really intelligent. This week the podcast is hosted by Ed Chalstrey and introduces Christina Last.
In this episode we talk to Dr Nira Chamberlain, president of The Institute of Mathematics and its Applications. We talk with Nira about Black History Month, mathematicians though history that have inspired him, and how mathematics can cross racial, geographical and cultural boundaries.
The hosts were joined by Dr Nicol Turner Lee to discuss her research on public policy, designed to enable equitable access to technology and digital equity. We talk about themes in her recent book on the Digitally Invisible and the real-life consequences of the growing digital divide. Nicol Turner Lee is a speaker, author and technology innovator. As well as a senior fellow in Governance Studies and Director of the Centre for Technology Innovation at the Brookings Institution in Washington D.C., where she also serves as Co-Editor in Chief of the blog, Tech-Tank.
We chat about all things science communication with two Turing colleagues: Ethics Research Fellow Mhairi Aitken and Science Writer James Lloyd. They discuss why we need science communicators in the first place, what makes for good communication, and what specific challenges are associated with communicating data science and AI research to the general public.
This week on the podcast, we bring you a conversation the hosts had last December with PhD candidate Elizabeth Seger. Elizabeth studies at the University of Cambridge and is a research assistant at the Leverhulme Centre for the Future of Intelligence. Talking about her work with The Alan Turing Institute, she explains how informed decision making in democracies is being impacted by modern technology, and in particular how online misinformation has affected the pandemic response. Find out more about the research.
In this episode hosts Jo Dungate and Rachel Winstanley speak to Andrew Holding, a Senior Research Associate at Cancer Research UK's (CRUK) Cambridge Institute and Turing Fellow. Andrew discusses how his research is using machine learning to understand the biology that underlies breast cancer to help improve treatments.
The hosts chat with Professor Robert Foley, who works on Human Evolution at the University of Cambridge and is a Turing Fellow. The conversation takes a broad view of how our understanding of human evolution has changed in recent decades and focuses in on the Turing's Palaeoanalytics project, which involves applying data science and machine learning methods to non-genomic data. Find out more about this project.
AI is widely lauded as a way of reducing the burden on human online content moderators. However, to understand whether AI could, and should, replace human moderators, we need to understand its strengths and limitations. In this episode our hosts speak to the researchers Paul Röttger and Bertie Vidgen to discuss how they are attempting to tackle online hate speech, in particular through their work on HateCheck – a suite of tests for hate speech detection models.
In an interview recorded last year, Jo and Ed are joined by Dr Omar A Guerrero, an Economist and Computational Social Scientist at The Alan Turing Institute and UCL Department of Economics, whose research is on economic behaviour and institutions from an interdisciplinary angle. The episode focuses on Policy Priority Inference (PPI): a technology developed in collaboration with the United Nations Development Programme. PPI is intended to be used to optimise government policy to meet sustainable development goals and identify the policy priorities that governments need to set if they are to adopt a specific development strategy. Read more about the research discussed in this episode.
This week on the podcast, the hosts are joined by Sören Mindermann and Mrinank Sharma, who are PhD students at the University of Oxford. Mrinank works as part of Oxford's Future of Humanity Institute, whilst Sören is a member of Oxford Applied and Theoretical Machine Learning Group. The episode focuses on the research they have recently published on inferring the effectiveness of government interventions against COVID-19, during the first wave of the pandemic in early 2020. You can read the research article for this work. 
In this episode the hosts were joined by Professor Sue Black to discuss her inspirational life story and career, as well as the initiatives she has set up to encourage more women into the tech sector and her hopes for the future. Sue Black – Professor of Computer Science and Technology Evangelist at Durham University – has set up initiatives such as BCSWomen and the social enterprise Techmums, to encourage more women into computing. She has received an OBE for services to technology and was also instrumental in the campaign to save Bletchley Park.
This week the hosts chat with Dr Dan Stowell, senior researcher at Queen Mary University of London and fellow of The Alan Turing Institute, about his work on addressing climate change via creating high-coverage open dataset of solar photovoltaic installations in the UK. It also happens to be research that podcast host Ed was involved in as you'll hear! You can check out the paper on this topic, published in Nature Scientific Data here.
On this episode of the podcast, we are joined by Lord Robert Winston to talk about engaging with the public about the science of combatting COVID-19. Professor of Science and Society and Emeritus Professor of Fertility Studies at Imperial College London, Robert has also had an incredible career in television, presenting the BBC’s The Secret Life of Twins, Child of Our Time and the BAFTA award-winning The Human Body. Professor Winston runs a research programme at the Institute of Reproductive and Developmental Biology at Imperial College that aims to improve human transplantation. He has over 300 scientific publications about human reproduction and the early stages of pregnancy.
This week the hosts are joined by Professor Tim Hubbard, who is Head of the Department of Medical and Molecular Genetics at King’s College London, and Associate Director of Health Data Research UK in London, as well as being the Head of Genome Analysis at Genomics England. They discuss the recent success of Deep Mind's AlphaFold protein structure prediction software at the CASP14 competition and other developments from the worlds of health data science and genomics.
On the first episode of season two, we are joined by Alexander Tokarev, a very recent PhD graduate from the University of Manchester. Alex does research in organisational psychology, personality psychology, and psychometrics. With a strong mathematical and statistical background, he applies these to psychology. He is here to tell us a little bit about modeling personality traits, in particular the ones known as the dark core. Sound effects courtesy of Brand Name Audio.
Ever wondered what you were signing up to when you click the 'accept all cookies' button that seems to appear on every website you visit? In the final episode of The Turing Podcast series one, the hosts are joined by Dr Rebecca Rumbul to talk about The Privacy Collective, an organisation that supports compensation claims arising out of the misuse of personal data on behalf of the general public, and how they're involved in with the largest data privacy case against GDPR breaches in history. To learn more, check out their website.
This week on The Turing Podcast, the hosts chat with James Walsh, a research assistant at The Alan Turing Institute, and Funmi Kesa, a PhD student at the University of Warwick, and hear about their work on “Project Odysseus”, one of The Alan Turing Institute’s key research projects in response to the COVID-19 Pandemic. By capturing activity in London to better understand 'busyness', the research aims to aid effective policy-making strategies for exiting lockdowns.
The ability to reproduce the research that other scientists have done to see whether the same results are obtained (or the same conclusions are reached) is an integral part of the scientific process, but are we doing it right and how difficult is it to do? This week, Ed is joined by Dr Kirstie Whittaker and Dr Sarah Gibson for a discussion about the reproducibility of scientific research, why this is such an important topic and what The Alan Turing Institute is doing to promote best practices in reproducible data science. Kirstie is the Programme Lead for Tools, Practices and Systems at The Alan Turing Institute and Sarah is a Research Software Engineer at the Institute who is also a fellow of the Software Sustainability Institute. Check out some of the projects mentioned in the interview such as The Turing Way at https://the-turing-way.netlify.app/ and Binder at https://mybinder.org/
Today, September 16th, is International Identity Day! To mark the occasion, the hosts are joined by Carsten Maple, who is Professor at the University of Warwick and Fellow of The Alan Turing Institute, for a conversation about the trustworthiness of digital identity systems, some of the related work going on at the institute and other “open” initiatives in the digital identity research space. Identification has come a long way since the 1990s when paper-based registries and documents, such as hard copy passports, were the only option for fully verifying a person’s identity. The last decade has seen the proliferation of digital identity systems, both national and commercial, meaning that identities can be verified and checked securely, cheaply and at scale. Will digital identity systems will achieve fair and just outcomes for citizens? That remains an open question!
This week on The Turing Podcast we're joined for a second time by Alan Turing Institute fellow Dr Peter Tennant of the Leeds Institute for Data Analytics, for a discussion about the scientific communities' response to the COVID-19 pandemic. Peter is an expert not only in data science, but also Epidemiology and causal inference. The discussion took place in June when fewer of the UK’s lockdown restrictions had been lifted.
This week The Turing Podcast welcomes our second external guest interviewee: Tom Chivers. Tom is a science writer and journalist who has previously worked for the Daily Telegraph and Buzzfeed UK, but now writes for the online publication UnHerd. His writing often focusses on topics such as rationalism and Artificial Intelligence and he has authored a popular book The AI Does Not Hate You: Superintelligence, Rationality and the Race to Save the World. In this episode, the discussion revolves around the media coverage of the COVID-19 pandemic, Tom's experience as data-savvy journalist and the challenges of accurately reporting on rapidly evolving science, at a time when public hunger for information is high and misinformation abounds! You can keep up with Tom’s writing here, or follow him on Twitter.
On the podcast this week, the hosts chat with Dr Chris Hicks and Dr David Butler, who work as post-doctoral researchers in security and cryptography at The Alan Turing Institute. In an episode that focusses on one of the projects the institute has undertaken to help tackle the pandemic, they discuss how to build a privacy-preserving system for issuing and verifying COVID-19 antibody certificates, a technology that could be used to help with the easing of pandemic measures in some scenarios. If you’d like to learn more about this research, check out their paper here.
In the last couple of decades, we've all been witness to the huge advances in personal computing and the astonishing rise of consumer technology. We now live on a planet that's more connected than ever before, with over 3.5 billion smartphone users, many of whom use social media on a daily basis. But where is consumer technology headed and what exciting developments are there on the horizon? In this episode of The Turing Podcast, the hosts speak to Daniel Rotar, CEO and Founder of ZONEofTECH: one of the largest UK based Technology YouTube channels, for a conversation that ranges from the latest developments in Augmented Reality (AR) technology and smartphone design to the intersection of AI research and consumer tech. You can watch and subscribe to ZONEofTECH here.
In this week's podcast, the hosts speak with Dr Eric Daub, a Principal Research Data Scientist at The Alan Turing Institute, about his involvement in measuring, monitoring, and analysing the performance of the world’s largest 3D printed metal structure: a 12 metre-long stainless steel bridge crossing one of Amsterdam's canals.
This week's episode takes us back the pre-COVID-19 era, when premier league football was still a thing! Dr Nick Barlow of The Alan Turing Institute's Research Engineering Group chats to the podcast hosts about his side project ""AIrsenal"", a machine learning manager for Fantasy Premier League. You can find out more about the project and even contribute to the code by visiting here and here.
Viruses like SARS-CoV-2, which causes COVID-19, aren't the only problem we face globally when it comes to infectious disease. Bacterial pathogens are constantly evolving, presenting a challenge for the worlds healthcare systems as more of the antibiotics we rely on to kill bacteria fail on those that have evolved antimicrobial resistance. In this episode, we chat with Victoria Carr, a PhD student in Bioinformatics at the Centre for Host-Microbiome Interactions at King's College London and The Alan Turing Institute, who describes her research comparing antimicrobial resistance in mouth and gut microbes and developing software to find ""mobile genetic elements"" (DNA sequences that can change position within a genome) and their association with antibiotic resistance genes.
Image recognition and classification is a hot topic in AI research, and these tools are increasingly being utilised by biologists with the aim to classify and distinguish diseases. In this episode, Bea Costa Gomes, a PhD candidate and Turing enrichment student at The University of Manchester talks about her research into developing software that spots shape differences in the images of diseased brain cells, as well as her passion for Drosophila flies!
Remember back at school when you were taught that correlation doesn’t mean causation, that increased ice cream sales are correlated with sunnier weather but don’t cause the clouds to part? Peter Tennant, a fellow of the Alan Turing Institute based at Leeds Institute for Data Analytics explains why it’s important for scientists to become more confident in talking about causation, how ""causal inference"" methods are transforming the field of epidemiology and why AI isn’t typically best placed to make sensible assumptions about complex data. This episode was recorded before the COVID-19 lockdown began in the UK, but the topics discussed couldn’t be more relevant!
Large datasets and modern machine learning techniques are fast changing the field of Astrophysics and our understanding of the universe. In this episode, we chat with PhD candidates Tarek Allam and Gordon Yip, whose respective research at University College London’s Department of Physics and Astronomy focuses on the classification of supernova light curves to help constrain theories of Dark Energy and the atmospheric composition of remote exo-planets.
How can groups of people be empowered to share sensitive personal data such as medical records, with transparency about how they will be used? This is a question of particular relevance during the COVID-19 pandemic. In this episode, we're talking to Sylvie Delacroix, Turing Fellow and Professor at Birmingham Law School about the concept of data trusts and how they enable the sharing and safeguarding of data. https://datatrusts.uk/
How can data science support us in tackling the COVID-19 pandemic? Our Researcher at Large, Jon Crowcroft, speaks about how smartphones can be mobilised to track the COVID-19 pandemic. He also discusses how data science and AI research is playing a crucial role in finding solutions during the crisis.
Recorded a few months ago, in our first episode we speak to Dr Radka Jersakova, Research Data Scientist at The Institute about her project that applies AI to air traffic control simulators. Join us as we discuss how advances in AI may go some way paving the way towards putting more planes (safely) in the sky. 
"
https://www.turing.ac.uk/about-us/impact/co-designing-computing,"
In a high-performance computing (HPC) environment, such as a data centre with hundreds or thousands of interconnected computers, well-designed algorithms and architectures allow huge data analysis tasks to be performed. For example, classifying millions of images of tissue samples to identify whether they contain anomalous features that should be examined by a doctor.
While these high-performance systems operate well for some computing needs, they often run at less than half their full capacity for many data science and machine learning tasks. Researchers at The Alan Turing Institute have been working in collaboration with Intel to co-design better architectures for their HPC systems. The collaboration has looked at how to improve communication between multiple machines that are sharing the workload of massive analyses, as well as how to rethink the formatting of the data used in HPC, to improve performance on data science and machine learning problems.
The output of the work is not only helping Intel improve their products and services, but also enabling data scientists to manage and analyse massive datasets with greater efficiency, in a range of machine learning applications.
The Institute established a relationship with Intel that built on existing links between Turing researchers and the technology giant. Peter Boyle, a Turing Fellow from the University of Edinburgh, explains: “I had been working with Intel on HPC workloads in Edinburgh for a while. The Turing’s engagement helped grow that relationship further, expanding the scope to include a broad spectrum of AI and machine learning workloads. It allowed Intel to place two of their engineers in Edinburgh to help work on these active co-design projects.”
Katrina Payne, Business Development Manager at the Turing, says, “The organisational relationship between Turing and Intel is about putting a framework in place that facilitates direct, personal working relationships between individuals in each organisation.”
“Co-design… is mutually beneficial. We get better science and Intel get a better product”
Peter Boyle, Turing Fellow
“The co-design process involves identifying the elements of computer architecture that are limiting performance,” Boyle continues, “then trying to ‘change the rules of the game’; seeing whether it makes engineering and economic sense to change the computer architecture. If it does, it’s mutually beneficial – we get better science out of the product and Intel get a better product to sell”
A key aspect of the work looked at how to make communication between machines in HPC environments more efficient. AI and machine learning problems often involve the use of multi-layered ‘neural networks’, which are trained to learn the mapping between inputs and outputs. Each individual ‘neuron’ or node in the network is given a set of parameters (or weightings) which are iteratively adjusted. These adjustments help to form smart ‘neural pathways’ that optimise the network’s ability to fulfil certain tasks, e.g. accurately translating a sentence from one language to another.
Despite recent advances in training methods, as well as in hardware and network architectures, training these neural networks with data can take an impractically long time on a single machine. Distributed training across multiple machines allows for significantly more efficient development of neural networks. “If you have a thousand devices training a neural net rather than just one, you can potentially turn a three-year job into a one-day job,” Boyle explains.
The most common form of distributed training is data parallelism, in which each machine gets a different portion of the input data, but a complete copy of the network, and then each machine’s results are subsequently combined. “Sometimes millions of weights parameterise a neural network, so combining results requires efficient network communication,” Boyle says.
Boyle and his collaborators at Intel started by taking an existing benchmark algorithm – published by the Chinese technology company Baidu – that aims to reduce the amount of time spent communicating between different cores in a computing network. They applied the algorithm to Intel’s HPC Omni-Path Architecture (OPA) and identified where the code wasn’t running efficiently. “With help from Intel engineers we managed to enable more cores to drive the network at the same time, improving bandwidth and resulting in a 10 times improvement in speed,” says Boyle. The code that this work produced is now shipped as standard in Intel products.
“As part of our work we considered: suppose we have the freedom to change the hardware to be whatever the heck we wanted,” says Boyle. “This led us to try different floating-point formats.” Floating-point formats are used to represent, with a fixed number of bits (binary digits), numbers of different orders of magnitude, made up of ‘mantissa’ and ‘exponent’ bits. For example, in the number 1.011 x 2101, the mantissa is 011 and the exponent is 101 (the first 1 is ignored as all numbers in standard floating-point format start 1.something, and the 2 shows we’re in base 2, otherwise known as binary).
In order to ensure accuracy when working with neural networks, a 32-bit format is often used, but Boyle and his colleagues explored whether they could use a 16-bit format instead. Karl Solchenbach, Director of Exascale Labs Europe at Intel, explains: “If you can do the same calculation with the same accuracy with 16 bits rather than the standard 32 bits, that’s great! It saves you half the memory, it makes the calculations much faster, and you can save silicon space in hardware.”
“[This work] saves you half the memory, makes calculations much faster…and saves silicon space”
Karl Solchenbach, Director Exascale Labs Europe at Intel
“We discovered that the standard 16-bit IEEE [Institute of Electrical and Electronics Engineers] floating-point format has a problem in that it only has 5 exponent bits and the range of data that can be represented with this is insufficient for a lot of machine learning problems,” says Boyle. “Using standard software libraries across multiple neural network benchmarks, we varied the number of mantissa and exponent bits to see the effects on performance.” They found that by changing the 16-bit format to 8 exponent bits and 7 mantissa bits, they were able to train neural networks that had previously failed to train with existing formats. A simple seeming solution with significant benefits.
One of the main takeaways from this work has been seeing the power of co-design and strong working relationships. Boyle says: “The relationship between scientists and engineers, like those at Intel, needs to be evidence-based, bottom-up and well-founded on an individual level.”
As well as the work described here, Intel has also been working with Turing Fellow Kenneth Heafield at the University of Edinburgh on training neural networks, and developing the related hardware, to be better at translating millions of words of online text.
“We’ve seen a positive impact on our architecture as a result of our work with the Turing”
Anil Rao, Vice-President, Data Center Group and General Manager of Data Center Security and System Architecture at Intel
On the future, Anil Rao, Vice-President, Data Center Group and General Manager of Data Center Security and System Architecture at Intel, says: “We’ve seen a positive impact on our architecture as a result of our work with the Turing and continue to work together to develop other similar successful projects.”

"
https://www.turing.ac.uk/events/turing-roche-knowledge-share-series-ai-precision-medicine,"
An event series for Turing-Roche partnership updates, knowledge sharing and new perspectives. Find out more about the series.
 
This event will be on the theme of AI in precision/personalised medicine. We will explore how AI's insights may help each patient's unique needs and empower clinician decision-making.
We'll be hearing from Dr Samaneh Abbasi, Senior Data Scientist and Data Science Network Co-Lead on the PHC Data analytics and Imaging (DAI) team at Roche and Professor Zoe Kourtzi, Professor of Cognitive Computational Neuroscience at the University of Cambridge and Turing University Lead.
 
Samaneh will be speaking about automatic detection of molecular subtypes as one of the prognostic factors in breast cancer directly from H&E images, while in clinics IHC staining is usually used to determine this factor.
 
Zoe will be discussing the EDoN initiative which brings together global experts in data science, digital technology and neurodegeneration to develop a digital tool for the early detection of diseases that cause dementia.
 
We also hope to have discussions about this area pre and post the event via our Turing-Roche Slack Workspace which you can join here.
You can watch a recording of this event here. 



"
https://www.turing.ac.uk/leap,"

[email protected]
"
https://www.turing.ac.uk/events/machine-learning-and-data-challenges-ransomware-and-cloud,"
Registration closed.
This workshop will follow the Chatham House Rule.
In 2019 data breaches are predicted to be the most prevalent form of attack against industry. Ransomware along with DDoS attacks, which attack or disrupt data flows, are close behind in the pecking order. Yet, on the other side, the digital data relating to the circumstances of those attacks is vital for identifying what went wrong and analysing how to improve systems so that similar problems do not occur again.
There is a massive challenge to get hold of such data, which would be used to learn how to make systems safer, due to: 
It is therefore not surprising that organisations are reluctant to participate in projects and share their experiences and any data that could be invaluable to the research mission.
This climate of distrust is unlikely to change without the establishment of common protocols and shared best practice for the anonymisation and safe sharing of data.
This collaborative workshop aims to work toward developing protocols and principles for data sharing in research. The workshop will address the following questions:

This event is intended to bootstrap a new data security interest group at the center of which is developing the principle of data sharing. We are therefore keen to encourage attendance from all of the key players and sectors in this area, including, but not limited to, organisations, cloud providers, and data scientists.
Registration from 09:30, event starts at 10:00.

"
https://www.turing.ac.uk/people/guest-speakers/natalie-banner,"
Dr Natalie Banner leads Understanding Patient Data (UPD), an initiative hosted at Wellcome to ensure uses of patient data for care and research are more visible, understandable and trustworthy. This involves creating accessible information resources about data use, commissioning and collating public attitudes and engagement research, and channelling these insights into policy and decision-making to ensure that diverse public views and values are embedded in data governance. UPD’s work includes exploring how to create trustworthy frameworks for developing new data-driven technologies in healthcare and research, focused on ensuring benefits back to patients and the health system.
Natalie formerly managed Wellcome’s Parliamentary advocacy on GDPR and data protection, leading the research sector response to ensure UK legislation and regulation created a supportive environment for research using patient and health-related data. Prior to joining Wellcome, Natalie was a postdoctoral research fellow in philosophy at King’s College London.
"
https://www.turing.ac.uk/research/research-projects/impacts-climate-change-and-heat-health,"
The climate emergency is bringing critical changes to our ability to live healthy and prosperous lives.  A changing climate and increasing temperatures, especially in urban areas, means that, more than ever before, we need to understand the complex relationships between our human behaviour, the environment in which we live, and our health in order to develop effective mitigation strategies.
This project brings together expertise from across the UK to develop the most detailed model to date of how projected future extreme heat events will affect different population groups, and inequalities in those effects, and aid in developing interventions that will, ultimately, save lives. The tools will be developed in concert with local authorities to support their decision-making around strategies to tackle the climate emergency locally.
Our interaction with the natural environment plays a crucial role in all aspects of society: our health, wealth, safety, education, and future prosperity. Climate change will bring fundamental changes to our environment – changes that have the potential to pose significant threats to people’s health and wellbeing. The UKCP18 climate projections indicate “a greater chance of warmer, wetter winters and hotter, drier summers” . Increased temperatures will pose a series of challenges related to health and well-being, especially in urban areas where there is the potential for great inequalities in the effects experienced by different people. For example, high density, low income populations in the centres of our cities may be the most affected by hotter summers but the least well-equipped to ventilate their homes due to factors such as air pollution, noise, low quality housing, and the risk of crime with open windows etc.
Estimating the risks associated with higher temperatures in epidemiological studies and exposures used in health impact analyses are almost exclusively based on aggregate measures of heat (e.g. averages of measurements in an urban area or of model outputs) with the assumption that all members of the population experience the same temperatures. In reality, different members of the population will spend different amounts of time in different locations, i.e. outdoors and indoors in different types of building stock.
The ability to produce high quality, disaggregated information on heat exposures experienced by different population groups will provide a step-change in our understanding of the adverse effects associated with higher temperatures. The ability to generate information on the exposures experienced by different population groups will be essential in developing adaptation measures that provide cooler, healthier, temperatures in times of extreme heat in homes and public spaces such as schools.
The aim of this project is to develop models that will provide new information on ‘personal exposures’ to environmental hazards, specifically those related to exposure to future increased temperatures associated with climate change. Based on a microsimulation modelling approach that integrates environmental conditions with human behaviour, the results will feed into health impact analyses and highlight population groups that may be disproportionately at risk and inform local adaptation strategies to climate change risks. The project will build upon the DyME (Dynamic Models for Environments) framework and tools that were created as part of the Royal Society RAMP Urban Analytics programme that accelerated development of technological innovation in the use of dynamic microsimulation models in real-world applications.
Integrating the foundations DyMe: the QUANT and SPENSER platforms for urban mobility and dynamic microsimulation, with the Joint Centre for Excellence in Environmental Intelligence’s Climate Impacts Mitigation Adaptation and Resilience (CLIMAR) framework and existing work on user interfaces in DyME the outputs of the project will include new data products, tools and interfaces for estimating the local effects of climate and heat on health.
The project will work with Cornwall County Council to explore one of their key interests related to climate change: the effects of changing temperatures on active travel, damp in homes (with warmer, wetter winters) and other health related outcomes. The outputs from DyME-CHH will feed directly into The Local Climate Adaptation Tool (LCAT) that supports evidence-based adaptation by local authorities and public bodies. LCAT brings together climate information, adaptation options and health impact evidence to help users understand the health implications of climate change in their local area. Importantly, LCAT also generates recommendations of appropriate adaptation approaches, based on the best available evidence that will support the health and wellbeing of local people.
Other examples include feeding into the City of Bradford Metropolitan Council’s use of data analytics to provide information on the impacts of climate change and sustainability, and working with the Department for Education who are particularly interested in the role of schools from a context of climate change and sustainability. The outputs of this work will link with the Connected Bradford dataset to meet their green agenda goals. This will allow a better understanding of the impact of school building design on outcomes for children and young people. In Bristol, the project will link the outputs of ‘personal exposures’ to increased heat for different population groups to housing stock and feed directly into their aims to have its estate climate resilient by 2030.
Arielle Bennett, [email protected]
Cornwall Country Council
City of Bradford Metropolitan Council
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/francis-crick-institute,"
The Francis Crick Institute and The Alan Turing Institute have entered into a partnership to facilitate growth in data-centric biomedical science research collaborations. The partnership aims to bring leading data science and medical research closer together, and achieve transformational change in fundamental and applied research, talent development and public engagement.
In August 2020, the two organisations created a research collaboration framework agreement that is expected to drive further collaborative projects to the benefit of both partners.
For more information on the projects as a result of this partnership, please see the ‘Funded research’ links below.
A Joint Steering Committee has been stood up to govern this partnership and related project activity. Joint Steering Committee members include:
For further information about the partnership, please contact [email protected].
 
 
 
"
https://www.turing.ac.uk/research/research-areas/social-data-science/ethics,"
"
https://www.turing.ac.uk/news/publications/context-aware-learning-using-transferable-features-classification-breast-cancer,"
Convolutional neural networks (CNNs) have been recently used for a variety of histology image analysis. However, availability of a large dataset is a major prerequisite for training a CNN which limits its use by the computational pathology community. In previous studies, CNNs have demonstrated their potential in terms of feature generalizability and transferability accompanied with better performance. Considering these traits of CNN, we propose a simple yet effective method which leverages the strengths of CNN combined with the advantages of including contextual information, particularly designed for a small dataset. Our method consists of two main steps: first it uses the activation features of CNN trained for a patch-based classification and then it trains a separate classifier using features of overlapping patches to perform image-based classification using the contextual information. The proposed framework outperformed the state-of-the-art method for breast cancer classification.
Awan R., Koohbanani N.A., Shaban M., Lisowska A., Rajpoot N. (2018) Context-Aware Learning Using Transferable Features for Classification of Breast Cancer Histology Images. In: Campilho A., Karray F., ter Haar Romeny B. (eds) Image Analysis and Recognition. ICIAR 2018 International Conference Image Analysis and Recognition Lecture Notes in Computer Science, vol 10882. Springer, Cham
"
https://www.turing.ac.uk/people/guest-speakers/gina-neff,"
Gina Neff is the Executive Director of the Minderoo Centre for Technology & Democracy at the University of Cambridge. Her research focuses on the effects of the rapid expansion of our digital information environment on workers and workplaces and in our everyday lives.
Professor Neff holds a Ph.D. in sociology from Columbia University and advises international organisations including UNESCO, the OECD and the Women’s Forum for the Economy and Society. She chairs the International Scientific Committee of the UK’s Trusted Autonomous Systems programme, is associate director of the ESRC Digital Good Network and is a member of the Strategic Advisory Network for the UK’s Economic and Social Research Council.
Professor Neff serves on the board of directors for the Social Science Research Council.
"
https://www.turing.ac.uk/events/2nd-cardiff-nlp-summer-workshop,"
This free 2-day workshop is on the topic of Natural Language Processing (NLP). NLP is a branch of AI that deals with language, and spans across many disciplines that make use of text data. NLP is vital in bridging the gap between human language and computers, enabling machines to understand, process, and generate natural language. Its applications are wide-ranging and impactful, transforming the way we communicate, access information, and interact with technology.
The in-person workshop will consist of a mix of invited speakers from research institutions (including Turing Research Fellows) and industry. It is aimed at NLP practitioners and researchers, but all are welcome. PhD students and early career researchers working with NLP are especially encouraged to attend.
The venue for the event  is the Abacws Building, the brand new home of the School of Computer Science and Mathematics at Cardiff University. The expression of interest period is now finished. However, there are still some spaces available. Please contact the event organisers via this webpage for latest availability  (registration deadline is June 9th ).     
Register now
"
https://www.turing.ac.uk/research/research-areas/applied-mathematics/mathematical-physics,"
"
https://www.turing.ac.uk/people/doctoral-students/cohort/2019,"
"
https://www.turing.ac.uk/research/defence-and-national-security,"
To protect the UK, its people and the places they inhabit
The explosion of human connectivity and information availability creates new ways in which bad actors can cause harm to people, businesses, and societies. The exponential rise in volumes of data makes it harder for defence and security agencies to identify, and act on insights.
The prevalence of networked systems and the sharing of private data give rise to new insecurities and risks, both technical and social. Further, the speed of technological change poses a unique challenge to the development of new laws, policy, and governance. The UK must continue to stay ahead in researching, developing, and integrating technologies to meet these challenges, or risk falling behind on the global stage.
Data science and AI present an opportunity to meet these new threats. This includes better analysis of data at scale, helping to find key intelligence amongst an expanse of data and make informed decisions from resulting analysis; building resilient and secure methods of protecting networks and privacy; and developing evidence-based assessments of emerging technologies to help policymakers develop timely and effective legislation and policy.
The Alan Turing Institute is uniquely placed to lead this endeavour. We have demonstrable experience of close engagement with the national security and defence communities, nationally and internationally.
We act as an authoritative convener for diverse expertise in key areas across academia. We not only have teams of world class researchers, but we also have dedicated teams helping to translate the research into operations, both from an engineering and a policy standpoint. Importantly, we are driven by societal impact and public good. This independence is invaluable, and helps the Turing contribute to the missions our partners face.
‘The prosperity and security of the United Kingdom faces a continued threat from state actors, terrorists, and serious criminals. AI capabilities will be fundamental to our future ability to protect the UK. They will enable analysts to better analyse ever-increasing volumes of data much faster, improving the quality and timeliness of decision-making. Keeping the UK’s citizens safe and prosperous will increasingly depend on the success of these systems.  
Our partnership with the Turing supports government to advance the research and development of new critical capabilities in this important space. We look forward to continuing to work with them to protect the UK, its people, and the places they inhabit, as outlined in their new strategy published today‘ 
Alex van Someren, Chief Scientific Adviser for National Security 
The Institute is developing a set of missions to sit under the Defence and National Security Grand Challenge, to be launched soon. These will focus on scientific areas such as multi-modal data and signals analysis, human machine teaming, behavioural analysis, cyber security, reinforcement learning, AI explainability, model security, privacy technology, and digital twins.
We need collaborators to help us to solve this Grand Challenge. Sign up to our newsletter to stay in touch with the latest news.

"
https://www.turing.ac.uk/people/guest-speakers/iulia-georgescu,"
Iulia is the Chief Editor of Nature Reviews Physics to which she brings together her experience as a reviews author and commissioning editor. In 2012, Iulia joined the team at Nature Physics as an Associate Editor and then Senior Editor, where she handled manuscripts in various subject areas and oversaw numerous editorial projects.
Before that Iulia was a postdoctoral researcher at RIKEN Advanced Science Institute, Japan and at the University of Basel, Switzerland, where she worked on quantum simulation and the characterization of Coulomb crystals of ions. In 2008, she obtained her Ph.D. from the University of Tokyo, having studied quantum information and simulation using trapped ions. 
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/british-library,"
www.bl.uk
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/accenture,"
Accenture and The Alan Turing Institute have been working together since 2017, during which time they have conducted joint research projects into new techniques for network analytics for use in fraud detection, for example, in telecommunications. They have also run Data Study Groups to tackle challenges facing AI professionals, such as fairness in algorithmic decision-making.
In 2020 we launched a five-year strategic partnership with Accenture that will sit within the finance and economics programme. Our partnership will build on an existing three-year alliance to create a world-leading strategic partnership in advanced data science, data analytics and artificial intelligence (AI) research with a focus on delivering substantial business and societal value.
Some of the activities the partnership plans to undertake include conducting joint research in data science and AI across many areas including: 
The partnership will also deliver engagement activities to help transform executives’ understanding of how AI and data science can be used as a tool for business. This will include guest lectures and seminars as well as internship opportunities for a range of students across the Turing network with Accenture and its clients.

Read an edited version of Adrian Smith’s keynote speech given at the Accenture-Turing Innovation Symposium on 18 November 2021. This virtual event provided a unique opportunity for industry and academia to come together to discuss the research that is pioneering the conscientious uses of data science and AI, in areas including privacy, explainability, fairness and security.
Read here.
 
 

The government sees the National AI Strategy as fundamental to the UK’s economic growth over the next ten years. Mark Girolami, Chief Scientist of The Alan Turing Institute, and Accenture's Fernando Lucini discuss what this means for business and society. As a global AI powerhouse, UK business is already seeing opportunities to grow and transform.
 
The latest episode of the Turing Podcast features a special roundtable discussion with Accenture about career options in the data science sector. Hosts Jo Dungate and Bea Costa Gomes were joined by three influential figures in AI and data science: Henrietta Ridley (Data Science Manager at Accenture), Alice Aspinall (Senior Manager at Mudano) and Kirstie Whitaker (the Turing’s Programme Director for Tools, Practices and Systems). 
Click to listen
 
You can view sessions from the Accenture Innovation Symposium below.
Adrian Smith
In addition to his role at the Turing, Adrian is President of the Royal Society, and a member of the government’s national AI council. In this talk, Adrian discusses the UK’s national data and AI strategies; focusing in on the key challenges to industry.
Mihaela van der Schaar
Mihaela is John Humphrey Plummer Professor of Machine Learning, Artificial Intelligence and Medicine at the University of Cambridge, and Turing Fellow. The Current emphasis of her research is on synthetic data with applications to medicine, finance and education. In this talk she focuses on new evaluation metrics for synthetic data generation.
Timothy Dodwell
Timothy is a senior lecturer in industrial applied mathematics within the Institute of Data Science and AI at the University of Exeter, and Turing Fellow. His work addresses the challenge of building a more sustainable aviation industry by building a virtual test pyramid. In this talk he focuses on deploying reinforcement learning in safety critical systems, as well as the Turing’s prosperity partnership with NATS.
Maria Liakata
Maria is a Professor in Natural Language Processing (NLP) at Queen Mary University of London and Turing Fellow. Her work utilises language data obtained from wide-spread use of digital technology such as social media as well as mobile phone data to develop novel natural language processing methods for automatically capturing changes in user behaviour over time. In this talk she looks at the applicability to mental health as NLP techniques will help provide experts with evidence for personalised changes in mood and cognition from everyday use of digital technologies.
Jose Miguel Hernandez Lobato
Jose is a lecturer in Machine Learning at the Department of Engineering of the University of Cambridge. His work focuses on increasing robustness in machine learning methods using probabilistic approaches. This talk addresses key concerns around providing training data collected under differing conditions.
 
The COVID-19 pandemic brought into sharp focus the need for new data science and AI skills. Looking ahead, it’s clear we’re going to need data-informed non-therapeutic interventions to limit any resurgence of the virus for years to come.
Click to read blog
 
 
 
Professor Zoe Kourtzi gave a talk at Accenture as part of Mental Health Week 2021.
Click to watch video
 
 
How can you identify whether a problem or opportunity – whether it’s from industry, third sector, government or academia – is a suitable candidate for an AI-driven approach?
Click to read blog
 
Addressing the UK’s AI skills gap has never been more important. In this blog, Ray Eitel-Porter (Global Responsible AI Lead at Accenture), discusses the scale of the challenge, addressing diversity, and AI learning in schools.
Click to read blog
 
Applied Intelligence collaboration is at the heart of Accenture does. Read more about their work with some of the world’s leading academic and research institutions worldwide.
Click to read blog

 
Click to read blog
 

 
On 7 October 2020, Accenture and The Alan Turing Institute partnered to host the Innovation Symposium, featuring the latest in applied data science and artificial intelligence research.
Click to watch video

 
Accenture’s challenge to the Turing's Data Study Group researchers was to develop a tool that can promote fairness in algorithmic decision-making across the financial services sector. Read more about the outcomes of the challenge.
Click to read more
 
The big data revolution is changing everything. Along with increasing computing power, data is the engine of data science and AI. It is having an impact across social and economic life through social media, new forms of public service delivery or the transformation of industries and employment.
Click to read blog
 
Accenture Turing strategic partnership: Expression of interest (Data sharing) – Through this call we are seeking to fund projects that fall under the “Data Sharing” research theme.
Accenture and The Alan Turing Institute establish strategic partnership – A partnership to conduct AI and data science research and develop applications that solve real-world problems.
Accenture challenge: Fairness in algorithmic decision-making – Accenture’s challenge to Turing DSG researchers was to develop a tool that can promote fairness in algorithmic decision-making across the financial services sector.
The Alan Turing Institute and Accenture launch a new alliance to combat fraud and money laundering – The Alan Turing Institute and Accenture announced that they will work together to develop next generation artificial intelligence and data science tools, with the first in a series of projects to look at the security challenges faced by large business and government organisations.
Accenture is a global professional services company with leading capabilities in digital, cloud and security. Combining unmatched experience and specialised skills across more than 40 industries, Accenture offer Strategy and Consulting, Interactive, Technology and Operations services – all powered by the world’s largest network of Advanced Technology and Intelligent Operations centres. Their 537,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. They embrace the power of change to create value and shared success for clients, people, shareholders, partners and communities. Visit at www.accenture.com.
The finance and economics programme brings together leading experts in data science, machine learning, finance and the social sciences, from both academia and industry to tackle the most challenging questions by producing world-leading research with significant impact. We inform public policy and enable trusted, research-led thought leadership. The programme works closely with government and the industry to exploit the potential of new technologies in the financial sector and economic research, and to position the UK as the leader in these areas.
In 2020 we launched a five-year Strategic Partnership with Accenture that will sit within the finance and economics programme. Our partnership will build on an existing three-year alliance to create a world-leading strategic partnership in advanced data science, data analytics and artificial intelligence (AI) research with a focus on delivering substantial business and societal value via:
We will achieve our vision through four data science and AI pillars: (1) research sponsorship, (2) students and scholarship, (3) skills and training, and (4) use case commercialisation.
"
https://www.turing.ac.uk/research/interest-groups/developing-best-practice-collecting-cybersecurity-data,"
This interest group will bring key data stakeholders together with Data Scientists and Artificial Intelligence experts in order to organise and develop best practices with regard to collecting Cybersecurity Data. It will seek to develop acceptable standards for collecting data, analysing and using that data in a way that is both ethically safe, minimal in bias and also useful for building insights into how all organisations (public, private and third sector) can be made more cybersecure in the future.
This will be a hard task from the outset because collecting and processing data from key stakeholders that relates to their own security presents many challenges to their existing organisational and occupational cultures. Not least, without checks, the data may perpetuate bias. Victims may be reluctant to share it, for example, as they may be concerned that sharing attack data could damage their reputation if publicly revealed. They may, as is sometimes the case, not even be aware that sensitive information has been stolen, even though an attack may have taken place. Furthermore, requests for data may be made just after a cybercrime incident when the organisation is either most risk-averse, or not be capable of assessing the damage, or even be in a position to report it. In the worst-case scenario, the data could even provide an attacker with vital information that can be used to make further attacks.
Guided by the principle of developing better security for all, the interest group will work together to develop best practices that will enable participants to manage expectations and achieve the best science possible in the circumstances.
The project will employ a grounded method which begins with the participants own perspectives about their sector's specific needs and requirements. Such an approach enables ideas to progress from the ground up. The project, however, will seek solutions based upon practices and (ethical) practicalities rather than theory. It is anticipated that a dataset will be co-created between organisations and researchers rather than produced by one and consumed by the other.
The aims of this interest group are directly related to the goals of the ATI: 
During a time when the cybersecurity stakes are heightened by new types and intensities of threats, especially those which can retard the economic recovery from COVID-19, why is there such reluctance to participate in cybersecurity research since all parties have a common interest to do so?
Why do researchers have a problem in accessing organisations and data?
What cybersecurity data will help researchers get the job done? Are researchers thinking in or out of the box? Do expectations match the data that is available?
Can the gap between what researchers want to do with cybersecurity data and what organisations want it to be used for be reconciled?
Given that there is evidence that cyber attackers are using all data at their disposal to attack us should we not be doing the same to protect ourselves?
What can we (victims, agencies and researchers) do to work together to solve a problem that affects us all?
How can viable working partnerships be created that engenders trust between participants?
Click here to join us and request sign-up
Stephen McGough[email protected]
David Wall[email protected]
"
https://www.turing.ac.uk/research/research-projects/endogenous-production-networks,"
Production networks live at the heart of modern economies and global value chains. Shocks such as demand shifts, technological change, and the emergence (death) of new (old) industries generate responses that propagate across their linkages. Most analytic tools analyse some of these shocks by considering production networks to be exogenous, while others that try to explain their formation have limited real-world applicability. We develop models of how such networks emerge from the learning process of firms under uncertainty; without having perfect knowledge of the nature of technology (also known as production functions). These models can be calibrated with publicly available input-output datasets and deployed with various types of production functions, facilitating the analysis of structural transformations such as drastic technological changes. The tools produced by this project can be used to support evidence-based policy making, for example, when formulating national industrial strategies or when considering supporting emergent industries.
"
https://www.turing.ac.uk/news/publications/inference-simulation-and-application-latent-trawl-model-extreme-values,"
V. Courgeau and A. E. D. Veraart (2018). Inference, simulation and application of a latent trawl model for extreme values. Work in progress.
"
https://www.turing.ac.uk/people/guest-speakers/michael-tildesley,"
Dr Michael Tildesley is a Reader in the Zeeman Institute for Systems Biology and Infectious Disease Epidemiology Research at the University of Warwick. He completed his Ph.D. in Astrophysical Fluid Dynamics at the University of Cambridge in 2003, but has been working in the field of mathematical epidemiology since then. His research focuses upon the development of models of infectious diseases and their utility as predictive tools. He has a particular interest in the predictive power of models in the early stages of disease outbreaks, when there is significant uncertainty regarding the spread of disease.
Dr Tildesley has strong links with policy makers, working closely with the Department for the Environment, Food and Rural Affairs, the US Department of Agriculture (USDA), the World Organisation for Animal Health (OIE) and the Food and Agriculture Organisation (FAO) upon strategies for disease control.
Dr Tildesley is currently a member of the Scientific Pandemic Influenza Modelling group (SPI-M), the modelling subgroup of SAGE, and has been working throughout the COVID-19 pandemic upon the development of models to inform a range of policy questions in the UK.
"
https://www.turing.ac.uk/people/enrichment-students/victoria-smith,"
Victoria is an Enrichment Student and a PhD student at the UKRI AI Enabled Healthcare Systems CDT at University College London (UCL). Her research consists of natural language processing (NLP) and text mining for information extraction in biomedical literature. Victoria's background lies in Neuroscience, achieving her MSci from UCL where her research focused on immune cell gene expression in Alzheimer's Disease. Victoria is excited by how NLP and machine learning techniques can be used to ultimately improve health outcomes. 
Victoria's research focuses on automating information extraction of pharmacometric data from the literature using natural languages processing and machine learning approaches. She aims to produce a centralised and standardised pharmacometric database to improve predictions in drug discovery and for special populations in the clinic. Victoria's research focuses on text classification, named-entity recognition, entity linking and relation extraction for biomedical text and tables (semi-structured). In particular, Victoria is interested in text data augmentation, synthetic data creation to cope with imbalanced data and active learning to improve performance on poorly labelled data. 
"
https://www.turing.ac.uk/news/publications/mapreader-computer-vision-pipeline-semantic-exploration-maps-scale,"
We present MapReader, a free, open-source software library written in Python for analyzing large map collections. MapReader allows users with little computer vision expertise to i) retrieve maps via web-servers; ii) preprocess and divide them into patches; iii) annotate patches; iv) train, fine-tune, and evaluate deep neural network models; and v) create structured data about map content. We demonstrate how MapReader enables historians to interpret a collection of ≈16K nineteenth-century maps of Britain (≈30.5M patches), foregrounding the challenge of translating visual markers into machine-readable data. We present a case study focusing on rail and buildings. We also show how the outputs from the MapReader pipeline can be linked to other, external datasets. We release ≈62K manually annotated patches used here for training and evaluating the models.
Kasra Hosseini, Daniel C. S. Wilson, Kaspar Beelen, and Katherine McDonough. 2022. MapReader: a computer vision pipeline for the semantic exploration of maps at scale. In Proceedings of the 6th ACM SIGSPATIAL International Workshop on Geospatial Humanities (GeoHumanities '22). Association for Computing Machinery, New York, NY, USA, 8–19.
"
https://www.turing.ac.uk/partnering-turing/current-partnerships-and-collaborations/mi5,"
"
